<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>GooleDevops</title>
    <link>https://cloudblog.withgoogle.com/products/devops-sre/rss/</link>
    <description></description>
    <item>
      <title>GitOps your service orchestrations</title>
      <link>https://cloud.google.com/blog/topics/developers-practitioners/gitsops-service-orchestration/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://opengitops.dev/&#34; target=&#34;_blank&#34;&gt;GitOps&lt;/a&gt; takes DevOps best practices used for application development (such as version control and CI/CD) and applies them to infrastructure automation. In GitOps, the Git repository serves as the source of truth and the CD pipeline is responsible for building, testing, and deploying the application code and the underlying infrastructure.&lt;/p&gt;&lt;p&gt;Nowadays, an application is not just code running on infrastructure that you own and operate. It is usually a set of first-party and third-party microservices working together in an event-driven architecture or with a central service orchestrator such as &lt;a href=&#34;https://cloud.google.com/workflows&#34;&gt;Workflows&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Service orchestrations, which have their own definition files and deployment cycles, can benefit from a GitOps approach. This blog post describes how to set up a simple Git-driven development, testing, and deployment pipeline for Workflows using &lt;a href=&#34;https://cloud.google.com/build&#34;&gt;Cloud Build&lt;/a&gt;. &lt;/p&gt;&lt;h3&gt;Architecture&lt;/h3&gt;&lt;p&gt;Let’s take a look at the overall approach. &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;GitOps Blog 1&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_1.max-1000x1000.png&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Architechture&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In this approach, you have a staging branch where you make changes to a workflow. This triggers a Cloud Build configuration that deploys a test staging workflow and runs some staging tests against it. If all tests pass, Cloud Build deploys the staging workflow. After more manual testing of the staging workflow, you merge changes from the staging branch to the main branch. This triggers the same Cloud Build configuration to deploy a test production workflow, run more production tests, and if all tests pass, deploy the production workflow.&lt;/p&gt;&lt;p&gt;This approach enables you to have an automated and staged rollout of workflow changes with tests along the way to minimize risk. &lt;/p&gt;&lt;h3&gt;Configuration&lt;/h3&gt;&lt;p&gt;Setting up such an automated workflow deployment pipeline is straightforward. &lt;/p&gt;&lt;p&gt;First, you need a workflow definition file that can benefit from such automation. You can use one of your workflow definition files or &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;workflow.yaml&lt;/code&gt;&lt;/a&gt;, which simply returns &lt;code&gt;Hello World&lt;/code&gt;. &lt;/p&gt;&lt;p&gt;Next, define a Cloud Build configuration file (see &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/cloudbuild.yaml&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;cloudbuild.yaml&lt;/code&gt;&lt;/a&gt;) with all the stages. In this configuration, Cloud Build deploys a test workflow with the branch name and commit hash, runs the workflow and captures the output, deletes the test workflow, and tests the workflow with the supplied test script. If all the tests pass, it deploys the final workflow in the branch.&lt;/p&gt;&lt;p&gt;Tests for the branch are defined in appropriate &lt;code&gt;test-{branchname}.sh&lt;/code&gt; files. For example, &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-staging.sh&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;test-staging.sh&lt;/code&gt;&lt;/a&gt; runs against the workflows deployed in the staging branch and only checks the workflow execution state. On the other hand, &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-master.sh&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;test-main.sh&lt;/code&gt;&lt;/a&gt; runs against the main branch, checks the workflow execution state, and also checks the output of the execution. You can add more tests as you see fit.&lt;/p&gt;&lt;h3&gt;Connect your repository to Cloud Build&lt;/h3&gt;&lt;p&gt;Now that you have the basic configuration in place, you connect your (or &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;workflows-demos&lt;/code&gt;&lt;/a&gt;) repository to Cloud Build before creating triggers. Follow the instructions &lt;a href=&#34;https://cloud.google.com/build/docs/automating-builds/github/connect-repo-github#connecting_a_github_repository&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Create a Cloud Build trigger&lt;/h3&gt;&lt;p&gt;You now create a Cloud Build trigger to watch for commits to the main and staging branches. General instructions are &lt;a href=&#34;https://cloud.google.com/build/docs/automating-builds/create-manage-triggers&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Go to the &lt;code&gt;Create Trigger&lt;/code&gt; section of Cloud Build in the console and create a trigger with the following properties:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Name: &lt;code&gt;workflows-trigger&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Event: Push to a branch&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Repository: &lt;code&gt;GoogleCloudPlatform/workflows-demos&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Branch: &lt;code&gt;^main$|^staging$&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Included files filter: &lt;code&gt;gitops/workflow.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Configuration type: Cloud build configuration file&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud build configuration file location: &lt;code&gt;gitops/cloudbuild.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Add a Substitution variable with key/value: &lt;code&gt;_WORKFLOW_NAME&lt;/code&gt; and &lt;code&gt;workflows-gitops&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Test the staging workflow&lt;/h3&gt;&lt;p&gt;You’re now ready to test the build pipeline with the staging branch. &lt;/p&gt;&lt;p&gt;Switch to the staging branch:&lt;/p&gt;&lt;p&gt;&lt;code&gt;git checkout staging&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Change &lt;code&gt;Hello World&lt;/code&gt; in &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;workflow.yaml&lt;/code&gt;&lt;/a&gt; to &lt;code&gt;Bye World&lt;/code&gt;:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;- init:\r\n assign:\r\n- - message: &#34;Hello World&#34;\r\n+ - message: &#34;Bye World&#34;&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e099b360890&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Commit and push the change to the staging branch:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;git add workflow.yaml\r\ngit commit -m &#34;Update workflow.yaml in staging&#34;\r\ngit push&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e099ac423d0&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;You should see the trigger running:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;GOB 2&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_2.max-1000x1000.png&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Build trigger running&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;After a few seconds, the build (including all its stages) is successful:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;BOB 3&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_3.max-1000x1000.png&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Staging build details&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;And a staging workflow has been deployed:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;GOB 4&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_4.max-1000x1000.png&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Workflows staging&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Test the production workflow&lt;/h3&gt;&lt;p&gt;Once you&#39;re ready to deploy the staging workflow to production, simply merge the staging branch to the main branch.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;git checkout main\r\ngit merge staging\r\ngit push&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e0998a25b50&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In this case, however, the build fails:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;GOB 5&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_5.max-1000x1000.png&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Production build details&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;This is because the test script for the production workflow in &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-master.sh&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;test-main.sh&lt;/code&gt;&lt;/a&gt; is expecting to see &lt;code&gt;Hello World&lt;/code&gt; as output of the workflow.&lt;/p&gt;&lt;p&gt;You need to go back to the staging branch, and change Bye World in &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;workflow.yaml&lt;/code&gt;&lt;/a&gt; back to &lt;code&gt;Hello World&lt;/code&gt;:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;- init:\r\n assign:\r\n- - message: &#34;Bye World&#34;\r\n+ - message: &#34;Hello World&#34;&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e0998a25e50&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Check in your changes to staging, see the build succeed, and merge to main. Finally, you should also see the build succeed and see that a production workflow has been deployed alongside staging:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;GOB 6&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_7.max-1000x1000.png&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Workflows production&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;&lt;b&gt;Next steps&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;This post covered how to set up a Git-driven development, testing, and deployment pipeline for Workflows using Cloud Build. All the details and sample configuration files are in our &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/tree/master/gitops&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;workflows-demos/gitops&lt;/code&gt;&lt;/a&gt; repository. &lt;/p&gt;&lt;p&gt;Of course, Cloud Build is not the only way to set up such a pipeline. &lt;a href=&#34;https://github.com/features/actions&#34; target=&#34;_blank&#34;&gt;GitHub Actions&lt;/a&gt; is another useful tool that can help to set up similar service orchestration pipelines. Feel free to contribute to our repository with GitHub Actions based pipelines and reach out to me on Twitter &lt;a href=&#34;https://twitter.com/meteatamel&#34; target=&#34;_blank&#34;&gt;@meteatamel&lt;/a&gt; for any questions or feedback.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c59=""><div _ngcontent-c59="" innerhtml="&lt;p&gt;In this approach, you have a staging branch where you make changes to a workflow. This triggers a Cloud Build configuration that deploys a test staging workflow and runs some staging tests against it. If all tests pass, Cloud Build deploys the staging workflow. After more manual testing of the staging workflow, you merge changes from the staging branch to the main branch. This triggers the same Cloud Build configuration to deploy a test production workflow, run more production tests, and if all tests pass, deploy the production workflow.&lt;/p&gt;&lt;p&gt;This approach enables you to have an automated and staged rollout of workflow changes with tests along the way to minimize risk.&amp;#160;&lt;/p&gt;&lt;h3&gt;Configuration&lt;/h3&gt;&lt;p&gt;Setting up such an automated workflow deployment pipeline is straightforward.&amp;#160;&lt;/p&gt;&lt;p&gt;First, you need a workflow definition file that can benefit from such automation. You can use one of your workflow definition files or &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;workflow.yaml&lt;/code&gt;&lt;/a&gt;, which simply returns &lt;code&gt;Hello World&lt;/code&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;Next, define a Cloud Build configuration file (see &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/cloudbuild.yaml&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;cloudbuild.yaml&lt;/code&gt;&lt;/a&gt;) with all the stages. In this configuration, Cloud Build deploys a test workflow with the branch name and commit hash, runs the workflow and captures the output, deletes the test workflow, and tests the workflow with the supplied test script. If all the tests pass, it deploys the final workflow in the branch.&lt;/p&gt;&lt;p&gt;Tests for the branch are defined in appropriate &lt;code&gt;test-{branchname}.sh&lt;/code&gt; files. For example, &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-staging.sh&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;test-staging.sh&lt;/code&gt;&lt;/a&gt; runs against the workflows deployed in the staging branch and only checks the workflow execution state. On the other hand, &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-master.sh&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;test-main.sh&lt;/code&gt;&lt;/a&gt; runs against the main branch, checks the workflow execution state, and also checks the output of the execution. You can add more tests as you see fit.&lt;/p&gt;&lt;h3&gt;Connect your repository to Cloud Build&lt;/h3&gt;&lt;p&gt;Now that you have the basic configuration in place, you connect your (or &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;workflows-demos&lt;/code&gt;&lt;/a&gt;) repository to Cloud Build before creating triggers. Follow the instructions &lt;a href=&#34;https://cloud.google.com/build/docs/automating-builds/github/connect-repo-github#connecting_a_github_repository&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Create a Cloud Build trigger&lt;/h3&gt;&lt;p&gt;You now create a Cloud Build trigger to watch for commits to the main and staging branches. General instructions are &lt;a href=&#34;https://cloud.google.com/build/docs/automating-builds/create-manage-triggers&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Go to the &lt;code&gt;Create Trigger&lt;/code&gt; section of Cloud Build in the console and create a trigger with the following properties:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Name: &lt;code&gt;workflows-trigger&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Event: Push to a branch&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Repository: &lt;code&gt;GoogleCloudPlatform/workflows-demos&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Branch: &lt;code&gt;^main$|^staging$&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Included files filter: &lt;code&gt;gitops/workflow.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Configuration type: Cloud build configuration file&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud build configuration file location: &lt;code&gt;gitops/cloudbuild.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Add a Substitution variable with key/value: &lt;code&gt;_WORKFLOW_NAME&lt;/code&gt; and &lt;code&gt;workflows-gitops&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Test the staging workflow&lt;/h3&gt;&lt;p&gt;You&amp;#8217;re now ready to test the build pipeline with the staging branch.&amp;#160;&lt;/p&gt;&lt;p&gt;Switch to the staging branch:&lt;/p&gt;&lt;p&gt;&lt;code&gt;git checkout staging&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Change &lt;code&gt;Hello World&lt;/code&gt; in &lt;a href=&#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;workflow.yaml&lt;/code&gt;&lt;/a&gt; to &lt;code&gt;Bye World&lt;/code&gt;:&lt;/p&gt;"><p>In this approach, you have a staging branch where you make changes to a workflow. This triggers a Cloud Build configuration that deploys a test staging workflow and runs some staging tests against it. If all tests pass, Cloud Build deploys the staging workflow. After more manual testing of the staging workflow, you merge changes from the staging branch to the main branch. This triggers the same Cloud Build configuration to deploy a test production workflow, run more production tests, and if all tests pass, deploy the production workflow.</p><p>This approach enables you to have an automated and staged rollout of workflow changes with tests along the way to minimize risk. </p><h3>Configuration</h3><p>Setting up such an automated workflow deployment pipeline is straightforward. </p><p>First, you need a workflow definition file that can benefit from such automation. You can use one of your workflow definition files or <a href="https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://github.com" track-metadata-module="post"><code>workflow.yaml</code></a>, which simply returns <code>Hello World</code>. </p><p>Next, define a Cloud Build configuration file (see <a href="https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/cloudbuild.yaml" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://github.com" track-metadata-module="post"><code>cloudbuild.yaml</code></a>) with all the stages. In this configuration, Cloud Build deploys a test workflow with the branch name and commit hash, runs the workflow and captures the output, deletes the test workflow, and tests the workflow with the supplied test script. If all the tests pass, it deploys the final workflow in the branch.</p><p>Tests for the branch are defined in appropriate <code>test-{branchname}.sh</code> files. For example, <a href="https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-staging.sh" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://github.com" track-metadata-module="post"><code>test-staging.sh</code></a> runs against the workflows deployed in the staging branch and only checks the workflow execution state. On the other hand, <a href="https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-master.sh" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://github.com" track-metadata-module="post"><code>test-main.sh</code></a> runs against the main branch, checks the workflow execution state, and also checks the output of the execution. You can add more tests as you see fit.</p><h3>Connect your repository to Cloud Build</h3><p>Now that you have the basic configuration in place, you connect your (or <a href="https://github.com/GoogleCloudPlatform/workflows-demos" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://github.com" track-metadata-module="post"><code>workflows-demos</code></a>) repository to Cloud Build before creating triggers. Follow the instructions <a href="https://cloud.google.com/build/docs/automating-builds/github/connect-repo-github#connecting_a_github_repository" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/build/docs/automating-builds/github/connect-repo-github#connecting_a_github_repository" track-metadata-module="post">here</a>.</p><h3>Create a Cloud Build trigger</h3><p>You now create a Cloud Build trigger to watch for commits to the main and staging branches. General instructions are <a href="https://cloud.google.com/build/docs/automating-builds/create-manage-triggers" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/build/docs/automating-builds/create-manage-triggers" track-metadata-module="post">here</a>.</p><p>Go to the <code>Create Trigger</code> section of Cloud Build in the console and create a trigger with the following properties:</p><ul><li><p>Name: <code>workflows-trigger</code></p></li><li><p>Event: Push to a branch</p></li><li><p>Repository: <code>GoogleCloudPlatform/workflows-demos</code></p></li><li><p>Branch: <code>^main$|^staging$</code></p></li><li><p>Included files filter: <code>gitops/workflow.yaml</code></p></li><li><p>Configuration type: Cloud build configuration file</p></li><li><p>Cloud build configuration file location: <code>gitops/cloudbuild.yaml</code></p></li><li><p>Add a Substitution variable with key/value: <code>_WORKFLOW_NAME</code> and <code>workflows-gitops</code></p></li></ul><h3>Test the staging workflow</h3><p>You’re now ready to test the build pipeline with the staging branch. </p><p>Switch to the staging branch:</p><p><code>git checkout staging</code></p><p>Change <code>Hello World</code> in <a href="https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml" target="_blank" track-type="inline link" track-name="11" track-metadata-eventdetail="https://github.com" track-metadata-module="post"><code>workflow.yaml</code></a> to <code>Bye World</code>:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Mete Atamel&lt;/name&gt;&lt;title&gt; Developer Advocate&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 16 Sep 2022 12:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Building a secure CI/CD pipeline using Google Cloud built-in services</title>
      <link>https://cloud.google.com/blog/products/devops-sre/devsecops-and-cicd-using-google-cloud-built-in-services/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;DevOps is a concept that allows software development teams to release software in an automated and stable manner. DevOps itself is not just one thing; it&#39;s a combination of culture and technology, which together make the implementation of DevOps successful.&lt;/p&gt;&lt;p&gt;In this blog, we will be focusing on the tools and technology side of DevOps. At the core of the technical aspect of DevOps, the concept is Continuous Integration and Continuous Delivery (CI/CD). The idea behind CI/CD concept is to create an automated software delivery pipeline that continuously deploys the new software releases in an automated fashion.&lt;/p&gt;&lt;p&gt;The flow begins with the developers committing the code changes to a source code repository, which automatically triggers the delivery pipeline (henceforth called CI/CD pipeline) by building and deploying the code changes into various environments, from non-prod environments to production environments.&lt;/p&gt;&lt;p&gt;Also, as we build the CI/CD pipelines for faster and more reliable software delivery, the security aspect should not be ignored and must be incorporated into the pipeline right from the beginning. When we build our source code, we typically use various open-source libraries and container images. Having some security safeguards within the CI/CD pipeline is imperative to ensure that the software we are building and deploying is free from any vulnerability. Additionally, it&#39;s equally important to control what type of code/container image should be allowed to be deployed on your target runtime environment.&lt;/p&gt;&lt;p&gt;Security is everyone&#39;s responsibility. &lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-tech-shifting-left-on-security&#34;&gt;Shifting left&lt;/a&gt; on security is a DevOps practice that allows you to address security concerns early in the software development lifecycle. Vulnerability scanning of container images, putting security policies in place through Binary Authorization, and allowing approved/trusted images to be deployed on GKE are a couple of ways to implement this policy to make your CI/CD pipelines more secure.&lt;/p&gt;&lt;p&gt;What are we building?&lt;/p&gt;&lt;p&gt;This blog post will show how to build a secure CI/CD pipeline using Google Cloud&#39;s built-in services. We will create a secure software delivery pipeline that builds a sample Node.js application as a container image and deploys it on GKE clusters.&lt;/p&gt;&lt;p&gt;How are we building the CI/CD pipeline?&lt;/p&gt;&lt;p&gt;We&#39;re going to use the following Google Cloud built-in services to build the pipeline:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/build&#34;&gt;Cloud Build&lt;/a&gt; - Cloud Build is an entirely serverless CI/CD platform that allows you to automate your build, test, and deploy tasks.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry&#34;&gt;Artifact Registry&lt;/a&gt; - Artifact Registry is a secure service to store and manage your build artifacts.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Cloud Deploy&lt;/a&gt; - Cloud Deploy is a fully managed Continuous Delivery service for GKE and Anthos.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/binary-authorization&#34;&gt;Binary Authorization&lt;/a&gt; - Binary Authorization provides deployment time security controls for GKE and Cloud Run deployments.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;GKE&lt;/a&gt; - GKE is a fully managed Kubernetes platform.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Google Pub/Sub&lt;/a&gt; - Pub/Sub is a serverless messaging platform.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/functions&#34;&gt;Cloud Functions&lt;/a&gt; - Cloud Functions is a serverless platform to run your code.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;We use GitHub as a source code repository and Sendgrid APIs to send email notifications for approval and error logging.&lt;/p&gt;&lt;p&gt;The CI/CD pipeline is set up so that a Cloud Build trigger is configured to sense any code pushed to a particular repository and branch in a GitHub repository and automatically starts the build process.&lt;/p&gt;&lt;p&gt;Below is the flow of how the CI/CD pipeline is set up without any security policy enforcement:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Developer checks in the code to a GitHub repo.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and starts the &#39;build&#39; process. A successful build results in a docker container image.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The container image is stored in the Artifact Registry.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Deploy is configured to go through an approval step before deploying the image to the Production GKE cluster.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A Cloud Function sends an email to a pre-configured email id, notifying you that a Cloud Deploy rollout requires your approval. The email receiver can approve or reject the deployment to the production GKE cluster. Cloud Function code can be found &lt;a href=&#34;https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/cloud-function/index.js&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;To secure this CI/CD pipeline, we will use a couple of Google Cloud&#39;s built-in features and services. First, we will enable vulnerability scans on Artifact Registry, an out-of-the-box feature. Then finally, we will create a security policy using the Binary Authorization service, which only allows a specific image to be deployed to your GKE cluster.&lt;/p&gt;&lt;p&gt;Below is the flow when we try to build and deploy a container image that has vulnerabilities present:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Developer checks in the code to a GitHub repo.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the &#39;build&#39; process.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The build process fails with the error message that vulnerabilities were found in the image.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Below is the flow when we try to deploy a container image to GKE, which violates a Binary Authorization policy:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Developer checks in the code to a GitHub repo.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the &#39;build&#39; process. A successful build results in a docker container image.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The container image is stored in Artifact Registry.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Deploy fails as the GKE clusters reject the incoming image as it violates the existing Binary Authorization policy. Please note that an approval email is still triggered before the production deployment via the Cloud Function; the email receiver is expected to reject this release based on the failures in the previous stages.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Once the deployment fails due to the Binary Authorization policy violation, Cloud Function sends an email to a pre-configured email id about the deployment failure. Cloud Function code can be found &lt;a href=&#34;https://github.com/sysdesign-code/dev-sec-ops-demo/tree/main/cloud-function/deployment-notification&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Note: The deployment fails after the timeout value is exceeded, set for Cloud Deploy, which is 10 minutes by default, but you can change this value according to your requirements, see &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt;&lt;p&gt;Note: The Cloud Function code provided for the rollout approval email and deployment failure notification is under the folder cloud-functions in this repo. You will still have to create these cloud functions with this code in your Google Cloud project to receive email notifications.&lt;/p&gt;&lt;h2&gt;Solution Architecture&lt;/h2&gt;&lt;p&gt;The CI/CD pipeline is constructed by combining the aforementioned Google Cloud services. Cloud Build is at the center of automating the pipeline, which contains all the steps we need to build and deploy our container image. Cloud Build executes the steps defined in a YAML file sequentially. It&#39;s quite flexible in terms of how you want to define your &#39;build&#39; and &#39;deploy&#39; process, and the service ensures to execute those steps reliably every time.&lt;/p&gt;&lt;p&gt;Below are solution diagrams of how the CI/CD pipeline is set up :&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/A_secure_CICD_pipeline.max-2800x2800.jpeg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;A secure CI:CD pipeline.jpeg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/A_secure_CICD_pipeline.max-1000x1000.jpeg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;As the last step of our CI process, the Cloud Build YAML triggers the Cloud Deploy service, and the container image is deployed to three different GKE clusters. Cloud Deploy automatically emits multiple notifications to pub/Sub topics throughout the deployment process. We are using Cloud Functions to listen to these Pub/Sub topics to send appropriate email notifications about the deployment status and required approvals.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/B_secure_CICD_pipeline.max-2800x2800.jpeg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;B secure CI:CD pipeline.jpeg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/B_secure_CICD_pipeline.max-1000x1000.jpeg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h2&gt;Step-by-Step instructions for creating the CI/CD pipeline&lt;/h2&gt;&lt;h3&gt;I. Prerequisites&lt;/h3&gt;&lt;p&gt;These steps are required to set up and prepare your GCP environment. We highly recommend you create a new GCP Project as you will run multiple cloud services within the region &#34;us-central1&#34;.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Fork the following GitHub Repo: &lt;a href=&#34;https://github.com/sysdesign-code/dev-sec-ops-demo&#34; target=&#34;_blank&#34;&gt;https://github.com/sysdesign-code/dev-sec-ops-demo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Create a new GCP Project, follow the steps here around how to provision and create one: &lt;a href=&#34;https://cloud.google.com/resource-manager/docs/creating-managing-projects&#34;&gt;https://cloud.google.com/resource-manager/docs/creating-managing-projects&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Once your new project is created, enable Cloud SDK to allow CLI access for &lt;code&gt;gcloud&lt;/code&gt; either in Cloud Shell or your local workstation. Follow the steps here: &lt;a href=&#34;https://cloud.google.com/sdk/docs/install&#34;&gt;https://cloud.google.com/sdk/docs/install&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Once you&#39;ve enabled CLI access, either through your Cloud Shell or local workstation, validate or set your project ID:&lt;br/&gt;&lt;code&gt;gcloud config set project YOUR_PROJECT_ID&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Run the following one-time script &lt;code&gt;/scripts/gcp_env_setup.sh&lt;/code&gt;, which creates and provisions the necessary GCP cloud services required to create the DevSecOps CI/CD pipeline for deploying a sample docker application.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/C_secure_CICD_pipeline.max-2800x2800.jpeg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;C secure CI:CD pipeline.jpeg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/C_secure_CICD_pipeline.max-1000x1000.jpeg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Here are all the service deployments that will occur once the script finishes:&lt;/p&gt;&lt;p&gt;a) Enables all the required cloud service APIs such as Cloud Build, Binary Authorization, Kubernetes Service, Artifact Registry, Cloud Deploy, and many more.&lt;/p&gt;&lt;p&gt;b) Create three (3) GKE clusters for test, staging, and production to show image rollout deployments across these clusters using Cloud Deploy.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/D_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;D secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/D_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;c) Bind all the necessary IAM roles and permissions for Cloud Build and Cloud Deploy.&lt;/p&gt;&lt;p&gt;d) Create a Binary Authorization attestor, associated container note, cryptographic KMS key, and all the associated IAM roles and permissions to allow container note access for the attestor.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/E_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;E secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/E_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;By default, the binary authorization policy allows for all images to be deployed to GCP. Later, we will update this policy only to allow attestor-approved images to be deployed to specific GKE clusters.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/F_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;F secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/F_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;e) Create the Artifact Registry repository where the docker image will be stored.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/G_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;G secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/G_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;f) Finally, create two Pub/Sub topics and Cloud Functions which will allow for email approvals for any GKE deployment to production and error reporting if a release fails.&lt;br/&gt;NOTE&lt;br/&gt;&lt;ol&gt;&lt;li&gt;Before you run the script, please validate if your new GCP project already contains a &#34;default&#34; VPC and subnetwork. If you already have a &#34;default&#34; VPC, please go through the script and COMMENT out lines 53-55 which reference the creation of a default VPC and subnetwork. If you already have one, this step is not needed.&lt;br/&gt;&lt;/li&gt;&lt;li&gt;By default, the creation of GKE clusters uses the &#34;default&#34; VPC subnetwork. If you prefer to use a non-default VPC, update the GKE cluster creation commands, starting at line 157, and update the &lt;code&gt;--subnetwork&lt;/code&gt; value for all 3 GKE clusters.&lt;br/&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;6. To execute the script, run the following command: &lt;code&gt;sh /scripts/gcp_env_setup.sh&lt;/code&gt;&lt;/p&gt;&lt;p&gt;g) This script will approximately take 20-22 minutes to complete. Once finished, the output should look similar to something like &lt;a href=&#34;https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/scripts/gcp_env_setup_OUTPUT.txt&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;7. Create a SendGRID API Key. Follow the instructions: &lt;a href=&#34;https://app.sendgrid.com/guide/integrate&#34; target=&#34;_blank&#34;&gt;https://app.sendgrid.com/guide/integrate&lt;/a&gt; to create a free &#34;Web API&#34; email integration for cURL and its associated API key. Take note and save your key value and verify the integration. The key details will be needed when you create the Cloud Deploy approval process later in this blog. Note: Using SendGRID APIs DOES require you to create a user account.&lt;/p&gt;&lt;h3&gt;II. Configure Cloud Build&lt;/h3&gt;&lt;p&gt;This step requires integrating your git repository (from Pre-Requisites, Step 1) as a managed repository to GCP&#39;s cloud build service and creating the necessary Trigger. The goal of this integration is that any updates you make to your application within your GitHub repository will automatically kick off a Cloud Build deployment which will create, enable and deploy your application to GKE.&lt;/p&gt;&lt;p&gt;Create the GitHub Repository Integration for Cloud Build :&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;To start, from your GCP Console homepage, type &#34;Cloud Build&#34; within the search bar and select this service.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;From the left-hand panel, click on &#34;Triggers&#34;. And click on &#34;Connect Repository.&#34;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Select the source as &#34;GitHub (Cloud Build GitHub App)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Authenticate the connection with your GitHub credentials, select the forked repository, and click &#34;Connect&#34;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Once the integration is done, you will see your newly added repository under &#34;Triggers&#34; -&amp;gt; &#34;Manage Repositories.&#34;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/H_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;H secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/H_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Create a Trigger for Cloud Build&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;From the &#34;Triggers&#34; page, click on &#34;+ Create Trigger&#34;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Enter/Select the following values for the Trigger:&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Name: &lt;code&gt;CI/CD-blog-trigger&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Region: &lt;code&gt;us-central1&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Description: &lt;code&gt;Deploy Docker Image using GCP CI/CD cloud services.&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Event: &lt;code&gt;Push to a branch.&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Repository: Select your forked repository&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Branch: &lt;code&gt;^main$&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Configuration: &lt;code&gt;Cloud Build Configuration File (YAML or JSON)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Location: &lt;code&gt;Repository&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Build configuration file location: &lt;code&gt;/ cloudbuild.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Under &#34;Advanced&#34;, add the following TWO environment variables and their values: &lt;code&gt;_CONTAINER_REPO_NAME: test-repo _SEVERITY: CRITICAL&lt;/code&gt; &lt;br/&gt;NOTE: The value of these env variables is case sensitive.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/I_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;I secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/I_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;3. After the environment values are entered/selected, click &#34;Create&#34;.&lt;/p&gt;&lt;p&gt;Once the Trigger is created, it will look like the following:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/J_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;J secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/J_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;III. Create Cloud Deploy Pipeline&lt;/h3&gt;&lt;p&gt;Now that we have created GitHub integration and Cloud Build Trigger, the next step is to create the Cloud Deploy pipeline. This will deploy the container image to the three GKE environments: &#34;test,&#34; &#34;staging,&#34; and &#34;prod&#34; once the image release for all three environments is created through Cloud Build. The requirement for image release requires a Cloud Deploy pipeline.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Edit the clouddeploy.yaml file with your GCP project ID.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Within the file, update lines 22, 32, and 42 with your respective GCP project ID&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/K_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;K secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/K_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;3. Once this is updated, save the file.&lt;/p&gt;4. Either through Cloud Shell or your local workstation, run the following GCP command to create the environment variables and the Cloud Deploy pipeline called &lt;code&gt;ci-cd-test&lt;/code&gt;:&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;$ PROJECT_ID=&amp;lt;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;gt;\r\n$ LOCATION=us-central1\r\n$ gcloud deploy apply --file clouddeploy.yaml --region=$LOCATION --project=$PROJECT_ID&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3ee4e8ede710&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;NOTE: If you run into issues with a failed Cloud Deploy pipeline creation, delete the pipeline using the following gcloud command:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;gcloud deploy delivery-pipelines delete ci-cd-test --region=us-central1 --force&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3ee4e8ede350&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;5. Once the pipeline is created, here is what the output will look like:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;$ gcloud deploy apply --file clouddeploy.yaml --region=$LOCATION --project=$PROJECT_ID\r\nWaiting for the operation on resource projects/&amp;lt;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;gt;/locations/us-central1/deliveryPipelines/ci-cd-test...done. \r\nCreated Cloud Deploy resource: projects/&amp;lt;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;gt;/locations/us-central1/deliveryPipelines/ci-cd-test.\r\nWaiting for the operation on resource projects/&amp;lt;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;gt;/locations/us-central1/targets/test...done. \r\nCreated Cloud Deploy resource: projects/&amp;lt;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;gt;/locations/us-central1/targets/test.\r\nWaiting for the operation on resource projects/&amp;lt;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;gt;/locations/us-central1/targets/staging...done. \r\nCreated Cloud Deploy resource: projects/&amp;lt;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;gt;/locations/us-central1/targets/staging.\r\nWaiting for the operation on resource projects/&amp;lt;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;gt;/locations/us-central1/targets/prod...done. \r\nCreated Cloud Deploy resource: projects/&amp;lt;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;gt;/locations/us-central1/targets/prod.&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3ee4ea052850&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;6. From your GCP Console homepage, type &#34;Cloud Deploy&#34; within the search bar and select this service. From the main page, you will see the newly created pipeline.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/L_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;L secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/L_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;IV. Configure email notifications for GKE production cluster deployment&lt;/h3&gt;&lt;p&gt;As part of a typical CI/CD process, any deployment of production workloads requires some form of approval process by DevOps engineers. Cloud Deploy allows you to inject an &#39;approval&#39; step before deploying a rollout to the next target. We have created this approval check in our pipeline before the deployment to the &#39;prod&#39; GKE cluster. Once the pipeline reaches the step to deploy the rollout to the &#39;prod&#39; GKE cluster, it emits a message in the &lt;code&gt;clouddeploy-approvals&lt;/code&gt; Pub/Sub topic. We have created a Cloud Function to listen to this topic and implement logic to send email notifications via Sendgrid. You can use any other library to send emails via Cloud Functions.&lt;/p&gt;&lt;p&gt;The one-time script has created a Pub/Sub topic and Cloud Function, allowing your cloud build release to send an approver email.&lt;/p&gt;&lt;p&gt;To validate that the Pub/Sub topics and Cloud Function was created, go to those respective services and ensure they were created.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;From your GCP Console homepage, type &#34;Pub/Sub&#34; within the search bar and select this service. There will be two Pub/Sub topics, and they&#39;re called &lt;code&gt;clouddeploy-approvals&lt;/code&gt; and &lt;code&gt;clouddeploy-operations&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;From your GCP Console homepage, type &#34;Cloud Functions&#34; within the search bar and select this service. There will be two Cloud Functions, called &lt;code&gt;cd-approval&lt;/code&gt; and &lt;code&gt;cd-deploy-notification&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click on &lt;code&gt;cd-approval&lt;/code&gt; and select &#34;Variables&#34;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click the &#34;Edit&#34; button and expand the `Runtime, build, connections and security settings.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Scroll down until you get to the &#34;Runtime environment variables.&#34; Here you will update the following three variables.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/M_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;M secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/M_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;For &lt;code&gt;FROM_EMAIL&lt;/code&gt;, enter a secondary email account; it could be @gmail or any other domain of your choice. For &lt;code&gt;TO_EMAIL&lt;/code&gt;, select a primary email. For instance, the email of a DevOps Engineer who will be the approver of all production workload deployments to GKE. For &lt;code&gt;SENDGRID_API_KEY&lt;/code&gt;, you will enter your API Key, starting with &#34;SG.&#34;. If you haven&#39;t already, refer to the Prerequisites section above, step 6, around creating this key.&lt;/p&gt;&lt;p&gt;6. After you&#39;ve updated the cloud function environment variables, click &#34;Next&#34; and &#34;Deploy&#34; the updated function. It will take about 1-2 minutes. Once completed, the function will have a green check mark to validate its running.&lt;/p&gt;&lt;p&gt;7. Repeat steps 4-6 from above for the other cloud function of &lt;code&gt;cd-approval&lt;/code&gt;.&lt;/p&gt;&lt;h2&gt;Step-by-step instructions of testing and validating the GCP CI/CD pipeline&lt;/h2&gt;&lt;p&gt;Now that all the GCP prerequisites and environment setup is complete for Cloud Build, Cloud Deploy, and Email approvals, we&#39;ll next deploy the image to GKE and initiate the pipeline testing.&lt;/p&gt;&lt;p&gt;A couple of items to note during this test, we&#39;re going to show a &#34;Happy&#34; and &#34;Vulnerable&#34; Image deployment path to GKE.&lt;/p&gt;&lt;p&gt;The &#34;Happy&#34; path will show a successful deployment of the end-to-end pipeline across nine steps for a clean image deployment to GKE. &#34;Clean&#34; refers to the docker image with non-critical vulnerabilities. This path will also update the Binary Authorization policy that allows only the &#34;Happy&#34; image to be deployed to GKE&#39;s &#34;test&#34;, &#34;staging&#34;, and eventually &#34;production&#34; environments, which a DevOps engineer will approve.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/N_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;N secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/N_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The &#34;Vulnerable&#34; docker path will show a failed deployment of the end-to-end pipeline across seven steps. The pipeline will fail in 2 of these steps because the image has:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Certain vulnerabilities must be addressed before the image can be stored in the Artifact Registry.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A failed deployment to GKE because this is a non-approved image without attestation, violating the updated Binary Authorization policy from the &#34;Happy&#34; path.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;When Binary Authorization is enabled, its default policy allows all images to be deployed to the GKE target environments without attestation. In the &#34;Happy&#34; path, we will update the default Binary Authorization policy where only a specific docker image is approved for deployment to GKE. GKE will reject any other image not approved by the Binary Authorization policy at the deployment time.&lt;/p&gt;&lt;p&gt;To allow other images to be deployed to GKE through an active binary authorization policy, update the following script &lt;code&gt;/scripts/create_binauthz_policy.sh&lt;/code&gt; where you can sign the image digest to the existing attestor and allow for that image deployment to GKE.&lt;/p&gt;&lt;p&gt;In the following sections, we&#39;ll go into further detail describing both paths of image deployment to GKE.&lt;/p&gt;&lt;h3&gt;I. Run Cloud Build configuration file for &#34;Happy&#34; path&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Ensure your GitHub repo is connected as a repository in Cloud Build. Refer to the &#34;Create the GitHub Repository Integration for Cloud Build&#34; section on how to do this.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ensure your Cloud Build trigger called &lt;code&gt;CI/CD-blog-trigger&lt;/code&gt; is created. Refer to the section &#34;Create a Trigger for Cloud Build&#34; on how to do this.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Since the Trigger is already enabled, any updates to your repository will trigger this Cloud Build deployment.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Open up the &lt;code&gt;cloudbuild.yaml&lt;/code&gt; from your GitHub repo. This is the cloud build configuration file for the &#34;Happy&#34; Docker path.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;To kick off the build, make any update to your codebase such as update the &lt;code&gt;/src/static/js&lt;/code&gt; file for any cosmetic change.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;After you&#39;ve made the change, push the changes to your GitHub repo.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;From the GCP Console, go to the Cloud Build service and click on &#34;History&#34;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Since the Trigger is enabled and integrated with your GitHub page, the build is automatically kicked off, and you can click the custom build number to see the log details.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/O_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;O secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/O_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;II. Validate image deployment for &#34;Happy&#34; path&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Within that build, steps 7-9 highlight the image deployment to GKE through Cloud Deploy. If you click on step 9, the result of the build states that the deployment to &#34;prod&#34; is awaiting approval.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/P_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;P secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/P_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;2. Go to the Cloud Deploy homepage from the GCP Console and click on the &lt;code&gt;ci-cd-test&lt;/code&gt; pipeline.&lt;/p&gt;&lt;p&gt;3. Within the pipeline, click on the release associated with the latest cloud build deployment. Here you see that the &#34;Happy&#34; image is deployed successfully to both &#34;test&#34; and &#34;staging&#34;, but there&#39;s an approval process required for the &#34;prod&#34; cluster.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Q_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Q secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Q_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;4. From the GCP Console, search for Kubernetes Engine; from the left-hand navigation, click on &#34;Workloads.&#34; Here you can see that the image deployment is successful in the two &#34;test&#34; and &#34;staging&#34; GKE environments.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/R_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;R secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/R_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;5. Now that the deployment is queued for production, check your primary email and validate that you received a notification for approval. It will look something like this.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/S_secure_CICD_pipeline_1.1000062520000400.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;S secure CI:CD pipeline.png&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/S_secure_CICD_pipeline_1.1000062520000400.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;6. From the email, click the &lt;code&gt;here&lt;/code&gt; hyperlink and it will take you to the Cloud deploy pipeline page.&lt;/p&gt;&lt;p&gt;7. From the Pipeline page, approve or reject the release so the deployment can be pushed to &#34;prod&#34; in GKE. In this case, we will approve.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/T_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;T secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/T_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;7. If you go back to the Kubernetes workload page, you&#39;ll see that the image rollout to prod was successful.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/U_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;U secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/U_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In parallel, validate your Cloud Deploy, continuous deployment pipeline also confirms a successful rollout.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/V_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;V secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/V_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;III. Run Cloud Build configuration file for &#34;Vulnerable&#34; path (container image has vulnerabilities)&lt;/h3&gt;&lt;p&gt;We will show two failure paths with this deployment: image vulnerabilities and Binary Authorization policy enforcement.&lt;/p&gt;&lt;p&gt;A. First, failed deployment to push docker image to Artifact Registry because of severity-specific vulnerabilities -&lt;/p&gt;&lt;p&gt;1. Ensure your GitHub Repo is connected as a repository in Cloud Build. Refer to the &#34;Create the GitHub Repository Integration for Cloud Build&#34; section on how to do this.&lt;/p&gt;&lt;p&gt;2. Ensure your Cloud Build Trigger called &lt;code&gt;CI/CD-blog-trigger&lt;/code&gt; is created. Refer to the section &#34;Create a Trigger for Cloud Build&#34; on how to do this.&lt;/p&gt;&lt;p&gt;3. Since the Trigger is already enabled, any updates to your repository will trigger this cloud build deployment.&lt;/p&gt;&lt;p&gt;4. View the &lt;code&gt;cloudbuild-vulnerable.yaml&lt;/code&gt; file from your GitHub repo. This is the cloud build configuration file for the &#34;Vulnerable&#34; Docker path.&lt;/p&gt;&lt;p&gt;5. Edit the existing Trigger with the following:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Click on the ellipses next to &#34;RUN&#34; and update the &#34;Cloud Build configuration file location&#34; to be: &lt;code&gt;cloudbuild-vulnerable.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Update the &#34;_SEVERITY&#34; environment variable value to be &lt;code&gt;HIGH&lt;/code&gt;. We&#39;re changing the severity of the vulnerabilities because the vulnerability check will either PASS or FAIL a cloud build deployment if the image contains ANY &lt;code&gt;HIGH&lt;/code&gt; vulnerabilities.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Save the Trigger and validate its status as &#34;Enabled&#34;.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;6. To kick off the build, make any update to your codebase, such as updating the &lt;code&gt;/src/static/js&lt;/code&gt; file for any cosmetic change. After you&#39;ve made the change, push the changes to your GitHub repo.&lt;/p&gt;&lt;p&gt;7. From the GCP Console, go to the Cloud Build service and click on &#34;History&#34;.&lt;/p&gt;8. The build will fail in &lt;code&gt;Step 2: Check For Vulnerabilities within the Image&lt;/code&gt; because this image contains HIGH vulnerabilities, and cloud build will NOT push this image to be stored in the artifact registry.&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/W_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;W secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/W_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;B. Second, a failed image deployment to GKE because of Binary Authorization policy enforcement -&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Go back to the Trigger configuration for this build and change the &#34;_SEVERITY&#34; environment variable value to &lt;code&gt;CRITICAL&lt;/code&gt; instead of &#34;HIGH&#34;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;To kick off the build, make any update to your codebase such as update the &lt;code&gt;/src/static/js&lt;/code&gt; file for any cosmetic change. After you&#39;ve made the change, push the changes to your GitHub repo.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;From the GCP Console, go to the Cloud Deploy pipeline &lt;code&gt;ci-cd-test&lt;/code&gt; and check the results of this latest release.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;From the Cloud Deploy pipeline page, approximately 10 minutes later, the build for &#34;test&#34; and &#34;staging&#34; will eventually fail because the Kubernetes manifest file for this docker image timed out.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/X_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;X secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/X_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;You can change the timeout period to be shorter; additional details can be found &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;p&gt;5. From the GCP Console, go to the GKE page and click on &#34;Workloads&#34;. Here you will see the image deployments to both the &#34;test&#34; and &#34;staging&#34; GKE environments failed. The reason being is binary authorization policy enforcement. The &#34;vulnerable&#34; docker image is not approved for deployment.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Y_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Y secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Y_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;6. In parallel to a failed deployment to any of the GKE staging environments, Cloud Function &lt;code&gt;cd-deploy-notification&lt;/code&gt; will send the following email to check the logs for the pipeline.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/S_secure_CICD_pipeline_1.1000062520000400.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;S secure CI:CD pipeline.png&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/S_secure_CICD_pipeline_1.1000062520000400.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;7. From the email, click on &lt;code&gt;here to see deployment logs&lt;/code&gt;, and it will take you to the log files within cloud build around additional details on the failure of the release rollout to GKE.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Z_secure_CICD_pipeline.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Z secure CI:CD pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Z_secure_CICD_pipeline.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h2&gt;Conclusion and further reading&lt;/h2&gt;&lt;p&gt;In this blog post, we built a secure CI/CD pipeline using Google Cloud&#39;s built-in services.&lt;/p&gt;&lt;p&gt;We learned how we can secure a CI/CD pipeline using Google Cloud&#39;s built-in services, such as Binary Authorization and Vulnerability scanning of the container images. We only saw one way to put some control on specific images that can be deployed to a GKE cluster. Binary Authorization also offers &lt;a href=&#34;https://cloud.google.com/binary-authorization/docs/overview#attestations&#34;&gt;Build Verification&lt;/a&gt;, in which Binary Authorization uses attestations to verify that an image was built by a specific build system or continuous integration (CI) pipeline such as Cloud Build.&lt;/p&gt;&lt;p&gt;Additionally, Binary Authorization also writes all the events where the deployment of a container image is blocked due to the constraints defined by the security policy to the audit logs. You can create alerts on these log entries and notify the appropriate team members about the blocked deployment events.&lt;/p&gt;&lt;p&gt;Lastly, all of the services used to build and secure the CI/CD pipelines are serverless, which makes it very easy to spin up the whole infrastructure within a few minutes without worrying about maintaining or managing it, so that your teams can focus on building and releasing software in a faster, reliable and cost efficient manner.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout_external&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><article><article-header-block></article-header-block><div><div><article-author-block><div><div><p> Nitin Vashishtha </p><p> Customer Engineer </p></div><p><span> September 13, 2022 </span></p></div></article-author-block></div><article-cta _nghost-c58=""><div _ngcontent-c58=""><h4 _ngcontent-c58=""><span _ngcontent-c58="">Google Cloud Next &#39;22</span></h4><p _ngcontent-c58=""><span _ngcontent-c58="">Register for our flagship event October 11–13.</span></p><p><a _ngcontent-c58="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="Next22_registration" track-metadata-eventdetail="https://cloud.withgoogle.com/next?utm_source=cgc-blog&amp;utm_medium=blog&amp;utm_campaign=FY22-Q4-global-ENTD217-onlineevent-er-next-2022-mc&amp;utm_content=left_hand_rail_blog&amp;utm_term=-" href="https://cloud.withgoogle.com/next?utm_source=cgc-blog&amp;utm_medium=blog&amp;utm_campaign=FY22-Q4-global-ENTD217-onlineevent-er-next-2022-mc&amp;utm_content=left_hand_rail_blog&amp;utm_term=-"><span _ngcontent-c58="">Register Now</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;DevOps is a concept that allows software development teams to release software in an automated and stable manner. DevOps itself is not just one thing; it&#39;s a combination of culture and technology, which together make the implementation of DevOps successful.&lt;/p&gt;&lt;p&gt;In this blog, we will be focusing on the tools and technology side of DevOps. At the core of the technical aspect of DevOps, the concept is Continuous Integration and Continuous Delivery (CI/CD). The idea behind CI/CD concept is to create an automated software delivery pipeline that continuously deploys the new software releases in an automated fashion.&lt;/p&gt;&lt;p&gt;The flow begins with the developers committing the code changes to a source code repository, which automatically triggers the delivery pipeline (henceforth called CI/CD pipeline) by building and deploying the code changes into various environments, from non-prod environments to production environments.&lt;/p&gt;&lt;p&gt;Also, as we build the CI/CD pipelines for faster and more reliable software delivery, the security aspect should not be ignored and must be incorporated into the pipeline right from the beginning. When we build our source code, we typically use various open-source libraries and container images. Having some security safeguards within the CI/CD pipeline is imperative to ensure that the software we are building and deploying is free from any vulnerability. Additionally, it&#39;s equally important to control what type of code/container image should be allowed to be deployed on your target runtime environment.&lt;/p&gt;&lt;p&gt;Security is everyone&#39;s responsibility. &lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-tech-shifting-left-on-security&#34;&gt;Shifting left&lt;/a&gt; on security is a DevOps practice that allows you to address security concerns early in the software development lifecycle. Vulnerability scanning of container images, putting security policies in place through Binary Authorization, and allowing approved/trusted images to be deployed on GKE are a couple of ways to implement this policy to make your CI/CD pipelines more secure.&lt;/p&gt;&lt;p&gt;What are we building?&lt;/p&gt;&lt;p&gt;This blog post will show how to build a secure CI/CD pipeline using Google Cloud&#39;s built-in services. We will create a secure software delivery pipeline that builds a sample Node.js application as a container image and deploys it on GKE clusters.&lt;/p&gt;&lt;p&gt;How are we building the CI/CD pipeline?&lt;/p&gt;&lt;p&gt;We&#39;re going to use the following Google Cloud built-in services to build the pipeline:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/build&#34;&gt;Cloud Build&lt;/a&gt; - Cloud Build is an entirely serverless CI/CD platform that allows you to automate your build, test, and deploy tasks.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry&#34;&gt;Artifact Registry&lt;/a&gt; - Artifact Registry is a secure service to store and manage your build artifacts.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Cloud Deploy&lt;/a&gt; - Cloud Deploy is a fully managed Continuous Delivery service for GKE and Anthos.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/binary-authorization&#34;&gt;Binary Authorization&lt;/a&gt; - Binary Authorization provides deployment time security controls for GKE and Cloud Run deployments.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;GKE&lt;/a&gt; - GKE is a fully managed Kubernetes platform.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Google Pub/Sub&lt;/a&gt; - Pub/Sub is a serverless messaging platform.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/functions&#34;&gt;Cloud Functions&lt;/a&gt; - Cloud Functions is a serverless platform to run your code.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;We use GitHub as a source code repository and Sendgrid APIs to send email notifications for approval and error logging.&lt;/p&gt;&lt;p&gt;The CI/CD pipeline is set up so that a Cloud Build trigger is configured to sense any code pushed to a particular repository and branch in a GitHub repository and automatically starts the build process.&lt;/p&gt;&lt;p&gt;Below is the flow of how the CI/CD pipeline is set up without any security policy enforcement:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Developer checks in the code to a GitHub repo.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and starts the &#39;build&#39; process. A successful build results in a docker container image.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The container image is stored in the Artifact Registry.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Deploy is configured to go through an approval step before deploying the image to the Production GKE cluster.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A Cloud Function sends an email to a pre-configured email id, notifying you that a Cloud Deploy rollout requires your approval. The email receiver can approve or reject the deployment to the production GKE cluster. Cloud Function code can be found &lt;a href=&#34;https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/cloud-function/index.js&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;To secure this CI/CD pipeline, we will use a couple of Google Cloud&#39;s built-in features and services. First, we will enable vulnerability scans on Artifact Registry, an out-of-the-box feature. Then finally, we will create a security policy using the Binary Authorization service, which only allows a specific image to be deployed to your GKE cluster.&lt;/p&gt;&lt;p&gt;Below is the flow when we try to build and deploy a container image that has vulnerabilities present:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Developer checks in the code to a GitHub repo.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the &#39;build&#39; process.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The build process fails with the error message that vulnerabilities were found in the image.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Below is the flow when we try to deploy a container image to GKE, which violates a Binary Authorization policy:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Developer checks in the code to a GitHub repo.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the &#39;build&#39; process. A successful build results in a docker container image.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The container image is stored in Artifact Registry.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Deploy fails as the GKE clusters reject the incoming image as it violates the existing Binary Authorization policy. Please note that an approval email is still triggered before the production deployment via the Cloud Function; the email receiver is expected to reject this release based on the failures in the previous stages.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Once the deployment fails due to the Binary Authorization policy violation, Cloud Function sends an email to a pre-configured email id about the deployment failure. Cloud Function code can be found &lt;a href=&#34;https://github.com/sysdesign-code/dev-sec-ops-demo/tree/main/cloud-function/deployment-notification&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Note: The deployment fails after the timeout value is exceeded, set for Cloud Deploy, which is 10 minutes by default, but you can change this value according to your requirements, see &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt;&lt;p&gt;Note: The Cloud Function code provided for the rollout approval email and deployment failure notification is under the folder cloud-functions in this repo. You will still have to create these cloud functions with this code in your Google Cloud project to receive email notifications.&lt;/p&gt;&lt;h2&gt;Solution Architecture&lt;/h2&gt;&lt;p&gt;The CI/CD pipeline is constructed by combining the aforementioned Google Cloud services. Cloud Build is at the center of automating the pipeline, which contains all the steps we need to build and deploy our container image. Cloud Build executes the steps defined in a YAML file sequentially. It&#39;s quite flexible in terms of how you want to define your &#39;build&#39; and &#39;deploy&#39; process, and the service ensures to execute those steps reliably every time.&lt;/p&gt;&lt;p&gt;Below are solution diagrams of how the CI/CD pipeline is set up :&lt;/p&gt;"><p>DevOps is a concept that allows software development teams to release software in an automated and stable manner. DevOps itself is not just one thing; it&#39;s a combination of culture and technology, which together make the implementation of DevOps successful.</p><p>In this blog, we will be focusing on the tools and technology side of DevOps. At the core of the technical aspect of DevOps, the concept is Continuous Integration and Continuous Delivery (CI/CD). The idea behind CI/CD concept is to create an automated software delivery pipeline that continuously deploys the new software releases in an automated fashion.</p><p>The flow begins with the developers committing the code changes to a source code repository, which automatically triggers the delivery pipeline (henceforth called CI/CD pipeline) by building and deploying the code changes into various environments, from non-prod environments to production environments.</p><p>Also, as we build the CI/CD pipelines for faster and more reliable software delivery, the security aspect should not be ignored and must be incorporated into the pipeline right from the beginning. When we build our source code, we typically use various open-source libraries and container images. Having some security safeguards within the CI/CD pipeline is imperative to ensure that the software we are building and deploying is free from any vulnerability. Additionally, it&#39;s equally important to control what type of code/container image should be allowed to be deployed on your target runtime environment.</p><p>Security is everyone&#39;s responsibility. <a href="https://cloud.google.com/architecture/devops/devops-tech-shifting-left-on-security" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/architecture/devops/devops-tech-shifting-left-on-security" track-metadata-module="post">Shifting left</a> on security is a DevOps practice that allows you to address security concerns early in the software development lifecycle. Vulnerability scanning of container images, putting security policies in place through Binary Authorization, and allowing approved/trusted images to be deployed on GKE are a couple of ways to implement this policy to make your CI/CD pipelines more secure.</p><p>What are we building?</p><p>This blog post will show how to build a secure CI/CD pipeline using Google Cloud&#39;s built-in services. We will create a secure software delivery pipeline that builds a sample Node.js application as a container image and deploys it on GKE clusters.</p><p>How are we building the CI/CD pipeline?</p><p>We&#39;re going to use the following Google Cloud built-in services to build the pipeline:</p><ol><li><p><a href="https://cloud.google.com/build" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/build" track-metadata-module="post">Cloud Build</a> - Cloud Build is an entirely serverless CI/CD platform that allows you to automate your build, test, and deploy tasks.</p></li><li><p><a href="https://cloud.google.com/artifact-registry" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/artifact-registry" track-metadata-module="post">Artifact Registry</a> - Artifact Registry is a secure service to store and manage your build artifacts.</p></li><li><p><a href="https://cloud.google.com/deploy" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">Cloud Deploy</a> - Cloud Deploy is a fully managed Continuous Delivery service for GKE and Anthos.</p></li><li><p><a href="https://cloud.google.com/binary-authorization" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/binary-authorization" track-metadata-module="post">Binary Authorization</a> - Binary Authorization provides deployment time security controls for GKE and Cloud Run deployments.</p></li><li><p><a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">GKE</a> - GKE is a fully managed Kubernetes platform.</p></li><li><p><a href="https://cloud.google.com/pubsub" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/pubsub" track-metadata-module="post">Google Pub/Sub</a> - Pub/Sub is a serverless messaging platform.</p></li><li><p><a href="https://cloud.google.com/functions" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/functions" track-metadata-module="post">Cloud Functions</a> - Cloud Functions is a serverless platform to run your code.</p></li></ol><p>We use GitHub as a source code repository and Sendgrid APIs to send email notifications for approval and error logging.</p><p>The CI/CD pipeline is set up so that a Cloud Build trigger is configured to sense any code pushed to a particular repository and branch in a GitHub repository and automatically starts the build process.</p><p>Below is the flow of how the CI/CD pipeline is set up without any security policy enforcement:</p><ol><li><p>Developer checks in the code to a GitHub repo.</p></li><li><p>A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and starts the &#39;build&#39; process. A successful build results in a docker container image.</p></li><li><p>The container image is stored in the Artifact Registry.</p></li><li><p>The Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.</p></li><li><p>Cloud Deploy is configured to go through an approval step before deploying the image to the Production GKE cluster.</p></li><li><p>A Cloud Function sends an email to a pre-configured email id, notifying you that a Cloud Deploy rollout requires your approval. The email receiver can approve or reject the deployment to the production GKE cluster. Cloud Function code can be found <a href="https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/cloud-function/index.js" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://github.com" track-metadata-module="post">here</a></p></li></ol><p>To secure this CI/CD pipeline, we will use a couple of Google Cloud&#39;s built-in features and services. First, we will enable vulnerability scans on Artifact Registry, an out-of-the-box feature. Then finally, we will create a security policy using the Binary Authorization service, which only allows a specific image to be deployed to your GKE cluster.</p><p>Below is the flow when we try to build and deploy a container image that has vulnerabilities present:</p><ol><li><p>Developer checks in the code to a GitHub repo.</p></li><li><p>A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the &#39;build&#39; process.</p></li><li><p>The build process fails with the error message that vulnerabilities were found in the image.</p></li></ol><p>Below is the flow when we try to deploy a container image to GKE, which violates a Binary Authorization policy:</p><ol><li><p>Developer checks in the code to a GitHub repo.</p></li><li><p>A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the &#39;build&#39; process. A successful build results in a docker container image.</p></li><li><p>The container image is stored in Artifact Registry.</p></li><li><p>The Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.</p></li><li><p>Cloud Deploy fails as the GKE clusters reject the incoming image as it violates the existing Binary Authorization policy. Please note that an approval email is still triggered before the production deployment via the Cloud Function; the email receiver is expected to reject this release based on the failures in the previous stages.</p></li><li><p>Once the deployment fails due to the Binary Authorization policy violation, Cloud Function sends an email to a pre-configured email id about the deployment failure. Cloud Function code can be found <a href="https://github.com/sysdesign-code/dev-sec-ops-demo/tree/main/cloud-function/deployment-notification" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://github.com" track-metadata-module="post">here</a>.</p></li></ol><p>Note: The deployment fails after the timeout value is exceeded, set for Cloud Deploy, which is 10 minutes by default, but you can change this value according to your requirements, see <a href="https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout" track-metadata-module="post">here</a> for more details.</p><p>Note: The Cloud Function code provided for the rollout approval email and deployment failure notification is under the folder cloud-functions in this repo. You will still have to create these cloud functions with this code in your Google Cloud project to receive email notifications.</p><h2>Solution Architecture</h2><p>The CI/CD pipeline is constructed by combining the aforementioned Google Cloud services. Cloud Build is at the center of automating the pipeline, which contains all the steps we need to build and deploy our container image. Cloud Build executes the steps defined in a YAML file sequentially. It&#39;s quite flexible in terms of how you want to define your &#39;build&#39; and &#39;deploy&#39; process, and the service ensures to execute those steps reliably every time.</p><p>Below are solution diagrams of how the CI/CD pipeline is set up :</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>As the last step of our CI process, the Cloud Build YAML triggers the Cloud Deploy service, and the container image is deployed to three different GKE clusters. Cloud Deploy automatically emits multiple notifications to pub/Sub topics throughout the deployment process. We are using Cloud Functions to listen to these Pub/Sub topics to send appropriate email notifications about the deployment status and required approvals.</p></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h2&gt;Step-by-Step instructions for creating the CI/CD pipeline&lt;/h2&gt;&lt;h3&gt;I. Prerequisites&lt;/h3&gt;&lt;p&gt;These steps are required to set up and prepare your GCP environment. We highly recommend you create a new GCP Project as you will run multiple cloud services within the region &amp;#34;us-central1&amp;#34;.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Fork the following GitHub Repo: &lt;a href=&#34;https://github.com/sysdesign-code/dev-sec-ops-demo&#34; target=&#34;_blank&#34;&gt;https://github.com/sysdesign-code/dev-sec-ops-demo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Create a new GCP Project, follow the steps here around how to provision and create one: &lt;a href=&#34;https://cloud.google.com/resource-manager/docs/creating-managing-projects&#34;&gt;https://cloud.google.com/resource-manager/docs/creating-managing-projects&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Once your new project is created, enable Cloud SDK to allow CLI access for &lt;code&gt;gcloud&lt;/code&gt; either in Cloud Shell or your local workstation. Follow the steps here: &lt;a href=&#34;https://cloud.google.com/sdk/docs/install&#34;&gt;https://cloud.google.com/sdk/docs/install&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Once you&#39;ve enabled CLI access, either through your Cloud Shell or local workstation, validate or set your project ID:&lt;br&gt;&lt;code&gt;gcloud config set project YOUR_PROJECT_ID&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Run the following one-time script &lt;code&gt;/scripts/gcp_env_setup.sh&lt;/code&gt;, which creates and provisions the necessary GCP cloud services required to create the DevSecOps CI/CD pipeline for deploying a sample docker application.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;"><h2>Step-by-Step instructions for creating the CI/CD pipeline</h2><h3>I. Prerequisites</h3><p>These steps are required to set up and prepare your GCP environment. We highly recommend you create a new GCP Project as you will run multiple cloud services within the region &#34;us-central1&#34;.</p><ol><li><p>Fork the following GitHub Repo: <a href="https://github.com/sysdesign-code/dev-sec-ops-demo" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="https://github.com" track-metadata-module="post">https://github.com/sysdesign-code/dev-sec-ops-demo</a></p></li><li><p>Create a new GCP Project, follow the steps here around how to provision and create one: <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/resource-manager/docs/creating-managing-projects" track-metadata-module="post">https://cloud.google.com/resource-manager/docs/creating-managing-projects</a></p></li><li><p>Once your new project is created, enable Cloud SDK to allow CLI access for <code>gcloud</code> either in Cloud Shell or your local workstation. Follow the steps here: <a href="https://cloud.google.com/sdk/docs/install" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/sdk/docs/install" track-metadata-module="post">https://cloud.google.com/sdk/docs/install</a></p></li><li><p>Once you&#39;ve enabled CLI access, either through your Cloud Shell or local workstation, validate or set your project ID:<br/><code>gcloud config set project YOUR_PROJECT_ID</code></p></li><li><p>Run the following one-time script <code>/scripts/gcp_env_setup.sh</code>, which creates and provisions the necessary GCP cloud services required to create the DevSecOps CI/CD pipeline for deploying a sample docker application.</p></li></ol></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;Here are all the service deployments that will occur once the script finishes:&lt;/p&gt;&lt;p&gt;a) Enables all the required cloud service APIs such as Cloud Build, Binary Authorization, Kubernetes Service, Artifact Registry, Cloud Deploy, and many more.&lt;/p&gt;&lt;p&gt;b) Create three (3) GKE clusters for test, staging, and production to show image rollout deployments across these clusters using Cloud Deploy. &lt;/p&gt;"><p>Here are all the service deployments that will occur once the script finishes:</p><p>a) Enables all the required cloud service APIs such as Cloud Build, Binary Authorization, Kubernetes Service, Artifact Registry, Cloud Deploy, and many more.</p><p>b) Create three (3) GKE clusters for test, staging, and production to show image rollout deployments across these clusters using Cloud Deploy. </p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;c) Bind all the necessary IAM roles and permissions for Cloud Build and Cloud Deploy.&lt;/p&gt;&lt;p&gt;d) Create a Binary Authorization attestor, associated container note, cryptographic KMS key, and all the associated IAM roles and permissions to allow container note access for the attestor. &lt;/p&gt;"><p>c) Bind all the necessary IAM roles and permissions for Cloud Build and Cloud Deploy.</p><p>d) Create a Binary Authorization attestor, associated container note, cryptographic KMS key, and all the associated IAM roles and permissions to allow container note access for the attestor. </p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>By default, the binary authorization policy allows for all images to be deployed to GCP. Later, we will update this policy only to allow attestor-approved images to be deployed to specific GKE clusters. </p></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>e) Create the Artifact Registry repository where the docker image will be stored.</p></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="f) Finally, create two Pub/Sub topics and Cloud Functions which will allow for email approvals for any GKE deployment to production and error reporting if a release fails.&lt;br&gt;NOTE&lt;br&gt;&lt;ol&gt;&lt;li&gt;Before you run the script, please validate if your new GCP project already contains a &amp;#34;default&amp;#34; VPC and subnetwork. If you already have a &amp;#34;default&amp;#34; VPC, please go through the script and COMMENT out lines 53-55 which reference the creation of a default VPC and subnetwork. If you already have one, this step is not needed.&lt;br&gt;&lt;/li&gt;&lt;li&gt;By default, the creation of GKE clusters uses the &amp;#34;default&amp;#34; VPC subnetwork. If you prefer to use a non-default VPC, update the GKE cluster creation commands, starting at line 157, and update the &lt;code&gt;--subnetwork&lt;/code&gt; value for all 3 GKE clusters.&lt;br&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;6. To execute the script, run the following command:&amp;#160;&lt;code&gt;sh /scripts/gcp_env_setup.sh&lt;/code&gt;&lt;/p&gt;&lt;p&gt;g) This script will approximately take 20-22 minutes to complete. Once finished, the output should look similar to something like &lt;a href=&#34;https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/scripts/gcp_env_setup_OUTPUT.txt&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;7. Create a SendGRID API Key. Follow the instructions: &lt;a href=&#34;https://app.sendgrid.com/guide/integrate&#34; target=&#34;_blank&#34;&gt;https://app.sendgrid.com/guide/integrate&lt;/a&gt; to create a free &amp;#34;Web API&amp;#34; email integration for cURL and its associated API key. Take note and save your key value and verify the integration. The key details will be needed when you create the Cloud Deploy approval process later in this blog. Note: Using SendGRID APIs DOES require you to create a user account.&lt;/p&gt;&lt;h3&gt;II. Configure Cloud Build&lt;/h3&gt;&lt;p&gt;This step requires integrating your git repository (from Pre-Requisites, Step 1) as a managed repository to GCP&#39;s cloud build service and creating the necessary Trigger. The goal of this integration is that any updates you make to your application within your GitHub repository will automatically kick off a Cloud Build deployment which will create, enable and deploy your application to GKE.&lt;/p&gt;&lt;p&gt;Create the GitHub Repository Integration for Cloud Build :&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;To start, from your GCP Console homepage, type &amp;#34;Cloud Build&amp;#34; within the search bar and select this service.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;From the left-hand panel, click on &amp;#34;Triggers&amp;#34;. And click on &amp;#34;Connect Repository.&amp;#34;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Select the source as &amp;#34;GitHub (Cloud Build GitHub App)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Authenticate the connection with your GitHub credentials, select the forked repository, and click &amp;#34;Connect&amp;#34;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Once the integration is done, you will see your newly added repository under &amp;#34;Triggers&amp;#34; -&amp;gt; &amp;#34;Manage Repositories.&amp;#34;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;"><p>f) Finally, create two Pub/Sub topics and Cloud Functions which will allow for email approvals for any GKE deployment to production and error reporting if a release fails.<br/>NOTE</p><ol><li>Before you run the script, please validate if your new GCP project already contains a &#34;default&#34; VPC and subnetwork. If you already have a &#34;default&#34; VPC, please go through the script and COMMENT out lines 53-55 which reference the creation of a default VPC and subnetwork. If you already have one, this step is not needed.<br/></li><li>By default, the creation of GKE clusters uses the &#34;default&#34; VPC subnetwork. If you prefer to use a non-default VPC, update the GKE cluster creation commands, starting at line 157, and update the <code>--subnetwork</code> value for all 3 GKE clusters.<br/></li></ol><p>6. To execute the script, run the following command: <code>sh /scripts/gcp_env_setup.sh</code></p><p>g) This script will approximately take 20-22 minutes to complete. Once finished, the output should look similar to something like <a href="https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/scripts/gcp_env_setup_OUTPUT.txt" target="_blank" track-type="inline link" track-name="15" track-metadata-eventdetail="https://github.com" track-metadata-module="post">this</a>.</p><p>7. Create a SendGRID API Key. Follow the instructions: <a href="https://app.sendgrid.com/guide/integrate" target="_blank" track-type="inline link" track-name="16" track-metadata-eventdetail="https://app.sendgrid.com" track-metadata-module="post">https://app.sendgrid.com/guide/integrate</a> to create a free &#34;Web API&#34; email integration for cURL and its associated API key. Take note and save your key value and verify the integration. The key details will be needed when you create the Cloud Deploy approval process later in this blog. Note: Using SendGRID APIs DOES require you to create a user account.</p><h3>II. Configure Cloud Build</h3><p>This step requires integrating your git repository (from Pre-Requisites, Step 1) as a managed repository to GCP&#39;s cloud build service and creating the necessary Trigger. The goal of this integration is that any updates you make to your application within your GitHub repository will automatically kick off a Cloud Build deployment which will create, enable and deploy your application to GKE.</p><p>Create the GitHub Repository Integration for Cloud Build :</p><ol><li><p>To start, from your GCP Console homepage, type &#34;Cloud Build&#34; within the search bar and select this service.</p></li><li><p>From the left-hand panel, click on &#34;Triggers&#34;. And click on &#34;Connect Repository.&#34;</p></li><li><p>Select the source as &#34;GitHub (Cloud Build GitHub App)</p></li><li><p>Authenticate the connection with your GitHub credentials, select the forked repository, and click &#34;Connect&#34;.</p></li><li><p>Once the integration is done, you will see your newly added repository under &#34;Triggers&#34; -&gt; &#34;Manage Repositories.&#34;</p></li></ol></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;Create a Trigger for Cloud Build&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;From the &amp;#34;Triggers&amp;#34; page, click on &amp;#34;+ Create Trigger&amp;#34;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Enter/Select the following values for the Trigger:&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Name: &lt;code&gt;CI/CD-blog-trigger&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Region: &lt;code&gt;us-central1&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Description: &lt;code&gt;Deploy Docker Image using GCP CI/CD cloud services.&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Event: &lt;code&gt;Push to a branch.&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Repository: Select your forked repository&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Branch: &lt;code&gt;^main$&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Configuration: &lt;code&gt;Cloud Build Configuration File (YAML or JSON)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Location: &lt;code&gt;Repository&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Build configuration file location: &lt;code&gt;/ cloudbuild.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Under &amp;#34;Advanced&amp;#34;, add the following TWO environment variables and their values: &lt;code&gt;_CONTAINER_REPO_NAME: test-repo _SEVERITY: CRITICAL&lt;/code&gt; &lt;br&gt;NOTE: The value of these env variables is case sensitive.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;"><p>Create a Trigger for Cloud Build</p><ol><li><p>From the &#34;Triggers&#34; page, click on &#34;+ Create Trigger&#34;</p></li><li><p>Enter/Select the following values for the Trigger:</p></li></ol><ul><li><p>Name: <code>CI/CD-blog-trigger</code></p></li><li><p>Region: <code>us-central1</code></p></li><li><p>Description: <code>Deploy Docker Image using GCP CI/CD cloud services.</code></p></li><li><p>Event: <code>Push to a branch.</code></p></li><li><p>Repository: Select your forked repository</p></li><li><p>Branch: <code>^main$</code></p></li><li><p>Configuration: <code>Cloud Build Configuration File (YAML or JSON)</code></p></li><li><p>Location: <code>Repository</code></p></li><li><p>Cloud Build configuration file location: <code>/ cloudbuild.yaml</code></p></li><li><p>Under &#34;Advanced&#34;, add the following TWO environment variables and their values: <code>_CONTAINER_REPO_NAME: test-repo _SEVERITY: CRITICAL</code> <br/>NOTE: The value of these env variables is case sensitive.</p></li></ul></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;3. After the environment values are entered/selected, click &amp;#34;Create&amp;#34;.&lt;/p&gt;&lt;p&gt;Once the Trigger is created, it will look like the following:&lt;/p&gt;"><p>3. After the environment values are entered/selected, click &#34;Create&#34;.</p><p>Once the Trigger is created, it will look like the following:</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h3&gt;III. Create Cloud Deploy Pipeline&lt;/h3&gt;&lt;p&gt;Now that we have created GitHub integration and Cloud Build Trigger, the next step is to create the Cloud Deploy pipeline. This will deploy the container image to the three GKE environments: &amp;#34;test,&amp;#34; &amp;#34;staging,&amp;#34; and &amp;#34;prod&amp;#34; once the image release for all three environments is created through Cloud Build. The requirement for image release requires a Cloud Deploy pipeline.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Edit the clouddeploy.yaml file with your GCP project ID.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Within the file, update lines 22, 32, and 42 with your respective GCP project ID &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;"><h3>III. Create Cloud Deploy Pipeline</h3><p>Now that we have created GitHub integration and Cloud Build Trigger, the next step is to create the Cloud Deploy pipeline. This will deploy the container image to the three GKE environments: &#34;test,&#34; &#34;staging,&#34; and &#34;prod&#34; once the image release for all three environments is created through Cloud Build. The requirement for image release requires a Cloud Deploy pipeline.</p><ol><li><p>Edit the clouddeploy.yaml file with your GCP project ID.</p></li><li><p>Within the file, update lines 22, 32, and 42 with your respective GCP project ID </p></li></ol></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;3. Once this is updated, save the file.&lt;/p&gt;4. Either through Cloud Shell or your local workstation, run the following GCP command to create the environment variables and the Cloud Deploy pipeline called &lt;code&gt;ci-cd-test&lt;/code&gt;:"><p>3. Once this is updated, save the file.</p><p>4. Either through Cloud Shell or your local workstation, run the following GCP command to create the environment variables and the Cloud Deploy pipeline called <code>ci-cd-test</code>:</p></div></paragraph-block></div><div><article-code-block _nghost-c61=""><pre _ngcontent-c61="">  <code _ngcontent-c61="">$ PROJECT_ID=&lt;&lt;YOUR_PROJECT_ID&gt;&gt;
</code><code _ngcontent-c61="">$ LOCATION=us-central1
</code><code _ngcontent-c61="">$ gcloud deploy apply --file clouddeploy.yaml --region=$LOCATION --project=$PROJECT_ID</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c60=""><p>NOTE: If you run into issues with a failed Cloud Deploy pipeline creation, delete the pipeline using the following gcloud command:</p></paragraph-block></div><div><article-code-block _nghost-c61=""><pre _ngcontent-c61="">  <code _ngcontent-c61="">gcloud deploy delivery-pipelines delete ci-cd-test --region=us-central1 --force</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c60=""><p>5. Once the pipeline is created, here is what the output will look like:</p></paragraph-block></div><div><article-code-block _nghost-c61=""><pre _ngcontent-c61="">  <code _ngcontent-c61="">$ gcloud deploy apply --file clouddeploy.yaml --region=$LOCATION --project=$PROJECT_ID
</code><code _ngcontent-c61="">Waiting for the operation on resource projects/&lt;&lt;YOUR_PROJECT_ID&gt;&gt;/locations/us-central1/deliveryPipelines/ci-cd-test...done.   
</code><code _ngcontent-c61="">Created Cloud Deploy resource: projects/&lt;&lt;YOUR_PROJECT_ID&gt;&gt;/locations/us-central1/deliveryPipelines/ci-cd-test.
</code><code _ngcontent-c61="">Waiting for the operation on resource projects/&lt;&lt;YOUR_PROJECT_ID&gt;&gt;/locations/us-central1/targets/test...done.   
</code><code _ngcontent-c61="">Created Cloud Deploy resource: projects/&lt;&lt;YOUR_PROJECT_ID&gt;&gt;/locations/us-central1/targets/test.
</code><code _ngcontent-c61="">Waiting for the operation on resource projects/&lt;&lt;YOUR_PROJECT_ID&gt;&gt;/locations/us-central1/targets/staging...done.   
</code><code _ngcontent-c61="">Created Cloud Deploy resource: projects/&lt;&lt;YOUR_PROJECT_ID&gt;&gt;/locations/us-central1/targets/staging.
</code><code _ngcontent-c61="">Waiting for the operation on resource projects/&lt;&lt;YOUR_PROJECT_ID&gt;&gt;/locations/us-central1/targets/prod...done.   
</code><code _ngcontent-c61="">Created Cloud Deploy resource: projects/&lt;&lt;YOUR_PROJECT_ID&gt;&gt;/locations/us-central1/targets/prod.</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c60=""><p>6. From your GCP Console homepage, type &#34;Cloud Deploy&#34; within the search bar and select this service. From the main page, you will see the newly created pipeline. </p></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h3&gt;IV. Configure email notifications for GKE production cluster deployment&lt;/h3&gt;&lt;p&gt;As part of a typical CI/CD process, any deployment of production workloads requires some form of approval process by DevOps engineers. Cloud Deploy allows you to inject an &#39;approval&#39; step before deploying a rollout to the next target. We have created this approval check in our pipeline before the deployment to the &#39;prod&#39; GKE cluster. Once the pipeline reaches the step to deploy the rollout to the &#39;prod&#39; GKE cluster, it emits a message in the &lt;code&gt;clouddeploy-approvals&lt;/code&gt; Pub/Sub topic. We have created a Cloud Function to listen to this topic and implement logic to send email notifications via Sendgrid. You can use any other library to send emails via Cloud Functions.&lt;/p&gt;&lt;p&gt;The one-time script has created a Pub/Sub topic and Cloud Function, allowing your cloud build release to send an approver email.&lt;/p&gt;&lt;p&gt;To validate that the Pub/Sub topics and Cloud Function was created, go to those respective services and ensure they were created.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;From your GCP Console homepage, type &amp;#34;Pub/Sub&amp;#34; within the search bar and select this service. There will be two Pub/Sub topics, and they&#39;re called &lt;code&gt;clouddeploy-approvals&lt;/code&gt; and &lt;code&gt;clouddeploy-operations&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;From your GCP Console homepage, type &amp;#34;Cloud Functions&amp;#34; within the search bar and select this service. There will be two Cloud Functions, called &lt;code&gt;cd-approval&lt;/code&gt; and &lt;code&gt;cd-deploy-notification&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click on &lt;code&gt;cd-approval&lt;/code&gt; and select &amp;#34;Variables&amp;#34;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click the &amp;#34;Edit&amp;#34; button and expand the `Runtime, build, connections and security settings.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Scroll down until you get to the &amp;#34;Runtime environment variables.&amp;#34; Here you will update the following three variables.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;"><h3>IV. Configure email notifications for GKE production cluster deployment</h3><p>As part of a typical CI/CD process, any deployment of production workloads requires some form of approval process by DevOps engineers. Cloud Deploy allows you to inject an &#39;approval&#39; step before deploying a rollout to the next target. We have created this approval check in our pipeline before the deployment to the &#39;prod&#39; GKE cluster. Once the pipeline reaches the step to deploy the rollout to the &#39;prod&#39; GKE cluster, it emits a message in the <code>clouddeploy-approvals</code> Pub/Sub topic. We have created a Cloud Function to listen to this topic and implement logic to send email notifications via Sendgrid. You can use any other library to send emails via Cloud Functions.</p><p>The one-time script has created a Pub/Sub topic and Cloud Function, allowing your cloud build release to send an approver email.</p><p>To validate that the Pub/Sub topics and Cloud Function was created, go to those respective services and ensure they were created.</p><ol><li><p>From your GCP Console homepage, type &#34;Pub/Sub&#34; within the search bar and select this service. There will be two Pub/Sub topics, and they&#39;re called <code>clouddeploy-approvals</code> and <code>clouddeploy-operations</code>.</p></li><li><p>From your GCP Console homepage, type &#34;Cloud Functions&#34; within the search bar and select this service. There will be two Cloud Functions, called <code>cd-approval</code> and <code>cd-deploy-notification</code>.</p></li><li><p>Click on <code>cd-approval</code> and select &#34;Variables&#34;.</p></li><li><p>Click the &#34;Edit&#34; button and expand the `Runtime, build, connections and security settings.</p></li><li><p>Scroll down until you get to the &#34;Runtime environment variables.&#34; Here you will update the following three variables.</p></li></ol></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;For &lt;code&gt;FROM_EMAIL&lt;/code&gt;, enter a secondary email account; it could be @gmail or any other domain of your choice. For &lt;code&gt;TO_EMAIL&lt;/code&gt;, select a primary email. For instance, the email of a DevOps Engineer who will be the approver of all production workload deployments to GKE. For &lt;code&gt;SENDGRID_API_KEY&lt;/code&gt;, you will enter your API Key, starting with &amp;#34;SG.&amp;#34;. If you haven&#39;t already, refer to the Prerequisites section above, step 6, around creating this key.&lt;/p&gt;&lt;p&gt;6. After you&#39;ve updated the cloud function environment variables, click &amp;#34;Next&amp;#34; and &amp;#34;Deploy&amp;#34; the updated function. It will take about 1-2 minutes. Once completed, the function will have a green check mark to validate its running.&lt;/p&gt;&lt;p&gt;7. Repeat steps 4-6 from above for the other cloud function of &lt;code&gt;cd-approval&lt;/code&gt;.&lt;/p&gt;&lt;h2&gt;Step-by-step instructions of testing and validating the GCP CI/CD pipeline&lt;/h2&gt;&lt;p&gt;Now that all the GCP prerequisites and environment setup is complete for Cloud Build, Cloud Deploy, and Email approvals, we&#39;ll next deploy the image to GKE and initiate the pipeline testing.&lt;/p&gt;&lt;p&gt;A couple of items to note during this test, we&#39;re going to show a &amp;#34;Happy&amp;#34; and &amp;#34;Vulnerable&amp;#34; Image deployment path to GKE.&lt;/p&gt;&lt;p&gt;The &amp;#34;Happy&amp;#34; path will show a successful deployment of the end-to-end pipeline across nine steps for a clean image deployment to GKE. &amp;#34;Clean&amp;#34; refers to the docker image with non-critical vulnerabilities. This path will also update the Binary Authorization policy that allows only the &amp;#34;Happy&amp;#34; image to be deployed to GKE&#39;s &amp;#34;test&amp;#34;, &amp;#34;staging&amp;#34;, and eventually &amp;#34;production&amp;#34; environments, which a DevOps engineer will approve.&lt;/p&gt;"><p>For <code>FROM_EMAIL</code>, enter a secondary email account; it could be @gmail or any other domain of your choice. For <code>TO_EMAIL</code>, select a primary email. For instance, the email of a DevOps Engineer who will be the approver of all production workload deployments to GKE. For <code>SENDGRID_API_KEY</code>, you will enter your API Key, starting with &#34;SG.&#34;. If you haven&#39;t already, refer to the Prerequisites section above, step 6, around creating this key.</p><p>6. After you&#39;ve updated the cloud function environment variables, click &#34;Next&#34; and &#34;Deploy&#34; the updated function. It will take about 1-2 minutes. Once completed, the function will have a green check mark to validate its running.</p><p>7. Repeat steps 4-6 from above for the other cloud function of <code>cd-approval</code>.</p><h2>Step-by-step instructions of testing and validating the GCP CI/CD pipeline</h2><p>Now that all the GCP prerequisites and environment setup is complete for Cloud Build, Cloud Deploy, and Email approvals, we&#39;ll next deploy the image to GKE and initiate the pipeline testing.</p><p>A couple of items to note during this test, we&#39;re going to show a &#34;Happy&#34; and &#34;Vulnerable&#34; Image deployment path to GKE.</p><p>The &#34;Happy&#34; path will show a successful deployment of the end-to-end pipeline across nine steps for a clean image deployment to GKE. &#34;Clean&#34; refers to the docker image with non-critical vulnerabilities. This path will also update the Binary Authorization policy that allows only the &#34;Happy&#34; image to be deployed to GKE&#39;s &#34;test&#34;, &#34;staging&#34;, and eventually &#34;production&#34; environments, which a DevOps engineer will approve.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;The &amp;#34;Vulnerable&amp;#34; docker path will show a failed deployment of the end-to-end pipeline across seven steps. The pipeline will fail in 2 of these steps because the image has:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Certain vulnerabilities must be addressed before the image can be stored in the Artifact Registry.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A failed deployment to GKE because this is a non-approved image without attestation, violating the updated Binary Authorization policy from the &amp;#34;Happy&amp;#34; path.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;When Binary Authorization is enabled, its default policy allows all images to be deployed to the GKE target environments without attestation. In the &amp;#34;Happy&amp;#34; path, we will update the default Binary Authorization policy where only a specific docker image is approved for deployment to GKE. GKE will reject any other image not approved by the Binary Authorization policy at the deployment time.&lt;/p&gt;&lt;p&gt;To allow other images to be deployed to GKE through an active binary authorization policy, update the following script &lt;code&gt;/scripts/create_binauthz_policy.sh&lt;/code&gt; where you can sign the image digest to the existing attestor and allow for that image deployment to GKE.&lt;/p&gt;&lt;p&gt;In the following sections, we&#39;ll go into further detail describing both paths of image deployment to GKE.&lt;/p&gt;&lt;h3&gt;I. Run Cloud Build configuration file for &amp;#34;Happy&amp;#34; path&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Ensure your GitHub repo is connected as a repository in Cloud Build. Refer to the &amp;#34;Create the GitHub Repository Integration for Cloud Build&amp;#34; section on how to do this.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ensure your Cloud Build trigger called &lt;code&gt;CI/CD-blog-trigger&lt;/code&gt; is created. Refer to the section &amp;#34;Create a Trigger for Cloud Build&amp;#34; on how to do this.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Since the Trigger is already enabled, any updates to your repository will trigger this Cloud Build deployment.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Open up the &lt;code&gt;cloudbuild.yaml&lt;/code&gt; from your GitHub repo. This is the cloud build configuration file for the &amp;#34;Happy&amp;#34; Docker path.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;To kick off the build, make any update to your codebase such as update the &lt;code&gt;/src/static/js&lt;/code&gt; file for any cosmetic change.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;After you&#39;ve made the change, push the changes to your GitHub repo.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;From the GCP Console, go to the Cloud Build service and click on &amp;#34;History&amp;#34;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Since the Trigger is enabled and integrated with your GitHub page, the build is automatically kicked off, and you can click the custom build number to see the log details. &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;"><p>The &#34;Vulnerable&#34; docker path will show a failed deployment of the end-to-end pipeline across seven steps. The pipeline will fail in 2 of these steps because the image has:</p><ul><li><p>Certain vulnerabilities must be addressed before the image can be stored in the Artifact Registry.</p></li><li><p>A failed deployment to GKE because this is a non-approved image without attestation, violating the updated Binary Authorization policy from the &#34;Happy&#34; path.</p></li></ul><p>When Binary Authorization is enabled, its default policy allows all images to be deployed to the GKE target environments without attestation. In the &#34;Happy&#34; path, we will update the default Binary Authorization policy where only a specific docker image is approved for deployment to GKE. GKE will reject any other image not approved by the Binary Authorization policy at the deployment time.</p><p>To allow other images to be deployed to GKE through an active binary authorization policy, update the following script <code>/scripts/create_binauthz_policy.sh</code> where you can sign the image digest to the existing attestor and allow for that image deployment to GKE.</p><p>In the following sections, we&#39;ll go into further detail describing both paths of image deployment to GKE.</p><h3>I. Run Cloud Build configuration file for &#34;Happy&#34; path</h3><ol><li><p>Ensure your GitHub repo is connected as a repository in Cloud Build. Refer to the &#34;Create the GitHub Repository Integration for Cloud Build&#34; section on how to do this.</p></li><li><p>Ensure your Cloud Build trigger called <code>CI/CD-blog-trigger</code> is created. Refer to the section &#34;Create a Trigger for Cloud Build&#34; on how to do this.</p></li><li><p>Since the Trigger is already enabled, any updates to your repository will trigger this Cloud Build deployment.</p></li><li><p>Open up the <code>cloudbuild.yaml</code> from your GitHub repo. This is the cloud build configuration file for the &#34;Happy&#34; Docker path.</p></li><li><p>To kick off the build, make any update to your codebase such as update the <code>/src/static/js</code> file for any cosmetic change.</p></li><li><p>After you&#39;ve made the change, push the changes to your GitHub repo.</p></li><li><p>From the GCP Console, go to the Cloud Build service and click on &#34;History&#34;.</p></li><li><p>Since the Trigger is enabled and integrated with your GitHub page, the build is automatically kicked off, and you can click the custom build number to see the log details. </p></li></ol></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h3&gt;II. Validate image deployment for &amp;#34;Happy&amp;#34; path&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Within that build, steps 7-9 highlight the image deployment to GKE through Cloud Deploy. If you click on step 9, the result of the build states that the deployment to &amp;#34;prod&amp;#34; is awaiting approval.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;"><h3>II. Validate image deployment for &#34;Happy&#34; path</h3><ol><li><p>Within that build, steps 7-9 highlight the image deployment to GKE through Cloud Deploy. If you click on step 9, the result of the build states that the deployment to &#34;prod&#34; is awaiting approval.</p></li></ol></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;2. Go to the Cloud Deploy homepage from the GCP Console and click on the &lt;code&gt;ci-cd-test&lt;/code&gt; pipeline.&lt;/p&gt;&lt;p&gt;3. Within the pipeline, click on the release associated with the latest cloud build deployment. Here you see that the &amp;#34;Happy&amp;#34; image is deployed successfully to both &amp;#34;test&amp;#34; and &amp;#34;staging&amp;#34;, but there&#39;s an approval process required for the &amp;#34;prod&amp;#34; cluster.&lt;/p&gt;"><p>2. Go to the Cloud Deploy homepage from the GCP Console and click on the <code>ci-cd-test</code> pipeline.</p><p>3. Within the pipeline, click on the release associated with the latest cloud build deployment. Here you see that the &#34;Happy&#34; image is deployed successfully to both &#34;test&#34; and &#34;staging&#34;, but there&#39;s an approval process required for the &#34;prod&#34; cluster.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>4. From the GCP Console, search for Kubernetes Engine; from the left-hand navigation, click on &#34;Workloads.&#34; Here you can see that the image deployment is successful in the two &#34;test&#34; and &#34;staging&#34; GKE environments. </p></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>5. Now that the deployment is queued for production, check your primary email and validate that you received a notification for approval. It will look something like this. </p></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;6. From the email, click the &lt;code&gt;here&lt;/code&gt; hyperlink and it will take you to the Cloud deploy pipeline page.&lt;/p&gt;&lt;p&gt;7. From the Pipeline page, approve or reject the release so the deployment can be pushed to &amp;#34;prod&amp;#34; in GKE. In this case, we will approve.&lt;/p&gt;"><p>6. From the email, click the <code>here</code> hyperlink and it will take you to the Cloud deploy pipeline page.</p><p>7. From the Pipeline page, approve or reject the release so the deployment can be pushed to &#34;prod&#34; in GKE. In this case, we will approve.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>7. If you go back to the Kubernetes workload page, you&#39;ll see that the image rollout to prod was successful. </p></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>In parallel, validate your Cloud Deploy, continuous deployment pipeline also confirms a successful rollout. </p></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h3&gt;III. Run Cloud Build configuration file for &amp;#34;Vulnerable&amp;#34; path (container image has vulnerabilities)&lt;/h3&gt;&lt;p&gt;We will show two failure paths with this deployment: image vulnerabilities and Binary Authorization policy enforcement.&lt;/p&gt;&lt;p&gt;A. First, failed deployment to push docker image to Artifact Registry because of severity-specific vulnerabilities -&lt;/p&gt;&lt;p&gt;1. Ensure your GitHub Repo is connected as a repository in Cloud Build. Refer to the &amp;#34;Create the GitHub Repository Integration for Cloud Build&amp;#34; section on how to do this.&lt;/p&gt;&lt;p&gt;2. Ensure your Cloud Build Trigger called &lt;code&gt;CI/CD-blog-trigger&lt;/code&gt; is created. Refer to the section &amp;#34;Create a Trigger for Cloud Build&amp;#34; on how to do this.&lt;/p&gt;&lt;p&gt;3. Since the Trigger is already enabled, any updates to your repository will trigger this cloud build deployment.&lt;/p&gt;&lt;p&gt;4. View the &lt;code&gt;cloudbuild-vulnerable.yaml&lt;/code&gt; file from your GitHub repo. This is the cloud build configuration file for the &amp;#34;Vulnerable&amp;#34; Docker path.&lt;/p&gt;&lt;p&gt;5. Edit the existing Trigger with the following:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Click on the ellipses next to &amp;#34;RUN&amp;#34; and update the &amp;#34;Cloud Build configuration file location&amp;#34; to be: &lt;code&gt;cloudbuild-vulnerable.yaml&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Update the &amp;#34;_SEVERITY&amp;#34; environment variable value to be &lt;code&gt;HIGH&lt;/code&gt;. We&#39;re changing the severity of the vulnerabilities because the vulnerability check will either PASS or FAIL a cloud build deployment if the image contains ANY &lt;code&gt;HIGH&lt;/code&gt; vulnerabilities.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Save the Trigger and validate its status as &amp;#34;Enabled&amp;#34;.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;6. To kick off the build, make any update to your codebase, such as updating the &lt;code&gt;/src/static/js&lt;/code&gt; file for any cosmetic change. After you&#39;ve made the change, push the changes to your GitHub repo.&lt;/p&gt;&lt;p&gt;7. From the GCP Console, go to the Cloud Build service and click on &amp;#34;History&amp;#34;.&lt;/p&gt;8. The build will fail in &lt;code&gt;Step 2: Check For Vulnerabilities within the Image&lt;/code&gt; because this image contains HIGH vulnerabilities, and cloud build will NOT push this image to be stored in the artifact registry."><h3>III. Run Cloud Build configuration file for &#34;Vulnerable&#34; path (container image has vulnerabilities)</h3><p>We will show two failure paths with this deployment: image vulnerabilities and Binary Authorization policy enforcement.</p><p>A. First, failed deployment to push docker image to Artifact Registry because of severity-specific vulnerabilities -</p><p>1. Ensure your GitHub Repo is connected as a repository in Cloud Build. Refer to the &#34;Create the GitHub Repository Integration for Cloud Build&#34; section on how to do this.</p><p>2. Ensure your Cloud Build Trigger called <code>CI/CD-blog-trigger</code> is created. Refer to the section &#34;Create a Trigger for Cloud Build&#34; on how to do this.</p><p>3. Since the Trigger is already enabled, any updates to your repository will trigger this cloud build deployment.</p><p>4. View the <code>cloudbuild-vulnerable.yaml</code> file from your GitHub repo. This is the cloud build configuration file for the &#34;Vulnerable&#34; Docker path.</p><p>5. Edit the existing Trigger with the following:</p><ul><li><p>Click on the ellipses next to &#34;RUN&#34; and update the &#34;Cloud Build configuration file location&#34; to be: <code>cloudbuild-vulnerable.yaml</code></p></li><li><p>Update the &#34;_SEVERITY&#34; environment variable value to be <code>HIGH</code>. We&#39;re changing the severity of the vulnerabilities because the vulnerability check will either PASS or FAIL a cloud build deployment if the image contains ANY <code>HIGH</code> vulnerabilities.</p></li><li><p>Save the Trigger and validate its status as &#34;Enabled&#34;.</p></li></ul><p>6. To kick off the build, make any update to your codebase, such as updating the <code>/src/static/js</code> file for any cosmetic change. After you&#39;ve made the change, push the changes to your GitHub repo.</p><p>7. From the GCP Console, go to the Cloud Build service and click on &#34;History&#34;.</p><p>8. The build will fail in <code>Step 2: Check For Vulnerabilities within the Image</code> because this image contains HIGH vulnerabilities, and cloud build will NOT push this image to be stored in the artifact registry.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;B. Second, a failed image deployment to GKE because of Binary Authorization policy enforcement -&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Go back to the Trigger configuration for this build and change the &amp;#34;_SEVERITY&amp;#34; environment variable value to &lt;code&gt;CRITICAL&lt;/code&gt; instead of &amp;#34;HIGH&amp;#34;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;To kick off the build, make any update to your codebase such as update the &lt;code&gt;/src/static/js&lt;/code&gt; file for any cosmetic change. After you&#39;ve made the change, push the changes to your GitHub repo.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;From the GCP Console, go to the Cloud Deploy pipeline &lt;code&gt;ci-cd-test&lt;/code&gt; and check the results of this latest release.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;From the Cloud Deploy pipeline page, approximately 10 minutes later, the build for &amp;#34;test&amp;#34; and &amp;#34;staging&amp;#34; will eventually fail because the Kubernetes manifest file for this docker image timed out. &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;"><p>B. Second, a failed image deployment to GKE because of Binary Authorization policy enforcement -</p><ol><li><p>Go back to the Trigger configuration for this build and change the &#34;_SEVERITY&#34; environment variable value to <code>CRITICAL</code> instead of &#34;HIGH&#34;.</p></li><li><p>To kick off the build, make any update to your codebase such as update the <code>/src/static/js</code> file for any cosmetic change. After you&#39;ve made the change, push the changes to your GitHub repo.</p></li><li><p>From the GCP Console, go to the Cloud Deploy pipeline <code>ci-cd-test</code> and check the results of this latest release.</p></li><li><p>From the Cloud Deploy pipeline page, approximately 10 minutes later, the build for &#34;test&#34; and &#34;staging&#34; will eventually fail because the Kubernetes manifest file for this docker image timed out. </p></li></ol></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;You can change the timeout period to be shorter; additional details can be found &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;p&gt;5. From the GCP Console, go to the GKE page and click on &amp;#34;Workloads&amp;#34;. Here you will see the image deployments to both the &amp;#34;test&amp;#34; and &amp;#34;staging&amp;#34; GKE environments failed. The reason being is binary authorization policy enforcement. The &amp;#34;vulnerable&amp;#34; docker image is not approved for deployment.&lt;/p&gt;"><p>You can change the timeout period to be shorter; additional details can be found <a href="https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout" track-type="inline link" track-name="17" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout" track-metadata-module="post">here</a></p><p>5. From the GCP Console, go to the GKE page and click on &#34;Workloads&#34;. Here you will see the image deployments to both the &#34;test&#34; and &#34;staging&#34; GKE environments failed. The reason being is binary authorization policy enforcement. The &#34;vulnerable&#34; docker image is not approved for deployment.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>6. In parallel to a failed deployment to any of the GKE staging environments, Cloud Function <code>cd-deploy-notification</code> will send the following email to check the logs for the pipeline. </p></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>7. From the email, click on <code>here to see deployment logs</code>, and it will take you to the log files within cloud build around additional details on the failure of the release rollout to GKE.</p></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h2&gt;Conclusion and further reading&lt;/h2&gt;&lt;p&gt;In this blog post, we built a secure CI/CD pipeline using Google Cloud&#39;s built-in services.&lt;/p&gt;&lt;p&gt;We learned how we can secure a CI/CD pipeline using Google Cloud&#39;s built-in services, such as Binary Authorization and Vulnerability scanning of the container images. We only saw one way to put some control on specific images that can be deployed to a GKE cluster. Binary Authorization also offers &lt;a href=&#34;https://cloud.google.com/binary-authorization/docs/overview#attestations&#34;&gt;Build Verification&lt;/a&gt;, in which Binary Authorization uses attestations to verify that an image was built by a specific build system or continuous integration (CI) pipeline such as Cloud Build.&lt;/p&gt;&lt;p&gt;Additionally, Binary Authorization also writes all the events where the deployment of a container image is blocked due to the constraints defined by the security policy to the audit logs. You can create alerts on these log entries and notify the appropriate team members about the blocked deployment events.&lt;/p&gt;&lt;p&gt;Lastly, all of the services used to build and secure the CI/CD pipelines are serverless, which makes it very easy to spin up the whole infrastructure within a few minutes without worrying about maintaining or managing it, so that your teams can focus on building and releasing software in a faster, reliable and cost efficient manner.&lt;/p&gt;"><h2>Conclusion and further reading</h2><p>In this blog post, we built a secure CI/CD pipeline using Google Cloud&#39;s built-in services.</p><p>We learned how we can secure a CI/CD pipeline using Google Cloud&#39;s built-in services, such as Binary Authorization and Vulnerability scanning of the container images. We only saw one way to put some control on specific images that can be deployed to a GKE cluster. Binary Authorization also offers <a href="https://cloud.google.com/binary-authorization/docs/overview#attestations" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/binary-authorization/docs/overview#attestations" track-metadata-module="post">Build Verification</a>, in which Binary Authorization uses attestations to verify that an image was built by a specific build system or continuous integration (CI) pipeline such as Cloud Build.</p><p>Additionally, Binary Authorization also writes all the events where the deployment of a container image is blocked due to the constraints defined by the security policy to the audit logs. You can create alerts on these log entries and notify the appropriate team members about the blocked deployment events.</p><p>Lastly, all of the services used to build and secure the CI/CD pipelines are serverless, which makes it very easy to spin up the whole infrastructure within a few minutes without worrying about maintaining or managing it, so that your teams can focus on building and releasing software in a faster, reliable and cost efficient manner.</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c59=""></article-up-1to3-block></section></div></article></div>]]></content:encoded>
      <author>&lt;name&gt;Nitin Vashishtha&lt;/name&gt;&lt;title&gt;Customer Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 13 Sep 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Kubernetes control plane metrics in GKE</title>
      <link>https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-control-plane-metrics-are-generally-available/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;An essential aspect of operating any application is the ability to observe the health and performance of that application and of the underlying infrastructure to quickly resolve issues as they arise. &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE) already provides &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging&#34;&gt;audit logs&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs&#34;&gt;operational logs&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics&#34;&gt;metrics&lt;/a&gt; along with &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/observing&#34;&gt;out-of-the-box dashboards&lt;/a&gt; and automatic &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/kubernetes-engine&#34;&gt;error reporting&lt;/a&gt; to facilitate running reliable applications at scale. Using these logs and metrics, &lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;Cloud Operations&lt;/a&gt; provides the alerts, monitoring dashboards and a &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Logs Explorer&lt;/a&gt; to quickly detect, troubleshoot and resolve issues.&lt;/p&gt;&lt;h3&gt;Introducing Kubernetes control plane metrics and why they matter&lt;/h3&gt;&lt;p&gt;In addition to these existing sources of telemetry data, we are excited to announce that we are now exposing &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#control-plane-metrics&#34;&gt;Kubernetes control plane metrics&lt;/a&gt;, which are now Generally Available. With GKE, Google fully manages the Kubernetes control plane; however, when troubleshooting issues it can be helpful to have access to certain metrics emitted by the Kubernetes control plane.&lt;/p&gt;&lt;p&gt;As part of our vision to make Kubernetes easier to use and easier to operate, these control plane metrics are directly integrated with Cloud Monitoring, so you don&#39;t need to manage any metric collection or scrape config.&lt;/p&gt;&lt;p&gt;For example, to understand the health of the API server, you can use metrics like &lt;code&gt;apiserver_request_total&lt;/code&gt; and &lt;code&gt;apiserver_request_duration_seconds&lt;/code&gt; to track the load that the API server is experiencing, the fraction of API server requests that return errors, and the response latency for requests received by the API server. Also, &lt;code&gt;apiserver_storage_objects&lt;/code&gt; can be very useful to monitor the saturation of the API server, especially if you’re using custom controllers. Breakdown this metric by the resource label to find out which Kubernetes custom resource or controller is problematic.&lt;/p&gt;&lt;p&gt;When a pod is created it is initially placed in a &#34;pending&#34; state, indicating it hasn&#39;t yet been scheduled on a node. In a healthy cluster, pending pods are relatively quickly scheduled on a node, providing the workload the resources it needs to run. However, a sustained increase in the number of pending pods may indicate a problem scheduling those pods, which may be caused by insufficient resources or inappropriate configuration. Metrics like &lt;code&gt;scheduler_pending_pods, scheduler_schedule_attempts_total&lt;/code&gt;, &lt;code&gt;scheduler_preemption_attempts_total, scheduler_preemption_victims&lt;/code&gt; , and &lt;code&gt;scheduler_scheduling_attempt_duration_seconds&lt;/code&gt; can alert you to potential scheduling issues, so you can act quickly to ensure sufficient resources are available for your pods. Using these metrics in combination will help you better understand the health of your cluster. For instance, if &lt;code&gt;scheduler_preemption_attempts_total&lt;/code&gt; goes up, it means that there are higher priority pods available to be scheduled and the Scheduler is preempting some running pods. However, if the value of &lt;code&gt;scheduler_pending_pods&lt;/code&gt; is also increasing, this may indicate that you don’t have enough resources to allocate the higher priority pods.&lt;/p&gt;&lt;br/&gt;&lt;p&gt;If the Kubernetes scheduler is still unable to find a suitable node for a pod, then the pod will eventually be marked as unschedulable. Kubernetes control plane metrics provide you visibility into pod scheduling errors and unschedulable pods. A spike in either means that the Kubernetes scheduler isn&#39;t able to find an appropriate node on which to run many of your pods, which may ultimately impair the performance of your application. In many cases, a high rate of unschedulable pods will not resolve itself until you take some action to address the underlying cause. A good first place to start troubleshooting the issue is to look for recent &lt;code&gt;FailedScheduling&lt;/code&gt; events. (If you have &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/installing#available-logs&#34;&gt;GKE system logs&lt;/a&gt; enabled, then all Kubernetes events are available in Cloud Logging.) These &lt;code&gt;FailedScheduling&lt;/code&gt; events include a message (for instance, &#34;0/6 nodes are available: 6 Insufficient cpu.&#34;) that very helpfully describes exactly why the pod wasn&#39;t able to be scheduled on any nodes, giving you guidance on how to address the problem.&lt;/p&gt;&lt;p&gt;A final example: If you see scheduling jobs is very slow, then one possible cause is that a third-party webhooks might be introducing significant latency, causing the API server to take a long time to schedule a job. Kubernetes control plane metrics such as &lt;code&gt;apiserver_admission_webhook_admission_duration_seconds&lt;/code&gt; can expose the admission webhook latency, helping you identify the root cause of slow job scheduling and mitigate the issue.&lt;/p&gt;&lt;h3&gt;Displayed in context&lt;/h3&gt;&lt;p&gt;Not only are we making these additional Kubernetes control plane metrics available, we&#39;re also excited to announce that all of these metrics are &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/view-observability-metrics&#34;&gt;displayed in the Kubernetes Engine section of the Cloud Console&lt;/a&gt;, making it easy to identify and investigate issues in-context as you&#39;re managing your GKE clusters.&lt;/p&gt;&lt;p&gt;To view these control plane metrics, go to the Kubernetes clusters section of the Cloud Console, select the &#34;Observability&#34; tab, and select &#34;Control plane&#34;:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;control_plane_metrics_screenshot.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/control_plane_metrics_screenshot.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Since all Kubernetes control plane metrics are ingested into Cloud Monitoring, you can &lt;a href=&#34;https://cloud.google.com/monitoring/alerts/using-alerting-ui&#34;&gt;create alerting policies in Cloud Alerting&lt;/a&gt; so you&#39;re notified as soon as something needs your attention.&lt;/p&gt;&lt;h3&gt;PromQL compatible&lt;/h3&gt;&lt;p&gt;When you enable Kubernetes control plane metrics for your GKE clusters, all metrics are collected using &lt;a href=&#34;https://g.co/cloud/managedprometheus&#34; target=&#34;_blank&#34;&gt;Google Cloud Managed Service for Prometheus&lt;/a&gt;. This means the metrics are sent to Cloud Monitoring in the same GCP project as your Kubernetes cluster and can be &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/query&#34;&gt;queried using PromQL&lt;/a&gt; via the Cloud Monitoring API and Metrics explorer.&lt;/p&gt;&lt;p&gt;For example, you can monitor any spikes in the 99th percentile API server response latency using this PromQL query:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;sum by (instance, verb) (histogram_quantile(0.99, rate(apiserver_request_duration_seconds_bucket{cluster=&#34;cluster-name&#34;}[5m])))&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e5ea65472d0&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Third-party support&lt;/h3&gt;&lt;p&gt;If you monitor your GKE cluster using popular third party observability tools, any third party observability tool can ingest these Kubernetes control plane metrics using the Cloud Monitoring API.&lt;/p&gt;&lt;p&gt;For example, if you&#39;re a Datadog customer and you&#39;ve enabled Kubernetes control plane metrics for your GKE cluster, then Datadog provides enhanced visualizations that include Kubernetes control plane metrics from the API server, scheduler, and controller manager.&lt;/p&gt;&lt;h3&gt;Pricing&lt;/h3&gt;&lt;p&gt;All Kubernetes control plane metrics are charged at the &lt;a href=&#34;https://cloud.devsite.corp.google.com/stackdriver/pricing#monitoring-pricing-summary&#34; target=&#34;_blank&#34;&gt;standard price for metrics&lt;/a&gt; ingested from Google Cloud Managed Service for Prometheus.&lt;/p&gt;&lt;h3&gt;Get started&lt;/h3&gt;&lt;p&gt;GKE clusters running control plane version 1.23.6 or later can now access metrics from the Kubernetes API server, Scheduler, and Controller Manager. Kubernetes control plane metrics are not available for GKE Autopilot clusters.&lt;/p&gt;&lt;p&gt;The following gcloud command will update a cluster to enable the collection of metrics from the API server, scheduler, and controller manager:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;gcloud container clusters update [CLUSTER_ID] \\\r\n --zone=[ZONE] \\\r\n --project=[PROJECT_ID] \\\r\n --monitoring=SYSTEM,API_SERVER,SCHEDULER,CONTROLLER_MANAGER&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e5ea53bf950&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Kubernetes control plane metrics can also be &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#enable-control-plane-metrics&#34;&gt;configured using Terraform&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Learn more about &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#configuring_collection_of_control_plane_metrics&#34;&gt;configuring the collection of control plane metrics&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Google Cloud Managed Service for Prometheus is now generally available&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Announcing the GA of Google Cloud Managed Service for Prometheus for the collection, storage, and querying of Kubernetes metrics.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c65=""><div _ngcontent-c65="" innerhtml="&lt;p&gt;An essential aspect of operating any application is the ability to observe the health and performance of that application and of the underlying infrastructure to quickly resolve issues as they arise. &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE) already provides &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging&#34;&gt;audit logs&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs&#34;&gt;operational logs&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics&#34;&gt;metrics&lt;/a&gt; along with &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/observing&#34;&gt;out-of-the-box dashboards&lt;/a&gt; and automatic &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/kubernetes-engine&#34;&gt;error reporting&lt;/a&gt; to facilitate running reliable applications at scale. Using these logs and metrics, &lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;Cloud Operations&lt;/a&gt; provides the alerts, monitoring dashboards and a &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Logs Explorer&lt;/a&gt; to quickly detect, troubleshoot and resolve issues.&lt;/p&gt;&lt;h3&gt;Introducing Kubernetes control plane metrics and why they matter&lt;/h3&gt;&lt;p&gt;In addition to these existing sources of telemetry data, we are excited to announce that we are now exposing &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#control-plane-metrics&#34;&gt;Kubernetes control plane metrics&lt;/a&gt;, which are now Generally Available. With GKE, Google fully manages the Kubernetes control plane; however, when troubleshooting issues it can be helpful to have access to certain metrics emitted by the Kubernetes control plane.&lt;/p&gt;&lt;p&gt;As part of our vision to make Kubernetes easier to use and easier to operate, these control plane metrics are directly integrated with Cloud Monitoring, so you don&#39;t need to manage any metric collection or scrape config.&lt;/p&gt;&lt;p&gt;For example, to understand the health of the API server, you can use metrics like &lt;code&gt;apiserver_request_total&lt;/code&gt; and &lt;code&gt;apiserver_request_duration_seconds&lt;/code&gt; to track the load that the API server is experiencing, the fraction of API server requests that return errors, and the response latency for requests received by the API server. Also, &lt;code&gt;apiserver_storage_objects&lt;/code&gt; can be very useful to monitor the saturation of the API server, especially if you&amp;#8217;re using custom controllers. Breakdown this metric by the resource label to find out which Kubernetes custom resource or controller is problematic.&lt;/p&gt;&lt;p&gt;When a pod is created it is initially placed in a &amp;#34;pending&amp;#34; state, indicating it hasn&#39;t yet been scheduled on a node. In a healthy cluster, pending pods are relatively quickly scheduled on a node, providing the workload the resources it needs to run. However, a sustained increase in the number of pending pods may indicate a problem scheduling those pods, which may be caused by insufficient resources or inappropriate configuration. Metrics like &lt;code&gt;scheduler_pending_pods, scheduler_schedule_attempts_total&lt;/code&gt;, &lt;code&gt;scheduler_preemption_attempts_total, scheduler_preemption_victims&lt;/code&gt; , and &lt;code&gt;scheduler_scheduling_attempt_duration_seconds&lt;/code&gt; can alert you to potential scheduling issues, so you can act quickly to ensure sufficient resources are available for your pods. Using these metrics in combination will help you better understand the health of your cluster. For instance, if &lt;code&gt;scheduler_preemption_attempts_total&lt;/code&gt; goes up, it means that there are higher priority pods available to be scheduled and the Scheduler is preempting some running pods. However, if the value of &lt;code&gt;scheduler_pending_pods&lt;/code&gt; is also increasing, this may indicate that you don&amp;#8217;t have enough resources to allocate the higher priority pods.&lt;/p&gt;&lt;br&gt;&lt;p&gt;If the Kubernetes scheduler is still unable to find a suitable node for a pod, then the pod will eventually be marked as unschedulable. Kubernetes control plane metrics provide you visibility into pod scheduling errors and unschedulable pods. A spike in either means that the Kubernetes scheduler isn&#39;t able to find an appropriate node on which to run many of your pods, which may ultimately impair the performance of your application. In many cases, a high rate of unschedulable pods will not resolve itself until you take some action to address the underlying cause. A good first place to start troubleshooting the issue is to look for recent &lt;code&gt;FailedScheduling&lt;/code&gt; events. (If you have &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/installing#available-logs&#34;&gt;GKE system logs&lt;/a&gt; enabled, then all Kubernetes events are available in Cloud Logging.) These &lt;code&gt;FailedScheduling&lt;/code&gt; events include a message (for instance, &amp;#34;0/6 nodes are available: 6 Insufficient cpu.&amp;#34;) that very helpfully describes exactly why the pod wasn&#39;t able to be scheduled on any nodes, giving you guidance on how to address the problem.&lt;/p&gt;&lt;p&gt;A final example: If you see scheduling jobs is very slow, then one possible cause is that a third-party webhooks might be introducing significant latency, causing the API server to take a long time to schedule a job. Kubernetes control plane metrics such as &lt;code&gt;apiserver_admission_webhook_admission_duration_seconds&lt;/code&gt; can expose the admission webhook latency, helping you identify the root cause of slow job scheduling and mitigate the issue.&lt;/p&gt;&lt;h3&gt;Displayed in context&lt;/h3&gt;&lt;p&gt;Not only are we making these additional Kubernetes control plane metrics available, we&#39;re also excited to announce that all of these metrics are &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/view-observability-metrics&#34;&gt;displayed in the Kubernetes Engine section of the Cloud Console&lt;/a&gt;, making it easy to identify and investigate issues in-context as you&#39;re managing your GKE clusters.&lt;/p&gt;&lt;p&gt;To view these control plane metrics, go to the Kubernetes clusters section of the Cloud Console, select the &amp;#34;Observability&amp;#34; tab, and select &amp;#34;Control plane&amp;#34;:&lt;/p&gt;"><p>An essential aspect of operating any application is the ability to observe the health and performance of that application and of the underlying infrastructure to quickly resolve issues as they arise. <a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">Google Kubernetes Engine</a> (GKE) already provides <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging" track-metadata-module="post">audit logs</a>, <a href="https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs" track-metadata-module="post">operational logs</a>, and <a href="https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics" track-metadata-module="post">metrics</a> along with <a href="https://cloud.google.com/stackdriver/docs/solutions/gke/observing" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/solutions/gke/observing" track-metadata-module="post">out-of-the-box dashboards</a> and automatic <a href="https://cloud.google.com/error-reporting/docs/setup/kubernetes-engine" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/setup/kubernetes-engine" track-metadata-module="post">error reporting</a> to facilitate running reliable applications at scale. Using these logs and metrics, <a href="https://cloud.google.com/products/operations" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/products/operations" track-metadata-module="post">Cloud Operations</a> provides the alerts, monitoring dashboards and a <a href="https://cloud.google.com/logging/docs/view/logs-explorer-interface" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/logs-explorer-interface" track-metadata-module="post">Logs Explorer</a> to quickly detect, troubleshoot and resolve issues.</p><h3>Introducing Kubernetes control plane metrics and why they matter</h3><p>In addition to these existing sources of telemetry data, we are excited to announce that we are now exposing <a href="https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#control-plane-metrics" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#control-plane-metrics" track-metadata-module="post">Kubernetes control plane metrics</a>, which are now Generally Available. With GKE, Google fully manages the Kubernetes control plane; however, when troubleshooting issues it can be helpful to have access to certain metrics emitted by the Kubernetes control plane.</p><p>As part of our vision to make Kubernetes easier to use and easier to operate, these control plane metrics are directly integrated with Cloud Monitoring, so you don&#39;t need to manage any metric collection or scrape config.</p><p>For example, to understand the health of the API server, you can use metrics like <code>apiserver_request_total</code> and <code>apiserver_request_duration_seconds</code> to track the load that the API server is experiencing, the fraction of API server requests that return errors, and the response latency for requests received by the API server. Also, <code>apiserver_storage_objects</code> can be very useful to monitor the saturation of the API server, especially if you’re using custom controllers. Breakdown this metric by the resource label to find out which Kubernetes custom resource or controller is problematic.</p><p>When a pod is created it is initially placed in a &#34;pending&#34; state, indicating it hasn&#39;t yet been scheduled on a node. In a healthy cluster, pending pods are relatively quickly scheduled on a node, providing the workload the resources it needs to run. However, a sustained increase in the number of pending pods may indicate a problem scheduling those pods, which may be caused by insufficient resources or inappropriate configuration. Metrics like <code>scheduler_pending_pods, scheduler_schedule_attempts_total</code>, <code>scheduler_preemption_attempts_total, scheduler_preemption_victims</code> , and <code>scheduler_scheduling_attempt_duration_seconds</code> can alert you to potential scheduling issues, so you can act quickly to ensure sufficient resources are available for your pods. Using these metrics in combination will help you better understand the health of your cluster. For instance, if <code>scheduler_preemption_attempts_total</code> goes up, it means that there are higher priority pods available to be scheduled and the Scheduler is preempting some running pods. However, if the value of <code>scheduler_pending_pods</code> is also increasing, this may indicate that you don’t have enough resources to allocate the higher priority pods.</p><p>If the Kubernetes scheduler is still unable to find a suitable node for a pod, then the pod will eventually be marked as unschedulable. Kubernetes control plane metrics provide you visibility into pod scheduling errors and unschedulable pods. A spike in either means that the Kubernetes scheduler isn&#39;t able to find an appropriate node on which to run many of your pods, which may ultimately impair the performance of your application. In many cases, a high rate of unschedulable pods will not resolve itself until you take some action to address the underlying cause. A good first place to start troubleshooting the issue is to look for recent <code>FailedScheduling</code> events. (If you have <a href="https://cloud.google.com/stackdriver/docs/solutions/gke/installing#available-logs" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/solutions/gke/installing#available-logs" track-metadata-module="post">GKE system logs</a> enabled, then all Kubernetes events are available in Cloud Logging.) These <code>FailedScheduling</code> events include a message (for instance, &#34;0/6 nodes are available: 6 Insufficient cpu.&#34;) that very helpfully describes exactly why the pod wasn&#39;t able to be scheduled on any nodes, giving you guidance on how to address the problem.</p><p>A final example: If you see scheduling jobs is very slow, then one possible cause is that a third-party webhooks might be introducing significant latency, causing the API server to take a long time to schedule a job. Kubernetes control plane metrics such as <code>apiserver_admission_webhook_admission_duration_seconds</code> can expose the admission webhook latency, helping you identify the root cause of slow job scheduling and mitigate the issue.</p><h3>Displayed in context</h3><p>Not only are we making these additional Kubernetes control plane metrics available, we&#39;re also excited to announce that all of these metrics are <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/view-observability-metrics" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine/docs/how-to/view-observability-metrics" track-metadata-module="post">displayed in the Kubernetes Engine section of the Cloud Console</a>, making it easy to identify and investigate issues in-context as you&#39;re managing your GKE clusters.</p><p>To view these control plane metrics, go to the Kubernetes clusters section of the Cloud Console, select the &#34;Observability&#34; tab, and select &#34;Control plane&#34;:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Nathan Beach&lt;/name&gt;&lt;title&gt;Group Product Manager, Google Kubernetes Engine&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 08 Sep 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Announcing public availability of Google Cloud Certificate Manager</title>
      <link>https://cloud.google.com/blog/products/identity-security/introducing-general-availability-of-google-cloud-certificate-manager/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Today we are pleased to announce that &lt;a href=&#34;https://cloud.google.com/certificate-manager/docs/overview&#34;&gt;Cloud Certificate Manager&lt;/a&gt; is now in general availability. Cloud Certificate Manager enables our users to acquire, manage, and deploy public Transport Layer Security (TLS) certificates at scale for use with your Google Cloud workloads. TLS certificates are required to secure browser connections and transactions. &lt;/p&gt;&lt;p&gt;Cloud Certificate Manager supports both self-managed and Google-managed certificates, as well as wildcard certificates, and has monitoring capabilities to alert for expiring certificates. &lt;/p&gt;&lt;h3&gt;Scale to support as many domains as you need&lt;/h3&gt;&lt;p&gt;Since our public preview announcement supporting the &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/simplify-saas-scale-tls-certificate-management&#34;&gt;SaaS use cases&lt;/a&gt;, we have scaled the solution to serve millions of managed domains. &lt;a href=&#34;https://il.linkedin.com/in/alonkochba&#34; target=&#34;_blank&#34;&gt;Alon Kochba&lt;/a&gt;, head of web performance at &lt;a href=&#34;https://www.wix.com/&#34; target=&#34;_blank&#34;&gt;Wix&lt;/a&gt;, shared how Certificate Manager’s scale and performance helped them lighten their workload.&lt;/p&gt;&lt;p&gt;“As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. Google Cloud&#39;s Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to deploy our own custom solution for terminating SSL,” Kochba said. &lt;/p&gt;&lt;h3&gt;Streamline your migrations&lt;/h3&gt;&lt;p&gt;You can now deploy a new certificate globally in minutes and greatly simplify and accelerate the deployment of TLS for SaaS offerings. Coupled with support for &lt;a href=&#34;https://cloud.google.com/certificate-manager/docs/dns-authorizations#:~:text=When%20you%20create%20a%20DNS,the%20steps%20in%20this%20section.&#34;&gt;DNS Authorizations&lt;/a&gt;, you can now streamline your workload migrations without major disruptions. &lt;a href=&#34;https://www.linkedin.com/in/jameshartig&#34; target=&#34;_blank&#34;&gt;James Hartig&lt;/a&gt;, co-founder of &lt;a href=&#34;https://www.getadmiral.com/&#34; target=&#34;_blank&#34;&gt;GetAdmiral.com&lt;/a&gt;, shared this with Google after the migration experience.&lt;/p&gt;&lt;p&gt;“I just wanted to say thank you so much for the release of Certificate Manager and its support for SaaS use cases. We just completed our migration to using Google to terminate TLS and everything went really smoothly and we couldn&#39;t be happier.” &lt;/p&gt;&lt;h3&gt;Automate with Kubernetes &amp;amp; self-service ACME certificate enrollment&lt;/h3&gt;&lt;p&gt;We have further introduced a number of automation and observability features including:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-security#certificate-manager&#34;&gt;Kubernetes integration&lt;/a&gt; in public preview with Cloud Certificate Manager&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api&#34;&gt;Self-service ACME certificate enrollment&lt;/a&gt;, now in public preview&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The ability to track Certificate Manager usage in the billing dashboard&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We also have started work on incorporating &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/certificate_manager_certificate&#34; target=&#34;_blank&#34;&gt;Terraform automation&lt;/a&gt; with Cloud Certificate Manager, which will simplify your workload automation.&lt;/p&gt;&lt;p&gt;During the certificate manager private preview of the ACME &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api&#34;&gt;certificate enrollment capability&lt;/a&gt;, our users have acquired millions of certificates for their self-managed TLS deployments. Each of these certificates comes from &lt;a href=&#34;http://pki.goog&#34; target=&#34;_blank&#34;&gt;Google Trust Services&lt;/a&gt;, which means our users get the same TLS device compatibility and scalability we demand for our own services. Our Cloud users get this benefit even when they manage the certificate and private key themselves–all for free. &lt;/p&gt;&lt;p&gt;We look forward to you using &lt;a href=&#34;https://cloud.google.com/certificate-manager/docs/overview&#34;&gt;Certificate Manager&lt;/a&gt; and these new capabilities to improve the reliability of your services and help encourage further adoption of TLS.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/identity-security/how-google-cloud-blocked-largest-layer-7-ddos-attack-at-46-million-rps/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_security.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;How Google Cloud blocked the largest Layer 7 DDoS attack at 46 million rps&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;By anticipating a DDOS attack, a Google Cloud customer was able to stop it before it took down their site. They just weren’t expecting it...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;p&gt;Today we are pleased to announce that &lt;a href=&#34;https://cloud.google.com/certificate-manager/docs/overview&#34;&gt;Cloud Certificate Manager&lt;/a&gt; is now in general availability. Cloud Certificate Manager enables our users to acquire, manage, and deploy public Transport Layer Security (TLS) certificates at scale for use with your Google Cloud workloads. TLS certificates are required to secure browser connections and transactions.&amp;#160;&lt;/p&gt;&lt;p&gt;Cloud Certificate Manager supports both self-managed and Google-managed certificates, as well as wildcard certificates, and has monitoring capabilities to alert for expiring certificates.&amp;#160;&lt;/p&gt;&lt;h3&gt;Scale to support as many domains as you need&lt;/h3&gt;&lt;p&gt;Since our public preview announcement supporting the &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/simplify-saas-scale-tls-certificate-management&#34;&gt;SaaS use cases&lt;/a&gt;, we have scaled the solution to serve millions of managed domains. &lt;a href=&#34;https://il.linkedin.com/in/alonkochba&#34; target=&#34;_blank&#34;&gt;Alon Kochba&lt;/a&gt;, head of web performance at &lt;a href=&#34;https://www.wix.com/&#34; target=&#34;_blank&#34;&gt;Wix&lt;/a&gt;, shared how Certificate Manager&amp;#8217;s scale and performance helped them lighten their workload.&lt;/p&gt;&lt;p&gt;&amp;#8220;As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. Google Cloud&#39;s Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to deploy our own custom solution for terminating SSL,&amp;#8221; Kochba said.&amp;#160;&lt;/p&gt;&lt;h3&gt;Streamline your migrations&lt;/h3&gt;&lt;p&gt;You can now deploy a new certificate globally in minutes and greatly simplify and accelerate the deployment of TLS for SaaS offerings. Coupled with support for &lt;a href=&#34;https://cloud.google.com/certificate-manager/docs/dns-authorizations#:~:text=When%20you%20create%20a%20DNS,the%20steps%20in%20this%20section.&#34;&gt;DNS Authorizations&lt;/a&gt;, you can now streamline your workload migrations without major disruptions. &lt;a href=&#34;https://www.linkedin.com/in/jameshartig&#34; target=&#34;_blank&#34;&gt;James Hartig&lt;/a&gt;, co-founder of &lt;a href=&#34;https://www.getadmiral.com/&#34; target=&#34;_blank&#34;&gt;GetAdmiral.com&lt;/a&gt;, shared this with Google after the migration experience.&lt;/p&gt;&lt;p&gt;&amp;#8220;I just wanted to say thank you so much for the release of Certificate Manager and its support for SaaS use cases. We just completed our migration to using Google to terminate TLS and everything went really smoothly and we couldn&#39;t be happier.&amp;#8221;&amp;#160;&lt;/p&gt;&lt;h3&gt;Automate with Kubernetes &amp;amp; self-service ACME certificate enrollment&lt;/h3&gt;&lt;p&gt;We have further introduced a number of automation and observability features including:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-security#certificate-manager&#34;&gt;Kubernetes integration&lt;/a&gt; in public preview with Cloud Certificate Manager&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api&#34;&gt;Self-service ACME certificate enrollment&lt;/a&gt;, now in public preview&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The ability to track Certificate Manager usage in the billing dashboard&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We also have started work on incorporating &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/certificate_manager_certificate&#34; target=&#34;_blank&#34;&gt;Terraform automation&lt;/a&gt; with Cloud Certificate Manager, which will simplify your workload automation.&lt;/p&gt;&lt;p&gt;During the certificate manager private preview of the ACME &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api&#34;&gt;certificate enrollment capability&lt;/a&gt;, our users have acquired millions of certificates for their self-managed TLS deployments. Each of these certificates comes from &lt;a href=&#34;http://pki.goog&#34; target=&#34;_blank&#34;&gt;Google Trust Services&lt;/a&gt;, which means our users get the same TLS device compatibility and scalability we demand for our own services. Our Cloud users get this benefit even when they manage the certificate and private key themselves&amp;#8211;all for free.&amp;#160;&lt;/p&gt;&lt;p&gt;We look forward to you using &lt;a href=&#34;https://cloud.google.com/certificate-manager/docs/overview&#34;&gt;Certificate Manager&lt;/a&gt; and these new capabilities to improve the reliability of your services and help encourage further adoption of TLS.&lt;/p&gt;"><p>Today we are pleased to announce that <a href="https://cloud.google.com/certificate-manager/docs/overview" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/certificate-manager/docs/overview" track-metadata-module="post">Cloud Certificate Manager</a> is now in general availability. Cloud Certificate Manager enables our users to acquire, manage, and deploy public Transport Layer Security (TLS) certificates at scale for use with your Google Cloud workloads. TLS certificates are required to secure browser connections and transactions. </p><p>Cloud Certificate Manager supports both self-managed and Google-managed certificates, as well as wildcard certificates, and has monitoring capabilities to alert for expiring certificates. </p><h3>Scale to support as many domains as you need</h3><p>Since our public preview announcement supporting the <a href="https://cloud.google.com/blog/products/identity-security/simplify-saas-scale-tls-certificate-management" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/blog/products/identity-security/simplify-saas-scale-tls-certificate-management" track-metadata-module="post">SaaS use cases</a>, we have scaled the solution to serve millions of managed domains. <a href="https://il.linkedin.com/in/alonkochba" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://il.linkedin.com" track-metadata-module="post">Alon Kochba</a>, head of web performance at <a href="https://www.wix.com/" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://www.wix.com" track-metadata-module="post">Wix</a>, shared how Certificate Manager’s scale and performance helped them lighten their workload.</p><p>“As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. Google Cloud&#39;s Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to deploy our own custom solution for terminating SSL,” Kochba said. </p><h3>Streamline your migrations</h3><p>You can now deploy a new certificate globally in minutes and greatly simplify and accelerate the deployment of TLS for SaaS offerings. Coupled with support for <a href="https://cloud.google.com/certificate-manager/docs/dns-authorizations#:~:text=When%20you%20create%20a%20DNS,the%20steps%20in%20this%20section." track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/certificate-manager/docs/dns-authorizations#:~:text=When%20you%20create%20a%20DNS,the%20steps%20in%20this%20section." track-metadata-module="post">DNS Authorizations</a>, you can now streamline your workload migrations without major disruptions. <a href="https://www.linkedin.com/in/jameshartig" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://www.linkedin.com" track-metadata-module="post">James Hartig</a>, co-founder of <a href="https://www.getadmiral.com/" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://www.getadmiral.com" track-metadata-module="post">GetAdmiral.com</a>, shared this with Google after the migration experience.</p><p>“I just wanted to say thank you so much for the release of Certificate Manager and its support for SaaS use cases. We just completed our migration to using Google to terminate TLS and everything went really smoothly and we couldn&#39;t be happier.” </p><h3>Automate with Kubernetes &amp; self-service ACME certificate enrollment</h3><p>We have further introduced a number of automation and observability features including:</p><ul><li><p><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-security#certificate-manager" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-security#certificate-manager" track-metadata-module="post">Kubernetes integration</a> in public preview with Cloud Certificate Manager</p></li></ul><ul><li><p><a href="https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api" track-metadata-module="post">Self-service ACME certificate enrollment</a>, now in public preview</p></li><li><p>The ability to track Certificate Manager usage in the billing dashboard</p></li></ul><p>We also have started work on incorporating <a href="https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/certificate_manager_certificate" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://registry.terraform.io" track-metadata-module="post">Terraform automation</a> with Cloud Certificate Manager, which will simplify your workload automation.</p><p>During the certificate manager private preview of the ACME <a href="https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api" track-metadata-module="post">certificate enrollment capability</a>, our users have acquired millions of certificates for their self-managed TLS deployments. Each of these certificates comes from <a href="http://pki.goog" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="http://pki.goog" track-metadata-module="post">Google Trust Services</a>, which means our users get the same TLS device compatibility and scalability we demand for our own services. Our Cloud users get this benefit even when they manage the certificate and private key themselves–all for free. </p><p>We look forward to you using <a href="https://cloud.google.com/certificate-manager/docs/overview" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/certificate-manager/docs/overview" track-metadata-module="post">Certificate Manager</a> and these new capabilities to improve the reliability of your services and help encourage further adoption of TLS.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Babi Seal&lt;/name&gt;&lt;title&gt;Product Manager, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 24 Aug 2022 19:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Accelerate your developer productivity with Query Library</title>
      <link>https://cloud.google.com/blog/products/devops-sre/increase-developer-productivity-with-query-library/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Our goal in &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt; is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you’re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical. &lt;/p&gt;&lt;p&gt;That’s why we recently launched a &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; and other &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging&#34;&gt;new features&lt;/a&gt; to make querying your logs even easier. The &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; in Cloud Logging makes it easier to find logs faster by using common queries.&lt;/p&gt;&lt;h3&gt;Build queries faster with our templates&lt;/h3&gt;&lt;p&gt;The new &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#search-text&#34;&gt;text search&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus&#34;&gt;drop-down&lt;/a&gt; features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logging-query-language&#34;&gt;Logging query language&lt;/a&gt; necessary for you. The Query Library extends this simplicity with templates for common GCP queries.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Query Library.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you’ll notice the following details:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query categories&lt;/b&gt; – Each query is broken down into categories that can be used to easily narrow down to relevant queries. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query occurrences&lt;/b&gt; – To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query details&lt;/b&gt; – Each query has a description along with the Logging query &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Run/Stream&lt;/b&gt; – Run the query or start streaming logs right from the library&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Save&lt;/b&gt; – Save the query in your list of saved queries&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 Query Library.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;The road ahead&lt;/h3&gt;&lt;p&gt;We’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Logs Explorer&lt;/a&gt; and join the discussion in our &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Cloud Operations page&lt;/a&gt; on the Google Cloud Community site.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-deploy-gets-continuous-delivery-productivity-enhancements/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header_nEzKg5F.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Google Cloud Deploy gets continuous delivery productivity enhancements&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;In this latest release, Google Cloud Deploy got improved onboarding, delivery pipeline management and additional enterprise features.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-content-stream-block><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;Our goal in &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt; is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you&amp;#8217;re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical.&amp;#160;&lt;/p&gt;&lt;p&gt;That&amp;#8217;s why we recently launched a &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; and other &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging&#34;&gt;new features&lt;/a&gt; to make querying your logs even easier. The &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; in Cloud Logging makes it easier to find logs faster by using common queries.&lt;/p&gt;&lt;h3&gt;Build queries faster with our templates&lt;/h3&gt;&lt;p&gt;The new &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#search-text&#34;&gt;text search&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus&#34;&gt;drop-down&lt;/a&gt; features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logging-query-language&#34;&gt;Logging query language&lt;/a&gt; necessary for you. The Query Library extends this simplicity with templates for common GCP queries.&lt;/p&gt;"><p>Our goal in <a href="https://cloud.google.com/logging" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/logging" track-metadata-module="post">Cloud Logging</a> is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you’re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical. </p><p>That’s why we recently launched a <a href="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-metadata-module="post">Query Library</a> and other <a href="https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging" track-metadata-module="post">new features</a> to make querying your logs even easier. The <a href="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-metadata-module="post">Query Library</a> in Cloud Logging makes it easier to find logs faster by using common queries.</p><h3>Build queries faster with our templates</h3><p>The new <a href="https://cloud.google.com/logging/docs/view/building-queries#search-text" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/building-queries#search-text" track-metadata-module="post">text search</a> and <a href="https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus" track-metadata-module="post">drop-down</a> features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the <a href="https://cloud.google.com/logging/docs/view/logging-query-language" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/logging-query-language" track-metadata-module="post">Logging query language</a> necessary for you. The Query Library extends this simplicity with templates for common GCP queries.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you&amp;#8217;ll notice the following details:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query categories&lt;/b&gt; &amp;#8211;&amp;#160;Each query is broken down into categories that can be used to easily narrow down to relevant queries.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query occurrences&lt;/b&gt; &amp;#8211;&amp;#160;To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query details&lt;/b&gt; &amp;#8211;&amp;#160;Each query has a description along with the Logging query&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Run/Stream&lt;/b&gt; &amp;#8211;&amp;#160;Run the query or start streaming logs right from the library&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Save&lt;/b&gt; &amp;#8211;&amp;#160;Save the query in your list of saved queries&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;"><p>The <a href="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-metadata-module="post">Query Library</a> is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you’ll notice the following details:</p><ul><li><p><b>Query categories</b> – Each query is broken down into categories that can be used to easily narrow down to relevant queries. </p></li><li><p><b>Query occurrences</b> – To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project. </p></li><li><p><b>Query details</b> – Each query has a description along with the Logging query </p></li><li><p><b>Run/Stream</b> – Run the query or start streaming logs right from the library</p></li><li><p><b>Save</b> – Save the query in your list of saved queries</p></li></ul></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h3&gt;The road ahead&lt;/h3&gt;&lt;p&gt;We&amp;#8217;re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven&amp;#8217;t already, get started with the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Logs Explorer&lt;/a&gt; and join the discussion in our &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Cloud Operations page&lt;/a&gt; on the Google Cloud Community site.&lt;/p&gt;"><h3>The road ahead</h3><p>We’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the <a href="https://cloud.google.com/logging/docs/view/logs-explorer-interface" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/logs-explorer-interface" track-metadata-module="post">Logs Explorer</a> and join the discussion in our <a href="https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">Cloud Operations page</a> on the Google Cloud Community site.</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Charles Baer&lt;/name&gt;&lt;title&gt;Product Manager, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 11 Aug 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Snooze your alert policies in Cloud Monitoring</title>
      <link>https://cloud.google.com/blog/products/devops-sre/snooze-your-alert-policies-cloud-monitoring/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Does your development team want to snooze alerts during non-business hours? Or proactively prevent the creation of expected alerts for an upcoming expected maintenance window? Cloud Alerting in Google&#39;s Cloud operations suite now supports the ability to snooze alert policies for a given period of time. You can create a Snooze by providing specific alert policies and a time period. During this window, if the alert policy is violated, no incidents or notifications are created. When the window ends, the alerting behavior resumes as normal. &lt;/p&gt;&lt;p&gt;Your team can use this feature in a variety of ways. One example is to avoid being paged for non-production environments over the weekend. Another way is to plan for a known maintenance window or cutover period. You can also quiet the noise during a growing outage, among other approaches. &lt;/p&gt;&lt;p&gt;To create a Snooze, go to Monitoring &amp;gt;  Alerting. See the new table with Snoozes and click on Create Snooze. You provide the name of the Snooze, time period, and select the desired Alert Policies. After you select the criteria, a table lists recent Incidents that match this criteria. Events like those won&#39;t cause an alert when the snooze is active.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Snooze_alerts.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Snooze alerts.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Snooze_alerts.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;You will  see a timeline visualization of all past, active, and upcoming Snoozes. If you’d like to adjust the duration, you can go back and edit the details. For more information, please see the documentation.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Snooze_alerts.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 Snooze alerts.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Snooze_alerts.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In the future, we’ll expand this functionality to allow snoozing by labels. You’ll be able to temporarily silence by the resource, system, metric, and custom labels which will allow you to snooze all alert policies in a specific environment, zone, or team. This functionality will be extended to be supported in the API, allowing you to create Snoozes programmatically for regularly repeating events.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Add severity levels to your alert policies in Cloud Monitoring&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Add static and dynamic severity levels to your alert policies for easier triaging and include these in notifications when sent to 3rd par...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;p&gt;Does your development team want to snooze alerts during non-business hours? Or proactively prevent the creation of expected alerts for an upcoming expected maintenance window? Cloud Alerting in Google&#39;s Cloud operations suite now supports the ability to snooze alert policies for a given period of time. You can create a Snooze by providing specific alert policies and a time period. During this window, if the alert policy is violated, no incidents or notifications are created. When the window ends, the alerting behavior resumes as normal.&amp;#160;&lt;/p&gt;&lt;p&gt;Your team can use this feature in a variety of ways. One example is to avoid being paged for non-production environments over the weekend. Another way is to plan for a known maintenance window or cutover period. You can also quiet the noise during a growing outage, among other approaches.&amp;#160;&lt;/p&gt;&lt;p&gt;To create a Snooze, go to Monitoring &amp;gt;&amp;#160; Alerting. See the new table with Snoozes and click on Create Snooze. You provide the name of the Snooze, time period, and select the desired Alert Policies. After you select the criteria, a table lists recent Incidents that match this criteria. Events like those won&#39;t cause an alert when the snooze is active.&lt;/p&gt;"><p>Does your development team want to snooze alerts during non-business hours? Or proactively prevent the creation of expected alerts for an upcoming expected maintenance window? Cloud Alerting in Google&#39;s Cloud operations suite now supports the ability to snooze alert policies for a given period of time. You can create a Snooze by providing specific alert policies and a time period. During this window, if the alert policy is violated, no incidents or notifications are created. When the window ends, the alerting behavior resumes as normal. </p><p>Your team can use this feature in a variety of ways. One example is to avoid being paged for non-production environments over the weekend. Another way is to plan for a known maintenance window or cutover period. You can also quiet the noise during a growing outage, among other approaches. </p><p>To create a Snooze, go to Monitoring &gt;  Alerting. See the new table with Snoozes and click on Create Snooze. You provide the name of the Snooze, time period, and select the desired Alert Policies. After you select the criteria, a table lists recent Incidents that match this criteria. Events like those won&#39;t cause an alert when the snooze is active.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Alisa Goldstein&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 11 Aug 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Accelerate your developer productivity with Query Library</title>
      <link>https://cloud.google.com/blog/products/devops-sre/increase-developer-productivity-with-query-library/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Our goal in &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt; is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you’re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical. &lt;/p&gt;&lt;p&gt;That’s why we recently launched a &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; and other &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging&#34;&gt;new features&lt;/a&gt; to make querying your logs even easier. The &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; in Cloud Logging makes it easier to find logs faster by using common queries.&lt;/p&gt;&lt;h3&gt;Build queries faster with our templates&lt;/h3&gt;&lt;p&gt;The new &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#search-text&#34;&gt;text search&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus&#34;&gt;drop-down&lt;/a&gt; features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logging-query-language&#34;&gt;Logging query language&lt;/a&gt; necessary for you. The Query Library extends this simplicity with templates for common GCP queries.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Query Library.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you’ll notice the following details:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query categories&lt;/b&gt; – Each query is broken down into categories that can be used to easily narrow down to relevant queries. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query occurrences&lt;/b&gt; – To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query details&lt;/b&gt; – Each query has a description along with the Logging query &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Run/Stream&lt;/b&gt; – Run the query or start streaming logs right from the library&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Save&lt;/b&gt; – Save the query in your list of saved queries&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 Query Library.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;The road ahead&lt;/h3&gt;&lt;p&gt;We’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Logs Explorer&lt;/a&gt; and join the discussion in our &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Cloud Operations page&lt;/a&gt; on the Google Cloud Community site.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-deploy-gets-continuous-delivery-productivity-enhancements/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header_nEzKg5F.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Google Cloud Deploy gets continuous delivery productivity enhancements&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;In this latest release, Google Cloud Deploy got improved onboarding, delivery pipeline management and additional enterprise features.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-content-stream-block><div><paragraph-block _nghost-c59=""><div _ngcontent-c59="" innerhtml="&lt;p&gt;Our goal in &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt; is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you&amp;#8217;re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical.&amp;#160;&lt;/p&gt;&lt;p&gt;That&amp;#8217;s why we recently launched a &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; and other &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging&#34;&gt;new features&lt;/a&gt; to make querying your logs even easier. The &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; in Cloud Logging makes it easier to find logs faster by using common queries.&lt;/p&gt;&lt;h3&gt;Build queries faster with our templates&lt;/h3&gt;&lt;p&gt;The new &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#search-text&#34;&gt;text search&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus&#34;&gt;drop-down&lt;/a&gt; features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logging-query-language&#34;&gt;Logging query language&lt;/a&gt; necessary for you. The Query Library extends this simplicity with templates for common GCP queries.&lt;/p&gt;"><p>Our goal in <a href="https://cloud.google.com/logging" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/logging" track-metadata-module="post">Cloud Logging</a> is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you’re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical. </p><p>That’s why we recently launched a <a href="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-metadata-module="post">Query Library</a> and other <a href="https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging" track-metadata-module="post">new features</a> to make querying your logs even easier. The <a href="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-metadata-module="post">Query Library</a> in Cloud Logging makes it easier to find logs faster by using common queries.</p><h3>Build queries faster with our templates</h3><p>The new <a href="https://cloud.google.com/logging/docs/view/building-queries#search-text" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/building-queries#search-text" track-metadata-module="post">text search</a> and <a href="https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus" track-metadata-module="post">drop-down</a> features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the <a href="https://cloud.google.com/logging/docs/view/logging-query-language" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/logging-query-language" track-metadata-module="post">Logging query language</a> necessary for you. The Query Library extends this simplicity with templates for common GCP queries.</p></div></paragraph-block></div><div><paragraph-block _nghost-c59=""><div _ngcontent-c59="" innerhtml="&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you&amp;#8217;ll notice the following details:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query categories&lt;/b&gt; &amp;#8211;&amp;#160;Each query is broken down into categories that can be used to easily narrow down to relevant queries.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query occurrences&lt;/b&gt; &amp;#8211;&amp;#160;To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query details&lt;/b&gt; &amp;#8211;&amp;#160;Each query has a description along with the Logging query&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Run/Stream&lt;/b&gt; &amp;#8211;&amp;#160;Run the query or start streaming logs right from the library&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Save&lt;/b&gt; &amp;#8211;&amp;#160;Save the query in your list of saved queries&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;"><p>The <a href="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/building-queries#library-queries" track-metadata-module="post">Query Library</a> is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you’ll notice the following details:</p><ul><li><p><b>Query categories</b> – Each query is broken down into categories that can be used to easily narrow down to relevant queries. </p></li><li><p><b>Query occurrences</b> – To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project. </p></li><li><p><b>Query details</b> – Each query has a description along with the Logging query </p></li><li><p><b>Run/Stream</b> – Run the query or start streaming logs right from the library</p></li><li><p><b>Save</b> – Save the query in your list of saved queries</p></li></ul></div></paragraph-block></div><div><paragraph-block _nghost-c59=""><div _ngcontent-c59="" innerhtml="&lt;h3&gt;The road ahead&lt;/h3&gt;&lt;p&gt;We&amp;#8217;re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven&amp;#8217;t already, get started with the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Logs Explorer&lt;/a&gt; and join the discussion in our &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Cloud Operations page&lt;/a&gt; on the Google Cloud Community site.&lt;/p&gt;"><h3>The road ahead</h3><p>We’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the <a href="https://cloud.google.com/logging/docs/view/logs-explorer-interface" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/logs-explorer-interface" track-metadata-module="post">Logs Explorer</a> and join the discussion in our <a href="https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">Cloud Operations page</a> on the Google Cloud Community site.</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Charles Baer&lt;/name&gt;&lt;title&gt;Product Manager, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 11 Aug 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Snooze your alert policies in Cloud Monitoring</title>
      <link>https://cloud.google.com/blog/products/devops-sre/snooze-your-alert-policies-cloud-monitoring/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Does your development team want to snooze alerts during non-business hours? Or proactively prevent the creation of expected alerts for an upcoming expected maintenance window? Cloud Alerting in Google&#39;s Cloud operations suite now supports the ability to snooze alert policies for a given period of time. You can create a Snooze by providing specific alert policies and a time period. During this window, if the alert policy is violated, no incidents or notifications are created. When the window ends, the alerting behavior resumes as normal. &lt;/p&gt;&lt;p&gt;Your team can use this feature in a variety of ways. One example is to avoid being paged for non-production environments over the weekend. Another way is to plan for a known maintenance window or cutover period. You can also quiet the noise during a growing outage, among other approaches. &lt;/p&gt;&lt;p&gt;To create a Snooze, go to Monitoring &amp;gt;  Alerting. See the new table with Snoozes and click on Create Snooze. You provide the name of the Snooze, time period, and select the desired Alert Policies. After you select the criteria, a table lists recent Incidents that match this criteria. Events like those won&#39;t cause an alert when the snooze is active.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Snooze_alerts.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Snooze alerts.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Snooze_alerts.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;You will  see a timeline visualization of all past, active, and upcoming Snoozes. If you’d like to adjust the duration, you can go back and edit the details. For more information, please see the documentation.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Snooze_alerts.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 Snooze alerts.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Snooze_alerts.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In the future, we’ll expand this functionality to allow snoozing by labels. You’ll be able to temporarily silence by the resource, system, metric, and custom labels which will allow you to snooze all alert policies in a specific environment, zone, or team. This functionality will be extended to be supported in the API, allowing you to create Snoozes programmatically for regularly repeating events.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Add severity levels to your alert policies in Cloud Monitoring&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Add static and dynamic severity levels to your alert policies for easier triaging and include these in notifications when sent to 3rd par...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Alisa Goldstein&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 11 Aug 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Accelerate your developer productivity with Query Library</title>
      <link>https://cloud.google.com/blog/products/devops-sre/increase-developer-productivity-with-query-library/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Our goal in &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt; is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you’re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical. &lt;/p&gt;&lt;p&gt;That’s why we recently launched a &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; and other &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging&#34;&gt;new features&lt;/a&gt; to make querying your logs even easier. The &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; in Cloud Logging makes it easier to find logs faster by using common queries.&lt;/p&gt;&lt;h3&gt;Build queries faster with our templates&lt;/h3&gt;&lt;p&gt;The new &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#search-text&#34;&gt;text search&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus&#34;&gt;drop-down&lt;/a&gt; features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logging-query-language&#34;&gt;Logging query language&lt;/a&gt; necessary for you. The Query Library extends this simplicity with templates for common GCP queries.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Query Library.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries&#34;&gt;Query Library&lt;/a&gt; is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you’ll notice the following details:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query categories&lt;/b&gt; – Each query is broken down into categories that can be used to easily narrow down to relevant queries. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query occurrences&lt;/b&gt; – To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Query details&lt;/b&gt; – Each query has a description along with the Logging query &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Run/Stream&lt;/b&gt; – Run the query or start streaming logs right from the library&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Save&lt;/b&gt; – Save the query in your list of saved queries&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 Query Library.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;The road ahead&lt;/h3&gt;&lt;p&gt;We’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Logs Explorer&lt;/a&gt; and join the discussion in our &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Cloud Operations page&lt;/a&gt; on the Google Cloud Community site.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-deploy-gets-continuous-delivery-productivity-enhancements/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header_nEzKg5F.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Google Cloud Deploy gets continuous delivery productivity enhancements&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;In this latest release, Google Cloud Deploy got improved onboarding, delivery pipeline management and additional enterprise features.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Charles Baer&lt;/name&gt;&lt;title&gt;Product Manager, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 11 Aug 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Google Cloud Deploy gets continuous delivery productivity enhancements</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-gets-continuous-delivery-productivity-enhancements/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Since &lt;a href=&#34;http://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt; became &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-now-ga&#34;&gt;generally available&lt;/a&gt; in January 2022, we’ve remained focused on our core mission: making it easier to establish and operate software continuous delivery to a &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; environment. &lt;/p&gt;&lt;p&gt;Through ongoing conversations with developers, DevOps engineers, and business decision makers alike, we’ve received feedback about onboarding speed, delivery pipeline management, and expanding enterprise features.Today, we are pleased to introduce numerous feature additions to Google Cloud Deploy in these areas. &lt;/p&gt;&lt;h3&gt;Faster onboarding&lt;/h3&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/skaffold&#34;&gt;Skaffold&lt;/a&gt; is an open source tool that orchestrates continuous development, continuous integration (CI), and continuous delivery (CD), and it’s &lt;a href=&#34;https://cloud.google.com/deploy/docs/using-skaffold&#34;&gt;integral to Google Cloud Deploy&lt;/a&gt;. Through Skaffold and Google Cloud Deploy, the &lt;a href=&#34;https://cloud.google.com/deploy/docs/using-skaffold/getting-started-skaffold#using_skaffold_for_local_development&#34;&gt;local application development loop&lt;/a&gt; is &lt;a href=&#34;https://cloud.google.com/deploy/docs/using-skaffold/getting-started-skaffold#using_skaffold_for_cicd&#34;&gt;seamlessly connected&lt;/a&gt; to a continuous delivery capability, bringing consistency to your end-to-end software delivery lifecycle tooling. &lt;/p&gt;&lt;p&gt;This may be the first time your team is using Skaffold. To help, Google Cloud Deploy can now &lt;a href=&#34;https://cloud.google.com/deploy/docs/using-skaffold/getting-started-skaffold#have_generate_your_skaffoldyaml&#34;&gt;generate a Skaffold configuration&lt;/a&gt; for single manifest applications when one is not present. &lt;/p&gt;&lt;p&gt;When you create a &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#release&#34;&gt;release&lt;/a&gt;, the new ‘gcloud deploy releases create … &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/deploy/releases/create#--from-k8s-manifest&#34;&gt;--from-k8s-manifest&lt;/a&gt;‘ command provides an application manifest, and generates a Skaffold configuration. This lets your application development teams and continuous delivery operators familiarize themselves with Google Cloud Deploy, reducing early-stage configuration and learning friction as they establish their continuous delivery capabilities. &lt;/p&gt;&lt;p&gt;When you use this option, you can review the generated Skaffold configuration, and as your comfort with Skaffold configuration and Google Cloud Deploy increases, you can develop your own Skaffold configurations tailored to your specific delivery pipeline needs.&lt;/p&gt;&lt;h3&gt;Delivery pipeline management&lt;/h3&gt;&lt;p&gt;Continuous &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;delivery pipelines&lt;/a&gt; are always in use. New releases navigate a progression sequence as they make their way out to the production &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#target&#34;&gt;target&lt;/a&gt;. The journey, however, isn’t always smooth. In that case, you may need to manage your delivery pipeline and related resources more discretely. &lt;/p&gt;&lt;p&gt;With the addition of &lt;a href=&#34;https://cloud.google.com/deploy/docs/suspend-pipeline&#34;&gt;delivery pipeline suspension&lt;/a&gt;, you can now temporarily pause problematic delivery pipelines to restrict all release and rollout activity. By pausing the activity, you can undertake an investigation to identify problems and their root cause. &lt;/p&gt;&lt;p&gt;Sometimes it isn’t the delivery pipeline that has a problem, but rather a release. Through &lt;a href=&#34;https://cloud.google.com/deploy/docs/abandon-release&#34;&gt;release abandonment&lt;/a&gt;, you can prohibit application releases that have a feature defect, outdated library, or other identified issues from being deployed further. Release abandonment ensures an undesired release won’t be used again, while keeping it available for issue review and troubleshooting.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/suspended_pipeline_blog.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;suspended_pipeline_blog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/suspended_pipeline_blog.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;A suspended delivery pipeline and abandoned releases&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;When reviewing or troubleshooting release application manifest issues, you may want to compare application manifests between releases and target environments to determine when an application configuration changed and why. But comparing applications manifests can be hard, requiring you to use the command line to locate and diff multiple files.&lt;/p&gt;&lt;p&gt;To help, Google Cloud Deploy now has a &lt;a href=&#34;https://cloud.google.com/deploy/docs/view-release#viewing_release_artifacts&#34;&gt;Release inspector&lt;/a&gt;, which makes it easy to review application manifests and compare against releases and targets within a delivery pipeline.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;release-inspector.gif&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/release-inspector.gif&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Reviewing and comparing application manifests with the Release Inspector&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#rollout&#34;&gt;Rollout&lt;/a&gt; listings within the Google Cloud Deploy console have, to date, been limited to a specific release or target. A complete delivery pipeline rollout listing (and filtering) has been a standing request, and you can now find it on the delivery pipeline details page.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/rollouts_tab_blog.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;rollouts_tab_blog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/rollouts_tab_blog.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Delivery pipeline details now with complete Rollouts listing&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Finally, &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution environments&lt;/a&gt; are an important part of configuring custom render and deploy environments. In addition to the ability to specify custom worker pools, Cloud Storage buckets, and service accounts, we’ve added an &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files#executionconfigs&#34;&gt;execution timeout&lt;/a&gt; to better support long-running deployments. &lt;/p&gt;&lt;h3&gt;Expanded enterprise features&lt;/h3&gt;&lt;p&gt;Enterprise environments frequently have numerous requirements to be able to operate, such as security controls, logging, Terraform support, and regional availability.&lt;/p&gt;&lt;p&gt;In a &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-now-ga&#34;&gt;previous blog post&lt;/a&gt;, we announced support for VPC Security Controls (VPC-SC) in Preview. We are pleased to announce that &lt;a href=&#34;https://cloud.google.com/vpc-service-controls/docs/supported-products#table_deploy&#34;&gt;Google Cloud Deploy VPC-SC&lt;/a&gt; is now generally available. We’ve also &lt;a href=&#34;https://cloud.google.com/deploy/docs/securing/cmek&#34;&gt;documented&lt;/a&gt; how you can configure customer managed encryption keys (CMEK) with services that depend on Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;There are also times when reviewing manifest-render and application deployment logs may not be sufficient for troubleshooting. For these situations, we’ve added Google Cloud Deploy service &lt;a href=&#34;https://cloud.google.com/deploy/docs/platform-logs&#34;&gt;platform logs&lt;/a&gt;, which may provide additional details towards issue resolution.&lt;/p&gt;&lt;p&gt;Terraform plays an important role in deploying Google Cloud resources. You can now deploy Google Cloud Deploy &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/clouddeploy_delivery_pipeline&#34; target=&#34;_blank&#34;&gt;delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/clouddeploy_target&#34; target=&#34;_blank&#34;&gt;target&lt;/a&gt; resources using Google Cloud Platform’s &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs&#34; target=&#34;_blank&#34;&gt;Terraform provider&lt;/a&gt;. With this, you can now deploy Google Cloud Deploy resources as part of a broader Google Cloud Platform resource deployment.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/regions&#34;&gt;Regional availability&lt;/a&gt; is important for businesses that need a regional service presence. Google Cloud Deploy is now available in an additional nine &lt;a href=&#34;https://cloud.google.com/about/locations&#34;&gt;regions&lt;/a&gt;, bringing the total number of Google Cloud Deploy worldwide regions to 15.&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software delivery capability, and it’s our hope that Google Cloud Deploy will help you implement complete CI/CD pipelines. And we’re just getting started. Stay tuned as we introduce exciting new capabilities and features to Google Cloud Deploy in the months to come. &lt;/p&gt;&lt;p&gt;In the meantime, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploy-app-gke&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-deploy-now-ga/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Google Cloud Deploy, now GA, makes it easier to do continuous delivery to GKE&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Google Cloud Deploy managed service, now GA, makes it easier to do continuous delivery to Google Kubernetes Engine&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;S. Bogdan&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/blog_post_header_nEzKg5F.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 10 Aug 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Managing the Looker ecosystem at scale with SRE and DevOps practices</title>
      <link>https://cloud.google.com/blog/products/devops-sre/using-devops-and-sre-principles-to-manage-looker/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Many organizations struggle to create data-driven cultures where each employee is empowered to make decisions based on data. This is especially true for enterprises with a variety of systems and tools in use across different teams. If you are a leader, manager, or executive focused on how your team can leverage Google&#39;s SRE practices or wider DevOps practices, definitely you are in the right place!&lt;/p&gt;&lt;h3&gt;What do today’s enterprises or mature start-ups look like?&lt;/h3&gt;&lt;p&gt;Today large organizations are often segmented into hundreds of small teams which are often working around data in the magnitude of several petabytes and in a wide variety of raw forms. ‘Working around data’ could mean any of the following: generating, facilitating, consuming, processing, visualizing or feeding back into the system. Due to a wide variety of responsibilities, the skill sets also vary to a large extent. Numerous people and teams work with data, with jobs that span the entire data ecosystem:&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Centralizing data from raw sources and systems&lt;/li&gt;&lt;li&gt;Maintaining and transforming data in a warehouse&lt;/li&gt;&lt;li&gt;Managing access controls and permissions for the data&lt;/li&gt;&lt;li&gt;Modeling data&lt;/li&gt;&lt;li&gt;Doing ad-hoc data analysis and exploration&lt;/li&gt;&lt;li&gt;Building visualizations and reports&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Nevertheless, a common goal across all these teams is keeping services running and downstream customers happy. In other words, the organization might be divided internally, however, they all have the mission to leverage the data to make better business decisions. Hence, despite silos and different subgoals, destiny for all these teams is intertwined for the organization to thrive. To support such a diverse set of data sources and the teams supporting them, Looker supports over &lt;a href=&#34;https://docs.looker.com/setup-and-management/database-config&#34; target=&#34;_blank&#34;&gt;60 dialects&lt;/a&gt; (input from a data source) and over &lt;a href=&#34;https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/delivering-looks-explores#delivery_options_for_third-party_integrations&#34; target=&#34;_blank&#34;&gt;35 destinations&lt;/a&gt; (output to a new data source).&lt;/p&gt;&lt;p&gt;Below is a simplified* picture of how the Looker ecosystem is central to a data-rich organization.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Simplified.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Simplified.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Simplified.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Simplified* Looker ecosystem in a data-rich environment&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;*The picture hides the complexity of team(s) accountable for each data source. It also hides how a data source may have dependencies on other sources. &lt;a href=&#34;https://marketplace.looker.com/&#34; target=&#34;_blank&#34;&gt;Looker Marketplace&lt;/a&gt; can also play an important role in your ecosystem.&lt;/p&gt;&lt;h3&gt;What role can DevOps and SRE practices play?&lt;/h3&gt;&lt;p&gt;In the most ideal state, all these teams will be in harmony as a single-threaded organization with all the internal processes so smooth that everyone is empowered to experiment (i.e. fail, learn, iterate and repeat all the time). With increasing organizational complexities, it is incredibly challenging to achieve such a state because there will be overhead and misaligned priorities. This is where we look up to the guiding principles of DevOps and SRE practices. In case you are not familiar with Google SRE practices, &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering&#34;&gt;here&lt;/a&gt; is a starting point. The core of DevOps and SRE practices are mature communication and collaboration practices. &lt;/p&gt;&lt;p&gt;Let’s focus on the best practices which could help us with our Looker ecosystem.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Have joint goals&lt;/b&gt;. There should be some goals which are a shared responsibility across two or more teams. This helps establish a culture of psychological safety and transparency across teams.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Visualize how the data flows across the organization&lt;/b&gt;. This enables an understanding how each team plays their role and how to work with them better.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Agree on the&lt;/b&gt;&lt;a href=&#34;https://sre.google/sre-book/monitoring-distributed-systems/&#34; target=&#34;_blank&#34;&gt;&lt;b&gt;Golden Signals&lt;/b&gt;&lt;/a&gt; &lt;b&gt;(aka core metrics)&lt;/b&gt;. These could mean data freshness, data accuracy, latency on centralized dashboards etc. These signals allow teams to set their &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons&#34;&gt;error budgets&lt;/a&gt; and &lt;a href=&#34;https://sre.google/sre-book/service-level-objectives/&#34; target=&#34;_blank&#34;&gt;SLIs&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Agree on communication and collaboration methods that work across teams&lt;/b&gt;. &lt;/p&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Regular bidirectional communication modes - have shared &lt;a href=&#34;https://support.google.com/chat/answer/7659784?hl=en&#34; target=&#34;_blank&#34;&gt;Google Chat spaces&lt;/a&gt;/&lt;a href=&#34;https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling-slack&#34; target=&#34;_blank&#34;&gt;slack channels&lt;/a&gt;. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Focus on artifacts such as jointly owned documentations pages, shared roadmap items, reusable tooling, etc. For example, &lt;a href=&#34;https://docs.looker.com/admin-options/system-activity/sa-dashboards&#34; target=&#34;_blank&#34;&gt;System Activity Dashboards&lt;/a&gt; could be made available to all the relevant stakeholders and supplemented with notes tailored to your organization.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Set up regular forums where commonly discussed agenda items include major changes, expected downtime and postmortems around the core metrics. Among other agenda items, you could define/refine a common set of standards, for example centrally defined &lt;a href=&#34;https://docs.looker.com/reference/field-params/label-for-field&#34; target=&#34;_blank&#34;&gt;labels&lt;/a&gt;, &lt;a href=&#34;https://docs.looker.com/reference/field-params/group_label&#34; target=&#34;_blank&#34;&gt;group_labels&lt;/a&gt;, &lt;a href=&#34;https://docs.looker.com/reference/field-params/description&#34; target=&#34;_blank&#34;&gt;descriptions&lt;/a&gt;, etc. in the LookML to ensure there is a single terminology across the board.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Promote informal sharing opportunities such as lessons learned, &lt;a href=&#34;https://www.thinkwithgoogle.com/future-of-marketing/management-and-culture/passion-not-perks/&#34; target=&#34;_blank&#34;&gt;TGIFs&lt;/a&gt;, Brown bag sessions, and shadowing opportunities. Learning and teaching have an immense impact on how teams evolve. Teams often become closer with side projects that are slightly outside of their usual day-to-day duties.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Have mutually agreed upon change management practices&lt;/b&gt;. Each team has dependencies so making changes may have an impact on other teams. Why not plan those changes systematically? For example, getting common standards across the &lt;a href=&#34;https://docs.looker.com/data-modeling/getting-started/advanced-deploy-mode&#34; target=&#34;_blank&#34;&gt;Advance deploy mode&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Promote continuous improvements&lt;/b&gt;. Keep looking for better, faster, cost-optimized versions of something important to the teams.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Revisit your data flow&lt;/b&gt;. After every major reorganization, ensure that organizational change has not broken the established mechanisms.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-pull_quote&#34;&gt;&lt;div class=&#34;uni-pull-quote h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;div class=&#34;uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;div class=&#34;uni-pull-quote__inner-wrapper h-c-copy h-c-copy&#34;&gt;&lt;q class=&#34;uni-pull-quote__text&#34;&gt;despite silos and different subgoals, destiny for all these teams is intertwined for the organization to thrive.&lt;/q&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Are you over-engineering?&lt;/h3&gt;&lt;p&gt;There is a possibility that in the process of maturing the ecosystem, we may end up in an overly engineered system - we may unintentionally add &lt;a href=&#34;https://landing.google.com/sre/sre-book/chapters/eliminating-toil/&#34; target=&#34;_blank&#34;&gt;toil&lt;/a&gt; to the environment. These are examples of toil that often stem from communication gaps. &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Meetings with no outcomes/action plans - This one is among the most common forms of toil, where the original intention of a meeting is no longer valid but the forum has not taken efforts to revisit their decision.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Unnecessary approvals - Being a single threaded team can often create unnecessary dependencies and your teams may lose the ability to make changes.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Unaligned maintenance windows - Changes across multiple teams may not be mutually exclusive hence if there is misalignment then it may create unforeseen impacts on the end user.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Fancy, but unnecessary tooling - Side projects, if not governed, may create unnecessary tooling which is not being used by the business. Collaborations are great when they solve real business problems, hence it is also required to refocus if the priorities are set right.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Gray areas - When you have a shared responsibility model, you also may end up in gray areas which are often gaps with no owner. This can lead to increased complexity in the long run. For example, having the flexibility to &lt;a href=&#34;https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling&#34; target=&#34;_blank&#34;&gt;schedule content delivery&lt;/a&gt; still requires collaboration to reduce &lt;a href=&#34;https://docs.looker.com/admin-options/scheduler/history&#34; target=&#34;_blank&#34;&gt;jobs with failures&lt;/a&gt; because it can impact the performance of your Looker instance.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Contradicting metrics - You may want to pay special attention to how teams are rewarded for internal metrics. For example, if a team focuses on accuracy of data and other one on freshness then at scale they may not align with one another.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;To summarize, we learned how data is handled in large organizations with Looker at its heart unifying a universal semantic model. To handle large amounts of diverse data, teams need to start with aligned goals and commit to strong collaboration. We also learned how DevOps and SRE practices can guide us navigate through these complexities. Lastly, we looked at some side effects of excessively structured systems. To go forward from here, it is highly recommended to start with an analysis of how data flows under your scope and how mature the collaboration is across multiple teams.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Further reading and resources&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/data-analytics/business-intelligence-for-cloud-data-with-looker&#34;&gt;Getting to know Looker – common use cases&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1&#34;&gt;Enterprise DevOps Guidebook&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons&#34;&gt;Know thy enemy: how to prioritize and communicate risks—CRE life lessons&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.oreilly.com/content/how-to-get-started-with-site-reliability-engineering-sre/&#34; target=&#34;_blank&#34;&gt;How to get started with site reliability engineering (SRE)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/data-analytics/lookers-universal-semantic-model&#34;&gt;Bring governance and trust to everyone with Looker’s universal semantic model&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;Related articles&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-sres-analyze-risks-to-evaluate-slos&#34;&gt;How SREs analyze risks to evaluate SLOs | Google Cloud Blog&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://help.looker.com/hc/en-us/articles/360001766908-Best-Practice-Create-a-Positive-Experience-for-Looker-Users&#34; target=&#34;_blank&#34;&gt;Best Practice: Create a Positive Experience for Looker Users&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://help.looker.com/hc/en-us/articles/360001784747-Best-Practice-LookML-Dos-and-Don-ts&#34; target=&#34;_blank&#34;&gt;Best Practice: LookML Dos and Don&#39;ts&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><router-outlet></router-outlet><dynamic-page><article-page><main id="jump-content"><promo-banner-block _nghost-c55=""></promo-banner-block><article><article-header-block></article-header-block><div><article-cta _nghost-c62=""><div _ngcontent-c62=""><h4 _ngcontent-c62=""><span _ngcontent-c62="">Try Google Cloud</span></h4><p _ngcontent-c62=""><span _ngcontent-c62="">Start building on Google Cloud with $300 in free credits and 20+ always free products.</span></p><p><a _ngcontent-c62="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="free trial" track-metadata-eventdetail="https://cloud.google.com/free/" href="https://cloud.google.com/free/"><span _ngcontent-c62="">Free Trial</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c64=""><div _ngcontent-c64="" innerhtml="&lt;p&gt;Many organizations struggle to create data-driven cultures where each employee is empowered to make decisions based on data. This is especially true for enterprises with a variety of systems and tools in use across different teams. If you are a leader, manager, or executive focused on how your team can leverage Google&#39;s SRE practices or wider DevOps practices, definitely you are in the right place!&lt;/p&gt;&lt;h3&gt;What do today&amp;#8217;s enterprises or mature start-ups look like?&lt;/h3&gt;&lt;p&gt;Today large organizations are often segmented into hundreds of small teams which are often working around data in the magnitude of several petabytes and in a wide variety of raw forms. &amp;#8216;Working around data&amp;#8217; could mean any of the following: generating, facilitating, consuming, processing, visualizing or feeding back into the system. Due to a wide variety of responsibilities, the skill sets also vary to a large extent. Numerous people and teams work with data, with jobs that span the entire data ecosystem:&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Centralizing data from raw sources and systems&lt;/li&gt;&lt;li&gt;Maintaining and transforming data in a warehouse&lt;/li&gt;&lt;li&gt;Managing access controls and permissions for the data&lt;/li&gt;&lt;li&gt;Modeling data&lt;/li&gt;&lt;li&gt;Doing ad-hoc data analysis and exploration&lt;/li&gt;&lt;li&gt;Building visualizations and reports&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Nevertheless, a common goal across all these teams is keeping services running and downstream customers happy. In other words, the organization might be divided internally, however, they all have the mission to leverage the data to make better business decisions. Hence, despite silos and different subgoals, destiny for all these teams is intertwined for the organization to thrive. To support such a diverse set of data sources and the teams supporting them, Looker supports over &lt;a href=&#34;https://docs.looker.com/setup-and-management/database-config&#34; target=&#34;_blank&#34;&gt;60 dialects&lt;/a&gt; (input from a data source) and over &lt;a href=&#34;https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/delivering-looks-explores#delivery_options_for_third-party_integrations&#34; target=&#34;_blank&#34;&gt;35 destinations&lt;/a&gt; (output to a new data source).&lt;/p&gt;&lt;p&gt;Below is a simplified* picture of how the Looker ecosystem is central to a data-rich organization.&lt;/p&gt;"><p>Many organizations struggle to create data-driven cultures where each employee is empowered to make decisions based on data. This is especially true for enterprises with a variety of systems and tools in use across different teams. If you are a leader, manager, or executive focused on how your team can leverage Google&#39;s SRE practices or wider DevOps practices, definitely you are in the right place!</p><h3>What do today’s enterprises or mature start-ups look like?</h3><p>Today large organizations are often segmented into hundreds of small teams which are often working around data in the magnitude of several petabytes and in a wide variety of raw forms. ‘Working around data’ could mean any of the following: generating, facilitating, consuming, processing, visualizing or feeding back into the system. Due to a wide variety of responsibilities, the skill sets also vary to a large extent. Numerous people and teams work with data, with jobs that span the entire data ecosystem:</p><ul><li>Centralizing data from raw sources and systems</li><li>Maintaining and transforming data in a warehouse</li><li>Managing access controls and permissions for the data</li><li>Modeling data</li><li>Doing ad-hoc data analysis and exploration</li><li>Building visualizations and reports</li></ul><p>Nevertheless, a common goal across all these teams is keeping services running and downstream customers happy. In other words, the organization might be divided internally, however, they all have the mission to leverage the data to make better business decisions. Hence, despite silos and different subgoals, destiny for all these teams is intertwined for the organization to thrive. To support such a diverse set of data sources and the teams supporting them, Looker supports over <a href="https://docs.looker.com/setup-and-management/database-config" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://docs.looker.com" track-metadata-module="post">60 dialects</a> (input from a data source) and over <a href="https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/delivering-looks-explores#delivery_options_for_third-party_integrations" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://docs.looker.com" track-metadata-module="post">35 destinations</a> (output to a new data source).</p><p>Below is a simplified* picture of how the Looker ecosystem is central to a data-rich organization.</p></div></paragraph-block></div><div><paragraph-block _nghost-c64=""><div _ngcontent-c64="" innerhtml="&lt;p&gt;*The picture hides the complexity of team(s) accountable for each data source. It also hides how a data source may have dependencies on other sources. &lt;a href=&#34;https://marketplace.looker.com/&#34; target=&#34;_blank&#34;&gt;Looker Marketplace&lt;/a&gt; can also play an important role in your ecosystem.&lt;/p&gt;&lt;h3&gt;What role can DevOps and SRE practices play?&lt;/h3&gt;&lt;p&gt;In the most ideal state, all these teams will be in harmony as a single-threaded organization with all the internal processes so smooth that everyone is empowered to experiment (i.e. fail, learn, iterate and repeat all the time). With increasing organizational complexities, it is incredibly challenging to achieve such a state because there will be overhead and misaligned priorities. This is where we look up to the guiding principles of DevOps and SRE practices. In case you are not familiar with Google SRE practices, &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering&#34;&gt;here&lt;/a&gt; is a starting point. The core of DevOps and SRE practices are mature communication and collaboration practices.&amp;#160;&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s focus on the best practices which could help us with our Looker ecosystem.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Have joint goals&lt;/b&gt;. There should be some goals which are a shared responsibility across two or more teams. This helps establish a culture of psychological safety and transparency across teams.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Visualize how the data flows across the organization&lt;/b&gt;. This enables an understanding how each team plays their role and how to work with them better.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Agree on the &lt;/b&gt;&lt;a href=&#34;https://sre.google/sre-book/monitoring-distributed-systems/&#34; target=&#34;_blank&#34;&gt;&lt;b&gt;Golden Signals&lt;/b&gt;&lt;/a&gt; &lt;b&gt;(aka core metrics)&lt;/b&gt;. These could mean data freshness, data accuracy, latency on centralized dashboards etc. These signals allow teams to set their &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons&#34;&gt;error budgets&lt;/a&gt; and &lt;a href=&#34;https://sre.google/sre-book/service-level-objectives/&#34; target=&#34;_blank&#34;&gt;SLIs&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Agree on communication and collaboration methods that work across teams&lt;/b&gt;.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Regular bidirectional communication modes - have shared &lt;a href=&#34;https://support.google.com/chat/answer/7659784?hl=en&#34; target=&#34;_blank&#34;&gt;Google Chat spaces&lt;/a&gt;/&lt;a href=&#34;https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling-slack&#34; target=&#34;_blank&#34;&gt;slack channels&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Focus on artifacts such as jointly owned documentations pages, shared roadmap items, reusable tooling, etc. For example, &lt;a href=&#34;https://docs.looker.com/admin-options/system-activity/sa-dashboards&#34; target=&#34;_blank&#34;&gt;System Activity Dashboards&lt;/a&gt; could be made available to all the relevant stakeholders and supplemented with notes tailored to your organization.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Set up regular forums where commonly discussed agenda items include major changes, expected downtime and postmortems around the core metrics. Among other agenda items, you could define/refine a common set of standards, for example centrally defined &lt;a href=&#34;https://docs.looker.com/reference/field-params/label-for-field&#34; target=&#34;_blank&#34;&gt;labels&lt;/a&gt;, &lt;a href=&#34;https://docs.looker.com/reference/field-params/group_label&#34; target=&#34;_blank&#34;&gt;group_labels&lt;/a&gt;, &lt;a href=&#34;https://docs.looker.com/reference/field-params/description&#34; target=&#34;_blank&#34;&gt;descriptions&lt;/a&gt;, etc. in the LookML to ensure there is a single terminology across the board.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Promote informal sharing opportunities such as lessons learned, &lt;a href=&#34;https://www.thinkwithgoogle.com/future-of-marketing/management-and-culture/passion-not-perks/&#34; target=&#34;_blank&#34;&gt;TGIFs&lt;/a&gt;, Brown bag sessions, and shadowing opportunities. Learning and teaching have an immense impact on how teams evolve. Teams often become closer with side projects that are slightly outside of their usual day-to-day duties.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Have mutually agreed upon change management practices&lt;/b&gt;. Each team has dependencies so making changes may have an impact on other teams. Why not plan those changes systematically? For example, getting common standards across the &lt;a href=&#34;https://docs.looker.com/data-modeling/getting-started/advanced-deploy-mode&#34; target=&#34;_blank&#34;&gt;Advance deploy mode&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Promote continuous improvements&lt;/b&gt;. Keep looking for better, faster, cost-optimized versions of something important to the teams.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Revisit your data flow&lt;/b&gt;. After every major reorganization, ensure that organizational change has not broken the established mechanisms.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;"><p>*The picture hides the complexity of team(s) accountable for each data source. It also hides how a data source may have dependencies on other sources. <a href="https://marketplace.looker.com/" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://marketplace.looker.com" track-metadata-module="post">Looker Marketplace</a> can also play an important role in your ecosystem.</p><h3>What role can DevOps and SRE practices play?</h3><p>In the most ideal state, all these teams will be in harmony as a single-threaded organization with all the internal processes so smooth that everyone is empowered to experiment (i.e. fail, learn, iterate and repeat all the time). With increasing organizational complexities, it is incredibly challenging to achieve such a state because there will be overhead and misaligned priorities. This is where we look up to the guiding principles of DevOps and SRE practices. In case you are not familiar with Google SRE practices, <a href="https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering" track-metadata-module="post">here</a> is a starting point. The core of DevOps and SRE practices are mature communication and collaboration practices. </p><p>Let’s focus on the best practices which could help us with our Looker ecosystem.</p><ol><li><p><b>Have joint goals</b>. There should be some goals which are a shared responsibility across two or more teams. This helps establish a culture of psychological safety and transparency across teams.</p></li><li><p><b>Visualize how the data flows across the organization</b>. This enables an understanding how each team plays their role and how to work with them better.</p></li><li><p><b>Agree on the </b><a href="https://sre.google/sre-book/monitoring-distributed-systems/" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://sre.google" track-metadata-module="post"><b>Golden Signals</b></a> <b>(aka core metrics)</b>. These could mean data freshness, data accuracy, latency on centralized dashboards etc. These signals allow teams to set their <a href="https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons" track-metadata-module="post">error budgets</a> and <a href="https://sre.google/sre-book/service-level-objectives/" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">SLIs</a>.</p></li><li><p><b>Agree on communication and collaboration methods that work across teams</b>. </p></li><ol><li><p>Regular bidirectional communication modes - have shared <a href="https://support.google.com/chat/answer/7659784?hl=en" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://support.google.com" track-metadata-module="post">Google Chat spaces</a>/<a href="https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling-slack" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://docs.looker.com" track-metadata-module="post">slack channels</a>. </p></li><li><p>Focus on artifacts such as jointly owned documentations pages, shared roadmap items, reusable tooling, etc. For example, <a href="https://docs.looker.com/admin-options/system-activity/sa-dashboards" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://docs.looker.com" track-metadata-module="post">System Activity Dashboards</a> could be made available to all the relevant stakeholders and supplemented with notes tailored to your organization.</p></li><li><p>Set up regular forums where commonly discussed agenda items include major changes, expected downtime and postmortems around the core metrics. Among other agenda items, you could define/refine a common set of standards, for example centrally defined <a href="https://docs.looker.com/reference/field-params/label-for-field" target="_blank" track-type="inline link" track-name="11" track-metadata-eventdetail="https://docs.looker.com" track-metadata-module="post">labels</a>, <a href="https://docs.looker.com/reference/field-params/group_label" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="https://docs.looker.com" track-metadata-module="post">group_labels</a>, <a href="https://docs.looker.com/reference/field-params/description" target="_blank" track-type="inline link" track-name="13" track-metadata-eventdetail="https://docs.looker.com" track-metadata-module="post">descriptions</a>, etc. in the LookML to ensure there is a single terminology across the board.</p></li><li><p>Promote informal sharing opportunities such as lessons learned, <a href="https://www.thinkwithgoogle.com/future-of-marketing/management-and-culture/passion-not-perks/" target="_blank" track-type="inline link" track-name="14" track-metadata-eventdetail="https://www.thinkwithgoogle.com" track-metadata-module="post">TGIFs</a>, Brown bag sessions, and shadowing opportunities. Learning and teaching have an immense impact on how teams evolve. Teams often become closer with side projects that are slightly outside of their usual day-to-day duties.</p></li></ol><li><p><b>Have mutually agreed upon change management practices</b>. Each team has dependencies so making changes may have an impact on other teams. Why not plan those changes systematically? For example, getting common standards across the <a href="https://docs.looker.com/data-modeling/getting-started/advanced-deploy-mode" target="_blank" track-type="inline link" track-name="15" track-metadata-eventdetail="https://docs.looker.com" track-metadata-module="post">Advance deploy mode</a>.</p></li><li><p><b>Promote continuous improvements</b>. Keep looking for better, faster, cost-optimized versions of something important to the teams.</p></li><li><p><b>Revisit your data flow</b>. After every major reorganization, ensure that organizational change has not broken the established mechanisms.</p></li></ol></div></paragraph-block></div><div><article-pull-quote-block><div><p><q>despite silos and different subgoals, destiny for all these teams is intertwined for the organization to thrive.</q></p></div></article-pull-quote-block></div><div><paragraph-block _nghost-c64=""><div _ngcontent-c64="" innerhtml="&lt;h3&gt;Are you over-engineering?&lt;/h3&gt;&lt;p&gt;There is a possibility that in the process of maturing the ecosystem, we may end up in an overly engineered system - we may unintentionally add &lt;a href=&#34;https://landing.google.com/sre/sre-book/chapters/eliminating-toil/&#34; target=&#34;_blank&#34;&gt;toil&lt;/a&gt; to the environment. These are examples of toil that often stem from communication gaps.&amp;#160;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Meetings with no outcomes/action plans - This one is among the most common forms of toil, where the original intention of a meeting is no longer valid but the forum has not taken efforts to revisit their decision.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Unnecessary approvals - Being a single threaded team can often create unnecessary dependencies and your teams may lose the ability to make changes.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Unaligned maintenance windows - Changes across multiple teams may not be mutually exclusive hence if there is misalignment then it may create unforeseen impacts on the end user.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Fancy, but unnecessary tooling - Side projects, if not governed, may create unnecessary tooling which is not being used by the business. Collaborations are great when they solve real business problems, hence it is also required to refocus if the priorities are set right.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Gray areas - When you have a shared responsibility model, you also may end up in gray areas which are often gaps with no owner. This can lead to increased complexity in the long run. For example, having the flexibility to &lt;a href=&#34;https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling&#34; target=&#34;_blank&#34;&gt;schedule content delivery&lt;/a&gt; still requires collaboration to reduce &lt;a href=&#34;https://docs.looker.com/admin-options/scheduler/history&#34; target=&#34;_blank&#34;&gt;jobs with failures&lt;/a&gt; because it can impact the performance of your Looker instance.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Contradicting metrics - You may want to pay special attention to how teams are rewarded for internal metrics. For example, if a team focuses on accuracy of data and other one on freshness then at scale they may not align with one another.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;To summarize, we learned how data is handled in large organizations with Looker at its heart unifying a universal semantic model. To handle large amounts of diverse data, teams need to start with aligned goals and commit to strong collaboration. We also learned how DevOps and SRE practices can guide us navigate through these complexities. Lastly, we looked at some side effects of excessively structured systems. To go forward from here, it is highly recommended to start with an analysis of how data flows under your scope and how mature the collaboration is across multiple teams.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Further reading and resources&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/data-analytics/business-intelligence-for-cloud-data-with-looker&#34;&gt;Getting to know Looker &amp;#8211; common use cases&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1&#34;&gt;Enterprise DevOps Guidebook&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons&#34;&gt;Know thy enemy: how to prioritize and communicate risks&amp;#8212;CRE life lessons&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.oreilly.com/content/how-to-get-started-with-site-reliability-engineering-sre/&#34; target=&#34;_blank&#34;&gt;How to get started with site reliability engineering (SRE)&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/data-analytics/lookers-universal-semantic-model&#34;&gt;Bring governance and trust to everyone with Looker&amp;#8217;s universal semantic model&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;Related articles&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-sres-analyze-risks-to-evaluate-slos&#34;&gt;How SREs analyze risks to evaluate SLOs | Google Cloud Blog&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://help.looker.com/hc/en-us/articles/360001766908-Best-Practice-Create-a-Positive-Experience-for-Looker-Users&#34; target=&#34;_blank&#34;&gt;Best Practice: Create a Positive Experience for Looker Users&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://help.looker.com/hc/en-us/articles/360001784747-Best-Practice-LookML-Dos-and-Don-ts&#34; target=&#34;_blank&#34;&gt;Best Practice: LookML Dos and Don&#39;ts&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;"><h3>Are you over-engineering?</h3><p>There is a possibility that in the process of maturing the ecosystem, we may end up in an overly engineered system - we may unintentionally add <a href="https://landing.google.com/sre/sre-book/chapters/eliminating-toil/" target="_blank" track-type="inline link" track-name="16" track-metadata-eventdetail="https://landing.google.com" track-metadata-module="post">toil</a> to the environment. These are examples of toil that often stem from communication gaps. </p><ol><li><p>Meetings with no outcomes/action plans - This one is among the most common forms of toil, where the original intention of a meeting is no longer valid but the forum has not taken efforts to revisit their decision.</p></li><li><p>Unnecessary approvals - Being a single threaded team can often create unnecessary dependencies and your teams may lose the ability to make changes.</p></li><li><p>Unaligned maintenance windows - Changes across multiple teams may not be mutually exclusive hence if there is misalignment then it may create unforeseen impacts on the end user.</p></li><li><p>Fancy, but unnecessary tooling - Side projects, if not governed, may create unnecessary tooling which is not being used by the business. Collaborations are great when they solve real business problems, hence it is also required to refocus if the priorities are set right.</p></li><li><p>Gray areas - When you have a shared responsibility model, you also may end up in gray areas which are often gaps with no owner. This can lead to increased complexity in the long run. For example, having the flexibility to <a href="https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling" target="_blank" track-type="inline link" track-name="17" track-metadata-eventdetail="https://docs.looker.com" track-metadata-module="post">schedule content delivery</a> still requires collaboration to reduce <a href="https://docs.looker.com/admin-options/scheduler/history" target="_blank" track-type="inline link" track-name="18" track-metadata-eventdetail="https://docs.looker.com" track-metadata-module="post">jobs with failures</a> because it can impact the performance of your Looker instance.</p></li><li><p>Contradicting metrics - You may want to pay special attention to how teams are rewarded for internal metrics. For example, if a team focuses on accuracy of data and other one on freshness then at scale they may not align with one another.</p></li></ol><h3>Conclusion</h3><p>To summarize, we learned how data is handled in large organizations with Looker at its heart unifying a universal semantic model. To handle large amounts of diverse data, teams need to start with aligned goals and commit to strong collaboration. We also learned how DevOps and SRE practices can guide us navigate through these complexities. Lastly, we looked at some side effects of excessively structured systems. To go forward from here, it is highly recommended to start with an analysis of how data flows under your scope and how mature the collaboration is across multiple teams.</p><p><b>Further reading and resources</b></p><ol><li><p><a href="https://cloud.google.com/blog/products/data-analytics/business-intelligence-for-cloud-data-with-looker" track-type="inline link" track-name="19" track-metadata-eventdetail="https://cloud.google.com/blog/products/data-analytics/business-intelligence-for-cloud-data-with-looker" track-metadata-module="post">Getting to know Looker – common use cases</a></p></li><li><p><a href="https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1" track-type="inline link" track-name="20" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1" track-metadata-module="post">Enterprise DevOps Guidebook</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons" track-type="inline link" track-name="21" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons" track-metadata-module="post">Know thy enemy: how to prioritize and communicate risks—CRE life lessons</a></p></li><li><p><a href="https://www.oreilly.com/content/how-to-get-started-with-site-reliability-engineering-sre/" target="_blank" track-type="inline link" track-name="22" track-metadata-eventdetail="https://www.oreilly.com" track-metadata-module="post">How to get started with site reliability engineering (SRE)</a></p></li><li><p><a href="https://cloud.google.com/blog/products/data-analytics/lookers-universal-semantic-model" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/blog/products/data-analytics/lookers-universal-semantic-model" track-metadata-module="post">Bring governance and trust to everyone with Looker’s universal semantic model</a></p></li></ol><p><b>Related articles</b></p><ol><li><p><a href="https://cloud.google.com/blog/products/devops-sre/how-sres-analyze-risks-to-evaluate-slos" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/how-sres-analyze-risks-to-evaluate-slos" track-metadata-module="post">How SREs analyze risks to evaluate SLOs | Google Cloud Blog</a></p></li><li><p><a href="https://help.looker.com/hc/en-us/articles/360001766908-Best-Practice-Create-a-Positive-Experience-for-Looker-Users" target="_blank" track-type="inline link" track-name="25" track-metadata-eventdetail="https://help.looker.com" track-metadata-module="post">Best Practice: Create a Positive Experience for Looker Users</a></p></li><li><p><a href="https://help.looker.com/hc/en-us/articles/360001784747-Best-Practice-LookML-Dos-and-Don-ts" target="_blank" track-type="inline link" track-name="26" track-metadata-eventdetail="https://help.looker.com" track-metadata-module="post">Best Practice: LookML Dos and Don&#39;ts</a></p></li></ol></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c63=""></article-up-1to3-block></section></div></article></main></article-page></dynamic-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Saurabh Bangad&lt;/name&gt;&lt;title&gt;Technical Account Manager, Middle East&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 29 Jul 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How Google got to rolling Linux releases for Desktops</title>
      <link>https://cloud.google.com/blog/topics/developers-practitioners/how-google-got-to-rolling-linux-releases-for-desktops/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;&lt;sup&gt;Hero image credit: Markus Teich&lt;/sup&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;At Google we run large production fleets that serve Google products like YouTube and Gmail. To support all our employees, including engineers, we also run a sizable corporate fleet with hundreds of thousands of devices across multiple platforms, models, and locations. To let each Googler work in the environment they are most productive in, we operate many OS-platforms including a Linux system. For a long time, our internal facing Linux distribution, Goobuntu, was based off of Ubuntu LTS releases. In 2018 we completed a move to a rolling release model based on Debian.  &lt;/p&gt;&lt;h3&gt;Upgrade Toil&lt;/h3&gt;&lt;p&gt;More than 15 years ago, Ubuntu was chosen as the base for the internal Linux distribution, as it was user-friendly, easy to use, and had lots of fancy extras. The Long Term Support (LTS) releases were picked as it was valued that Canonical provided 2+ years of security updates. &lt;br/&gt;&lt;/p&gt;&lt;p&gt;However, this two year release cycle for LTS releases also meant that we had to upgrade every machine in our fleet of over 100.000 devices before the end-of-life date of the OS. The complex nature of workloads run on corporate machines meant that reinstalling and fully customizing machines could be a difficult and time consuming operation. The productivity hit of having all engineers configure their workspace from scratch every two years was not a financially responsible option. &lt;br/&gt;&lt;/p&gt;&lt;p&gt;For each OS cycle, we had a rather large version jump in major packages that could require significant changes to software configuration. To automate this process, we wrote an unattended in-place upgrade tool that took care of a lot of the common case problems. This automation focused approach meant that most of the Google employees didn&#39;t have to manually upgrade their machines by re-installing them and recreating all their configuration. To make this possible, however, we needed to do comprehensive testing of the upgrade process and check that all major packages that had changed kept working (in Ubuntu this could be up to several thousands packages to upgrade between major versions). Sometimes it was hard to provide automation in the cases where deprecations happened and engineers had to make decisions on how to move forward. &lt;br/&gt;&lt;/p&gt;&lt;p&gt;This effort to upgrade our Goobuntu fleet usually took the better part of a year. With a two year support window there was only one year left until we had to go through the same process all over again for the next LTS. This entire process was a huge stress factor for our team, as we got hundreds of bugs with requests for help for corner cases. Once one upgrade was done there was a general sense of being “close to burnout” in the team that we barely could recover from until the next round of updates came about. Running off an LTS version also meant that some bugs encountered by users of our distribution might’ve already been fixed upstream, but those improvements might’ve never been backported to the LTS version.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;There was also a long tail of special-case upgrades that could sometimes drag on for several years. Handling this process was a huge change management challenge to get engineers to upgrade the machines that didn’t work in the automatic process. We got creative when motivating our users to upgrade their machines. Measures ranged from nagging messages on their UI, mails, scheduled reboots and even shutting down the machines, to raise awareness that there were still some machines in dire need of an upgrade. Sometimes this caught machines that people had totally forgotten about, like the one machine under a desk that was running a critical pipeline for something important, as it turned out.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;Rolling Releases&lt;/h3&gt;&lt;p&gt;When we designed gLinux Rodete (Rolling Debian Testing), we aimed at removing the two year upgrade cycle and instead spread out the load on the team throughout time. The general move to CI/CD in the industry has shown that smaller incremental changes are easier to control and rollback. Rolling releases with Linux distributions today are getting more common (Arch Linux, NixOS). &lt;/p&gt;&lt;p&gt;We considered going with other Linux distributions, but ended up choosing Debian because we again wanted to offer a smooth in-place migration. This included considerations towards the availability of packages in Debian, the large Debian community, and also the existing internal packages and tooling that were using the Debian format. While the Debian Stable track follows a roughly two-year jump between releases, the Debian testing track works as a rolling release, as it&#39;s the pool of all packages ingested and built from upstream, waiting for the next stable release to happen. &lt;/p&gt;&lt;p&gt;The time from upstream release to availability in testing is often just a few days (although during freeze periods before a Debian stable release, it can sometimes lag a few months behind). This means we can get much more granular changes in general and provide the newest software to our engineers at Google without having to wait longer periods.&lt;/p&gt;&lt;p&gt;This frequency of updates required us to redesign a lot of systems and processes. While originally intending more frequent releases, we found that for us, weekly releases were a sweet spot between moving quickly and allowing for proper release qualification, limiting the disruption to developer productivity.&lt;/p&gt;&lt;p&gt;Whenever we start a new release,  we take a snapshot of all the packages ingested from Debian at that time. After some acceptance tests, the new hermetic release candidate is then cautiously rolled out to a dedicated testing fleet and a 1% fleet wide canary. The canary is held intentionally over the course of a couple days to detect any problems with Debian packages or Google internal packages before it progresses to the entire fleet. &lt;/p&gt;&lt;h3&gt;Introducing Sieve&lt;/h3&gt;&lt;p&gt;To manage all these complex tasks from building all upstream packages from source, we have built a workflow system called Sieve. Whenever we see any new version of a Debian package, we start a new build. We build packages in package groups, to take into account separate packages that need to be upgraded together. Once the whole group has been built, we run a virtualized test suite to make sure none of our core components and developer workflows are broken. Each group is tested separately with a full system installation, boot and local test suite run on that version of the operating system. While builds for individual packages usually complete within minutes, these tests can take up to an hour given the complexity of the package group.&lt;/p&gt;&lt;p&gt;Once the packages are built and all the tests passed, we merge all the new packages with our latest pool of packages. When we cut a new release, we snapshot that pool with each package version locked in for that release. We then proceed to carefully guide this release to the fleet utilizing SRE principles like incremental canarying and monitoring the fleet health. &lt;/p&gt;&lt;p&gt;But not all builds succeed on the first attempt. If a package fails to build, we usually check for any known bugs with the Debian bug tracker and potentially report it, should it not be known already. Sometimes our release engineers have to become creative and apply local workarounds/patches to get a package to build within our ecosystem and later on drop those workarounds once upstream has released a fix.&lt;/p&gt;&lt;p&gt;One issue that we&#39;ve run into a few times, for example, is that in upstream Debian, packages are usually built in Debian unstable. After a few days, these already built packages migrate to Debian testing. In some cases it&#39;s possible, however, that a build-dependency is stuck in unstable and thus building within testing might not (yet) be feasible. We generally try to work upstream first in these cases so we reduce the complexity and maintenance burden to keep these local patches, while also giving back to the community. &lt;/p&gt;&lt;p&gt;If any of the steps fail, Sieve has a toolbox of tricks to retry builds. For example, when it starts the initial build of a group of packages, the system makes an educated guess of which dependencies need to be built together. But sometimes the version information provided in Debian source packages can be incomplete and this guess is wrong. For this reason, Sieve periodically retries building groups that failed. As the latest snapshot of our packages is a moving target, it could happen that after a seemingly independent package group gets added to the snapshot, a previously broken group unexpectedly builds and passes tests correctly. All these workflows are mostly automatic and this highlights the importance of thinking as an SRE in this field. When facing a failure, it usually seems easier to just fix a failing build once, but if we need to apply the same workaround over and over, putting the workaround in code will reduce the overall burden put on our engineers.&lt;/p&gt;&lt;p&gt;There are also some security benefits to building all of our binaries from source and having additional source code provenance that verifies the origin of the running binary. During a security incident for example, we are able to rebuild quickly and have confidence in the build working with a temporary patch, as we have been building all packages before, that land in our distribution. Additionally, we also reduce the trust envelope that we have to place into upstream Debian and the binary build artifacts produced by their infrastructure. Instead once the source code is ingested and the binary built verifiably, we can cryptographically attest that the running binary originated from exactly that source code.&lt;/p&gt;&lt;h3&gt;Upgrading to Rodete&lt;/h3&gt;&lt;p&gt;The last Goobuntu release was based on Ubuntu 14.04 LTS (Codename Trusty). Development on Rodete started in 2015 and it was quickly clear that we couldn’t just drop support for Trusty and require the entire engineering population to install a fresh new distribution. From the previous experience of updating in-place between LTS versions, we already had some good experience of knowing what awaited us with this migration. Because Ubuntu is a derivative from Debian and uses a lot of the same packaging infrastructure/formats (apt), it wasn’t a totally crazy idea to upgrade the fleet from Goobuntu 14.04 to Debian in-place. We reused some parts of our previous in-place upgrade tool, and worked to make it more reliable, by adding more automation and a lot more testing.&lt;/p&gt;&lt;p&gt;To make it easier to create such a tool, test it and maintain it for the duration of the migration, we chose to temporarily freeze gLinux Rodete as a snapshot of Debian testing on a specific date which we call baseline. We can advance this baseline at our own choosing, to balance what packages Sieve ingests. To reduce friction, we intentionally set the baseline of Rodete at the current Debian stable release in 2016 which was much closer to the general state of Ubuntu Trusty. That way we could separate in-place upgrading from Trusty to Debian and major package version changes that happened in Debian at a later date. &lt;/p&gt;&lt;p&gt;In 2017, we started to migrate the machines to Rodete and completed the last in place migrations by the end of 2018. We however still had a baseline of packages which at that point dated almost two years in the past. To catch up with Debian Testing, we started a team wide effort to focus on optimizing Sieve behavior and speed up the time needed to build / test packages. Replaying the upgrades in this incremental fashion and having a moving rolling release target that we control eased the workload for Google engineers and our team.&lt;/p&gt;&lt;p&gt;In early 2019 we started to shut down the last remnants of Goobuntu machines. Our baseline has also advanced to only lag behind by ~250 days which at the time meant we were using most of the package versions that were part of buster. By mid-2020 we finally fully caught up at the same time when Debian bullseye was released. We continue to move ahead our baseline and will probably already be using a similar version of the next Debian Stable release, before its release in mid 2023.&lt;/p&gt;&lt;h3&gt;Reaching Zen&lt;/h3&gt;&lt;p&gt;Today, the life of a gLinux team member looks very different. We have reduced the amount of engineering time and energy required for releases to one on-duty release engineer that rotates among team members. We no longer have a big push to upgrade our entire fleet. No more need for multi stage alpha, betas and GAs for new LTS releases while simultaneously chasing down older machines that still were running Ubuntu Precise or Lucid.&lt;/p&gt;&lt;p&gt;We also dramatically improved our security stance by operating our fleet closer to upstream releases. While Debian provides a good source of security patches for the stable and oldstable tracks, we realized that not every security hole that gets patches, necessarily has a Debian Security Advisory (DSA) or CVE number. Our rolling release schedule makes sure we patch security holes on the entire fleet quickly without compromising on stability, while previously security engineers had to carefully review each DSA and make sure the fix has made it to our fleet.&lt;/p&gt;&lt;p&gt;Our improved testing suite and integration tests with key partner teams that run critical developer systems also yielded a more stable experience using a Linux distribution that provides the latest versions of the Linux Kernel. Our strong longing for automating everything in the pipeline has significantly reduced toil and stress within the team. It is now also possible for us to report bugs and incompatibilities with other library versions while making sure that Google tools work better within the Linux ecosystem.&lt;/p&gt;&lt;p&gt;If you are interested in making rolling releases in your company a success, then consider to balance the needs of the company against upgrade agility. Being in control of our own moving target and baseline has helped to slow down whenever we encountered too many problems and broke any of our team SLOs. Our journey has ultimately reinforced our belief that incremental changes are better manageable than big bang releases. &lt;/p&gt;&lt;p&gt;If you are able to control the influx of new work and keep that predictable, we have made the experience that our engineers stay happier and are less stressed out. This ultimately lowered the team churn and made sure that we can build expertise instead of dealing with multiple burning fires at the same time.&lt;/p&gt;&lt;p&gt;In the future, we are planning to work even more closely with upstream Debian and contribute more of our internal patches to maintain the Debian package ecosystem.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div _ngcontent-c63="" innerhtml="&lt;p&gt;&lt;i&gt;&lt;sup&gt;Hero image credit: Markus Teich&lt;/sup&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;At Google we run large production fleets that serve Google products like YouTube and Gmail. To support all our employees, including engineers, we also run a sizable corporate fleet with hundreds of thousands of devices across multiple platforms, models, and locations. To let each Googler work in the environment they are most productive in, we operate many OS-platforms including a Linux system. For a long time, our internal facing Linux distribution, Goobuntu, was based off of Ubuntu LTS releases. In 2018 we completed a move to a rolling release model based on Debian.&amp;#160;&amp;#160;&lt;/p&gt;&lt;h3&gt;Upgrade Toil&lt;/h3&gt;&lt;p&gt;More than 15 years ago, Ubuntu was chosen as the base for the internal Linux distribution, as it was user-friendly, easy to use, and had lots of fancy extras. The Long Term Support (LTS) releases were picked as it was valued that Canonical provided 2+ years of security updates.&amp;#160;&lt;br&gt;&lt;/p&gt;&lt;p&gt;However, this two year release cycle for LTS releases also meant that we had to upgrade every machine in our fleet of over 100.000 devices before the end-of-life date of the OS. The complex nature of workloads run on corporate machines meant that reinstalling and fully customizing machines could be a difficult and time consuming operation. The productivity hit of having all engineers configure their workspace from scratch every two years was not a financially responsible option.&amp;#160;&lt;br&gt;&lt;/p&gt;&lt;p&gt;For each OS cycle, we had a rather large version jump in major packages that could require significant changes to software configuration. To automate this process, we wrote an unattended in-place upgrade tool that took care of a lot of the common case problems. This automation focused approach meant that most of the Google employees didn&#39;t have to manually upgrade their machines by re-installing them and recreating all their configuration. To make this possible, however, we needed to do comprehensive testing of the upgrade process and check that all major packages that had changed kept working (in Ubuntu this could be up to several thousands packages to upgrade between major versions). Sometimes it was hard to provide automation in the cases where deprecations happened and engineers had to make decisions on how to move forward.&amp;#160;&lt;br&gt;&lt;/p&gt;&lt;p&gt;This effort to upgrade our Goobuntu fleet usually took the better part of a year. With a two year support window there was only one year left until we had to go through the same process all over again for the next LTS. This entire process was a huge stress factor for our team, as we got hundreds of bugs with requests for help for corner cases. Once one upgrade was done there was a general sense of being &amp;#8220;close to burnout&amp;#8221; in the team that we barely could recover from until the next round of updates came about. Running off an LTS version also meant that some bugs encountered by users of our distribution might&amp;#8217;ve already been fixed upstream, but those improvements might&amp;#8217;ve never been backported to the LTS version.&lt;br&gt;&lt;/p&gt;&lt;p&gt;There was also a long tail of special-case upgrades that could sometimes drag on for several years. Handling this process was a huge change management challenge to get engineers to upgrade the machines that didn&amp;#8217;t work in the automatic process. We got creative when motivating our users to upgrade their machines. Measures ranged from nagging messages on their UI, mails, scheduled reboots and even shutting down the machines, to raise awareness that there were still some machines in dire need of an upgrade. Sometimes this caught machines that people had totally forgotten about, like the one machine under a desk that was running a critical pipeline for something important, as it turned out.&lt;br&gt;&lt;/p&gt;&lt;h3&gt;Rolling Releases&lt;/h3&gt;&lt;p&gt;When we designed gLinux Rodete (Rolling Debian Testing), we aimed at removing the two year upgrade cycle and instead spread out the load on the team throughout time. The general move to CI/CD in the industry has shown that smaller incremental changes are easier to control and rollback. Rolling releases with Linux distributions today are getting more common (Arch Linux, NixOS).&amp;#160;&lt;/p&gt;&lt;p&gt;We considered going with other Linux distributions, but ended up choosing Debian because we again wanted to offer a smooth in-place migration. This included considerations towards the availability of packages in Debian, the large Debian community, and also the existing internal packages and tooling that were using the Debian format. While the Debian Stable track follows a roughly two-year jump between releases, the Debian testing track works as a rolling release, as it&#39;s the pool of all packages ingested and built from upstream, waiting for the next stable release to happen.&amp;#160;&lt;/p&gt;&lt;p&gt;The time from upstream release to availability in testing is often just a few days (although during freeze periods before a Debian stable release, it can sometimes lag a few months behind). This means we can get much more granular changes in general and provide the newest software to our engineers at Google without having to wait longer periods.&lt;/p&gt;&lt;p&gt;This frequency of updates required us to redesign a lot of systems and processes. While originally intending more frequent releases, we found that for us, weekly releases were a sweet spot between moving quickly and allowing for proper release qualification, limiting the disruption to developer productivity.&lt;/p&gt;&lt;p&gt;Whenever we start a new release,&amp;#160; we take a snapshot of all the packages ingested from Debian at that time. After some acceptance tests, the new hermetic release candidate is then cautiously rolled out to a dedicated testing fleet and a 1% fleet wide canary. The canary is held intentionally over the course of a couple days to detect any problems with Debian packages or Google internal packages before it progresses to the entire fleet.&amp;#160;&lt;/p&gt;&lt;h3&gt;Introducing Sieve&lt;/h3&gt;&lt;p&gt;To manage all these complex tasks from building all upstream packages from source, we have built a workflow system called Sieve. Whenever we see any new version of a Debian package, we start a new build. We build packages in package groups, to take into account separate packages that need to be upgraded together. Once the whole group has been built, we run a virtualized test suite to make sure none of our core components and developer workflows are broken. Each group is tested separately with a full system installation, boot and local test suite run on that version of the operating system. While builds for individual packages usually complete within minutes, these tests can take up to an hour given the complexity of the package group.&lt;/p&gt;&lt;p&gt;Once the packages are built and all the tests passed, we merge all the new packages with our latest pool of packages. When we cut a new release, we snapshot that pool with each package version locked in for that release. We then proceed to carefully guide this release to the fleet utilizing SRE principles like incremental canarying and monitoring the fleet health.&amp;#160;&lt;/p&gt;&lt;p&gt;But not all builds succeed on the first attempt. If a package fails to build, we usually check for any known bugs with the Debian bug tracker and potentially report it, should it not be known already. Sometimes our release engineers have to become creative and apply local workarounds/patches to get a package to build within our ecosystem and later on drop those workarounds once upstream has released a fix.&lt;/p&gt;&lt;p&gt;One issue that we&#39;ve run into a few times, for example, is that in upstream Debian, packages are usually built in Debian unstable. After a few days, these already built packages migrate to Debian testing. In some cases it&#39;s possible, however, that a build-dependency is stuck in unstable and thus building within testing might not (yet) be feasible. We generally try to work upstream first in these cases so we reduce the complexity and maintenance burden to keep these local patches, while also giving back to the community.&amp;#160;&lt;/p&gt;&lt;p&gt;If any of the steps fail, Sieve has a toolbox of tricks to retry builds. For example, when it starts the initial build of a group of packages, the system makes an educated guess of which dependencies need to be built together. But sometimes the version information provided in Debian source packages can be incomplete and this guess is wrong. For this reason, Sieve periodically retries building groups that failed. As the latest snapshot of our packages is a moving target, it could happen that after a seemingly independent package group gets added to the snapshot, a previously broken group unexpectedly builds and passes tests correctly. All these workflows are mostly automatic and this highlights the importance of thinking as an SRE in this field. When facing a failure, it usually seems easier to just fix a failing build once, but if we need to apply the same workaround over and over, putting the workaround in code will reduce the overall burden put on our engineers.&lt;/p&gt;&lt;p&gt;There are also some security benefits to building all of our binaries from source and having additional source code provenance that verifies the origin of the running binary. During a security incident for example, we are able to rebuild quickly and have confidence in the build working with a temporary patch, as we have been building all packages before, that land in our distribution. Additionally, we also reduce the trust envelope that we have to place into upstream Debian and the binary build artifacts produced by their infrastructure. Instead once the source code is ingested and the binary built verifiably, we can cryptographically attest that the running binary originated from exactly that source code.&lt;/p&gt;&lt;h3&gt;Upgrading to Rodete&lt;/h3&gt;&lt;p&gt;The last Goobuntu release was based on Ubuntu 14.04 LTS (Codename Trusty). Development on Rodete started in 2015 and it was quickly clear that we couldn&amp;#8217;t just drop support for Trusty and require the entire engineering population to install a fresh new distribution. From the previous experience of updating in-place between LTS versions, we already had some good experience of knowing what awaited us with this migration. Because Ubuntu is a derivative from Debian and uses a lot of the same packaging infrastructure/formats (apt), it wasn&amp;#8217;t a totally crazy idea to upgrade the fleet from Goobuntu 14.04 to Debian in-place. We reused some parts of our previous in-place upgrade tool, and worked to make it more reliable, by adding more automation and a lot more testing.&lt;/p&gt;&lt;p&gt;To make it easier to create such a tool, test it and maintain it for the duration of the migration, we chose to temporarily freeze gLinux Rodete as a snapshot of Debian testing on a specific date which we call baseline. We can advance this baseline at our own choosing, to balance what packages Sieve ingests. To reduce friction, we intentionally set the baseline of Rodete at the current Debian stable release in 2016 which was much closer to the general state of Ubuntu Trusty. That way we could separate in-place upgrading from Trusty to Debian and major package version changes that happened in Debian at a later date.&amp;#160;&lt;/p&gt;&lt;p&gt;In 2017, we started to migrate the machines to Rodete and completed the last in place migrations by the end of 2018. We however still had a baseline of packages which at that point dated almost two years in the past. To catch up with Debian Testing, we started a team wide effort to focus on optimizing Sieve behavior and speed up the time needed to build / test packages. Replaying the upgrades in this incremental fashion and having a moving rolling release target that we control eased the workload for Google engineers and our team.&lt;/p&gt;&lt;p&gt;In early 2019 we started to shut down the last remnants of Goobuntu machines. Our baseline has also advanced to only lag behind by ~250 days which at the time meant we were using most of the package versions that were part of buster. By mid-2020 we finally fully caught up at the same time when Debian bullseye was released. We continue to move ahead our baseline and will probably already be using a similar version of the next Debian Stable release, before its release in mid 2023.&lt;/p&gt;&lt;h3&gt;Reaching Zen&lt;/h3&gt;&lt;p&gt;Today, the life of a gLinux team member looks very different. We have reduced the amount of engineering time and energy required for releases to one on-duty release engineer that rotates among team members. We no longer have a big push to upgrade our entire fleet. No more need for multi stage alpha, betas and GAs for new LTS releases while simultaneously chasing down older machines that still were running Ubuntu Precise or Lucid.&lt;/p&gt;&lt;p&gt;We also dramatically improved our security stance by operating our fleet closer to upstream releases. While Debian provides a good source of security patches for the stable and oldstable tracks, we realized that not every security hole that gets patches, necessarily has a Debian Security Advisory (DSA) or CVE number. Our rolling release schedule makes sure we patch security holes on the entire fleet quickly without compromising on stability, while previously security engineers had to carefully review each DSA and make sure the fix has made it to our fleet.&lt;/p&gt;&lt;p&gt;Our improved testing suite and integration tests with key partner teams that run critical developer systems also yielded a more stable experience using a Linux distribution that provides the latest versions of the Linux Kernel. Our strong longing for automating everything in the pipeline has significantly reduced toil and stress within the team. It is now also possible for us to report bugs and incompatibilities with other library versions while making sure that Google tools work better within the Linux ecosystem.&lt;/p&gt;&lt;p&gt;If you are interested in making rolling releases in your company a success, then consider to balance the needs of the company against upgrade agility. Being in control of our own moving target and baseline has helped to slow down whenever we encountered too many problems and broke any of our team SLOs. Our journey has ultimately reinforced our belief that incremental changes are better manageable than big bang releases.&amp;#160;&lt;/p&gt;&lt;p&gt;If you are able to control the influx of new work and keep that predictable, we have made the experience that our engineers stay happier and are less stressed out. This ultimately lowered the team churn and made sure that we can build expertise instead of dealing with multiple burning fires at the same time.&lt;/p&gt;&lt;p&gt;In the future, we are planning to work even more closely with upstream Debian and contribute more of our internal patches to maintain the Debian package ecosystem.&lt;/p&gt;" _nghost-c63=""><p><i><sup>Hero image credit: Markus Teich</sup></i></p><p>At Google we run large production fleets that serve Google products like YouTube and Gmail. To support all our employees, including engineers, we also run a sizable corporate fleet with hundreds of thousands of devices across multiple platforms, models, and locations. To let each Googler work in the environment they are most productive in, we operate many OS-platforms including a Linux system. For a long time, our internal facing Linux distribution, Goobuntu, was based off of Ubuntu LTS releases. In 2018 we completed a move to a rolling release model based on Debian.  </p><h3>Upgrade Toil</h3><p>More than 15 years ago, Ubuntu was chosen as the base for the internal Linux distribution, as it was user-friendly, easy to use, and had lots of fancy extras. The Long Term Support (LTS) releases were picked as it was valued that Canonical provided 2+ years of security updates. <br/></p><p>However, this two year release cycle for LTS releases also meant that we had to upgrade every machine in our fleet of over 100.000 devices before the end-of-life date of the OS. The complex nature of workloads run on corporate machines meant that reinstalling and fully customizing machines could be a difficult and time consuming operation. The productivity hit of having all engineers configure their workspace from scratch every two years was not a financially responsible option. <br/></p><p>For each OS cycle, we had a rather large version jump in major packages that could require significant changes to software configuration. To automate this process, we wrote an unattended in-place upgrade tool that took care of a lot of the common case problems. This automation focused approach meant that most of the Google employees didn&#39;t have to manually upgrade their machines by re-installing them and recreating all their configuration. To make this possible, however, we needed to do comprehensive testing of the upgrade process and check that all major packages that had changed kept working (in Ubuntu this could be up to several thousands packages to upgrade between major versions). Sometimes it was hard to provide automation in the cases where deprecations happened and engineers had to make decisions on how to move forward. <br/></p><p>This effort to upgrade our Goobuntu fleet usually took the better part of a year. With a two year support window there was only one year left until we had to go through the same process all over again for the next LTS. This entire process was a huge stress factor for our team, as we got hundreds of bugs with requests for help for corner cases. Once one upgrade was done there was a general sense of being “close to burnout” in the team that we barely could recover from until the next round of updates came about. Running off an LTS version also meant that some bugs encountered by users of our distribution might’ve already been fixed upstream, but those improvements might’ve never been backported to the LTS version.<br/></p><p>There was also a long tail of special-case upgrades that could sometimes drag on for several years. Handling this process was a huge change management challenge to get engineers to upgrade the machines that didn’t work in the automatic process. We got creative when motivating our users to upgrade their machines. Measures ranged from nagging messages on their UI, mails, scheduled reboots and even shutting down the machines, to raise awareness that there were still some machines in dire need of an upgrade. Sometimes this caught machines that people had totally forgotten about, like the one machine under a desk that was running a critical pipeline for something important, as it turned out.<br/></p><h3>Rolling Releases</h3><p>When we designed gLinux Rodete (Rolling Debian Testing), we aimed at removing the two year upgrade cycle and instead spread out the load on the team throughout time. The general move to CI/CD in the industry has shown that smaller incremental changes are easier to control and rollback. Rolling releases with Linux distributions today are getting more common (Arch Linux, NixOS). </p><p>We considered going with other Linux distributions, but ended up choosing Debian because we again wanted to offer a smooth in-place migration. This included considerations towards the availability of packages in Debian, the large Debian community, and also the existing internal packages and tooling that were using the Debian format. While the Debian Stable track follows a roughly two-year jump between releases, the Debian testing track works as a rolling release, as it&#39;s the pool of all packages ingested and built from upstream, waiting for the next stable release to happen. </p><p>The time from upstream release to availability in testing is often just a few days (although during freeze periods before a Debian stable release, it can sometimes lag a few months behind). This means we can get much more granular changes in general and provide the newest software to our engineers at Google without having to wait longer periods.</p><p>This frequency of updates required us to redesign a lot of systems and processes. While originally intending more frequent releases, we found that for us, weekly releases were a sweet spot between moving quickly and allowing for proper release qualification, limiting the disruption to developer productivity.</p><p>Whenever we start a new release,  we take a snapshot of all the packages ingested from Debian at that time. After some acceptance tests, the new hermetic release candidate is then cautiously rolled out to a dedicated testing fleet and a 1% fleet wide canary. The canary is held intentionally over the course of a couple days to detect any problems with Debian packages or Google internal packages before it progresses to the entire fleet. </p><h3>Introducing Sieve</h3><p>To manage all these complex tasks from building all upstream packages from source, we have built a workflow system called Sieve. Whenever we see any new version of a Debian package, we start a new build. We build packages in package groups, to take into account separate packages that need to be upgraded together. Once the whole group has been built, we run a virtualized test suite to make sure none of our core components and developer workflows are broken. Each group is tested separately with a full system installation, boot and local test suite run on that version of the operating system. While builds for individual packages usually complete within minutes, these tests can take up to an hour given the complexity of the package group.</p><p>Once the packages are built and all the tests passed, we merge all the new packages with our latest pool of packages. When we cut a new release, we snapshot that pool with each package version locked in for that release. We then proceed to carefully guide this release to the fleet utilizing SRE principles like incremental canarying and monitoring the fleet health. </p><p>But not all builds succeed on the first attempt. If a package fails to build, we usually check for any known bugs with the Debian bug tracker and potentially report it, should it not be known already. Sometimes our release engineers have to become creative and apply local workarounds/patches to get a package to build within our ecosystem and later on drop those workarounds once upstream has released a fix.</p><p>One issue that we&#39;ve run into a few times, for example, is that in upstream Debian, packages are usually built in Debian unstable. After a few days, these already built packages migrate to Debian testing. In some cases it&#39;s possible, however, that a build-dependency is stuck in unstable and thus building within testing might not (yet) be feasible. We generally try to work upstream first in these cases so we reduce the complexity and maintenance burden to keep these local patches, while also giving back to the community. </p><p>If any of the steps fail, Sieve has a toolbox of tricks to retry builds. For example, when it starts the initial build of a group of packages, the system makes an educated guess of which dependencies need to be built together. But sometimes the version information provided in Debian source packages can be incomplete and this guess is wrong. For this reason, Sieve periodically retries building groups that failed. As the latest snapshot of our packages is a moving target, it could happen that after a seemingly independent package group gets added to the snapshot, a previously broken group unexpectedly builds and passes tests correctly. All these workflows are mostly automatic and this highlights the importance of thinking as an SRE in this field. When facing a failure, it usually seems easier to just fix a failing build once, but if we need to apply the same workaround over and over, putting the workaround in code will reduce the overall burden put on our engineers.</p><p>There are also some security benefits to building all of our binaries from source and having additional source code provenance that verifies the origin of the running binary. During a security incident for example, we are able to rebuild quickly and have confidence in the build working with a temporary patch, as we have been building all packages before, that land in our distribution. Additionally, we also reduce the trust envelope that we have to place into upstream Debian and the binary build artifacts produced by their infrastructure. Instead once the source code is ingested and the binary built verifiably, we can cryptographically attest that the running binary originated from exactly that source code.</p><h3>Upgrading to Rodete</h3><p>The last Goobuntu release was based on Ubuntu 14.04 LTS (Codename Trusty). Development on Rodete started in 2015 and it was quickly clear that we couldn’t just drop support for Trusty and require the entire engineering population to install a fresh new distribution. From the previous experience of updating in-place between LTS versions, we already had some good experience of knowing what awaited us with this migration. Because Ubuntu is a derivative from Debian and uses a lot of the same packaging infrastructure/formats (apt), it wasn’t a totally crazy idea to upgrade the fleet from Goobuntu 14.04 to Debian in-place. We reused some parts of our previous in-place upgrade tool, and worked to make it more reliable, by adding more automation and a lot more testing.</p><p>To make it easier to create such a tool, test it and maintain it for the duration of the migration, we chose to temporarily freeze gLinux Rodete as a snapshot of Debian testing on a specific date which we call baseline. We can advance this baseline at our own choosing, to balance what packages Sieve ingests. To reduce friction, we intentionally set the baseline of Rodete at the current Debian stable release in 2016 which was much closer to the general state of Ubuntu Trusty. That way we could separate in-place upgrading from Trusty to Debian and major package version changes that happened in Debian at a later date. </p><p>In 2017, we started to migrate the machines to Rodete and completed the last in place migrations by the end of 2018. We however still had a baseline of packages which at that point dated almost two years in the past. To catch up with Debian Testing, we started a team wide effort to focus on optimizing Sieve behavior and speed up the time needed to build / test packages. Replaying the upgrades in this incremental fashion and having a moving rolling release target that we control eased the workload for Google engineers and our team.</p><p>In early 2019 we started to shut down the last remnants of Goobuntu machines. Our baseline has also advanced to only lag behind by ~250 days which at the time meant we were using most of the package versions that were part of buster. By mid-2020 we finally fully caught up at the same time when Debian bullseye was released. We continue to move ahead our baseline and will probably already be using a similar version of the next Debian Stable release, before its release in mid 2023.</p><h3>Reaching Zen</h3><p>Today, the life of a gLinux team member looks very different. We have reduced the amount of engineering time and energy required for releases to one on-duty release engineer that rotates among team members. We no longer have a big push to upgrade our entire fleet. No more need for multi stage alpha, betas and GAs for new LTS releases while simultaneously chasing down older machines that still were running Ubuntu Precise or Lucid.</p><p>We also dramatically improved our security stance by operating our fleet closer to upstream releases. While Debian provides a good source of security patches for the stable and oldstable tracks, we realized that not every security hole that gets patches, necessarily has a Debian Security Advisory (DSA) or CVE number. Our rolling release schedule makes sure we patch security holes on the entire fleet quickly without compromising on stability, while previously security engineers had to carefully review each DSA and make sure the fix has made it to our fleet.</p><p>Our improved testing suite and integration tests with key partner teams that run critical developer systems also yielded a more stable experience using a Linux distribution that provides the latest versions of the Linux Kernel. Our strong longing for automating everything in the pipeline has significantly reduced toil and stress within the team. It is now also possible for us to report bugs and incompatibilities with other library versions while making sure that Google tools work better within the Linux ecosystem.</p><p>If you are interested in making rolling releases in your company a success, then consider to balance the needs of the company against upgrade agility. Being in control of our own moving target and baseline has helped to slow down whenever we encountered too many problems and broke any of our team SLOs. Our journey has ultimately reinforced our belief that incremental changes are better manageable than big bang releases. </p><p>If you are able to control the influx of new work and keep that predictable, we have made the experience that our engineers stay happier and are less stressed out. This ultimately lowered the team churn and made sure that we can build expertise instead of dealing with multiple burning fires at the same time.</p><p>In the future, we are planning to work even more closely with upstream Debian and contribute more of our internal patches to maintain the Debian package ecosystem.</p></div></div>]]></content:encoded>
      <author>&lt;name&gt;Sven Mueller&lt;/name&gt;&lt;title&gt;&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/rodete-hero.max-800x800.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 12 Jul 2022 12:00:00 +0000</pubDate>
    </item>
    <item>
      <title>More support for structured logs in new version of Go logging library</title>
      <link>https://cloud.google.com/blog/products/devops-sre/more-support-for-structured-logs-in-new-version-of-go-logging-library/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The new version of the Google logging client library for Go has been released. Version 1.5 adds new features and bug fixes including new structured logging capabilities that complete last year&#39;s effort to enrich structured logging support in Google &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/libraries#client-libraries-install-go&#34;&gt;logging client libraries&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Here are few of the new features in v1.5:&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Faster and more robust way&lt;/b&gt; to detect and capture Google Cloud resources that the application is running on.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Automatic source location detection&lt;/b&gt; to support log observability for debugging and troubleshooting.&lt;/li&gt;&lt;li&gt;&lt;b&gt;W3C header&lt;/b&gt; &lt;a href=&#34;https://www.w3.org/TR/trace-context/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;traceparent&lt;/code&gt;&lt;/a&gt; for capturing tracing information within the logged entries.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Better control over batched ingestion&lt;/b&gt; of the log entries by supporting the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/entries/write#body.request_body.FIELDS.partial_success&#34;&gt;&lt;code&gt;partialSuccess&lt;/code&gt;&lt;/a&gt; flag within Logger instances.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Support for out-of-process ingestion&lt;/b&gt; with redirection of the logs to &lt;code&gt;stdout&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt; using a structured logging format.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Let&#39;s look into each closer:&lt;/p&gt;&lt;h3&gt;Resource detection&lt;/h3&gt;&lt;p&gt;Resource detection is an existing feature of the logging library. It detects a resource on which an application is running. Retrieves the resource&#39;s metadata. And implicitly adds this metadata to each log entry the application ingests using the library. It is especially useful for applications that run on Google Cloud since it collects a lot of resource&#39;s attributes from the &lt;a href=&#34;https://cloud.google.com/compute/docs/metadata/overview&#34;&gt;Metadata server&lt;/a&gt; of the resource. These attributes enrich ingested logs with additional information such as a location of the VM, a name of the container or a service Id of the AppEngine service. The below Json shows a sample of the retrieved information after detecting the resource as a GKE container and retrieving resource metadata according to the &lt;a href=&#34;https://cloud.google.com/monitoring/api/resources#tag_k8s_container&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;{\r\n &#34;type&#34;: &#34;k8s_container&#34;,\r\n &#34;labels&#34;: {\r\n &#34;project_id&#34;: &#34;dev-env-060122&#34;,\r\n &#34;location&#34;: &#34;us-central1-a&#34;,\r\n &#34;cluster_name&#34;: &#34;dev-test-cluster-47fg&#34;,\r\n &#34;namespace_name&#34;: &#34;default&#34;,\r\n &#34;pod_name&#34; : &#34;frontend-4fgd4&#34;,\r\n &#34;container_name&#34;: &#34;frontend-4fgd4-acgf12a5&#34;\r\n }\r\n}&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4c9ad10a50&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The implementation is optimized to avoid performance degradation during the data collection process. Previously, the heuristic for identifying the resource was heavily based on environment variables which could result in many false positives. Additionally, the implementation performed too many queries to the metadata server which could sometimes cause delayed responses. In the 1.5 release the heuristic was updated to use additional artifacts beside the environment variables in the resource detection logic and the number of the queries to the metadata server was reduced to a bare minimum. As a result, false detection of GCP resources is decreased by an order of magnitude and the performance penalties to run the heuristic in non-GCP resources is decreased as well. The change does not affect the ingestion process and does not require any changes in the application&#39;s code.&lt;/p&gt;&lt;h3&gt;Source location capturing&lt;/h3&gt;&lt;p&gt;It is useful to capture the location in code where the log was ingested. While the main usage is in troubleshooting and debugging it can be useful in other circumstances. In this version of the library you can configure your logger instance to capture the source location &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation&#34;&gt;metadata&lt;/a&gt; for each log entry ingested using &lt;a href=&#34;https://pkg.go.dev/cloud.google.com/go/logging#Logger.Log&#34;&gt;&lt;code&gt;Logger.Log()&lt;/code&gt;&lt;/a&gt; or &lt;a href=&#34;https://pkg.go.dev/cloud.google.com/go/logging#Logger.LogSync&#34;&gt;&lt;code&gt;Logger.LogSync()&lt;/code&gt;&lt;/a&gt; functions. Just pass the output of the &lt;a href=&#34;https://pkg.go.dev/cloud.google.com/go/logging#SourceLocationPopulation&#34;&gt;&lt;code&gt;SourceLocationPopulation()&lt;/code&gt;&lt;/a&gt; as a &lt;code&gt;LoggerOption&lt;/code&gt; argument in the call to &lt;a href=&#34;https://pkg.go.dev/cloud.google.com/go/logging#Client.Logger&#34;&gt;&lt;code&gt;Client.Logger()&lt;/code&gt;&lt;/a&gt; when creating a new instance of the logger. The following snippet creates a logger instance that adds source location metadata into each ingested log with severity set to Debug:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;logger := client.Logger(&#34;debug-logger&#34;,\r\n logging.SourceLocationPopulation(PopulateSourceLocationForDebugEntries))&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee876f50&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The function SourceLocationPopulation() accepts the following constants:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;logging.DoNotPopulateSourceLocation&lt;/code&gt; ‒ is a default configuration that prevents capturing the source location in the ingested logs&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;logging.PopulateSourceLocationForDebugEntries&lt;/code&gt; ‒ adds the source location metadata into logs with Debug severity.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;logging.AlwaysPopulateSourceLocation&lt;/code&gt; ‒ populates the source location in all ingested logs.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This feature has to be enabled explicitly because the operation of capturing the source location in Go may increase the total execution time of the log ingestion by a factor of 2. It is strongly discouraged to enable it for all ingested logs.&lt;/p&gt;&lt;h3&gt;Use W3C context header for tracing&lt;/h3&gt;&lt;p&gt;You could add tracing information with your logs in the previous versions of the library. The way to do it was directly, by providing trace and span identification and, optionally, the sampling flag. The following code demonstrates the manual setting of the trace and span identifiers:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;logger := client.Logger(&#34;my-log&#34;)\r\n// \u2026\r\nlogger.Log(\r\n logging.Entry{\r\n Payload: &#34;keep tracing&#34;,\r\n Trace: &#34;4bf92f3577b34da6a3ce929d0e0e4736&#34;,\r\n SpanID: &#34;00f067aa0ba902b7&#34;,\r\n })&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee8fa650&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Or indirectly, by passing an instance of the &lt;code&gt;http.Request&lt;/code&gt; as a part of the Http request metadata:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;logger := client.Logger(&#34;my-log&#34;)\r\n// \u2026\r\nfunc MyHandler(w http.ResponseWriter, r *http.Request) {\r\n logger.log(\r\n logging.Entry{\r\n Payload: &#34;My handler invoked&#34;,\r\n HttpRequest: &amp;amp;logging.HttpRequest{\r\n Request: r,\r\n },\r\n })\r\n}&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee8fa110&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In the latter case, the library will try to pull tracing information from the &lt;code&gt;x-cloud-tracing-context&lt;/code&gt; header. From this release, the library also supports W3C tracing context header. If both headers are present, the tracing information is captured from the W3C traceparent header.&lt;/p&gt;&lt;h3&gt;Out-of-process logs&#39; ingestion&lt;/h3&gt;&lt;p&gt;By default the library supports synchronous and asynchronous log ingestions by calling the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rpc/google.logging.v2#google.logging.v2.WriteLogEntriesRequest&#34;&gt;Cloud Logging API&lt;/a&gt; directly. In certain cases the log ingestion is better to be done using external &lt;a href=&#34;https://cloud.google.com/logging/docs/agent&#34;&gt;logging agents&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/run/docs/logging#a_note_about_logging_agents&#34;&gt;built-in support&lt;/a&gt; for logs collection. In this release, you can configure a logger instance to write logs to stdout or stderr instead of ingesting it to Cloud Logging directly. The following example creates a logger that redirects logs to stdout using &lt;a href=&#34;https://cloud.google.com/logging/docs/structured-logging#special-payload-fields&#34;&gt;specially formatted&lt;/a&gt; Json string:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;logger := client.Logger(&#34;not-ingesting-log&#34;, RedirectAsJSON(os.Stdout)\r\nlogger.Log(logging.Entry{Severity: logging.Debug, Payload: &#34;out of process log&#34;})&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee3a1c10&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The above code will print something like the following line to the standard output:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;{&#34;message&#34;:&#34;out of process log&#34;, &#34;severity&#34;:&#34;DEBUG&#34;, &#34;timestamp&#34;:&#34;seconds:1656381253&#34;}&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee3a1310&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In some circumstances, when the standard output cannot be used for printing logs, the logger can be configured to redirect output to the standard error (&lt;code&gt;os.Stderr&lt;/code&gt;) with the same effect.&lt;/p&gt;&lt;p&gt;There are a couple of things to be aware of when you use the out-of-process logging:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;Methods Logger.Log()&lt;/code&gt; and &lt;code&gt;Logger.LogSync()&lt;/code&gt; behave the same way when the logger is configured with the out-of-process logging option. They write the Jsonified logs to the provided io.Write writer. And an external logging agent determines the logs&#39; collection and ingestion.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;You do not have control over the Log ID. All logs that are ingested by the logging agent or the built-in support of the managed service (e.g. Cloud Run) will use the Log ID that is determined out-of-process.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;More control over batch ingestion&lt;/h3&gt;&lt;p&gt;When you ingest logs using Logger.Log() function, the asynchronous ingestion batches multiple log entries together and ingest them using the &lt;code&gt;entries.write&lt;/code&gt; &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/entries/write&#34;&gt;Logging API&lt;/a&gt;. If the ingestion of any of the aggregate logs fails, no logs get ingested. Starting with this release you can control this logic by opting in the partial success &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/entries/write#body.request_body.FIELDS.partial_success&#34;&gt;flag&lt;/a&gt;. When the flag is set, the Logging API tries to ingest all logs, even if some other log entry fails due to a permanent error such as INVALID_ARGUMENT or PERMISSION_DENIED. This option can be opted-in when creating a new logger using the &lt;code&gt;PartialSuccess&lt;/code&gt; logger option:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;logger := client.Logger(&#34;my-log&#34;, PartialSuccess())&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee3a1d10&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Wrapping up&lt;/h3&gt;&lt;p&gt;When you upgrade to version 1.5 you get a more robust and deterministic resource detection algorithm while keeping the behavior of the library unchanged. Additional functionality such as out-of-process ingestion, source location or batch ingestion control can be opted-in using the logger options. With these new features and fixes the behavior of the library becomes more deterministic and robust. &lt;/p&gt;&lt;p&gt;Learn more about the release at &lt;a href=&#34;https://pkg.go.dev/cloud.google.com/go/logging&#34;&gt;go.pkg.dev&lt;/a&gt;. Please also visit the library&#39;s project on &lt;a href=&#34;https://github.com/googleapis/google-cloud-go/tree/main/logging&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-logging-python-client-library-v3-0-0-release/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/logging.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Getting Started with Google Cloud Logging Python v3.0.0&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Learn how to manage your app&#39;s Python logs and related metadata using Google Cloud client libraries.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Leonid Yankulin&lt;/name&gt;&lt;title&gt;Developer Relations Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/logging_nZNFoFp.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 01 Jul 2022 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Cloud Monitoring metrics, now in Managed Service for Prometheus</title>
      <link>https://cloud.google.com/blog/products/devops-sre/promql-for-cloud-monitoring-metrics-now-available/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;According to a recent CNCF survey, &lt;a href=&#34;https://www.cncf.io/blog/2022/03/08/cloud-native-observability-microsurvey-prometheus-leads-the-way-but-hurdles-remain-to-understanding-the-health-of-systems/#:~:text=According%20to%20the%20survey%20report,%2C%20and%20Fluentd%20with%2046%25.&#34; target=&#34;_blank&#34;&gt;86% of the cloud native community reports that they use Prometheus for observability&lt;/a&gt;. As Prometheus becomes more of a standard, an increasing number of developers are becoming fluent in &lt;a href=&#34;https://prometheus.io/docs/prometheus/latest/querying/basics/&#34; target=&#34;_blank&#34;&gt;PromQL&lt;/a&gt;, Prometheus’ built-in query language. While it is a powerful, flexible, and expressive query language, PromQL is typically only able to query Prometheus time series data. Other sources of telemetry, such as metrics offered by your Cloud provider or metrics generated from logs, remain isolated in separate products and might require developers to learn new query tools in order to access them.&lt;/p&gt;&lt;h3&gt;Introducing PromQL for Google Cloud Monitoring metrics&lt;/h3&gt;&lt;p&gt;Prometheus metrics alone aren’t enough to get a single pane of glass view of your Cloud footprint. Cloud Monitoring provides &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_gcp&#34;&gt;over 1,000 free metrics&lt;/a&gt; that let you monitor and alert on your usage of Google Cloud services, including metrics for Compute Engine, Kubernetes Engine, Load Balancing, BigQuery, Cloud Storage, Pub/Sub, and more. We’re excited to announce that you can now &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/promql&#34;&gt;query all Cloud Monitoring metrics using PromQL and Managed Service for Prometheus&lt;/a&gt;, including &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_gcp&#34;&gt;Google Cloud system metrics&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_kubernetes&#34;&gt;Kubernetes metrics&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/logging/docs/logs-based-metrics&#34;&gt;log-based metrics&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/monitoring/custom-metrics?hl=en&#34;&gt;custom metrics&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;grafana system metrics.gif&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/grafana_system_metrics.gif&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Google Cloud metrics appear within Grafana and can be queried using PromQL.&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Because we built Managed Service for Prometheus on top of the &lt;a href=&#34;https://research.google/pubs/pub50652/&#34; target=&#34;_blank&#34;&gt;same planet-scale time series database&lt;/a&gt; as Cloud Monitoring, all your metrics are stored together and are queryable together. Metrics in Cloud Monitoring are automatically generated when you use Google Cloud services at no additional cost to you. View all your metrics in one place with the query language that developers already know and prefer, opening up possibilities such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Correlating spikes in traffic with Redis cache misses using &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_gcp#gcp-loadbalancing&#34;&gt;Cloud Load Balancing metrics&lt;/a&gt; and Prometheus’ &lt;a href=&#34;https://github.com/oliver006/redis_exporter&#34; target=&#34;_blank&#34;&gt;Redis exporter&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Graphing Cloud Logging’s &lt;a href=&#34;https://cloud.google.com/logging/docs/logs-based-metrics&#34;&gt;logs-based metrics&lt;/a&gt; alongside Prometheus metrics&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Alerting on your &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_gcp#gcp-compute&#34;&gt;Compute Engine utilization&lt;/a&gt; or your &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_gcp#gcp-pubsub&#34;&gt;Pub/Sub backlog size&lt;/a&gt; using PromQL and &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/rules-managed&#34;&gt;Managed Service for Prometheus’ rule evaluation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Substituting paid Istio metrics for their &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_istio&#34;&gt;free Google Cloud Istio or Anthos Service Mesh equivalent&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Exposing these metrics using PromQL means that developers who are familiar with Prometheus can start using all time series telemetry data without first having to learn a new query language. New members of your operations team can ramp up faster, as many industry hires will already be familiar with PromQL from previous experience.&lt;/p&gt;&lt;h3&gt;Why Managed Service for Prometheus&lt;/h3&gt;&lt;p&gt;In addition to PromQL for all metrics, Managed Service for Prometheus offers open-source monitoring combined with the scale and reliability of Google services. Additional benefits include: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#gmp-outside-gke&#34;&gt;Hybrid- and multi-cloud support&lt;/a&gt;, so you can centralize all your metrics across clouds and on-prem deployments&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Two-year retention of all Prometheus metrics, included in the price&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cost-effective monitoring on a per-sample basis&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Easy &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources&#34;&gt;cost identification and attribution&lt;/a&gt; using Cloud Monitoring&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Your choice of collection, with &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed&#34;&gt;managed collection&lt;/a&gt; for those who want a completely hands-off Prometheus experience and &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged&#34;&gt;self-deployed collection&lt;/a&gt; for those who want to keep using existing Prometheus configs&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;How to get started&lt;/h3&gt;&lt;p&gt;You can query Cloud Monitoring metrics with PromQL by using the &lt;a href=&#34;https://console.cloud.google.com/monitoring/prometheus&#34;&gt;interactive query page in Cloud Console&lt;/a&gt; or Grafana. To learn how to write PromQL for Google Cloud metrics, see &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/promql&#34;&gt;Mapping Cloud Monitoring metric names to PromQL&lt;/a&gt;. To configure a Grafana data source that can read all your metrics in Cloud Monitoring, see &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/query&#34;&gt;Configure a query user interface&lt;/a&gt; in the Managed Service for Prometheus documentation.&lt;/p&gt;&lt;p&gt;To query Prometheus data alongside Cloud Monitoring, you have to first get Prometheus data into the system. For instructions on configuring Managed Service for Prometheus ingestion, see &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed&#34;&gt;Get started with managed collection&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Google Cloud Managed Service for Prometheus is now generally available&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Announcing the GA of Google Cloud Managed Service for Prometheus for the collection, storage, and querying of Kubernetes metrics.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;Because we built Managed Service for Prometheus on top of the &lt;a href=&#34;https://research.google/pubs/pub50652/&#34; target=&#34;_blank&#34;&gt;same planet-scale time series database&lt;/a&gt; as Cloud Monitoring, all your metrics are stored together and are queryable together. Metrics in Cloud Monitoring are automatically generated when you use Google Cloud services at no additional cost to you. View all your metrics in one place with the query language that developers already know and prefer, opening up possibilities such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Correlating spikes in traffic with Redis cache misses using &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_gcp#gcp-loadbalancing&#34;&gt;Cloud Load Balancing metrics&lt;/a&gt; and Prometheus&amp;#8217; &lt;a href=&#34;https://github.com/oliver006/redis_exporter&#34; target=&#34;_blank&#34;&gt;Redis exporter&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Graphing Cloud Logging&amp;#8217;s &lt;a href=&#34;https://cloud.google.com/logging/docs/logs-based-metrics&#34;&gt;logs-based metrics&lt;/a&gt; alongside Prometheus metrics&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Alerting on your &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_gcp#gcp-compute&#34;&gt;Compute Engine utilization&lt;/a&gt; or your &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_gcp#gcp-pubsub&#34;&gt;Pub/Sub backlog size&lt;/a&gt; using PromQL and &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/rules-managed&#34;&gt;Managed Service for Prometheus&amp;#8217; rule evaluation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Substituting paid Istio metrics for their &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_istio&#34;&gt;free Google Cloud Istio or Anthos Service Mesh equivalent&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Exposing these metrics using PromQL means that developers who are familiar with Prometheus can start using all time series telemetry data without first having to learn a new query language. New members of your operations team can ramp up faster, as many industry hires will already be familiar with PromQL from previous experience.&lt;/p&gt;&lt;h3&gt;Why Managed Service for Prometheus&lt;/h3&gt;&lt;p&gt;In addition to PromQL for all metrics, Managed Service for Prometheus offers open-source monitoring combined with the scale and reliability of Google services. Additional benefits include:&amp;#160;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#gmp-outside-gke&#34;&gt;Hybrid- and multi-cloud support&lt;/a&gt;, so you can centralize all your metrics across clouds and on-prem deployments&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Two-year retention of all Prometheus metrics, included in the price&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cost-effective monitoring on a per-sample basis&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Easy &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources&#34;&gt;cost identification and attribution&lt;/a&gt; using Cloud Monitoring&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Your choice of collection, with &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed&#34;&gt;managed collection&lt;/a&gt; for those who want a completely hands-off Prometheus experience and &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged&#34;&gt;self-deployed collection&lt;/a&gt; for those who want to keep using existing Prometheus configs&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;How to get started&lt;/h3&gt;&lt;p&gt;You can query Cloud Monitoring metrics with PromQL by using the &lt;a href=&#34;https://console.cloud.google.com/monitoring/prometheus&#34;&gt;interactive query page in Cloud Console&lt;/a&gt; or Grafana. To learn how to write PromQL for Google Cloud metrics, see &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/promql&#34;&gt;Mapping Cloud Monitoring metric names to PromQL&lt;/a&gt;. To configure a Grafana data source that can read all your metrics in Cloud Monitoring, see &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/query&#34;&gt;Configure a query user interface&lt;/a&gt; in the Managed Service for Prometheus documentation.&lt;/p&gt;&lt;p&gt;To query Prometheus data alongside Cloud Monitoring, you have to first get Prometheus data into the system. For instructions on configuring Managed Service for Prometheus ingestion, see &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed&#34;&gt;Get started with managed collection&lt;/a&gt;.&lt;/p&gt;"><p>Because we built Managed Service for Prometheus on top of the <a href="https://research.google/pubs/pub50652/" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://research.google" track-metadata-module="post">same planet-scale time series database</a> as Cloud Monitoring, all your metrics are stored together and are queryable together. Metrics in Cloud Monitoring are automatically generated when you use Google Cloud services at no additional cost to you. View all your metrics in one place with the query language that developers already know and prefer, opening up possibilities such as:</p><ul><li><p>Correlating spikes in traffic with Redis cache misses using <a href="https://cloud.google.com/monitoring/api/metrics_gcp#gcp-loadbalancing" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/monitoring/api/metrics_gcp#gcp-loadbalancing" track-metadata-module="post">Cloud Load Balancing metrics</a> and Prometheus’ <a href="https://github.com/oliver006/redis_exporter" target="_blank" track-type="inline link" track-name="11" track-metadata-eventdetail="https://github.com" track-metadata-module="post">Redis exporter</a></p></li><li><p>Graphing Cloud Logging’s <a href="https://cloud.google.com/logging/docs/logs-based-metrics" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/logging/docs/logs-based-metrics" track-metadata-module="post">logs-based metrics</a> alongside Prometheus metrics</p></li><li><p>Alerting on your <a href="https://cloud.google.com/monitoring/api/metrics_gcp#gcp-compute" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/monitoring/api/metrics_gcp#gcp-compute" track-metadata-module="post">Compute Engine utilization</a> or your <a href="https://cloud.google.com/monitoring/api/metrics_gcp#gcp-pubsub" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/monitoring/api/metrics_gcp#gcp-pubsub" track-metadata-module="post">Pub/Sub backlog size</a> using PromQL and <a href="https://cloud.google.com/stackdriver/docs/managed-prometheus/rules-managed" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/managed-prometheus/rules-managed" track-metadata-module="post">Managed Service for Prometheus’ rule evaluation</a></p></li><li><p>Substituting paid Istio metrics for their <a href="https://cloud.google.com/monitoring/api/metrics_istio" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/monitoring/api/metrics_istio" track-metadata-module="post">free Google Cloud Istio or Anthos Service Mesh equivalent</a></p></li></ul><p>Exposing these metrics using PromQL means that developers who are familiar with Prometheus can start using all time series telemetry data without first having to learn a new query language. New members of your operations team can ramp up faster, as many industry hires will already be familiar with PromQL from previous experience.</p><h3>Why Managed Service for Prometheus</h3><p>In addition to PromQL for all metrics, Managed Service for Prometheus offers open-source monitoring combined with the scale and reliability of Google services. Additional benefits include: </p><ul><li><p><a href="https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#gmp-outside-gke" track-type="inline link" track-name="17" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#gmp-outside-gke" track-metadata-module="post">Hybrid- and multi-cloud support</a>, so you can centralize all your metrics across clouds and on-prem deployments</p></li><li><p>Two-year retention of all Prometheus metrics, included in the price</p></li><li><p>Cost-effective monitoring on a per-sample basis</p></li><li><p>Easy <a href="https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources" track-metadata-module="post">cost identification and attribution</a> using Cloud Monitoring</p></li><li><p>Your choice of collection, with <a href="https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed" track-type="inline link" track-name="19" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed" track-metadata-module="post">managed collection</a> for those who want a completely hands-off Prometheus experience and <a href="https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged" track-type="inline link" track-name="20" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged" track-metadata-module="post">self-deployed collection</a> for those who want to keep using existing Prometheus configs</p></li></ul><h3>How to get started</h3><p>You can query Cloud Monitoring metrics with PromQL by using the <a href="https://console.cloud.google.com/monitoring/prometheus" track-type="inline link" track-name="21" track-metadata-eventdetail="https://console.cloud.google.com/monitoring/prometheus" track-metadata-module="post">interactive query page in Cloud Console</a> or Grafana. To learn how to write PromQL for Google Cloud metrics, see <a href="https://cloud.google.com/stackdriver/docs/managed-prometheus/promql" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/managed-prometheus/promql" track-metadata-module="post">Mapping Cloud Monitoring metric names to PromQL</a>. To configure a Grafana data source that can read all your metrics in Cloud Monitoring, see <a href="https://cloud.google.com/stackdriver/docs/managed-prometheus/query" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/managed-prometheus/query" track-metadata-module="post">Configure a query user interface</a> in the Managed Service for Prometheus documentation.</p><p>To query Prometheus data alongside Cloud Monitoring, you have to first get Prometheus data into the system. For instructions on configuring Managed Service for Prometheus ingestion, see <a href="https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed" track-metadata-module="post">Get started with managed collection</a>.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Lee Yanco&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 30 Jun 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Query Insights for Cloud Spanner: troubleshoot performance issues with pre-built dashboards</title>
      <link>https://cloud.google.com/blog/products/databases/diagnose-query-performance-issues-with-cloud-spanner-query-insights/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Today, application development teams are more agile and are shipping features faster than ever before. In addition to these rapid development cycles and the rise of microservices architectures, the end-to-end ownership of feature development (and performance monitoring) has moved to a shared responsibility model between advanced database administrators and full-stack developers. However, most developers don’t have the years of experience or the time needed to debug complex query performance issues and database administrators are now a scarce resource in most organizations. As a result, there is a dire need for tools for developers and DBAs alike to quickly diagnose performance issues. &lt;/p&gt;&lt;h3&gt;Introducing Query Insights for Spanner&lt;/h3&gt;&lt;p&gt;We are delighted to announce the launch of &lt;a href=&#34;https://cloud.google.com/spanner/docs/using-query-insights&#34;&gt;Query Insights&lt;/a&gt; for Spanner,  a set of visualization tools that provide an easy way for developers and database administrators to quickly diagnose query performance issues on Spanner. Using Query Insights, users can now troubleshoot query performance in a self-serve way. We’ve designed Query Insights using familiar design patterns with world-class visualizations to provide an intuitive experience for anyone who is debugging issues with query performance on Spanner. Query Insights is available at no additional cost.&lt;/p&gt;&lt;p&gt;By using out-of-the-box visual dashboards and graphs, developers can visualize aberrant behavior like peaks and troughs in various performance metrics over a time-series and quickly identify problematic queries. Time series data provides significant value to organizations because it enables them to analyze important real-time and historical metrics. Data is valuable only if it’s easy to comprehend;. that’s where being able to view intuitive dashboards becomes a force multiplier for organizations looking to expose their time series data across teams.&lt;/p&gt;&lt;h3&gt;Follow a visual journey with pre-built dashboards&lt;/h3&gt;&lt;p&gt;With Query Insights, developers can seamlessly move from detection of database performance issues to diagnosis of problematic queries using a single interface. Query Insights will help identify query performance issues easily with pre-built dashboards. &lt;/p&gt;&lt;p&gt;The user could do this by following a simple journey where they can quickly &lt;b&gt;confirm&lt;/b&gt;, &lt;b&gt;identify&lt;/b&gt; and &lt;b&gt;analyze&lt;/b&gt; query performance issues. Let’s walk through an example scenario. &lt;/p&gt;&lt;h3&gt;Understand database performance&lt;/h3&gt;&lt;p&gt;This journey will start by the user &lt;a href=&#34;https://cloud.google.com/spanner/docs/monitoring-cloud#create-alert&#34;&gt;setting up an alert&lt;/a&gt; on Google Cloud Monitoring for CPU utilization going above a certain threshold. The alert could be configured in a way that if this threshold is crossed, the user will be notified with an email alert, with a link to the “Monitoring” dashboard.&lt;/p&gt;&lt;p&gt;Once the user receives this alert, they would click on the link in the email, and navigate to the “Monitoring” dashboard. If they observe high CPU Utilization and high read latencies, the possible root cause could be expensive queries. A spike in CPU Utilization could be a strong signal that the system is using more compute than it usually would, due to an inefficient query.&lt;/p&gt;&lt;p&gt;The next step is to identify which query might be the problem, this is where Query Insights comes in. The user can get to this tool by clicking on Query Insights in the left navigation of your Spanner Instance. Here, they can drill down into the CPU usage by query and observe that for a specific database, CPU Utilization (attributed to all queries) is spiking for a particular time window. This confirms that the CPU utilization is due to inefficient queries.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Insights.1000063020000552.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Query Insights.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Insights.1000063020000552.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Identifying a problematic query&lt;/h3&gt;&lt;p&gt;The user now observes the &lt;a href=&#34;https://cloud.google.com/spanner/docs/introspection/query-statistics#cpu-by-query&#34;&gt;TopN&lt;/a&gt; (Top queries by CPU Utilization) query graph to see the TopN queries by CPU Utilization. From the graph, it is very easy to visualize and identify the top queries which could be causing the spike in CPU Utilization.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Insights.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 Query Insights.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Insights.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In the above screenshot, we can see that the first query in the table is showing a clear spike at 10:33 PM consuming 48.81% of total CPU. This is  a clear indication that this query could be problematic, and the user should investigate further.&lt;/p&gt;&lt;h3&gt;Analyzing the query performance&lt;/h3&gt;&lt;p&gt;Once they have identified the problematic query, they can now drill down into this query shape to confirm, identify the root cause of the high CPU utilization. &lt;/p&gt;&lt;p&gt;They can do this by clicking on the Fingerprint ID for the specific query from the topN table, and navigating to the Query Details page where they will be able to see a list of metrics (Latency, CPU Utilization, Execution count, Rows Scanned / Rows Returned) over a time series for that specific query.  &lt;/p&gt;&lt;p&gt;In this example, we notice that the average number of rows scanned for this specific query are very high (~ 600k rows scanned to return ~ 12k rows), which could point to a poor query design, resulting in an inefficient query. We can also observe that latency is high (1.4s) for this query.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Query_Insights.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;3 Query Insights.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Query_Insights.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Fixing the issue&lt;/h3&gt;&lt;p&gt;To fix the problem in this scenario, the user could optimize this query by &lt;a href=&#34;https://cloud.google.com/spanner/docs/secondary-indexes#index-directive&#34;&gt;specifying a secondary index&lt;/a&gt; in the query using a FORCE_INDEX query hint to provide an index directive. This would provide more consistent performance, make the query more efficient, and lower CPU utilization for this query.&lt;/p&gt;&lt;p&gt;In the screenshot below, you can see that after specifying the index in the query, the query performance dramatically increases in terms of CPU, rows scanned (54K vs 630k) and also in terms of query latency (536 ns vs 1.4 s).&lt;/p&gt;&lt;p&gt;Unoptimized Query:&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/4_Query_Insights.1000064320000620.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;4 Query Insights.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/4_Query_Insights.1000064320000620.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Optimized Query:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/5_Query_Insights.1000064120000632.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;5 Query Insights.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/5_Query_Insights.1000064120000632.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;By following this simple visual journey, the user can easily detect, diagnose and debug inefficient queries on Spanner.&lt;/p&gt;&lt;h3&gt;Get started with Query Insights today&lt;/h3&gt;&lt;p&gt;To learn more about Query Insights, review the documentation &lt;a href=&#34;https://cloud.google.com/spanner/docs/using-query-insights&#34;&gt;here&lt;/a&gt;. Query Insights is enabled by default. In the Spanner console, you can click on Query Insights in the left navigation and start visualizing your query performance metrics! &lt;/p&gt;&lt;p&gt;New to Spanner? Get started in minutes &lt;a href=&#34;https://cloud.google.com/spanner&#34;&gt;with a new database&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/databases/seeing-into-the-performance-of-cloud-native-database-spanner/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/cloud_spanner.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Improved troubleshooting with Cloud Spanner introspection capabilities&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Cloud-native database Spanner has new introspection capabilities to monitor database performance and optimize application efficiency.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c77=""><div _ngcontent-c77="" innerhtml="&lt;p&gt;Today, application development teams are more agile and are shipping features faster than ever before. In addition to these rapid development cycles and the rise of microservices architectures, the end-to-end ownership of feature development (and performance monitoring) has moved to a shared responsibility model between advanced database administrators and full-stack developers. However, most developers don&amp;#8217;t have the years of experience or the time needed to debug complex query performance issues and database administrators are now a scarce resource in most organizations. As a result, there is a dire need for tools for developers and DBAs alike to quickly diagnose performance issues.&amp;#160;&lt;/p&gt;&lt;h3&gt;Introducing Query Insights for Spanner&lt;/h3&gt;&lt;p&gt;We are delighted to announce the launch of &lt;a href=&#34;https://cloud.google.com/spanner/docs/using-query-insights&#34;&gt;Query Insights&lt;/a&gt; for Spanner,&amp;#160; a set of visualization tools that provide an easy way for developers and database administrators to quickly diagnose query performance issues on Spanner. Using Query Insights, users can now troubleshoot query performance in a self-serve way. We&amp;#8217;ve designed Query Insights using familiar design patterns with world-class visualizations to provide an intuitive experience for anyone who is debugging issues with query performance on Spanner. Query Insights is available at no additional cost.&lt;/p&gt;&lt;p&gt;By using out-of-the-box visual dashboards and graphs, developers can visualize aberrant behavior like peaks and troughs in various performance metrics over a time-series and quickly identify problematic queries. Time series data provides significant value to organizations because it enables them to analyze important real-time and historical metrics. Data is valuable only if it&amp;#8217;s easy to comprehend;. that&amp;#8217;s where being able to view intuitive dashboards becomes a force multiplier for organizations looking to expose their time series data across teams.&lt;/p&gt;&lt;h3&gt;Follow a visual journey with pre-built dashboards&lt;/h3&gt;&lt;p&gt;With Query Insights, developers can seamlessly move from detection of database performance issues to diagnosis of problematic queries using a single interface. Query Insights will help identify query performance issues easily with pre-built dashboards.&amp;#160;&lt;/p&gt;&lt;p&gt;The user could do this by following a simple journey where they can quickly &lt;b&gt;confirm&lt;/b&gt;, &lt;b&gt;identify&lt;/b&gt; and &lt;b&gt;analyze&lt;/b&gt; query performance issues. Let&amp;#8217;s walk through an example scenario.&amp;#160;&lt;/p&gt;&lt;h3&gt;Understand database performance&lt;/h3&gt;&lt;p&gt;This journey will start by the user &lt;a href=&#34;https://cloud.google.com/spanner/docs/monitoring-cloud#create-alert&#34;&gt;setting up an alert&lt;/a&gt; on Google Cloud Monitoring for CPU utilization going above a certain threshold. The alert could be configured in a way that if this threshold is crossed, the user will be notified with an email alert, with a link to the &amp;#8220;Monitoring&amp;#8221; dashboard.&lt;/p&gt;&lt;p&gt;Once the user receives this alert, they would click on the link in the email, and navigate to the &amp;#8220;Monitoring&amp;#8221; dashboard. If they observe high CPU Utilization and high read latencies, the possible root cause could be expensive queries. A spike in CPU Utilization could be a strong signal that the system is using more compute than it usually would, due to an inefficient query.&lt;/p&gt;&lt;p&gt;The next step is to identify which query might be the problem, this is where Query Insights comes in. The user can get to this tool by clicking on Query Insights in the left navigation of your Spanner Instance. Here, they can drill down into the CPU usage by query and observe that for a specific database, CPU Utilization (attributed to all queries) is spiking for a particular time window. This confirms that the CPU utilization is due to inefficient queries.&lt;/p&gt;"><p>Today, application development teams are more agile and are shipping features faster than ever before. In addition to these rapid development cycles and the rise of microservices architectures, the end-to-end ownership of feature development (and performance monitoring) has moved to a shared responsibility model between advanced database administrators and full-stack developers. However, most developers don’t have the years of experience or the time needed to debug complex query performance issues and database administrators are now a scarce resource in most organizations. As a result, there is a dire need for tools for developers and DBAs alike to quickly diagnose performance issues. </p><h3>Introducing Query Insights for Spanner</h3><p>We are delighted to announce the launch of <a href="https://cloud.google.com/spanner/docs/using-query-insights" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/spanner/docs/using-query-insights" track-metadata-module="post">Query Insights</a> for Spanner,  a set of visualization tools that provide an easy way for developers and database administrators to quickly diagnose query performance issues on Spanner. Using Query Insights, users can now troubleshoot query performance in a self-serve way. We’ve designed Query Insights using familiar design patterns with world-class visualizations to provide an intuitive experience for anyone who is debugging issues with query performance on Spanner. Query Insights is available at no additional cost.</p><p>By using out-of-the-box visual dashboards and graphs, developers can visualize aberrant behavior like peaks and troughs in various performance metrics over a time-series and quickly identify problematic queries. Time series data provides significant value to organizations because it enables them to analyze important real-time and historical metrics. Data is valuable only if it’s easy to comprehend;. that’s where being able to view intuitive dashboards becomes a force multiplier for organizations looking to expose their time series data across teams.</p><h3>Follow a visual journey with pre-built dashboards</h3><p>With Query Insights, developers can seamlessly move from detection of database performance issues to diagnosis of problematic queries using a single interface. Query Insights will help identify query performance issues easily with pre-built dashboards. </p><p>The user could do this by following a simple journey where they can quickly <b>confirm</b>, <b>identify</b> and <b>analyze</b> query performance issues. Let’s walk through an example scenario. </p><h3>Understand database performance</h3><p>This journey will start by the user <a href="https://cloud.google.com/spanner/docs/monitoring-cloud#create-alert" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/spanner/docs/monitoring-cloud#create-alert" track-metadata-module="post">setting up an alert</a> on Google Cloud Monitoring for CPU utilization going above a certain threshold. The alert could be configured in a way that if this threshold is crossed, the user will be notified with an email alert, with a link to the “Monitoring” dashboard.</p><p>Once the user receives this alert, they would click on the link in the email, and navigate to the “Monitoring” dashboard. If they observe high CPU Utilization and high read latencies, the possible root cause could be expensive queries. A spike in CPU Utilization could be a strong signal that the system is using more compute than it usually would, due to an inefficient query.</p><p>The next step is to identify which query might be the problem, this is where Query Insights comes in. The user can get to this tool by clicking on Query Insights in the left navigation of your Spanner Instance. Here, they can drill down into the CPU usage by query and observe that for a specific database, CPU Utilization (attributed to all queries) is spiking for a particular time window. This confirms that the CPU utilization is due to inefficient queries.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Mohit Gulati&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 29 Jun 2022 15:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Incorporating quota regression detection into your release pipeline</title>
      <link>https://cloud.google.com/blog/products/devops-sre/ensuring-consistent-api-quota-limits-between-dev-and-prod/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;On Google Cloud, one of the ways an organization may want to enforce fairness in how much of a resource can be consumed is through the use of &lt;a href=&#34;https://cloud.google.com/docs/quota&#34;&gt;quotas&lt;/a&gt;. Limiting resource consumption on services is one way that companies can better manage their cloud costs. Oftentimes, people associate quotas with APIs to access that said resource. Although an endpoint may be able to handle a high number of Queries Per Second (QPS), the quota gives them a means to ensure that no one user or customer has monopoly over the available capacity. This is where fairness comes into play. It allows people to put limits that can be scoped per user or per customer and allows them to increase or lower those limits.&lt;/p&gt;&lt;p&gt;Although quota limits address the issue of fairness from a resource providers’ point of view — in this case, Google Cloud — you still need a way as the resource &lt;i&gt;consumer&lt;/i&gt; to ensure that those limits are adhered to and, just as importantly, ensure that you don’t inadvertently violate those limits. This is especially important in a continuous integration and continuous delivery (CI/CD) environment, where there is so much automation going on. CI/CD is heavily based on automating product releases and you want to ensure that the products released are always stable. This brings us to the issue of quota regression.&lt;/p&gt;&lt;h3&gt;What is quota regression and how can it occur? &lt;/h3&gt;&lt;p&gt;Quota regression refers to the unplanned change in an allocated quota that oftentimes results in a reduced capacity for resource consumption. &lt;/p&gt;&lt;p&gt;Let&#39;s take for example an accountant firm. I have many friends in this sector and they can never hang out with me during their busy season between January and April. At least, that’s the excuse. During the busy season, they have an extraordinarily high caseload, and a low caseload the rest of the year. Let’s assume that these caseloads actually have an immediate impact on your resource costs on Google Cloud. Since this high caseload only occurs at a particular point throughout the year, it may not be necessary to maintain a high quota at all times. It’s not financially prudent since resources are paid on a “per-usage” model. &lt;/p&gt;&lt;p&gt;If the accountant firm has an in-house engineering team that has built load-tests to ensure the system is functioning as intended, you would expect the load capacity to increase before the busy season. If the load test is being done in an environment separate from the serving one (which it should be due to reasons such as security and avoiding unnecessary access grants to data), this is where you might start to see a quota regression. An example of this is load testing in your non-prod Google Cloud project (e.g.&lt;i&gt;your-project-name-nonprod&lt;/i&gt;) and promoting images to your serving project (e.g.&lt;i&gt;your-project-name-prod&lt;/i&gt;).&lt;/p&gt;&lt;p&gt;In order for the load tests to pass, there must be a sufficient quota allocated to the &lt;i&gt;load testing environment&lt;/i&gt;. However, there exists a possibility that that quota has not been granted in the &lt;i&gt;serving environment&lt;/i&gt;. It could be due to simply an oversight in the process where the admin needed to request the additional quota in the serving environment, or it could be because that quota was reverted after a busy season and thus went unnoticed. Whatever the reason, it still depends on human intervention to assert that the quotas are consistent across environments. If this is missed, the firm can go into a busy season with passing load tests and still have a system outage due to lack of quota in the serving environment.&lt;/p&gt;&lt;h3&gt;Why not just use traditional monitoring?&lt;/h3&gt;&lt;p&gt;This brings to mind the argument of “&lt;a href=&#34;https://www.youtube.com/watch?v=x8FNVsbnwWE&#34; target=&#34;_blank&#34;&gt;Security Monitor vs Security Guard&lt;/a&gt;.” Even with monitoring to detect such inconsistencies, alerts can be ignored and alerts can be late. Alerts work if there is no automation tied to the behavior. In the example above, alerts may just suffice. However, in the context of CI/CD, it’s likely for a deployment that introduces a higher QPS on dependencies to be promoted from a lower environment to the serving environment, because the load tests pass if the lower environment has sufficient quota. The problem here is that now that deployment is automatically pushed to production with alerts probably occurring with the outage. &lt;/p&gt;&lt;p&gt;The best way to handle these scenarios is to incorporate an automated way of not just monitoring and alerting, but a means for preventing promotion of that regressive behavior to the serving environment. The last thing you want is new logic that requires a higher resource quota than what is granted being automatically promoted to prod.&lt;/p&gt;&lt;p&gt;Why not use existing checks in tests? The software engineering discipline offers several types of tests (unit, integration, performance, load, smoke, etc…), none of which address something as complex as cross-environment consistency. Most of them focus on the user and expected behaviors. The only test that really focuses on infrastructure is the load test, but a quota regression is not necessarily part of the load test. It&#39;s not something you&#39;re going to detect since a load test occurs in its own environment and is agnostic of where it&#39;s actually running. &lt;/p&gt;&lt;p&gt;In other words, a quota regression test needs to be aware of the environments — it needs an expected baseline environment where the load test occurs and an actual serving environment where the product will be deployed. What I am proposing is an environment aware test to be included in the suite of many other tests.&lt;/p&gt;&lt;h3&gt;Quota regression testing on Google Cloud&lt;/h3&gt;&lt;p&gt;Google Cloud already provides services that you can use to easily incorporate this feature. This is more of a systems architecture practice that you can exercise. &lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/service-infrastructure/docs/service-consumer-management/reference/rest&#34;&gt;Service Consumer Management API&lt;/a&gt; provides the tools you need to create your own quota regression test. Take for example the &lt;a href=&#34;https://cloud.google.com/service-infrastructure/docs/service-consumer-management/reference/rest/v1beta1/services.consumerQuotaMetrics.limits#ConsumerQuotaLimit&#34;&gt;ConsumerQuotaLimit&lt;/a&gt; Resource that’s returned via the &lt;a href=&#34;https://cloud.google.com/service-infrastructure/docs/service-consumer-management/reference/rest/v1beta1/services.consumerQuotaMetrics/list&#34;&gt;list api&lt;/a&gt;. For the remainder of this discussion, let’s assume an environment setup such as this:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/extremely_simple_deployme.1000065520001058.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;extremely simple deployment pipeline.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/extremely_simple_deployme.1000065520001058.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Diagram demonstrating an extremely simple deployment pipeline for a resource provider.&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In the diagram above, we have a simplified deployment pipeline:&lt;br/&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Developers submit code to some repository&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The Cloud Build build and deployment trigger gets fired&lt;/p&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Tests are run&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Deployment images are pushed if the prerequisite steps succeed&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;&lt;p&gt;Images are pushed to their respective environments (in this case build to dev, and previous dev to prod)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Quotas&lt;/b&gt; are defined for the endpoints on deployment&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Load Balancer makes the endpoints available to end users&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;Quota limits&lt;/b&gt;&lt;/p&gt;&lt;p&gt;With this mental model, let’s hone in on the role quotas play in the big picture. Let’s assume we have the following service definition for an endpoint called “&lt;i&gt;FooService&lt;/i&gt;”. The service name, metric label and quota limit value are what we care about for this example.&lt;br/&gt;&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;gRPC Cloud Endpoint Yaml Example&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;type: google.api.Service\r\nconfig_version: 3\r\nname: fooservice.endpoints.my-project-id.cloud.goog\r\ntitle: Foo Service gRPC Cloud Endpoints\r\napis:\r\n - name: com.foos.demo.proto.v1.FooService\r\nusage:\r\n rules:\r\n # ListFoos methods can be called without an API Key.\r\n - selector: com.foos.demo.proto.v1.FooService.ListFoos\r\n allow_unregistered_calls: true\r\n # GetFoo methods can be called without an API Key.\r\n - selector: com.foos.demo.proto.v1.FooService.GetFoo\r\n allow_unregistered_calls: true\r\n # UpdateFoo methods can be called without an API Key.\r\n - selector: com.foos.demo.proto.v1.FooService.UpdateFoo\r\n allow_unregistered_calls: true\r\nmetrics:\r\n - name: library.googleapis.com/read_calls\r\n display_name: &#34;Read Quota&#34;\r\n value_type: INT64\r\n metric_kind: DELTA\r\n - name: library.googleapis.com/write_calls\r\n display_name: &#34;Write Quota&#34;\r\n value_type: INT64\r\n metric_kind: DELTA\r\nquota:\r\n limits:\r\n - name: &#34;apiReadQpmPerProject&#34;\r\n metric: library.googleapis.com/read_calls\r\n unit: &#34;1/min/{project}&#34;\r\n values:\r\n STANDARD: 1\r\n - name: &#34;apiWriteQpmPerProject&#34;\r\n metric: library.googleapis.com/write_calls\r\n unit: &#34;1/min/{project}&#34;\r\n values:\r\n STANDARD: 1\r\n # By default, all calls are measured with a cost of 1:1 for QPM.\r\n # See https://github.com/googleapis/googleapis/blob/master/google/api/quota.proto\r\n metric_rules:\r\n - selector: &#34;*&#34;\r\n metric_costs:\r\n library.googleapis.com/read_calls: 1\r\n - selector: com.foos.demo.proto.v1.FooService.UpdateFoo\r\n metric_costs:\r\n library.googleapis.com/write_calls: 2&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e6e633e6750&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In our definition we’ve established:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Service Name: &lt;code&gt;fooservice.endpoints.my-project-id.cloud.goog&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Metric Label: &lt;code&gt;library.googleapis.com/read_calls&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Quota Limit: &lt;code&gt;1&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;With these elements defined, we’ve now restricted read calls to exactly one per minute for the &lt;b&gt;service&lt;/b&gt;. Given a project number, (e.g., 123456789) we can now issue a call to the Consumer Quota Metrics Service to &lt;a href=&#34;https://cloud.google.com/service-usage/docs/manage-quota?hl=en_US#displaying_service_quota&#34;&gt;display the service quota&lt;/a&gt;.&lt;p&gt;Example commands and output.&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;$ alias gcurl=\&#39;curl -H &#34;Authorization: Bearer $(gcloud auth print-access-token)&#34; -H &#34;Content-Type: application/json&#34;\&#39;\r\n$ gcurl https://serviceconsumermanagement.googleapis.com/v1beta1/services/fooservice.endpoints.my-project-id.cloud.goog/projects/my-project-id/consumerQuotaMetrics&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e6e4d888910&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Response example (truncated)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;{\r\n &#34;metrics&#34;: [\r\n {\r\n &#34;name&#34;: &#34;services/fooservice.endpoints.my-project-id.cloud.goog/projects/123456789/consumerQuotaMetrics/library.googleapis.com%2Fread_calls&#34;,\r\n &#34;displayName&#34;: &#34;Read Quota&#34;,\r\n &#34;consumerQuotaLimits&#34;: [\r\n {\r\n &#34;name&#34;: &#34;services/fooservice.endpoints.my-project-id.cloud.goog/projects/123456789/consumerQuotaMetrics/library.googleapis.com%2Fread_calls/limits/%2Fmin%2Fproject&#34;,\r\n &#34;unit&#34;: &#34;1/min/{project}&#34;,\r\n &#34;metric&#34;: &#34;library.googleapis.com/read_calls&#34;,\r\n &#34;quotaBuckets&#34;: [\r\n {\r\n &#34;effectiveLimit&#34;: &#34;1&#34;,\r\n &#34;defaultLimit&#34;: &#34;1&#34;\r\n }\r\n ]\r\n }\r\n ],\r\n &#34;metric&#34;: &#34;library.googleapis.com/read_calls&#34;\r\n }\r\n \u2026&#39;), (u&#39;language&#39;, u&#39;&#39;), (u&#39;caption&#39;, &amp;lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e6e621b8150&amp;gt;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In the above response, the most important thing to note is the effective limit for a given service’s metric. The &lt;a href=&#34;https://cloud.google.com/service-infrastructure/docs/service-consumer-management/reference/rest/v1beta1/services.consumerQuotaMetrics.limits#quotabucket&#34;&gt;effective limit&lt;/a&gt; is the limit being applied to a resource consumer when enforcing customer fairness as discussed earlier.&lt;/p&gt;&lt;p&gt;Now that we’ve established how to get the &lt;i&gt;effectiveLimit&lt;/i&gt; for a quota definition on a resource per project, we can define the assertion of quota consistency as: &lt;/p&gt;&lt;p&gt;&lt;i&gt;Load Test Environment Quota Effective Limit &amp;lt;= Serving Environment Quota Effective Limit &lt;/i&gt;&lt;/p&gt;&lt;p&gt;Having a test like this, you can then integrate that with something like Cloud Build to block the promotion of your image from the lower environment to your serving environment if that test fails to pass. That saves you from introducing regressive behavior from the new image into the serving environment that would otherwise result in an outage. &lt;/p&gt;&lt;h3&gt;The importance of early detection&lt;/h3&gt;&lt;p&gt;It’s not enough to alert on a detected quota regression and block the image promotion to prod. It’s better to raise alarms as soon as possible. If resources are lacking when it’s time to promote to production, you’re now faced with the problem of wrangling enough resources in time. This may not always be possible in the desired timeline; it’s possible that the resource provider needs to scale up its resources to handle the increase in quota. This is not always something that can just be done in a day. For example, is the service hosted on &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE)? Even with autoscale, what if the &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#limitations&#34;&gt;ip pool is exhausted&lt;/a&gt;? Cloud infrastructure changes, although elastic, are not instant. Part of production planning needs to account for the time needed to scale.&lt;/p&gt;&lt;p&gt;In summary, quota regression testing is a key component that should be added to the entire concept of handling overload and dealing with load balancing in any cloud service — not just Google Cloud. It is important for product stability with the dips and spikes in demands, which will inevitably show up as a problem in many spaces. If you continue to rely on human intervention to ensure consistency of your quota across your configurations, you will only guarantee that eventually, you will have an outage when that consistency is not met. For more on working with quotas, &lt;a href=&#34;https://cloud.google.com/docs/quota&#34;&gt;check out the documentation&lt;/a&gt;. &lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;5 principles for cloud-native architecture—what it is and how to master it&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Learn to maximize your use of Google Cloud by adopting a cloud-native architecture.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Nethaneel Edwards&lt;/name&gt;&lt;title&gt;Senior Software Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 28 Jun 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Top 5 use cases for Google Cloud Spot VMs explained + best practices</title>
      <link>https://cloud.google.com/blog/products/compute/google-cloud-spot-vm-use-cases-and-best-practices/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Cloud was built on the premise of flexible infrastructure that grows and shrinks with your application demands. Applications that can take advantage of this elastic infrastructure and scale horizontally with the demands of your application offer significant advantages over competitors by allowing infrastructure costs to scale up and down along with the demand. &lt;/p&gt;&lt;p&gt;Google Cloud’s Spot VMs enable our customers to make the most of our idle capacity where and when it is available. Spot VMs are offered at a &lt;a href=&#34;https://cloud.google.com/spot-vms&#34;&gt;significant discount&lt;/a&gt; from list price to drive maximum savings provided customers have flexible, stateless workloads that can handle preemption. Spot VMs can be reclaimed by Google (with a 30 second notice). When you deploy the right workloads on Spot VMs, you are able to maintain elasticity while also taking advantage of the best discounts Google has to offer.&lt;/p&gt;&lt;p&gt;This blog discusses a few common use cases and design patterns we have seen customers utilize Spot VMs for and discusses the best practices for these use cases. While this is not an exhaustive list, this blog serves as a template to help customers make the most of the Spot VM savings while still reaching their application and workload objectives. &lt;/p&gt;&lt;h3&gt;Media rendering&lt;/h3&gt;&lt;p&gt;Rendering workloads (such as rendering 2D or 3D elements) can be both compute and time intensive, requiring skilled IT resources to manage render farms. Job management becomes even more difficult when the render farm is at 100% utilization. Spot VMs are ideal resources for fault-tolerant rendering workloads; when combined with a &lt;a href=&#34;https://cloud.google.com/architecture/building-a-hybrid-render-farm#managing_queues&#34;&gt;queuing system&lt;/a&gt; customers can integrate the preemption notice to track preempted jobs. This allows you to build a render farm which benefits from reduced TCO. If your renderer supports &lt;a href=&#34;https://cloud.google.com/architecture/building-a-hybrid-render-farm#choosing_between_standard_and_preemptible_vms&#34;&gt;taking snapshots&lt;/a&gt; of in-progress renders at specified intervals, writing these snapshots to a persistent data store (&lt;a href=&#34;https://cloud.google.com/storage&#34;&gt;Cloud Storage&lt;/a&gt;) will limit any loss in work in the event the Spot VM is preempted. As subsequent Spot VMs are created, they can pick up where the old ones left off by using the snapshots on Cloud Storage. You can also leverage the new “&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/suspend-resume-instance&#34;&gt;suspend and resume a VM&lt;/a&gt;” feature which allows you to keep the VM instances during the preemption event but not incur any charges for it while the VM is not in use.&lt;/p&gt;&lt;p&gt;Additionally, we have helped customers combine local render farms in their existing datacenters with cloud-based render farms, allowing a &lt;a href=&#34;https://cloud.google.com/architecture/building-a-hybrid-render-farm&#34;&gt;hybrid approach&lt;/a&gt; for large or numerous render workloads without increasing their investment in their physical datacenters. Not only does this reduce their capital expenses, but it adds flexible scalability to the existing farm and provides a better experience for their business partners. &lt;/p&gt;&lt;h3&gt;Financial modeling&lt;/h3&gt;&lt;p&gt;Capital market firms have significant investments in their infrastructure to create state-of-the-art, world-class compute grids. Since compute grids began, in-house researchers leverage these large grids in physical datacenters to test their trading hypotheses and perform backtesting. But as the business grows, what happens when all the researchers each have a brilliant idea and want to test that out at the same time? Researchers then have to compete with one another for the same limited resources, which leads to queueing their jobs and increased lead times for testing their ideas. And in financial markets, time is always scarce. Enter cloud computing and Spot VMs. Capital market firms can use Google Cloud as an extension of their on-premises grid by spinning up temporary compute resources. Or they can go all in on cloud and build their grid in Google Cloud entirely. In either scenario, Spot VMs are ideal candidates for bursting research workloads given the transient nature of the workload and heavily discounted prices of VMs. This enables researchers to test more hypotheses at a lower cost per test, in turn producing better models for firms. Google Cloud Spot VM discounts not only apply to the VMs themselves, but also to any &lt;a href=&#34;http://cloud.google.com/gpu&#34;&gt;GPU accelerator&lt;/a&gt; attached to them, providing even more processing power to a firm looking to process larger more complex models. Once these jobs have completed, Spot VMs can be quickly spun down, maintaining strict control on costs. &lt;/p&gt;&lt;h3&gt;CI/CD pipelines&lt;/h3&gt;&lt;p&gt;Continuous integration (CI) and Continuous delivery (CD) tools are very common for the modern application developer. These tools allow developers to create a testing pipeline that enables developers and quality engineers to ensure the newly created code works with their environment and that the deployment process does not break anything during deployment. CI/CD tools and test environments are great workloads to run on Spot VMs since CI/CD pipelines are not mission-critical for most companies — a delay in deployment or testing by 15 minutes, or even a few hours, is not material to their business. This means that companies can lower the cost of operating their CI/CD pipeline significantly through the use of Spot VMs. &lt;/p&gt;&lt;p&gt;A simple example of this would be to install the Jenkins Master Server in a &lt;a href=&#34;https://cloud.google.com/compute/docs/instance-groups&#34;&gt;Managed Instance Group&lt;/a&gt; (MIG) with the &lt;a href=&#34;https://cloud.google.com/compute/docs/instances/spot#spot-with-instance-groups&#34;&gt;VM type set to Spot&lt;/a&gt;. If the VM gets preempted, the CI/CD pipelines will stall until the MIG can find resources again to spin up a new VM. The first reaction may be concern that Jenkins persists data locally, which is problematic for Spot VMs. However, customers can move the Jenkins directory (/var/lib/Jenkins) to &lt;a href=&#34;https://cloud.google.com/filestore&#34;&gt;Google Cloud Filestore&lt;/a&gt; and preserve this data. Then when the new Spot VM spins up, it will reconnect to the directory. In the case of a large-scale Jenkins deployment, build VMs can utilize Spot VMs as part of a MIG to scale as necessary while ensuring that the builds can be maintained with on-demand VMs. This blended approach removes any risk to the builds, while still allowing customers to save up to 91% in costs of the additional VMs versus traditional on-demand VMs.&lt;/p&gt;&lt;h3&gt;Web services and apps&lt;/h3&gt;&lt;p&gt;Large online retailers have found ways to drive massive increases in order volume. Typically companies like this target a specific time each month, such as the last day of the month, through a unique promotion process. This means that they are in many cases creating a Black Friday/Cyber Monday-style event, each and every month! In order to support this, companies traditionally used a “Build it like a stadium for Super Bowl Sunday” model. The issue with that, and a reason most professional sports teams have practice facilities, is that it’s very expensive to keep all the lights, climate control, and ancillary equipment running for the sole purpose of practice. 29-30 days of a month most infrastructure sits idle, wasting HVAC, electricity, etc. However, using the elasticity of cloud, we could manage this capacity and turn it up only when necessary. But to drive even more optimization and savings, we turn to Spot VMs. &lt;/p&gt;&lt;p&gt;Spot VMs really shine during these kinds of scale-out events. Imagine the above scenario: what if behind a load balancer we could have:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;One MIG to help scale the web frontends. This MIG will be sized with on-demand VMs to handle day-to-day traffic.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A second MIG for Spot VMs that scales up starting at 11:45pm the night prior to the end of month. The first and second MIG can now handle ~80-90% of the workload. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A third MIG of on-demand VMs that spins up as a workload bursts to handle any remaining traffic, should the Spot MIG not be able to find enough capacity, thus ensuring we’re meeting our SLAs as well as keeping costs as tight as possible. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Kubernetes&lt;/h3&gt;&lt;p&gt;Now you may say “Well that’s all well and good, but we’re a fully modernized container shop, using Google Kubernetes Engine (GKE).” You are in luck — Spot VMs are integrated with &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;GKE&lt;/a&gt;, enabling you to quickly and easily save on your GKE workloads by using &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms&#34;&gt;Spot VMs&lt;/a&gt; with standard GKE clusters or &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/autopilot-spot-pods&#34;&gt;Spot Pods&lt;/a&gt; with your &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview&#34;&gt;Autopilot&lt;/a&gt;clusters. GKE supports &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms#termination-graceful-shutdown&#34;&gt;gracefully shutting down&lt;/a&gt;Spot VMs, notifying your workloads that they will be shut down and giving them time to cleanly exit. GKE then automatically reschedules your deployments. With Spot Pods, you can use Kubernetes nodeSelectors and/or Node affinity to control the placement of spot workloads, striking the right balance between cost and availability across spot and on-demand compute.&lt;/p&gt;&lt;h3&gt;General best practices&lt;/h3&gt;&lt;p&gt;To &lt;a href=&#34;https://cloud.google.com/blog/products/compute/google-cloud-spot-vm&#34;&gt;take advantage of Spot VMs&lt;/a&gt;, your use case doesn’t have to be an exact match to any of those described above. If the workload is stateless, scalable, can be stopped and checkpointed in less than 30 seconds, or is location- and hardware-flexible, then they may be a good fit for Spot VMs.&lt;/p&gt;&lt;p&gt;There are many several actions you can take to help ensure your Spot workloads run as smoothly as possible. Below we outline a few best practices you should consider:&lt;/p&gt;&lt;p&gt;1. Deploy Spot behind&lt;a href=&#34;https://cloud.google.com/compute/docs/instance-groups/regional-migs&#34;&gt;Regional Managed Instance Groups (RMIGs)&lt;/a&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;RMIGs are a great fit for Spot workloads given the RMIG’s ability to recreate instances which are preempted.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Using your workload’s profile, determine the RMIG’s &lt;a href=&#34;https://cloud.google.com/compute/docs/instance-groups/regional-migs#target_distribution_shape&#34;&gt;target distribution shape&lt;/a&gt;. For example, with a batch research workload, you might select an ANY target distribution shape. This will allow for Spot instances to be distributed in any manner across the various zones, thereby taking advantage of any underutilized resources. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;You can use a mix of on-demand RMIGs and Spot RMIGs to maintain stateful applications while increasing availability in a cost effective manner.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;2. Ensure you have a &lt;a href=&#34;https://cloud.google.com/compute/docs/shutdownscript&#34;&gt;shutdown script&lt;/a&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;In the event of Spot VM preemptions, use a shutdown script to enable checkpointing to Cloud Storage for your workloads as well as perform any graceful shutdown processes.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;When drafting your shutdown script, test it out on an instance by either &lt;a href=&#34;https://cloud.google.com/compute/docs/instances/spot#preemption-process&#34;&gt;manually stopping or deleting&lt;/a&gt; the instance with the shutdown script attached and validate the intended behavior.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;3. Write check-point files to Cloud Storage.&lt;/p&gt;&lt;p&gt;4. Consider using multiple MIGs behind your load balancer.&lt;/p&gt;&lt;p&gt;Whether your workload is graphics rendering, financial modeling, scaled-out ecommerce, or any other stateless use case, Spot VMs are the best and easiest way to reduce your cost of operating it by more than 60%. By following the examples and best practices above, you can ensure that Spot VMs will create the right outcome. Get started today with a &lt;a href=&#34;https://console.cloud.google.com/freetrial&#34;&gt;free trial&lt;/a&gt; of Google Cloud. &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;sup&gt;Acknowledgement&lt;br/&gt;Special thanks to Dan Sheppard, Product Manager for Cloud Compute, for contributing to this post.&lt;/sup&gt;&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Stefan Salandy&lt;/name&gt;&lt;title&gt;Customer Engineer - Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 22 Jun 2022 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Take the 2022 Accelerate State of DevOps Survey</title>
      <link>https://cloud.google.com/blog/products/devops-sre/take-the-2022-state-of-devops-survey/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper&#34;&gt;State of DevOps report&lt;/a&gt; by Google Cloud and the DORA research team is the largest and longest running research of its kind with inputs from over 32,000 professionals worldwide. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region, can employ to drive better performance.  &lt;/p&gt;&lt;p&gt;Today, Google Cloud and the &lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DORA&lt;/a&gt; research team are excited to announce the launch of the &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;2022 State of DevOps survey&lt;/a&gt;. For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. As technology teams continue to accelerate and evolve, so do the quantity and sophistication of security threats. Security can no longer be an afterthought or the final step before delivery, it must be integrated throughout the software development process. &lt;/p&gt;&lt;h3&gt;Shift Left&lt;/h3&gt;&lt;p&gt;The industry must shift from reactive practices to proactive and diagnostic measures, where software teams should assume that their systems are already compromised and build security into their supply chain. In the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report&#34;&gt;2021 State of DevOps report&lt;/a&gt; we found that elite performers who met or exceeded their reliability targets were twice as likely to have shifted their security practices left, i.e., implemented security practices earlier on in the software development lifecycle, and deliver reliable software quickly, and safely. Not only that, but teams who integrate security best practices throughout their development process are 1.6 times more likely to meet or exceed their organizational goals.&lt;/p&gt;&lt;p&gt;But how do companies know where to start when it comes to getting security right? In last year’s report we found that companies can integrate security, improve software delivery and operational performance, and improve organizational performance by leveraging the following practices:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_DevOps_Survey.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 DevOps Survey.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_DevOps_Survey.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;2022 State of DevOps Survey&lt;/h3&gt;&lt;p&gt;Like the past six research reports, our goal this year is to perform detailed analysis to help teams benchmark their performance against the industry and provide strategies that teams can employ to improve their performance. For the first time in &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;last year’s report&lt;/a&gt;, high and elite performers make up two-thirds of respondents. We can confidently say that as the industry continues to accelerate its adoption of DevOps principles teams see meaningful benefits as a result.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_DevOps_Survey.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 DevOps Survey.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_DevOps_Survey.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance. &lt;/p&gt;&lt;p&gt;Achieving elite performance is a team endeavor and diverse, inclusive teams drive the best performance. The research program benefits from the participation of a diverse group of people. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. &lt;/p&gt;&lt;p&gt;This &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; is for everyone. No matter where you are on your DevOps journey, the size of your organization, your organization&#39;s industry, or how you identify. There are no right or wrong answers, in fact we often hear feedback that questions in the survey prompt ideas for improvement.  &lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; will remain open until midnight PDT on July 22, 2022. We look forward to hearing from you and your teams!&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c53=""><div _ngcontent-c53="" innerhtml="&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper&#34;&gt;State of DevOps report&lt;/a&gt; by Google Cloud and the DORA research team is the largest and longest running research of its kind with inputs from over 32,000 professionals worldwide. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region, can employ to drive better performance.&amp;#160;&amp;#160;&lt;/p&gt;&lt;p&gt;Today, Google Cloud and the &lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DORA&lt;/a&gt; research team are excited to announce the launch of the &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;2022 State of DevOps survey&lt;/a&gt;. For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. As technology teams continue to accelerate and evolve, so do the quantity and sophistication of security threats. Security can no longer be an afterthought or the final step before delivery, it must be integrated throughout the software development process.&amp;#160;&lt;/p&gt;&lt;h3&gt;Shift Left&lt;/h3&gt;&lt;p&gt;The industry must shift from reactive practices to proactive and diagnostic measures, where software teams should assume that their systems are already compromised and build security into their supply chain. In the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report&#34;&gt;2021 State of DevOps report&lt;/a&gt; we found that elite performers who met or exceeded their reliability targets were twice as likely to have shifted their security practices left, i.e., implemented security practices earlier on in the software development lifecycle, and deliver reliable software quickly, and safely. Not only that, but teams who integrate security best practices throughout their development process are 1.6 times more likely to meet or exceed their organizational goals.&lt;/p&gt;&lt;p&gt;But how do companies know where to start when it comes to getting security right? In last year&amp;#8217;s report we found that companies can integrate security, improve software delivery and operational performance, and improve organizational performance by leveraging the following practices:&lt;/p&gt;"><p>The <a href="https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper" track-metadata-module="post">State of DevOps report</a> by Google Cloud and the DORA research team is the largest and longest running research of its kind with inputs from over 32,000 professionals worldwide. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region, can employ to drive better performance.  </p><p>Today, Google Cloud and the <a href="https://www.devops-research.com/research.html" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://www.devops-research.com" track-metadata-module="post">DORA</a> research team are excited to announce the launch of the <a href="https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://google.qualtrics.com" track-metadata-module="post">2022 State of DevOps survey</a>. For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. As technology teams continue to accelerate and evolve, so do the quantity and sophistication of security threats. Security can no longer be an afterthought or the final step before delivery, it must be integrated throughout the software development process. </p><h3>Shift Left</h3><p>The industry must shift from reactive practices to proactive and diagnostic measures, where software teams should assume that their systems are already compromised and build security into their supply chain. In the <a href="https://cloud.google.com/blog/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report" track-metadata-module="post">2021 State of DevOps report</a> we found that elite performers who met or exceeded their reliability targets were twice as likely to have shifted their security practices left, i.e., implemented security practices earlier on in the software development lifecycle, and deliver reliable software quickly, and safely. Not only that, but teams who integrate security best practices throughout their development process are 1.6 times more likely to meet or exceed their organizational goals.</p><p>But how do companies know where to start when it comes to getting security right? In last year’s report we found that companies can integrate security, improve software delivery and operational performance, and improve organizational performance by leveraging the following practices:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Claire Peters&lt;/name&gt;&lt;title&gt;User Experience Researcher&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 26 May 2022 23:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Enterprise DevOps Guidebook - Chapter 1</title>
      <link>https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The Google Cloud DORA team has been hard at work releasing our yearly &lt;a href=&#34;https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper&#34;&gt;Accelerate State of DevOps report&lt;/a&gt;. This research provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region, can employ to drive better performance. Year over year, the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;State of DevOps&lt;/a&gt;report helps organizations benchmark themselves against others in the industry as elite, high, medium, or low performers and provides recommendations for how organizations can continually improve. &lt;/p&gt;&lt;p&gt;The table below highlights elite, high, medium, and low performers at a glance from the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;last report.&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Enterprise_DevOps_Guidebook.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Enterprise DevOps Guidebook.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Enterprise_DevOps_Guidebook.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;To give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the &lt;a href=&#34;https://cloud.google.com/resources/dora-enterprise-guidebook&#34;&gt;DevOps Enterprise Guidebook&lt;/a&gt;. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud’s DORA research to initiate performance improvements.&lt;/p&gt;&lt;p&gt;We will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;DORA’s resources&lt;/a&gt; to measure your performance and to begin your first DevOps team experiment. Some resources include the &lt;a href=&#34;https://www.devops-research.com/quickcheck.html&#34; target=&#34;_blank&#34;&gt;DevOps Quick check&lt;/a&gt;, where you can measure your teams’ software delivery performance in less than a minute with just five multiple choice questions, or a more indepth&lt;a href=&#34;https://cloud.google.com/camp&#34;&gt;capabilities assessment&lt;/a&gt;, an assessment we deploy in your organization that gives us a robust measurement of your organization’s capabilities as they pertain to software delivery.&lt;/p&gt;&lt;p&gt;Future chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the &lt;a href=&#34;https://cloud.google.com/resources/dora-enterprise-guidebook&#34;&gt;guidebook&lt;/a&gt; we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.&lt;/p&gt;&lt;h3&gt;2022 State of DevOps Survey&lt;/h3&gt;&lt;p&gt;For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance. &lt;/p&gt;&lt;p&gt;We invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;2022 State of DevOps survey&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c53=""><div _ngcontent-c53="" innerhtml="&lt;p&gt;To give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the &lt;a href=&#34;https://cloud.google.com/resources/dora-enterprise-guidebook&#34;&gt;DevOps Enterprise Guidebook&lt;/a&gt;. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud&amp;#8217;s DORA research to initiate performance improvements.&lt;/p&gt;&lt;p&gt;We will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;DORA&amp;#8217;s resources&lt;/a&gt; to measure your performance and to begin your first DevOps team experiment. Some resources include the &lt;a href=&#34;https://www.devops-research.com/quickcheck.html&#34; target=&#34;_blank&#34;&gt;DevOps Quick check&lt;/a&gt;, where you can measure your teams&amp;#8217; software delivery performance in less than a minute with just five multiple choice questions, or a more indepth&lt;a href=&#34;https://cloud.google.com/camp&#34;&gt; capabilities assessment&lt;/a&gt;, an assessment we deploy in your organization that gives us a robust measurement of your organization&amp;#8217;s capabilities as they pertain to software delivery.&lt;/p&gt;&lt;p&gt;Future chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the &lt;a href=&#34;https://cloud.google.com/resources/dora-enterprise-guidebook&#34;&gt;guidebook&lt;/a&gt; we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.&lt;/p&gt;&lt;h3&gt;2022 State of DevOps Survey&lt;/h3&gt;&lt;p&gt;For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance.&amp;#160;&lt;/p&gt;&lt;p&gt;We invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;2022 State of DevOps survey&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!&lt;/p&gt;"><p>To give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the <a href="https://cloud.google.com/resources/dora-enterprise-guidebook" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/resources/dora-enterprise-guidebook" track-metadata-module="post">DevOps Enterprise Guidebook</a>. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud’s DORA research to initiate performance improvements.</p><p>We will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use <a href="https://cloud.google.com/devops" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/devops" track-metadata-module="post">DORA’s resources</a> to measure your performance and to begin your first DevOps team experiment. Some resources include the <a href="https://www.devops-research.com/quickcheck.html" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://www.devops-research.com" track-metadata-module="post">DevOps Quick check</a>, where you can measure your teams’ software delivery performance in less than a minute with just five multiple choice questions, or a more indepth<a href="https://cloud.google.com/camp" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/camp" track-metadata-module="post"> capabilities assessment</a>, an assessment we deploy in your organization that gives us a robust measurement of your organization’s capabilities as they pertain to software delivery.</p><p>Future chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the <a href="https://cloud.google.com/resources/dora-enterprise-guidebook" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/resources/dora-enterprise-guidebook" track-metadata-module="post">guidebook</a> we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.</p><h3>2022 State of DevOps Survey</h3><p>For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance. </p><p>We invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our <a href="https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://google.qualtrics.com" track-metadata-module="post">2022 State of DevOps survey</a>.</p><p>The <a href="https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://google.qualtrics.com" track-metadata-module="post">survey</a> will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Rob Edwards&lt;/name&gt;&lt;title&gt;Technology Practice Lead, DevOps&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 26 May 2022 23:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Enterprise DevOps Guidebook - Chapter 1</title>
      <link>https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The Google Cloud DORA team has been hard at work releasing our yearly &lt;a href=&#34;https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper&#34;&gt;Accelerate State of DevOps report&lt;/a&gt;. This research provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region, can employ to drive better performance. Year over year, the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;State of DevOps&lt;/a&gt;report helps organizations benchmark themselves against others in the industry as elite, high, medium, or low performers and provides recommendations for how organizations can continually improve. &lt;/p&gt;&lt;p&gt;The table below highlights elite, high, medium, and low performers at a glance from the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;last report.&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Enterprise_DevOps_Guidebook.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Enterprise DevOps Guidebook.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Enterprise_DevOps_Guidebook.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;To give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the &lt;a href=&#34;https://cloud.google.com/resources/dora-enterprise-guidebook&#34;&gt;DevOps Enterprise Guidebook&lt;/a&gt;. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud’s DORA research to initiate performance improvements.&lt;/p&gt;&lt;p&gt;We will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;DORA’s resources&lt;/a&gt; to measure your performance and to begin your first DevOps team experiment. Some resources include the &lt;a href=&#34;https://www.devops-research.com/quickcheck.html&#34; target=&#34;_blank&#34;&gt;DevOps Quick check&lt;/a&gt;, where you can measure your teams’ software delivery performance in less than a minute with just five multiple choice questions, or a more indepth&lt;a href=&#34;https://cloud.google.com/camp&#34;&gt;capabilities assessment&lt;/a&gt;, an assessment we deploy in your organization that gives us a robust measurement of your organization’s capabilities as they pertain to software delivery.&lt;/p&gt;&lt;p&gt;Future chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the &lt;a href=&#34;https://cloud.google.com/resources/dora-enterprise-guidebook&#34;&gt;guidebook&lt;/a&gt; we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.&lt;/p&gt;&lt;h3&gt;2022 State of DevOps Survey&lt;/h3&gt;&lt;p&gt;For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance. &lt;/p&gt;&lt;p&gt;We invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;2022 State of DevOps survey&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;p&gt;To give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the &lt;a href=&#34;https://cloud.google.com/resources/dora-enterprise-guidebook&#34;&gt;DevOps Enterprise Guidebook&lt;/a&gt;. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud&amp;#8217;s DORA research to initiate performance improvements.&lt;/p&gt;&lt;p&gt;We will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;DORA&amp;#8217;s resources&lt;/a&gt; to measure your performance and to begin your first DevOps team experiment. Some resources include the &lt;a href=&#34;https://www.devops-research.com/quickcheck.html&#34; target=&#34;_blank&#34;&gt;DevOps Quick check&lt;/a&gt;, where you can measure your teams&amp;#8217; software delivery performance in less than a minute with just five multiple choice questions, or a more indepth&lt;a href=&#34;https://cloud.google.com/camp&#34;&gt; capabilities assessment&lt;/a&gt;, an assessment we deploy in your organization that gives us a robust measurement of your organization&amp;#8217;s capabilities as they pertain to software delivery.&lt;/p&gt;&lt;p&gt;Future chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the &lt;a href=&#34;https://cloud.google.com/resources/dora-enterprise-guidebook&#34;&gt;guidebook&lt;/a&gt; we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.&lt;/p&gt;&lt;h3&gt;2022 State of DevOps Survey&lt;/h3&gt;&lt;p&gt;For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance.&amp;#160;&lt;/p&gt;&lt;p&gt;We invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;2022 State of DevOps survey&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!&lt;/p&gt;"><p>To give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the <a href="https://cloud.google.com/resources/dora-enterprise-guidebook" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/resources/dora-enterprise-guidebook" track-metadata-module="post">DevOps Enterprise Guidebook</a>. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud’s DORA research to initiate performance improvements.</p><p>We will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use <a href="https://cloud.google.com/devops" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/devops" track-metadata-module="post">DORA’s resources</a> to measure your performance and to begin your first DevOps team experiment. Some resources include the <a href="https://www.devops-research.com/quickcheck.html" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://www.devops-research.com" track-metadata-module="post">DevOps Quick check</a>, where you can measure your teams’ software delivery performance in less than a minute with just five multiple choice questions, or a more indepth<a href="https://cloud.google.com/camp" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/camp" track-metadata-module="post"> capabilities assessment</a>, an assessment we deploy in your organization that gives us a robust measurement of your organization’s capabilities as they pertain to software delivery.</p><p>Future chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the <a href="https://cloud.google.com/resources/dora-enterprise-guidebook" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/resources/dora-enterprise-guidebook" track-metadata-module="post">guidebook</a> we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.</p><h3>2022 State of DevOps Survey</h3><p>For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance. </p><p>We invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our <a href="https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://google.qualtrics.com" track-metadata-module="post">2022 State of DevOps survey</a>.</p><p>The <a href="https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://google.qualtrics.com" track-metadata-module="post">survey</a> will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Rob Edwards&lt;/name&gt;&lt;title&gt;Technology Practice Lead, DevOps&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 26 May 2022 23:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Get more insights with the new version of the Node.js library</title>
      <link>https://cloud.google.com/blog/products/devops-sre/get-more-insights-with-the-new-version-of-the-nodejs-library/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We’re thrilled to announce the release of a new update to the &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging&#34; target=&#34;_blank&#34;&gt;Cloud Logging Library for Node.js&lt;/a&gt; with the key new features of improved error handling and writing structured logging to standard output which becomes handy if you run applications in serverless environments like Google Functions!&lt;/p&gt;&lt;p&gt;The latest v9.9.0 of &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging&#34; target=&#34;_blank&#34;&gt;Cloud Logging Library for Node.js&lt;/a&gt; makes it even easier for Node.js developers to send and read logs from Google Cloud providing real-time insight into what is happening in your application through comprehensive tools like &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Log Explorer&lt;/a&gt;. If you are a Node.js developer working with Google Cloud, now is a great time to try out &lt;a href=&#34;https://cloud.google.com/logging/docs&#34;&gt;Cloud Logging&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The latest features of the Node.js library are also integrated and available in other packages which are based on &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging&#34; target=&#34;_blank&#34;&gt;Cloud Logging Library for Node.js&lt;/a&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging-winston&#34; target=&#34;_blank&#34;&gt;@google-cloud/logging-winston&lt;/a&gt; - this package integrates &lt;a href=&#34;https://cloud.google.com/logging/docs&#34;&gt;Cloud Logging&lt;/a&gt; with the &lt;a href=&#34;https://www.npmjs.com/package/winston&#34; target=&#34;_blank&#34;&gt;Winston&lt;/a&gt; logging library. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging-bunyan&#34; target=&#34;_blank&#34;&gt;@google-cloud/logging-bunyan&lt;/a&gt; - this package integrates &lt;a href=&#34;https://cloud.google.com/logging/docs&#34;&gt;Cloud Logging&lt;/a&gt; with the &lt;a href=&#34;https://www.npmjs.com/package/bunyan&#34; target=&#34;_blank&#34;&gt;Bunyan&lt;/a&gt; logging library. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If you are unfamiliar with the &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging&#34; target=&#34;_blank&#34;&gt;Cloud Logging Library for Node.js&lt;/a&gt;, start by running following command to add the library to your project:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;npm install @google-cloud/logging&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Once the library is installed, you can use it in your project. Below, I demonstrate how to initialize the logging library, create a client assigned configured with a project ID,  and log a single entry &#39;Your log message&#39;:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;// Imports the Google Cloud client library \r\n const { Logging } = require(&#39;@google-cloud/logging&#39;);\r\n // Creates a client with predefined project Id and a path to\r\n // credentials JSON file to be used for auth with Cloud Logging\r\n const logging = new Logging(\r\n {\r\n projectId: &#39;your-project-id&#39;,\r\n keyFilename: &#39;/path/to/key.json&#39;,\r\n }\r\n );\r\n // Create a log with desired log name\r\n const log = logging.log(&#39;your-log-name&#39;);\r\n // Create a simple log entry without any metadata\r\n const entry = log.entry({}, &#39;Your log message&#39;);\r\n // Log your record!!!\r\n log.info(entry);&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Here&#39;s the log message generated by this code in Log Explorer:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;1 nodejs library.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_nodejs_library.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Two critical features of the latest &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging&#34; target=&#34;_blank&#34;&gt;Cloud Logging Library for Node.js&lt;/a&gt; release are writing structured log entries to standard output and error handling with a default callback. Let&#39;s dig in deeper. &lt;/p&gt;&lt;h3&gt;Writing structured log entries to standard output&lt;/h3&gt;&lt;p&gt;The &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59&#34; target=&#34;_blank&#34;&gt;LogSync&lt;/a&gt; class helps users write context-rich structured logs to &lt;code&gt;stdout&lt;/code&gt; or any other &lt;code&gt;Writable&lt;/code&gt; interface. This class extracts additional log properties like trace context from HTTP headers, and can be used to toggle between writing to the Cloud Logging endpoint or to &lt;code&gt;stdout&lt;/code&gt; during local development.&lt;/p&gt;&lt;p&gt;In addition, writing structured logging to &lt;code&gt;stdout&lt;/code&gt; can be integrated with a &lt;a href=&#34;https://cloud.google.com/logging/docs/agent/logging&#34;&gt;Logging agent&lt;/a&gt;. Once a log is written to &lt;code&gt;stdout&lt;/code&gt;, a Logging agent then picks up those logs and delivers those to Cloud Logging out-of-process. &lt;a href=&#34;https://cloud.google.com/logging/docs/agent/logging&#34;&gt;Logging agents&lt;/a&gt; can add more properties to each entry before streaming it to the Logging API.&lt;/p&gt;&lt;p&gt;We recommend serverless applications (i.e. applications running in Cloud Functions and Cloud Run) to use the &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59&#34; target=&#34;_blank&#34;&gt;LogSync&lt;/a&gt; class as async logs delivery may be dropped due to lack of CPU or other environmental factors  preventing the logs from being sent immediately to the Logging API. Cloud Functions and Cloud Run applications by their nature are ephemeral and can have a short lifespan which will cause logging data drops when an instance is shut down before the logs have been sent to Cloud Logging servers. &lt;/p&gt;&lt;p&gt;Today, Google Cloud managed services automatically install Logging agents for all Google serverless environments in the resources that they provision - this means that you can use &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59&#34; target=&#34;_blank&#34;&gt;LogSync&lt;/a&gt; in your application to seamlessly deliver logs to Cloud Logging through standard output.&lt;/p&gt;&lt;p&gt;Below is a sample how to use &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59&#34; target=&#34;_blank&#34;&gt;LogSync&lt;/a&gt; class:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;const { Logging } = require(&#39;@google-cloud/logging&#39;);\r\n const logging = new Logging(\r\n {\r\n projectId: &#39;your-project-id&#39;,\r\n keyFilename: &#39;/path/to/key.json&#39;,\r\n }\r\n );\r\n// Create a LogSync transport, defaulting to `process.stdout`\r\nconst log = logging.logSync(&#39;Your-log-name&#39;);\r\nconst entry = log.entry({}, &#39;Your log message&#39;);\r\nlog.write(entry);&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;If you use &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging-winston&#34; target=&#34;_blank&#34;&gt;@google-cloud/logging-winston&lt;/a&gt;  or &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging-bunyan&#34; target=&#34;_blank&#34;&gt;@google-cloud/logging-bunyan&lt;/a&gt; library, you can set the &lt;code&gt;redirectToStdout&lt;/code&gt; parameter in &lt;a href=&#34;https://github.com/googleapis/nodejs-logging-winston/blob/2470dbc2b225e77b78f8066477a2f63126e3b1cd/src/index.ts#L181&#34; target=&#34;_blank&#34;&gt;LoggingWinston&lt;/a&gt; or &lt;a href=&#34;https://github.com/googleapis/nodejs-logging-bunyan/blob/75d7811c277e4de87a2232db3ce856669964cb45/src/index.ts#L165&#34; target=&#34;_blank&#34;&gt;LoggingBunyan&lt;/a&gt; constructor options respectively. Below is a sample code how to redirect structured logging output to &lt;code&gt;stdout&lt;/code&gt; for &lt;a href=&#34;https://github.com/googleapis/nodejs-logging-winston/blob/2470dbc2b225e77b78f8066477a2f63126e3b1cd/src/index.ts#L181&#34; target=&#34;_blank&#34;&gt;LoggingWinston&lt;/a&gt; class:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;// Imports the Google Cloud client library for Winston\r\nconst {LoggingWinston} = require(&#39;@google-cloud/logging-winston&#39;);\r\n\r\n// Creates a client that writes logs to stdout\r\nconst loggingWinston = new LoggingWinston({\r\n projectId: &#39;your-project-id&#39;,\r\n keyFilename: &#39;/path/to/key.json&#39;,\r\n redirectToStdout: true,\r\n});&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Error Handling with a default callback&lt;/h3&gt;&lt;p&gt;The &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log.ts#L117&#34; target=&#34;_blank&#34;&gt;Log&lt;/a&gt; class provides users the ability to write and delete logs asynchronously. However, there are cases when log entries cannot be written or deleted and an error is thrown - if the error is not handled properly, it can crash the application. &lt;/p&gt;One possible way to handle the error is to await the log write/delete calls and wrap it with &lt;code&gt;try/catch&lt;/code&gt;. However, waiting for every write or delete call may introduce delays which could be avoided by simply adding a callback as shown below:&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;// Asynchronously write the log entry and handle response or \r\n // any errors in provided callback\r\n log.write(entry, err =&amp;gt; {\r\n if (err) {\r\n // The log entry was not written.\r\n console.log(err.message);\r\n } else {\r\n console.log(&#39;No error in write callback!&#39;);\r\n }\r\n });&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Adding a callback to each write or delete call is duplicate code and remembering to include it for each call may be toilsome, especially if  the code handling the error is always the same. To eliminate this burden, we introduced the ability to provide a default callback for the &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log.ts#L117&#34; target=&#34;_blank&#34;&gt;Log&lt;/a&gt; class which can be set through the &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log.ts#L59&#34; target=&#34;_blank&#34;&gt;LogOptions&lt;/a&gt; passed to the &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log.ts#L117&#34; target=&#34;_blank&#34;&gt;Log&lt;/a&gt; constructor as in example below:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;const {Logging} = require(&#39;@google-cloud/logging&#39;);\r\n const logging = new Logging();\r\n \r\n // Create options with default callback to be called on \r\n // every write/delete response or error\r\n const options = {\r\n defaultWriteDeleteCallback: function (err) {\r\n if (err) {\r\n console.log(&#39;Error is: &#39; + err);\r\n } else {\r\n console.log(&#39;No error, all is good!&#39;);\r\n }\r\n },\r\n };\r\n\r\n const log = logging.log(&#39;my-log&#39;, options);&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;If you use &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging-winston&#34; target=&#34;_blank&#34;&gt;@google-cloud/logging-winston&lt;/a&gt;  or &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging-bunyan&#34; target=&#34;_blank&#34;&gt;@google-cloud/logging-bunyan&lt;/a&gt; library, you can set the callback through &lt;code&gt;defaultCallback&lt;/code&gt; parameter in &lt;a href=&#34;https://github.com/googleapis/nodejs-logging-winston/blob/2470dbc2b225e77b78f8066477a2f63126e3b1cd/src/index.ts#L181&#34; target=&#34;_blank&#34;&gt;LoggingWinston&lt;/a&gt; or &lt;a href=&#34;https://github.com/googleapis/nodejs-logging-bunyan/blob/75d7811c277e4de87a2232db3ce856669964cb45/src/index.ts#L165&#34; target=&#34;_blank&#34;&gt;LoggingBunyan&lt;/a&gt; constructor options respectively. Here is an example of  how to set a default callback for &lt;a href=&#34;https://github.com/googleapis/nodejs-logging-winston/blob/2470dbc2b225e77b78f8066477a2f63126e3b1cd/src/index.ts#L181&#34; target=&#34;_blank&#34;&gt;LoggingWinston&lt;/a&gt; class:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;// Imports the Google Cloud client library for Winston\r\nconst {LoggingWinston} = require(&#39;@google-cloud/logging-winston&#39;);\r\n\r\n// Creates a client\r\nconst loggingWinston = new LoggingWinston({\r\n projectId: &#39;your-project-id&#39;,\r\n keyFilename: &#39;/path/to/key.json&#39;,\r\n defaultCallback: err =&amp;gt; {\r\n if (err) {\r\n console.log(&#39;Error occurred: &#39; + err);\r\n }\r\n },\r\n});&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Next Steps&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Now, when you integrate the &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging&#34; target=&#34;_blank&#34;&gt;Cloud Logging Library for Node.js&lt;/a&gt; in your project, you can start using the latest features. &lt;/li&gt;&lt;li&gt;To try the latest Node.js library in Google Cloud you can follow this quickstart walkthrough guide:&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--small h-c-grid__col h-c-grid__col--2 h-c-grid__col--offset-5 &#34;&gt;&lt;a href=&#34;https://console.cloud.google.com/?walkthrough_id=logging__logging-nodejs&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;guide me button.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/guide_me_button.0998009406980188.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;For more information on the latest check out for &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging&#34; target=&#34;_blank&#34;&gt;Cloud Logging Library for Node.js&lt;/a&gt; user guide.&lt;br/&gt;&lt;/li&gt;&lt;li&gt;For any feedback or contributions, feel free to open issues in our &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/issues&#34; target=&#34;_blank&#34;&gt;Cloud Logging Library for Node.js GitHub repo&lt;/a&gt;. Issues can be also opened for bugs, questions about library usage and new feature requests.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/managed-service-for-prometheus-offers-new-pricing-tier/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Introducing a high-usage tier for Managed Service for Prometheus&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;New pricing tier for our managed Prometheus service users with over 500 billion metric samples per month. Pricing for existing tiers redu...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c31=""><div _ngcontent-c31="" innerhtml="&lt;p&gt;Two critical features of the latest &lt;a href=&#34;https://www.npmjs.com/package/@google-cloud/logging&#34; target=&#34;_blank&#34;&gt;Cloud Logging Library for Node.js&lt;/a&gt; release are writing structured log entries to standard output and error handling with a default callback. Let&#39;s dig in deeper.&amp;#160;&lt;/p&gt;&lt;h3&gt;Writing structured log entries to standard output&lt;/h3&gt;&lt;p&gt;The &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59&#34; target=&#34;_blank&#34;&gt;LogSync&lt;/a&gt; class helps users write context-rich structured logs to &lt;code&gt;stdout&lt;/code&gt; or any other &lt;code&gt;Writable&lt;/code&gt; interface. This class extracts additional log properties like trace context from HTTP headers, and can be used to toggle between writing to the Cloud Logging endpoint or to &lt;code&gt;stdout&lt;/code&gt; during local development.&lt;/p&gt;&lt;p&gt;In addition, writing structured logging to &lt;code&gt;stdout&lt;/code&gt; can be integrated with a &lt;a href=&#34;https://cloud.google.com/logging/docs/agent/logging&#34;&gt;Logging agent&lt;/a&gt;. Once a log is written to &lt;code&gt;stdout&lt;/code&gt;, a Logging agent then picks up those logs and delivers those to Cloud Logging out-of-process. &lt;a href=&#34;https://cloud.google.com/logging/docs/agent/logging&#34;&gt;Logging agents&lt;/a&gt; can add more properties to each entry before streaming it to the Logging API.&lt;/p&gt;&lt;p&gt;We recommend serverless applications (i.e. applications running in Cloud Functions and Cloud Run) to use the &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59&#34; target=&#34;_blank&#34;&gt;LogSync&lt;/a&gt; class as async logs delivery may be dropped due to lack of CPU or other environmental factors&amp;#160; preventing the logs from being sent immediately to the Logging API. Cloud Functions and Cloud Run applications by their nature are ephemeral and can have a short lifespan which will cause logging data drops when an instance is shut down before the logs have been sent to Cloud Logging servers.&amp;#160;&lt;/p&gt;&lt;p&gt;Today, Google Cloud managed services automatically install Logging agents for all Google serverless environments in the resources that they provision - this means that you can use &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59&#34; target=&#34;_blank&#34;&gt;LogSync&lt;/a&gt; in your application to seamlessly deliver logs to Cloud Logging through standard output.&lt;/p&gt;&lt;p&gt;Below is a sample how to use &lt;a href=&#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59&#34; target=&#34;_blank&#34;&gt;LogSync&lt;/a&gt; class:&lt;/p&gt;"><p>Two critical features of the latest <a href="https://www.npmjs.com/package/@google-cloud/logging" target="_blank" track-type="inline link" track-name="13" track-metadata-eventdetail="https://www.npmjs.com" track-metadata-module="post">Cloud Logging Library for Node.js</a> release are writing structured log entries to standard output and error handling with a default callback. Let&#39;s dig in deeper. </p><h3>Writing structured log entries to standard output</h3><p>The <a href="https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59" target="_blank" track-type="inline link" track-name="14" track-metadata-eventdetail="https://github.com" track-metadata-module="post">LogSync</a> class helps users write context-rich structured logs to <code>stdout</code> or any other <code>Writable</code> interface. This class extracts additional log properties like trace context from HTTP headers, and can be used to toggle between writing to the Cloud Logging endpoint or to <code>stdout</code> during local development.</p><p>In addition, writing structured logging to <code>stdout</code> can be integrated with a <a href="https://cloud.google.com/logging/docs/agent/logging" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/logging/docs/agent/logging" track-metadata-module="post">Logging agent</a>. Once a log is written to <code>stdout</code>, a Logging agent then picks up those logs and delivers those to Cloud Logging out-of-process. <a href="https://cloud.google.com/logging/docs/agent/logging" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/logging/docs/agent/logging" track-metadata-module="post">Logging agents</a> can add more properties to each entry before streaming it to the Logging API.</p><p>We recommend serverless applications (i.e. applications running in Cloud Functions and Cloud Run) to use the <a href="https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59" target="_blank" track-type="inline link" track-name="17" track-metadata-eventdetail="https://github.com" track-metadata-module="post">LogSync</a> class as async logs delivery may be dropped due to lack of CPU or other environmental factors  preventing the logs from being sent immediately to the Logging API. Cloud Functions and Cloud Run applications by their nature are ephemeral and can have a short lifespan which will cause logging data drops when an instance is shut down before the logs have been sent to Cloud Logging servers. </p><p>Today, Google Cloud managed services automatically install Logging agents for all Google serverless environments in the resources that they provision - this means that you can use <a href="https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59" target="_blank" track-type="inline link" track-name="18" track-metadata-eventdetail="https://github.com" track-metadata-module="post">LogSync</a> in your application to seamlessly deliver logs to Cloud Logging through standard output.</p><p>Below is a sample how to use <a href="https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://github.com" track-metadata-module="post">LogSync</a> class:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Alexander Losovsky&lt;/name&gt;&lt;title&gt;Developer Relations Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/nodejs.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 20 May 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Maisons du Monde’s journey to a managed service for Prometheus</title>
      <link>https://cloud.google.com/blog/products/devops-sre/maisons-du-monde-improved-their-kubernetes-observability/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;Editor’s note&lt;/b&gt;: Today we hear from Maisons du Monde, a furniture and home decor company that was founded in France over 25 years ago. They have 357 stores across France, Italy, Spain, Belgium, Luxembourg, Germany, Austria, Switzerland, Netherlands, and Portugal, and are a Google Cloud customer. They worked with their Customer Engineer, Adrien Aflalo, to prepare this story. &lt;/i&gt;&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;Without telemetry data like logs and metrics, we are blind in production. Our team of Operations Engineers and Site Reliability Engineers (SRE) rely on metrics data in particular to run the &lt;a href=&#34;https://www.maisonsdumonde.com/UK/en&#34; target=&#34;_blank&#34;&gt;Maisons du Monde website&lt;/a&gt;, our APIs, and our omnichannel services in a secure and reliable manner. This means that choosing a metrics platform and provider is not just a technical decision, but one that’s critical to our business as well.&lt;/p&gt;&lt;p&gt;As we moved more of our operations to Google Kubernetes Engine, we evaluated new metrics platforms and about eight months ago decided on Prometheus. Prometheus is a good fit for our environment, which contains cloud native applications that are built on Kubernetes and run on ephemeral compute infrastructure. Although first we built and ran our own Prometheus environment, we decided that Google Cloud Managed Service for Prometheus is a better solution for us. It allows us to focus on using our metrics instead of managing metrics infrastructure, providing the following features: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;Long-term retention of metrics (2 years)&lt;/li&gt;&lt;li&gt;Seamless support for high availability of Prometheus instances &lt;/li&gt;&lt;li&gt;Scraping and evaluating rules using lightweight Kubernetes Custom Resources &lt;/li&gt;&lt;li&gt;A global query view &lt;/li&gt;&lt;li&gt;Fully managed collection and querying, out of the box  &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For other organizations that are running their own Prometheus or deciding whether to run their own Prometheus, we wanted to provide you a look into our journey moving from self-hosted Prometheus to using a managed Prometheus service:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;We first switched from traditional managed metrics services to Prometheus to support our growing Kubernetes environment.  &lt;/li&gt;&lt;li&gt;We then built out and maintained our own Prometheus environment, but realized we needed additional features to support production workloads (such as high availability and faster MTTR) and scale.&lt;/li&gt;&lt;li&gt;Next we investigated solutions to help support our use of Prometheus in production, which led us to adopting Thanos.&lt;/li&gt;&lt;li&gt;Then we discovered we were still investing lots of time and personnel to run these technologies and manage infrastructure. &lt;/li&gt;&lt;li&gt;Finally we adopted Google Cloud Managed Service for Prometheus, which so far has met our needs quite well.  &lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Open source Prometheus worked well when we had a smaller deployment &lt;/h3&gt;&lt;p&gt;Commonly, organizations approach metrics using either a full service monitoring and metrics storage tool or an open source database to store their metrics, displaying them using a visualization tool like Grafana.&lt;/p&gt;&lt;p&gt;For the past five years, we used managed monitoring and storage services from two providers. Using different vendors created complexity for our operations teams and it made it hard for metrics to be shared with the teams who would eventually be responsible for maintaining reliability: the application teams! &lt;b&gt;It is our belief that ownership of the metrics should reside with the application teams. It allows developers and product owners to maintain the metrics they deem essential for alerting and dashboarding. &lt;/b&gt;&lt;/p&gt;&lt;p&gt;After we made the switch to Prometheus, our experience was great—at first. Prometheus’ design allows each application to expose granular metrics which are gathered with a Prometheus collector and stored in a Prometheus database. &lt;/p&gt;&lt;p&gt;However, as we deployed it on an increasing number of GKE clusters used to run our production applications, we ran into some constraints. These included: &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Support for scaled management &lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;We’re moving our business towards an “infrastructure-as-code” model to deploy and manage resources because it is more efficient and results in fewer errors. We need a simple way to deploy Prometheus in each&lt;a href=&#34;https://kubernetes.io/&#34; target=&#34;_blank&#34;&gt;Kubernetes&lt;/a&gt; cluster by policy. &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Retention&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Prometheus’ default time series database retention is set to &lt;a href=&#34;https://prometheus.io/docs/prometheus/latest/storage/&#34; target=&#34;_blank&#34;&gt;15 days&lt;/a&gt;. The database retention is configurable, but it will increase your costs and resource consumption to keep your metrics on disk for longer periods. We need a better way to manage metrics retention for longer periods of time for all our Kubernetes clusters and applications.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Backup/Disaster recovery and restoration&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Business continuity considerations are important for any service used in production. In Prometheus we found disk failures and backups to be a pain point. We need ways to scalably backup and restore data on Prometheus instances when failures occur to avoid data loss.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Operational scalability&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Furthermore, Prometheus doesn’t offer a native sharding feature, which may be a strength from an administration or deployment point of view, but ends up being a weakness if you have multiple clusters to monitor. &lt;/p&gt;&lt;p&gt;Updates&lt;/p&gt;&lt;p&gt;Prometheus stores rules within a static file, which means you have to reboot your Prometheus instances in order to apply rules file updates.  &lt;/p&gt;&lt;h3&gt;Using Prometheus and Thanos to address some (but not all) needs &lt;/h3&gt;&lt;p&gt;After we ran into the issues raised above, we searched for ways to address them and found &lt;a href=&#34;http://thanos.io&#34; target=&#34;_blank&#34;&gt;Thanos&lt;/a&gt;. Thanos is an open-source project released in 2018 by &lt;a href=&#34;https://www.improbable.io/&#34; target=&#34;_blank&#34;&gt;Improbable.&lt;/a&gt; It helped us with multi-cluster management and data storage. Prometheus metrics can be sent to object storage services such as Google Cloud Storage, Azure Blob Storage, or AWS’ S3. &lt;/p&gt;&lt;p&gt;Like Prometheus, Thanos’ architecture is extensible by design:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_mdm.0484037508910657.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 mdm.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_mdm.0484037508910657.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;Architecture diagram from &lt;a href=&#34;https://github.com/thanos-io/thanos&#34;&gt;Thanos’ Github page&lt;/a&gt;&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Using Prometheus and Thanos together helped us solve the following issues, which we noted above:&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Retention&lt;/i&gt;&lt;/b&gt; &lt;/p&gt;&lt;p&gt;Unlike Prometheus, Thanos is query-based instead of collection-based. Thanos sidecars are deployed alongside Prometheus instances and gather only metrics they are asked to expose. Thanos’ &lt;a href=&#34;https://thanos.io/tip/thanos/getting-started.md/&#34; target=&#34;_blank&#34;&gt;documentation&lt;/a&gt; describes each role. If Prometheus retention has been configured, and metrics aren’t available on the local disk, it will ask its Store Gateway component to retrieve the metrics from the remote storage location. With this feature, we can address the metric retention issue raised with standalone Prometheus. Additionally, Thanos addresses some other common Prometheus needs:&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Federation&lt;/i&gt; &lt;/b&gt;&lt;/p&gt;&lt;p&gt;Thanos allows us to set up a global view of our multi-cluster environments, whereas Prometheus could not. This requires us to set up one &lt;a href=&#34;https://thanos.io/tip/components/query.md/&#34; target=&#34;_blank&#34;&gt;Querier&lt;/a&gt; per Kubernetes cluster and one Querier “federator,” which you can see in the diagram below.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_mdm.0648038612970773.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 mdm.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_mdm.0648038612970773.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;Our Architecture Diagram&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The Querier components can be added to our multi-cluster environments via the addition of a simple configuration (see example code below) to get a global view of our metrics.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;apiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n name: thanos-querier\r\n namespace: monitoring\r\n labels:\r\n app: thanos-querier\r\nspec:\r\n replicas: 3\r\n selector:\r\n matchLabels:\r\n app: thanos-querier\r\n template:\r\n metadata:\r\n labels:\r\n app: thanos-querier\r\n spec:\r\n containers:\r\n - name: thanos\r\n image: quay.io/thanos/thanos:v0.23.1\r\n args:\r\n - query\r\n - --log.level=debug\r\n - --query.replica-label=replica\r\n - --store=dnssrv+thanos-store-gateway:10901\r\n ports:\r\n - name: http\r\n containerPort: 10902\r\n - name: grpc\r\n containerPort: 10901\r\n livenessProbe:\r\n httpGet:\r\n port: http\r\n path: /-/healthy\r\n readinessProbe:\r\n httpGet:\r\n port: http\r\n path: /-/ready&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;&lt;sup&gt;Thanos querier deployment configuration example &lt;/sup&gt;&lt;/i&gt;&lt;/p&gt;&lt;h3&gt;Prometheus and Thanos works, but increased complexity&lt;/h3&gt;&lt;p&gt;Thanos helped us a lot by dealing with issues raised by standalone Prometheus. However, it came with a lot of components which increased our complexity. This led to the following downsides for us: &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Development time&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;The multi-cluster, global environment that we described above required engineering resources and time to set up and maintain. Our engineers’ time is very valuable, and we would rather spend it developing new features instead of maintaining a state-of-the-art metrics system. &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Increased infrastructure load&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Configuring each Kubernetes cluster with the Thanos Queriers to enable remote storage led to increased network bandwidth consumption. In addition, we had now more components added to Prometheus which meant more system consumption (CPU, RAM).   &lt;/p&gt;&lt;p&gt;After going through the initial deployment of Prometheus and then trying to solve problems on our own with yet another solution (Thanos), we decided that it was time to look into a managed alternative.&lt;/p&gt;&lt;h3&gt;Switching to a managed service: Google Cloud Managed Service for Prometheus &lt;/h3&gt;&lt;p&gt;In October of 2021, Google Cloud released the public preview of Managed Service for Prometheus, which we understood to be a drop-in replacement for an existing Prometheus stack.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_mdm.0426021708240395.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;3 mdm.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_mdm.0426021708240395.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Architecture diagram from &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Google Cloud Managed Service for Prometheus&lt;/a&gt; documentation&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Metrics for the service are retrieved by collectors, which are a fork of the open source Prometheus technology. The collectors send metrics to Google’s global time-series database named &lt;a href=&#34;https://research.google/pubs/pub50652/&#34; target=&#34;_blank&#34;&gt;Monarch&lt;/a&gt;, which removed the need for &lt;a href=&#34;https://thanos.io/&#34; target=&#34;_blank&#34;&gt;Thanos&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Google Cloud gave us two modes for using &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Managed Service for Prometheus.&lt;/a&gt; In our case we are using managed collection, which allows us to reduce the complexity of deploying and managing Prometheus instances. &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Managed Service for Prometheus&lt;/a&gt; provides an operator to configure Custom Resources (CRs) for scraping metrics, evaluating rules, and more. All our &lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34;&gt;Prometheus&lt;/a&gt; operations are handled by the Kubernetes operator. &lt;/p&gt;&lt;p&gt;In addition, this solution supports more current Prometheus use cases (e.g. migrating from ServiceMonitor to PodMonitoring scrape configs).  &lt;/p&gt;&lt;p&gt;Because we expect our metrics data to steadily grow alongside our company’s growth, we know that managing metrics at scale ourselves will likely become very painful. Google Cloud Managed Service for Prometheus helped us achieve scaled metrics infrastructure in a straightforward way, as a managed service, without devoting hundreds of servers to this effort. We want to focus our attention on building a functional and strategic metrics-based operations practice, instead of building a competency in managing long-term storage and Prometheus infrastructure.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/4_mdm.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;4 mdm.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/4_mdm.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Global solution architecture&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Managed Service for Prometheus is not the perfect solution for us yet as it can be deployed using the Google Cloud Console, gcloud cli, or the kubectl tool. We hear that Terraform support is in the works. We needed to add additional engineering resources to deploy it using Helm charts. We invested in building Helm charts to help automate some of our processes and are happy to share this code with you to make your journey easier. Please visit the &lt;a href=&#34;https://medium.com/maisonsdumonde/metrics-management-with-google-cloud-managed-service-for-prometheus-15e226f73257&#34; target=&#34;_blank&#34;&gt;Medium blog&lt;/a&gt; we wrote on this topic to see our code snippets.&lt;/p&gt;&lt;h3&gt;Get started on your journey &lt;/h3&gt;&lt;p&gt;To sum it all up, while the structure, capabilities, and ecosystem of Prometheus are a good fit for our business, running the infrastructure and software was not worth it for us at scale. That’s why we chose Managed Service for Prometheus. We use Google Cloud’s fully managed service to monitor and manage alert notifications for our workloads. It scales with our needs and does not require management or maintenance.&lt;/p&gt;&lt;p&gt;If you’re getting started soon with your own Managed Service for Prometheus deployment, we recommend you check out &lt;a href=&#34;https://youtu.be/X4qAEa8_JxQ&#34; target=&#34;_blank&#34;&gt;this video&lt;/a&gt; that walks you through the first steps or visit the &lt;a href=&#34;https://cloud.google.com/managed-prometheus&#34;&gt;Managed Service for Prometheus page&lt;/a&gt; for more information. You can also take the fast track and join our teams to work on our stack. Check out &lt;a href=&#34;https://recrutement.maisonsdumonde.com/postulez/offres-demploi/#page-fr---search---set-vacsearchfront_function-004---fonction-informatique-web-16&#34; target=&#34;_blank&#34;&gt;our website&lt;/a&gt; — we have a lot of &lt;a href=&#34;https://www.welcometothejungle.com/fr/companies/maisons-du-monde/jobs&#34; target=&#34;_blank&#34;&gt;open positions&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Google Cloud Managed Service for Prometheus is now generally available&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Announcing the GA of Google Cloud Managed Service for Prometheus for the collection, storage, and querying of Kubernetes metrics.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;&lt;i&gt;&lt;b&gt;Editor&amp;#8217;s note&lt;/b&gt;: Today we hear from Maisons du Monde, a furniture and home decor company that was founded in France over 25 years ago. They have 357 stores across France, Italy, Spain, Belgium, Luxembourg, Germany, Austria, Switzerland, Netherlands, and Portugal, and are a Google Cloud customer. They worked with their Customer Engineer, Adrien Aflalo, to prepare this story.&amp;#160;&lt;/i&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Without telemetry data like logs and metrics, we are blind in production. Our team of Operations Engineers and Site Reliability Engineers (SRE) rely on metrics data in particular to run the &lt;a href=&#34;https://www.maisonsdumonde.com/UK/en&#34; target=&#34;_blank&#34;&gt;Maisons du Monde website&lt;/a&gt;, our APIs, and our omnichannel services in a secure and reliable manner. This means that choosing a metrics platform and provider is not just a technical decision, but one that&amp;#8217;s critical to our business as well.&lt;/p&gt;&lt;p&gt;As we moved more of our operations to Google Kubernetes Engine, we evaluated new metrics platforms and about eight months ago decided on Prometheus. Prometheus is a good fit for our environment, which contains cloud native applications that are built on Kubernetes and run on ephemeral compute infrastructure. Although first we built and ran our own Prometheus environment, we decided that Google Cloud Managed Service for Prometheus is a better solution for us. It allows us to focus on using our metrics instead of managing metrics infrastructure, providing the following features:&amp;#160;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Long-term retention of metrics (2 years)&lt;/li&gt;&lt;li&gt;Seamless support for high availability of Prometheus instances&amp;#160;&lt;/li&gt;&lt;li&gt;Scraping and evaluating rules using lightweight Kubernetes Custom Resources&amp;#160;&lt;/li&gt;&lt;li&gt;A global query view&amp;#160;&lt;/li&gt;&lt;li&gt;Fully managed collection and querying, out of the box&amp;#160;&amp;#160;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For other organizations that are running their own Prometheus or deciding whether to run their own Prometheus, we wanted to provide you a look into our journey moving from self-hosted Prometheus to using a managed Prometheus service:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;We first switched from traditional managed metrics services to Prometheus to support our growing Kubernetes environment.&amp;#160;&amp;#160;&lt;/li&gt;&lt;li&gt;We then built out and maintained our own Prometheus environment, but realized we needed additional features to support production workloads (such as high availability and faster MTTR) and scale.&lt;/li&gt;&lt;li&gt;Next we investigated solutions to help support our use of Prometheus in production, which led us to adopting Thanos.&lt;/li&gt;&lt;li&gt;Then we discovered we were still investing lots of time and personnel to run these technologies and manage infrastructure.&amp;#160;&lt;/li&gt;&lt;li&gt;Finally we adopted Google Cloud Managed Service for Prometheus, which so far has met our needs quite well.&amp;#160;&amp;#160;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Open source Prometheus worked well when we had a smaller deployment&amp;#160;&lt;/h3&gt;&lt;p&gt;Commonly, organizations approach metrics using either a full service monitoring and metrics storage tool or an open source database to store their metrics, displaying them using a visualization tool like Grafana.&lt;/p&gt;&lt;p&gt;For the past five years, we used managed monitoring and storage services from two providers. Using different vendors created complexity for our operations teams and it made it hard for metrics to be shared with the teams who would eventually be responsible for maintaining reliability: the application teams! &lt;b&gt;It is our belief that ownership of the metrics should reside with the application teams. It allows developers and product owners to maintain the metrics they deem essential for alerting and dashboarding.&amp;#160;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;After we made the switch to Prometheus, our experience was great&amp;#8212;at first. Prometheus&amp;#8217; design allows each application to expose granular metrics which are gathered with a Prometheus collector and stored in a Prometheus database.&amp;#160;&lt;/p&gt;&lt;p&gt;However, as we deployed it on an increasing number of GKE clusters used to run our production applications, we ran into some constraints. These included:&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Support for scaled management&amp;#160;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;We&amp;#8217;re moving our business towards an &amp;#8220;infrastructure-as-code&amp;#8221; model to deploy and manage resources because it is more efficient and results in fewer errors. We need a simple way to deploy Prometheus in each&lt;a href=&#34;https://kubernetes.io/&#34; target=&#34;_blank&#34;&gt; Kubernetes&lt;/a&gt; cluster by policy.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Retention&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Prometheus&amp;#8217; default time series database retention is set to &lt;a href=&#34;https://prometheus.io/docs/prometheus/latest/storage/&#34; target=&#34;_blank&#34;&gt;15 days&lt;/a&gt;. The database retention is configurable, but it will increase your costs and resource consumption to keep your metrics on disk for longer periods. We need a better way to manage metrics retention for longer periods of time for all our Kubernetes clusters and applications.&amp;#160;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Backup/Disaster recovery and restoration&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Business continuity considerations are important for any service used in production. In Prometheus we found disk failures and backups to be a pain point. We need ways to scalably backup and restore data on Prometheus instances when failures occur to avoid data loss.&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;Operational scalability&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Furthermore, Prometheus doesn&amp;#8217;t offer a native sharding feature, which may be a strength from an administration or deployment point of view, but ends up being a weakness if you have multiple clusters to monitor.&amp;#160;&lt;/p&gt;&lt;p&gt;Updates&lt;/p&gt;&lt;p&gt;Prometheus stores rules within a static file, which means you have to reboot your Prometheus instances in order to apply rules file updates.&amp;#160;&amp;#160;&lt;/p&gt;&lt;h3&gt;Using Prometheus and Thanos to address some (but not all) needs&amp;#160;&lt;/h3&gt;&lt;p&gt;After we ran into the issues raised above, we searched for ways to address them and found &lt;a href=&#34;http://thanos.io&#34; target=&#34;_blank&#34;&gt;Thanos&lt;/a&gt;. Thanos is an open-source project released in 2018 by &lt;a href=&#34;https://www.improbable.io/&#34; target=&#34;_blank&#34;&gt;Improbable.&lt;/a&gt; It helped us with multi-cluster management and data storage. Prometheus metrics can be sent to object storage services such as Google Cloud Storage, Azure Blob Storage, or AWS&amp;#8217; S3.&amp;#160;&lt;/p&gt;&lt;p&gt;Like Prometheus, Thanos&amp;#8217; architecture is extensible by design:&lt;/p&gt;"><p><i><b>Editor’s note</b>: Today we hear from Maisons du Monde, a furniture and home decor company that was founded in France over 25 years ago. They have 357 stores across France, Italy, Spain, Belgium, Luxembourg, Germany, Austria, Switzerland, Netherlands, and Portugal, and are a Google Cloud customer. They worked with their Customer Engineer, Adrien Aflalo, to prepare this story. </i></p><hr/><p>Without telemetry data like logs and metrics, we are blind in production. Our team of Operations Engineers and Site Reliability Engineers (SRE) rely on metrics data in particular to run the <a href="https://www.maisonsdumonde.com/UK/en" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://www.maisonsdumonde.com" track-metadata-module="post">Maisons du Monde website</a>, our APIs, and our omnichannel services in a secure and reliable manner. This means that choosing a metrics platform and provider is not just a technical decision, but one that’s critical to our business as well.</p><p>As we moved more of our operations to Google Kubernetes Engine, we evaluated new metrics platforms and about eight months ago decided on Prometheus. Prometheus is a good fit for our environment, which contains cloud native applications that are built on Kubernetes and run on ephemeral compute infrastructure. Although first we built and ran our own Prometheus environment, we decided that Google Cloud Managed Service for Prometheus is a better solution for us. It allows us to focus on using our metrics instead of managing metrics infrastructure, providing the following features: </p><ul><li>Long-term retention of metrics (2 years)</li><li>Seamless support for high availability of Prometheus instances </li><li>Scraping and evaluating rules using lightweight Kubernetes Custom Resources </li><li>A global query view </li><li>Fully managed collection and querying, out of the box  </li></ul><p>For other organizations that are running their own Prometheus or deciding whether to run their own Prometheus, we wanted to provide you a look into our journey moving from self-hosted Prometheus to using a managed Prometheus service:</p><ol><li>We first switched from traditional managed metrics services to Prometheus to support our growing Kubernetes environment.  </li><li>We then built out and maintained our own Prometheus environment, but realized we needed additional features to support production workloads (such as high availability and faster MTTR) and scale.</li><li>Next we investigated solutions to help support our use of Prometheus in production, which led us to adopting Thanos.</li><li>Then we discovered we were still investing lots of time and personnel to run these technologies and manage infrastructure. </li><li>Finally we adopted Google Cloud Managed Service for Prometheus, which so far has met our needs quite well.  </li></ol><h3>Open source Prometheus worked well when we had a smaller deployment </h3><p>Commonly, organizations approach metrics using either a full service monitoring and metrics storage tool or an open source database to store their metrics, displaying them using a visualization tool like Grafana.</p><p>For the past five years, we used managed monitoring and storage services from two providers. Using different vendors created complexity for our operations teams and it made it hard for metrics to be shared with the teams who would eventually be responsible for maintaining reliability: the application teams! <b>It is our belief that ownership of the metrics should reside with the application teams. It allows developers and product owners to maintain the metrics they deem essential for alerting and dashboarding. </b></p><p>After we made the switch to Prometheus, our experience was great—at first. Prometheus’ design allows each application to expose granular metrics which are gathered with a Prometheus collector and stored in a Prometheus database. </p><p>However, as we deployed it on an increasing number of GKE clusters used to run our production applications, we ran into some constraints. These included: </p><p><b><i>Support for scaled management </i></b></p><p>We’re moving our business towards an “infrastructure-as-code” model to deploy and manage resources because it is more efficient and results in fewer errors. We need a simple way to deploy Prometheus in each<a href="https://kubernetes.io/" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://kubernetes.io" track-metadata-module="post"> Kubernetes</a> cluster by policy. </p><p><b><i>Retention</i></b></p><p>Prometheus’ default time series database retention is set to <a href="https://prometheus.io/docs/prometheus/latest/storage/" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://prometheus.io" track-metadata-module="post">15 days</a>. The database retention is configurable, but it will increase your costs and resource consumption to keep your metrics on disk for longer periods. We need a better way to manage metrics retention for longer periods of time for all our Kubernetes clusters and applications.  </p><p><b><i>Backup/Disaster recovery and restoration</i></b></p><p>Business continuity considerations are important for any service used in production. In Prometheus we found disk failures and backups to be a pain point. We need ways to scalably backup and restore data on Prometheus instances when failures occur to avoid data loss.</p><p><b><i>Operational scalability</i></b></p><p>Furthermore, Prometheus doesn’t offer a native sharding feature, which may be a strength from an administration or deployment point of view, but ends up being a weakness if you have multiple clusters to monitor. </p><p>Updates</p><p>Prometheus stores rules within a static file, which means you have to reboot your Prometheus instances in order to apply rules file updates.  </p><h3>Using Prometheus and Thanos to address some (but not all) needs </h3><p>After we ran into the issues raised above, we searched for ways to address them and found <a href="http://thanos.io" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="http://thanos.io" track-metadata-module="post">Thanos</a>. Thanos is an open-source project released in 2018 by <a href="https://www.improbable.io/" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://www.improbable.io" track-metadata-module="post">Improbable.</a> It helped us with multi-cluster management and data storage. Prometheus metrics can be sent to object storage services such as Google Cloud Storage, Azure Blob Storage, or AWS’ S3. </p><p>Like Prometheus, Thanos’ architecture is extensible by design:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Victor Ladouceur&lt;/name&gt;&lt;title&gt;SRE, Maisons du Monde&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 16 May 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Alerting on error log messages in Cloud SQL for SQL Server</title>
      <link>https://cloud.google.com/blog/products/databases/cloud-sql-alerting/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;With Cloud SQL for SQL Server, you can bring your existing SQL Server on-premises workloads to Google Cloud. Cloud SQL takes care of infrastructure, maintenance, and patching so you can focus on your application and users. A great way to take better care of your application is by monitoring the SQL Server error log for issues that may be affecting your users such as deadlocks, job failures, and changes in database health.&lt;/p&gt;&lt;h3&gt;Cloud SQL for SQL Server and Cloud Operations Suite &lt;/h3&gt;&lt;p&gt;You can monitor and alert on messages in the Cloud SQL for SQL Server error log using the Google Cloud Operations Suite. Operations Suite is Google’s Cloud Observability solution allowing customers to have visibility into their infrastructure and applications. Using Cloud Operations Suite, you can monitor and alert for multiple instances at scale, and can set up alerting through your preferred method such as PagerDuty, Slack, email or a custom webhook. &lt;/p&gt;&lt;p&gt;The tools we will be using from the Operations Suite are Cloud Monitoring and Cloud Logging. Cloud Logging allows you to view logs from applications and services and allows you to create custom metrics from those logs. Cloud Monitoring allows you to create alerting policies to notify you when metrics, health check, and uptime check results meet specified criteria. To demonstrate how this works, we will enable deadlock detection on our Cloud SQL Instance, create a log based metric to monitor when deadlocks are detected, and create an alerting policy on the newly created log based metric. The architecture for monitoring SQL Server error log messages is shown below:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_CloudSQLErrorLog.0444023208200400.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 CloudSQLErrorLog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_CloudSQLErrorLog.0444023208200400.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Configuration of Cloud SQL for SQL Server&lt;/h3&gt;&lt;p&gt;What you will need: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/sql/docs/sqlserver/quickstart&#34;&gt;Cloud SQL for SQL Server&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15&#34; target=&#34;_blank&#34;&gt;Azure Data Studio&lt;/a&gt; &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/sql/docs/sqlserver/sql-proxy&#34;&gt;Cloud SQL Proxy &lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Email Address for alert message&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Let&#39;s break down how you can set this up. First you need a Cloud SQL for SQL Server instance here are the steps to set up one quickly:&lt;/p&gt;&lt;p&gt;1 .In the Google Cloud Console, go to the &lt;a href=&#34;https://console.cloud.google.com/sql&#34;&gt;&lt;b&gt;Cloud SQL Instances&lt;/b&gt;&lt;/a&gt; page.&lt;/p&gt;&lt;p&gt;2. Click &lt;b&gt;Create Instance&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;3. Click &lt;b&gt;Choose SQL Server&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;4. Enter a name for &lt;b&gt;Instance ID&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;5. Enter a password for the sqlserver user.&lt;/p&gt;&lt;p&gt;6. Expand &lt;b&gt;Show Configuration Options&lt;/b&gt;&lt;/p&gt;&lt;p&gt;7. Under &lt;b&gt;Flags and Parameters&lt;/b&gt; add the following trace flags:&lt;br/&gt;a. 1222&lt;br/&gt;b. 1204&lt;/p&gt;&lt;p&gt;8. Click &lt;b&gt;Create Instance&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;If you already have a Cloud SQL for SQL Server instance you would need to edit your Cloud SQL for SQL Server Instance.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_CloudSQLErrorLog.1000064320000386.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 CloudSQLErrorLog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_CloudSQLErrorLog.1000064320000386.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In the edit screen you will need to go to “Flags and parameters” to add and enable SQL Server trace flags 1204 and 1222. These flags enable deadlock detection messages into the SQL Server error log. Your instance will need to be restarted after this change. More details on editing your Cloud SQL for SQL Server instance can be found &lt;a href=&#34;https://cloud.google.com/sql/docs/sqlserver/edit-instance&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_CloudSQLErrorLog.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;3 CloudSQLErrorLog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_CloudSQLErrorLog.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Connecting to your Cloud SQL for SQL Server instance&lt;/h3&gt;&lt;p&gt;Perform the following steps to connect to your Cloud SQL for SQL Server Instance from your local machine.&lt;/p&gt;&lt;p&gt;1. Install the &lt;a href=&#34;https://cloud.google.com/sdk/docs&#34;&gt;Google Cloud CLI&lt;/a&gt;. The Google Cloud CLI provides the gcloud CLI to interact with Cloud SQL and other Google Cloud services. The gcloud CLI uses the Admin API to access Cloud SQL, so you must &lt;a href=&#34;https://cloud.google.com/sql/docs/sqlserver/admin-api#enabling_the_api&#34;&gt;Enable the Admin API&lt;/a&gt; before using the gcloud CLI to access Cloud SQL.&lt;/p&gt;&lt;p&gt;2. In a bash shell command prompt or in Windows PowerShell, run the following command to initialize the gcloud CLI: gcloud auth login &lt;/p&gt;&lt;p&gt;3. Run the following command to authenticate the gcloud CLI: gcloud auth login&lt;/p&gt;&lt;p&gt;4. Download and install the Cloud SQL Auth proxy (see &lt;a href=&#34;https://cloud.google.com/sql/docs/sqlserver/connect-admin-proxy#install&#34;&gt;Installing the Cloud SQL Auth proxy&lt;/a&gt;). Note the location of the Cloud SQL Auth proxy because you will run the Cloud SQL Auth proxy in the next step.&lt;/p&gt;&lt;p&gt;5. Run the Cloud SQL Auth proxy by using a bash shell command prompt (or by using Windows PowerShell). Specifically, run the following command, replacing Instance-connection-name with the corresponding value from the Google Cloud Console&#39;s Overview tab (for your instance): ./cloud_sql_proxy -instances=INSTANCE_CONNECTION_NAME=tcp:1433&lt;/p&gt;&lt;p&gt;6. In Azure Data Studio Create a New Connection&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/4_CloudSQLErrorLog.1000063420000487.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;4 CloudSQLErrorLog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/4_CloudSQLErrorLog.1000063420000487.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;7. Enter the following values in the &lt;b&gt;Connection&lt;/b&gt; dialog:&lt;/p&gt;&lt;p&gt;a. For Server Type, enter &lt;b&gt;Microsoft SQL Server&lt;/b&gt;&lt;/p&gt;&lt;p&gt;b. For Server, enter 127.0.0.1 as the IP address of your SQL Server instance.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;For TCP connections, the Cloud SQL Auth proxy listens on localhost(127.0.0.1) by default and since we are using Cloud SQL Auth Proxy to connect Azure Data Studio to our Cloud SQL instance that is the IP address we must use.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;c. For Authentication, enter &lt;b&gt;SQL Login&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;d. For Login, enter &lt;b&gt;sqlserver&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;e. For Password, enter the password used when the instance was created.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/5_CloudSQLErrorLog.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;5 CloudSQLErrorLog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/5_CloudSQLErrorLog.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;8. Click Connect &lt;/p&gt;&lt;h3&gt;Creating a deadlock &lt;/h3&gt;&lt;p&gt;Now that you are connected to Azure Data Studio you can run the follow T-SQL code to create temporary tables on the SQL Server instance.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;CREATE TABLE ##Product (\r\n ProductId INT IDENTITY,\r\n ProductName VARCHAR(10),\r\n Description VARCHAR(12)\r\n)\r\nGO\r\nINSERT INTO ##Product (ProductName, Description)\r\nVALUES (&#39;Boat&#39;, &#39;Water&#39;), (&#39;Plane&#39;, &#39;Air&#39;), (&#39;Car&#39;, &#39;Ground&#39;)\r\nGO\r\nCREATE TABLE ##Vendor(\r\n VendorId INT IDENTITY,\r\n VendorName VARCHAR(10),\r\n State VARCHAR(2)\r\n)\r\nGO\r\nINSERT INTO ##Vendor (VendorName, State)\r\nVALUES (&#39;XYZ&#39;, &#39;NY&#39;), (&#39;ABC&#39;, &#39;OH&#39;)\r\nGO&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Next to create a deadlock you will need to open two query sessions in Azure Data Studio and you must run each command one step at a time in the order specified here:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/6_CloudSQLErrorLog.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;6 CloudSQLErrorLog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/6_CloudSQLErrorLog.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;You should receive an error saying one of your sessions was deadlocked.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/7_CloudSQLErrorLog.1000067920000475.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;7 CloudSQLErrorLog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/7_CloudSQLErrorLog.1000067920000475.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Viewing the error log in Log Explorer&lt;/h3&gt;&lt;p&gt;Now we can view the SQL Server Error Log by going to Cloud Logging in the Google Cloud Console. Logging can be found in the Operations section of the navigation bar or you can type “logging” into the search bar in Google Cloud Console.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/8_CloudSQLErrorLog.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;8 CloudSQLErrorLog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/8_CloudSQLErrorLog.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Now in Cloud Logging Log Explorer section you will want to create a query to filter the proper results:&lt;/p&gt;&lt;p&gt;Resource should be → Cloud SQL Database → Cloud SQL For SQL Server Instance Name&lt;/p&gt;&lt;p&gt;Log should be → Cloud SQL Log → sqlserver.err &lt;/p&gt;&lt;p&gt;Now you should be able to see the deadlock messages in the log.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/9_CloudSQLErrorLog.1000064720000445.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;9 CloudSQLErrorLog.png&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/9_CloudSQLErrorLog.1000064720000445.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Creating a custom log-based metric and alerting policy&lt;/h3&gt;&lt;p&gt;To identify a deadlock message to use for your custom metric, we should create a custom query filter in log explorer. You can enter the query below into log explorer.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;resource.type=&#34;cloudsql_database&#34; resource.labels.database_id=&#34;&amp;lt;YourGoogleCloudProject&amp;gt;:&amp;lt;YourCloudSQLInstance&amp;gt;&#34;\r\nlogName=&#34;projects/&amp;lt;YourGoogleCloudProject&amp;gt;/logs/cloudsql.googleapis.com%2Fsqlserver.err&#34;\r\ntextPayload=~&#34;Deadlock encountered .... Printing deadlock information&#34;&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&amp;lt;YourGoogleCloudProject&amp;gt; is the name of the project your Cloud SQL instance is in and &amp;lt;YourCloudSQLInstance&amp;gt; is the name of your Cloud SQL for SQL Server instance. &lt;/p&gt;&lt;p&gt;Now you will see the single deadlock log entry. In the query results section there is an &lt;b&gt;action&lt;/b&gt; button on the right hand side. Click action and select “Create metric”.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/10_CloudSQLErrorLog.max-2800x2800.png&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;10 CloudSQLErrorLog.png&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/10_CloudSQLErrorLog.max-1000x1000.png&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;This will open a new window called &lt;b&gt;Create logs metric&lt;/b&gt;. Here you can give your custom metric a name and description. Keep it as a counter metric and leave the unit as 1. Add any &lt;a href=&#34;https://cloud.google.com/resource-manager/docs/creating-managing-labels&#34;&gt;labels&lt;/a&gt; you like and click &lt;b&gt;Create Metric&lt;/b&gt;. A label is a key-value pair that helps you organize your Google Cloud resources.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;11 CloudSQLErrorLog.png&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/11_CloudSQLErrorLog.max-1000x1000.png&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;This will give you a new user-defined metric to monitor and track deadlocks. In the User-defined metrics section, click on the three dots on the right side of your custom metric name. You will see options to &lt;b&gt;View in Metrics Explorer&lt;/b&gt; and &lt;b&gt;Create alert from metric&lt;/b&gt;. If you want to view the metric in Metric Explorer you will need to trigger a new deadlock to see data. &lt;/p&gt;&lt;p&gt;Now, let’s create an alert policy. Click on &lt;b&gt;Create alert from metric&lt;/b&gt; to define an alerting policy for your new deadlock metric.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/12_CloudSQLErrorLog.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;12 CloudSQLErrorLog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/12_CloudSQLErrorLog.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Clicking the &lt;b&gt;Create alert from metric&lt;/b&gt; link should have taken you straight to the alerting policy UI, where you can create an alert and identify specific conditions in which that alert should fire. In the condition section, your custom metric should have already been selected for you. You can leave everything as default and then select &lt;b&gt;Notifications and name&lt;/b&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/13_CloudSQLErrorLog.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;13 CloudSQLErrorLog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/13_CloudSQLErrorLog.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Next decide who should be notified when this alert is triggered. Before you do that you need to set a &lt;a href=&#34;https://cloud.google.com/monitoring/support/notification-options#creating_channels&#34;&gt;notification channel&lt;/a&gt;. Notification channels can be an email address or it can be various integration tools such as Slack and PagerDuty.  After you select who should be notified, name your alert and add instructions on how to resolve the alert. Now save the alert and you are done! I would recommend you test out the new alert by forcing another deadlock. Congratulations - now you know how to create alerts based on SQL Server Error Log messages. &lt;/p&gt;&lt;p&gt;You can create these types of alerts for more than just deadlocks: you can set alerts to monitor for other messages that show up in the error log such as crash dumps, connections issues, and corruption.  You can also create alerts based on SQL Server Agent Log messages.  Here are a few more examples listed below with the string from the SQL error logs that you can use as the text Payload in your custom metric. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Agent XPs disabled:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SQL Server Error Logs will include the string &lt;code&gt;&#34;Configuration option &#39;Agent XPs&#39; changed from 1 to 0.&#34;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;SQL Server Agent Status:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SQL Server Agent Logs will include the string &lt;code&gt;“SQLServerAgent terminated&#34;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;Job Failures:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SQL Server Agent Logs will include a string similar to  “&lt;code&gt;SQL Server Scheduled Job &#39;demo&#39; (0xB83611A22D4FD74B8900ADDFDC9CDD9C) - Status: Failed - Invoked on:&lt;/code&gt;”&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Option below needs to be checked or set using tsql&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/14_CloudSQLErrorLog.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;14 CloudSQLErrorLog.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/14_CloudSQLErrorLog.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Database Status:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SQL Server Error logs will include one of the following strings:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;“&lt;code&gt;Database % cannot be opened. It has been marked SUSPECT&lt;/code&gt;”&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;“&lt;code&gt;Database % database is in emergency or suspect mode&lt;/code&gt;”&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;“&lt;code&gt;database % is marked EMERGENCY_MODE&lt;/code&gt;”&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;“&lt;code&gt;Database % cannot be opened because it is offline.&lt;/code&gt;”&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;“&lt;code&gt;Setting database option OFFLINE to ON for database&lt;/code&gt;”&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;Custom error messages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;Using SQL Agent jobs, the &lt;a href=&#34;https://docs.microsoft.com/en-us/sql/t-sql/language-elements/raiserror-transact-sql?view=sql-server-ver15&#34; target=&#34;_blank&#34;&gt;RAISERROR&lt;/a&gt; with log command can be used to write custom messages in the SQL error logs. This could be triggered when any application or database condition is met. One way to do this is to create a SQL Agent job and define a job step with a simple query like the one below. &lt;br/&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;declare\r\n @LongRunningJobThreshold int=300,\r\n @runningtime int=0\r\n \r\nselect @runningtime=max(er.total_elapsed_time)/1000\r\nfrom sys.dm_exec_requests er\r\ninner join sys.sysprocesses p on er.session_id = p.spid\r\nwhere p.program_name like &#39;%SQLAgent%&#39;\r\n \r\nif @runningtime &amp;gt; @LongRunningJobThreshold\r\nRAISERROR (&#39;long running sql agent job&#39;,16,1) with LOG&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Then schedule this to run every couple of minutes. This will produce an error log  message and textPayload as in the image below. The same steps can be used for alerting and monitoring as described above for notifications&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/15_CloudSQLErrorLog.max-2800x2800.png&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;15 CloudSQLErrorLog.png&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/15_CloudSQLErrorLog.max-1000x1000.png&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Thanks for reading.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><router-outlet></router-outlet><dynamic-page><article-page><main id="jump-content"><promo-banner-block _nghost-c56=""></promo-banner-block><article><article-header-block></article-header-block><div><div><article-author-block><div><div><p> Bryan Hamilton </p><p> Database Engineer, SQL Server, Google Cloud </p></div><p><span> May 16, 2022 </span></p></div></article-author-block></div><article-cta _nghost-c58=""><div _ngcontent-c58=""><h4 _ngcontent-c58=""><span _ngcontent-c58="">Try Google Cloud</span></h4><p _ngcontent-c58=""><span _ngcontent-c58="">Start building on Google Cloud with $300 in free credits and 20+ always free products.</span></p><p><a _ngcontent-c58="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="free trial" track-metadata-eventdetail="https://cloud.google.com/free/" href="https://cloud.google.com/free/"><span _ngcontent-c58="">Free Trial</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;With Cloud SQL for SQL Server, you can bring your existing SQL Server on-premises workloads to Google Cloud. Cloud SQL takes care of infrastructure, maintenance, and patching so you can focus on your application and users. A great way to take better care of your application is by monitoring the SQL Server error log for issues that may be affecting your users such as deadlocks, job failures, and changes in database health.&lt;/p&gt;&lt;h3&gt;Cloud SQL for SQL Server and Cloud Operations Suite&amp;#160;&lt;/h3&gt;&lt;p&gt;You can monitor and alert on messages in the Cloud SQL for SQL Server error log using the Google Cloud Operations Suite. Operations Suite is Google&amp;#8217;s Cloud Observability solution allowing customers to have visibility into their infrastructure and applications. Using Cloud Operations Suite, you can monitor and alert for multiple instances at scale, and can set up alerting through your preferred method such as PagerDuty, Slack, email or a custom webhook.&amp;#160;&lt;/p&gt;&lt;p&gt;The tools we will be using from the Operations Suite are Cloud Monitoring and Cloud Logging. Cloud Logging allows you to view logs from applications and services and allows you to create custom metrics from those logs. Cloud Monitoring allows you to create alerting policies to notify you when metrics, health check, and uptime check results meet specified criteria. To demonstrate how this works, we will enable deadlock detection on our Cloud SQL Instance, create a log based metric to monitor when deadlocks are detected, and create an alerting policy on the newly created log based metric. The architecture for monitoring SQL Server error log messages is shown below:&lt;/p&gt;"><p>With Cloud SQL for SQL Server, you can bring your existing SQL Server on-premises workloads to Google Cloud. Cloud SQL takes care of infrastructure, maintenance, and patching so you can focus on your application and users. A great way to take better care of your application is by monitoring the SQL Server error log for issues that may be affecting your users such as deadlocks, job failures, and changes in database health.</p><h3>Cloud SQL for SQL Server and Cloud Operations Suite </h3><p>You can monitor and alert on messages in the Cloud SQL for SQL Server error log using the Google Cloud Operations Suite. Operations Suite is Google’s Cloud Observability solution allowing customers to have visibility into their infrastructure and applications. Using Cloud Operations Suite, you can monitor and alert for multiple instances at scale, and can set up alerting through your preferred method such as PagerDuty, Slack, email or a custom webhook. </p><p>The tools we will be using from the Operations Suite are Cloud Monitoring and Cloud Logging. Cloud Logging allows you to view logs from applications and services and allows you to create custom metrics from those logs. Cloud Monitoring allows you to create alerting policies to notify you when metrics, health check, and uptime check results meet specified criteria. To demonstrate how this works, we will enable deadlock detection on our Cloud SQL Instance, create a log based metric to monitor when deadlocks are detected, and create an alerting policy on the newly created log based metric. The architecture for monitoring SQL Server error log messages is shown below:</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h3&gt;Configuration of Cloud SQL for SQL Server&lt;/h3&gt;&lt;p&gt;What you will need:&amp;#160;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/sql/docs/sqlserver/quickstart&#34;&gt;Cloud SQL for SQL Server&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15&#34; target=&#34;_blank&#34;&gt;Azure Data Studio&lt;/a&gt;&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/sql/docs/sqlserver/sql-proxy&#34;&gt;Cloud SQL Proxy&amp;#160;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Email Address for alert message&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Let&#39;s break down how you can set this up. First you need a Cloud SQL for SQL Server instance here are the steps to set up one quickly:&lt;/p&gt;&lt;p&gt;1 .In the Google Cloud Console, go to the &lt;a href=&#34;https://console.cloud.google.com/sql&#34;&gt;&lt;b&gt;Cloud SQL Instances&lt;/b&gt;&lt;/a&gt; page.&lt;/p&gt;&lt;p&gt;2. Click &lt;b&gt;Create Instance&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;3. Click &lt;b&gt;Choose SQL Server&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;4. Enter a name for &lt;b&gt;Instance ID&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;5. Enter a password for the sqlserver user.&lt;/p&gt;&lt;p&gt;6. Expand &lt;b&gt;Show Configuration Options&lt;/b&gt;&lt;/p&gt;&lt;p&gt;7. Under &lt;b&gt;Flags and Parameters&lt;/b&gt; add the following trace flags:&lt;br&gt;a. 1222&lt;br&gt;b. 1204&lt;/p&gt;&lt;p&gt;8. Click &lt;b&gt;Create Instance&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;If you already have a Cloud SQL for SQL Server instance you would need to edit your Cloud SQL for SQL Server Instance.&lt;/p&gt;"><h3>Configuration of Cloud SQL for SQL Server</h3><p>What you will need: </p><ul><li><p><a href="https://cloud.google.com/sql/docs/sqlserver/quickstart" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/sql/docs/sqlserver/quickstart" track-metadata-module="post">Cloud SQL for SQL Server</a></p></li><li><p><a href="https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://docs.microsoft.com" track-metadata-module="post">Azure Data Studio</a> </p></li><li><p><a href="https://cloud.google.com/sql/docs/sqlserver/sql-proxy" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/sql/docs/sqlserver/sql-proxy" track-metadata-module="post">Cloud SQL Proxy </a></p></li><li><p>Email Address for alert message</p></li></ul><p>Let&#39;s break down how you can set this up. First you need a Cloud SQL for SQL Server instance here are the steps to set up one quickly:</p><p>1 .In the Google Cloud Console, go to the <a href="https://console.cloud.google.com/sql" track-type="inline link" track-name="4" track-metadata-eventdetail="https://console.cloud.google.com/sql" track-metadata-module="post"><b>Cloud SQL Instances</b></a> page.</p><p>2. Click <b>Create Instance</b>.</p><p>3. Click <b>Choose SQL Server</b>.</p><p>4. Enter a name for <b>Instance ID</b>.</p><p>5. Enter a password for the sqlserver user.</p><p>6. Expand <b>Show Configuration Options</b></p><p>7. Under <b>Flags and Parameters</b> add the following trace flags:<br/>a. 1222<br/>b. 1204</p><p>8. Click <b>Create Instance</b>.</p><p>If you already have a Cloud SQL for SQL Server instance you would need to edit your Cloud SQL for SQL Server Instance.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>In the edit screen you will need to go to “Flags and parameters” to add and enable SQL Server trace flags 1204 and 1222. These flags enable deadlock detection messages into the SQL Server error log. Your instance will need to be restarted after this change. More details on editing your Cloud SQL for SQL Server instance can be found <a href="https://cloud.google.com/sql/docs/sqlserver/edit-instance" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/sql/docs/sqlserver/edit-instance" track-metadata-module="post">here</a>.</p></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h3&gt;Connecting to your Cloud SQL for SQL Server instance&lt;/h3&gt;&lt;p&gt;Perform the following steps to connect to your Cloud SQL for SQL Server Instance from your local machine.&lt;/p&gt;&lt;p&gt;1. Install the &lt;a href=&#34;https://cloud.google.com/sdk/docs&#34;&gt;Google Cloud CLI&lt;/a&gt;. The Google Cloud CLI provides the gcloud CLI to interact with Cloud SQL and other Google Cloud services. The gcloud CLI uses the Admin API to access Cloud SQL, so you must &lt;a href=&#34;https://cloud.google.com/sql/docs/sqlserver/admin-api#enabling_the_api&#34;&gt;Enable the Admin API&lt;/a&gt; before using the gcloud CLI to access Cloud SQL.&lt;/p&gt;&lt;p&gt;2. In a bash shell command prompt or in Windows PowerShell, run the following command to initialize the gcloud CLI:&amp;#160;gcloud auth login&amp;#160;&lt;/p&gt;&lt;p&gt;3. Run the following command to authenticate the gcloud CLI:&amp;#160;gcloud auth login&lt;/p&gt;&lt;p&gt;4. Download and install the Cloud SQL Auth proxy (see &lt;a href=&#34;https://cloud.google.com/sql/docs/sqlserver/connect-admin-proxy#install&#34;&gt;Installing the Cloud SQL Auth proxy&lt;/a&gt;). Note the location of the Cloud SQL Auth proxy because you will run the Cloud SQL Auth proxy in the next step.&lt;/p&gt;&lt;p&gt;5. Run the Cloud SQL Auth proxy by using a bash shell command prompt (or by using Windows PowerShell). Specifically, run the following command, replacing Instance-connection-name with the corresponding value from the Google Cloud Console&#39;s Overview tab (for your instance):&amp;#160;./cloud_sql_proxy -instances=INSTANCE_CONNECTION_NAME=tcp:1433&lt;/p&gt;&lt;p&gt;6. In Azure Data Studio Create a New Connection&lt;/p&gt;"><h3>Connecting to your Cloud SQL for SQL Server instance</h3><p>Perform the following steps to connect to your Cloud SQL for SQL Server Instance from your local machine.</p><p>1. Install the <a href="https://cloud.google.com/sdk/docs" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/sdk/docs" track-metadata-module="post">Google Cloud CLI</a>. The Google Cloud CLI provides the gcloud CLI to interact with Cloud SQL and other Google Cloud services. The gcloud CLI uses the Admin API to access Cloud SQL, so you must <a href="https://cloud.google.com/sql/docs/sqlserver/admin-api#enabling_the_api" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/sql/docs/sqlserver/admin-api#enabling_the_api" track-metadata-module="post">Enable the Admin API</a> before using the gcloud CLI to access Cloud SQL.</p><p>2. In a bash shell command prompt or in Windows PowerShell, run the following command to initialize the gcloud CLI: gcloud auth login </p><p>3. Run the following command to authenticate the gcloud CLI: gcloud auth login</p><p>4. Download and install the Cloud SQL Auth proxy (see <a href="https://cloud.google.com/sql/docs/sqlserver/connect-admin-proxy#install" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/sql/docs/sqlserver/connect-admin-proxy#install" track-metadata-module="post">Installing the Cloud SQL Auth proxy</a>). Note the location of the Cloud SQL Auth proxy because you will run the Cloud SQL Auth proxy in the next step.</p><p>5. Run the Cloud SQL Auth proxy by using a bash shell command prompt (or by using Windows PowerShell). Specifically, run the following command, replacing Instance-connection-name with the corresponding value from the Google Cloud Console&#39;s Overview tab (for your instance): ./cloud_sql_proxy -instances=INSTANCE_CONNECTION_NAME=tcp:1433</p><p>6. In Azure Data Studio Create a New Connection</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;7. Enter the following values in the &lt;b&gt;Connection&lt;/b&gt; dialog:&lt;/p&gt;&lt;p&gt;a. For Server Type, enter &lt;b&gt;Microsoft SQL Server&lt;/b&gt;&lt;/p&gt;&lt;p&gt;b. For Server, enter 127.0.0.1 as the IP address of your SQL Server instance.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;For TCP connections, the Cloud SQL Auth proxy listens on localhost(127.0.0.1) by default and since we are using Cloud SQL Auth Proxy to connect Azure Data Studio to our Cloud SQL instance that is the IP address we must use.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;c. For Authentication, enter &lt;b&gt;SQL Login&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;d. For Login, enter &lt;b&gt;sqlserver&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;e. For Password, enter the password used when the instance was created.&lt;/p&gt;"><p>7. Enter the following values in the <b>Connection</b> dialog:</p><p>a. For Server Type, enter <b>Microsoft SQL Server</b></p><p>b. For Server, enter 127.0.0.1 as the IP address of your SQL Server instance.</p><ul><li>For TCP connections, the Cloud SQL Auth proxy listens on localhost(127.0.0.1) by default and since we are using Cloud SQL Auth Proxy to connect Azure Data Studio to our Cloud SQL instance that is the IP address we must use.</li></ul><p>c. For Authentication, enter <b>SQL Login</b>.</p><p>d. For Login, enter <b>sqlserver</b>.</p><p>e. For Password, enter the password used when the instance was created.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;8. Click Connect&amp;#160;&lt;/p&gt;&lt;h3&gt;Creating a deadlock&amp;#160;&lt;/h3&gt;&lt;p&gt;Now that you are connected to Azure Data Studio you can run the follow T-SQL code to create temporary tables on the SQL Server instance.&lt;/p&gt;"><p>8. Click Connect </p><h3>Creating a deadlock </h3><p>Now that you are connected to Azure Data Studio you can run the follow T-SQL code to create temporary tables on the SQL Server instance.</p></div></paragraph-block></div><div><article-code-block _nghost-c61=""><pre _ngcontent-c61="">  <code _ngcontent-c61="">CREATE TABLE ##Product (
</code><code _ngcontent-c61="">   ProductId INT IDENTITY,
</code><code _ngcontent-c61="">   ProductName VARCHAR(10),
</code><code _ngcontent-c61="">   Description VARCHAR(12)
</code><code _ngcontent-c61="">)
</code><code _ngcontent-c61="">GO
</code><code _ngcontent-c61="">INSERT INTO ##Product (ProductName, Description)
</code><code _ngcontent-c61="">VALUES (&#39;Boat&#39;, &#39;Water&#39;), (&#39;Plane&#39;, &#39;Air&#39;), (&#39;Car&#39;, &#39;Ground&#39;)
</code><code _ngcontent-c61="">GO
</code><code _ngcontent-c61="">CREATE TABLE ##Vendor(
</code><code _ngcontent-c61="">   VendorId INT IDENTITY,
</code><code _ngcontent-c61="">   VendorName VARCHAR(10),
</code><code _ngcontent-c61="">   State VARCHAR(2)
</code><code _ngcontent-c61="">)
</code><code _ngcontent-c61="">GO
</code><code _ngcontent-c61="">INSERT INTO ##Vendor (VendorName, State)
</code><code _ngcontent-c61="">VALUES (&#39;XYZ&#39;, &#39;NY&#39;), (&#39;ABC&#39;, &#39;OH&#39;)
</code><code _ngcontent-c61="">GO</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c60=""><p>Next to create a deadlock you will need to open two query sessions in Azure Data Studio and you must run each command one step at a time in the order specified here:</p></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>You should receive an error saying one of your sessions was deadlocked. </p></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h3&gt;Viewing the error log in Log Explorer&lt;/h3&gt;&lt;p&gt;Now we can view the SQL Server Error Log by going to Cloud Logging in the Google Cloud Console. Logging can be found in the Operations section of the navigation bar or you can type &amp;#8220;logging&amp;#8221; into the search bar in Google Cloud Console.&lt;/p&gt;"><h3>Viewing the error log in Log Explorer</h3><p>Now we can view the SQL Server Error Log by going to Cloud Logging in the Google Cloud Console. Logging can be found in the Operations section of the navigation bar or you can type “logging” into the search bar in Google Cloud Console.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;Now in Cloud Logging Log Explorer section you will want to create a query to filter the proper results:&lt;/p&gt;&lt;p&gt;Resource should be &amp;#8594; Cloud SQL Database &amp;#8594; Cloud SQL For SQL Server Instance Name&lt;/p&gt;&lt;p&gt;Log should be &amp;#8594; Cloud SQL Log &amp;#8594; sqlserver.err&amp;#160;&lt;/p&gt;&lt;p&gt;Now you should be able to see the deadlock messages in the log.&lt;/p&gt;"><p>Now in Cloud Logging Log Explorer section you will want to create a query to filter the proper results:</p><p>Resource should be → Cloud SQL Database → Cloud SQL For SQL Server Instance Name</p><p>Log should be → Cloud SQL Log → sqlserver.err </p><p>Now you should be able to see the deadlock messages in the log.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h3&gt;Creating a custom log-based metric and alerting policy&lt;/h3&gt;&lt;p&gt;To identify a deadlock message to use for your custom metric, we should create a custom query filter in log explorer. You can enter the query below into log explorer.&lt;/p&gt;"><h3>Creating a custom log-based metric and alerting policy</h3><p>To identify a deadlock message to use for your custom metric, we should create a custom query filter in log explorer. You can enter the query below into log explorer.</p></div></paragraph-block></div><div><article-code-block _nghost-c61=""><pre _ngcontent-c61="">  <code _ngcontent-c61="">resource.type=&#34;cloudsql_database&#34; resource.labels.database_id=&#34;&lt;YourGoogleCloudProject&gt;:&lt;YourCloudSQLInstance&gt;&#34;
</code><code _ngcontent-c61="">logName=&#34;projects/&lt;YourGoogleCloudProject&gt;/logs/cloudsql.googleapis.com%2Fsqlserver.err&#34;
</code><code _ngcontent-c61="">textPayload=~&#34;Deadlock encountered .... Printing deadlock information&#34;</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;&amp;lt;YourGoogleCloudProject&amp;gt; is the name of the project your Cloud SQL instance is in and &amp;lt;YourCloudSQLInstance&amp;gt; is the name of your Cloud SQL for SQL Server instance.&amp;#160;&lt;/p&gt;&lt;p&gt;Now you will see the single deadlock log entry. In the query results section there is an &lt;b&gt;action&lt;/b&gt; button on the right hand side. Click action and select &amp;#8220;Create metric&amp;#8221;.&lt;/p&gt;"><p>&lt;YourGoogleCloudProject&gt; is the name of the project your Cloud SQL instance is in and &lt;YourCloudSQLInstance&gt; is the name of your Cloud SQL for SQL Server instance. </p><p>Now you will see the single deadlock log entry. In the query results section there is an <b>action</b> button on the right hand side. Click action and select “Create metric”.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>This will open a new window called <b>Create logs metric</b>. Here you can give your custom metric a name and description. Keep it as a counter metric and leave the unit as 1. Add any <a href="https://cloud.google.com/resource-manager/docs/creating-managing-labels" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/resource-manager/docs/creating-managing-labels" track-metadata-module="post">labels</a> you like and click <b>Create Metric</b>. A label is a key-value pair that helps you organize your Google Cloud resources.</p></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;This will give you a new user-defined metric to monitor and track deadlocks. In the User-defined metrics section, click on the three dots on the right side of your custom metric name. You will see options to &lt;b&gt;View in Metrics Explorer&lt;/b&gt; and &lt;b&gt;Create alert from metric&lt;/b&gt;. If you want to view the metric in Metric Explorer you will need to trigger a new deadlock to see data.&amp;#160;&lt;/p&gt;&lt;p&gt;Now, let&amp;#8217;s create an alert policy. Click on &lt;b&gt;Create alert from metric&lt;/b&gt; to define an alerting policy for your new deadlock metric.&lt;/p&gt;"><p>This will give you a new user-defined metric to monitor and track deadlocks. In the User-defined metrics section, click on the three dots on the right side of your custom metric name. You will see options to <b>View in Metrics Explorer</b> and <b>Create alert from metric</b>. If you want to view the metric in Metric Explorer you will need to trigger a new deadlock to see data. </p><p>Now, let’s create an alert policy. Click on <b>Create alert from metric</b> to define an alerting policy for your new deadlock metric.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><p>Clicking the <b>Create alert from metric</b> link should have taken you straight to the alerting policy UI, where you can create an alert and identify specific conditions in which that alert should fire. In the condition section, your custom metric should have already been selected for you. You can leave everything as default and then select <b>Notifications and name</b>.</p></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;Next decide who should be notified when this alert is triggered. Before you do that you need to set a &lt;a href=&#34;https://cloud.google.com/monitoring/support/notification-options#creating_channels&#34;&gt;notification channel&lt;/a&gt;. Notification channels can be an email address or it can be various integration tools such as Slack and PagerDuty.&amp;#160; After you select who should be notified, name your alert and add instructions on how to resolve the alert. Now save the alert and you are done! I would recommend you test out the new alert by forcing another deadlock. Congratulations - now you know how to create alerts based on SQL Server Error Log messages.&amp;#160;&lt;/p&gt;&lt;p&gt;You can create these types of alerts for more than just deadlocks: you can set alerts to monitor for other messages that show up in the error log such as crash dumps, connections issues, and corruption.&amp;#160; You can also create alerts based on SQL Server Agent Log messages.&amp;#160; Here are a few more examples listed below with the string from the SQL error logs that you can use as the text Payload in your custom metric.&amp;#160;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Agent XPs disabled:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SQL Server Error Logs will include the string &lt;code&gt;&amp;#34;Configuration option &#39;Agent XPs&#39; changed from 1 to 0.&amp;#34;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;SQL Server Agent Status:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SQL Server Agent Logs will include the string &lt;code&gt;&amp;#8220;SQLServerAgent terminated&amp;#34;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;Job Failures:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SQL Server Agent Logs will include a string similar to&amp;#160; &amp;#8220;&lt;code&gt;SQL Server Scheduled Job &#39;demo&#39; (0xB83611A22D4FD74B8900ADDFDC9CDD9C) - Status: Failed - Invoked on:&lt;/code&gt;&amp;#8221;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Option below needs to be checked or set using tsql&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;"><p>Next decide who should be notified when this alert is triggered. Before you do that you need to set a <a href="https://cloud.google.com/monitoring/support/notification-options#creating_channels" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/monitoring/support/notification-options#creating_channels" track-metadata-module="post">notification channel</a>. Notification channels can be an email address or it can be various integration tools such as Slack and PagerDuty.  After you select who should be notified, name your alert and add instructions on how to resolve the alert. Now save the alert and you are done! I would recommend you test out the new alert by forcing another deadlock. Congratulations - now you know how to create alerts based on SQL Server Error Log messages. </p><p>You can create these types of alerts for more than just deadlocks: you can set alerts to monitor for other messages that show up in the error log such as crash dumps, connections issues, and corruption.  You can also create alerts based on SQL Server Agent Log messages.  Here are a few more examples listed below with the string from the SQL error logs that you can use as the text Payload in your custom metric. </p><ul><li><p>Agent XPs disabled:</p></li><ul><li><p>SQL Server Error Logs will include the string <code>&#34;Configuration option &#39;Agent XPs&#39; changed from 1 to 0.&#34;</code></p></li></ul><li><p>SQL Server Agent Status:</p></li><ul><li><p>SQL Server Agent Logs will include the string <code>“SQLServerAgent terminated&#34;</code></p></li></ul><li><p>Job Failures:</p></li><ul><li><p>SQL Server Agent Logs will include a string similar to  “<code>SQL Server Scheduled Job &#39;demo&#39; (0xB83611A22D4FD74B8900ADDFDC9CDD9C) - Status: Failed - Invoked on:</code>”</p></li><li><p>Option below needs to be checked or set using tsql</p></li></ul></ul></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;ul&gt;&lt;li&gt;&lt;p&gt;Database Status:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;SQL Server Error logs will include one of the following strings:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&amp;#8220;&lt;code&gt;Database % cannot be opened. It has been marked SUSPECT&lt;/code&gt;&amp;#8221;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&amp;#8220;&lt;code&gt;Database % database is in emergency or suspect mode&lt;/code&gt;&amp;#8221;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&amp;#8220;&lt;code&gt;database % is marked EMERGENCY_MODE&lt;/code&gt;&amp;#8221;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&amp;#8220;&lt;code&gt;Database % cannot be opened because it is offline.&lt;/code&gt;&amp;#8221;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&amp;#8220;&lt;code&gt;Setting database option OFFLINE to ON for database&lt;/code&gt;&amp;#8221;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;Custom error messages&lt;/p&gt;&lt;/li&gt;&lt;li&gt;Using SQL Agent jobs, the &lt;a href=&#34;https://docs.microsoft.com/en-us/sql/t-sql/language-elements/raiserror-transact-sql?view=sql-server-ver15&#34; target=&#34;_blank&#34;&gt;RAISERROR&lt;/a&gt; with log command can be used to write custom messages in the SQL error logs. This could be triggered when any application or database condition is met. One way to do this is to create a SQL Agent job and define a job step with a simple query like the one below. &lt;br&gt;&lt;/li&gt;&lt;/ul&gt;"><ul><li><p>Database Status:</p></li><ul><li><p>SQL Server Error logs will include one of the following strings:</p></li><ul><li><p>“<code>Database % cannot be opened. It has been marked SUSPECT</code>”</p></li><li><p>“<code>Database % database is in emergency or suspect mode</code>”</p></li><li><p>“<code>database % is marked EMERGENCY_MODE</code>”</p></li><li><p>“<code>Database % cannot be opened because it is offline.</code>”</p></li><li><p>“<code>Setting database option OFFLINE to ON for database</code>”</p></li></ul></ul><li><p>Custom error messages</p></li><li>Using SQL Agent jobs, the <a href="https://docs.microsoft.com/en-us/sql/t-sql/language-elements/raiserror-transact-sql?view=sql-server-ver15" target="_blank" track-type="inline link" track-name="11" track-metadata-eventdetail="https://docs.microsoft.com" track-metadata-module="post">RAISERROR</a> with log command can be used to write custom messages in the SQL error logs. This could be triggered when any application or database condition is met. One way to do this is to create a SQL Agent job and define a job step with a simple query like the one below. <br/></li></ul></div></paragraph-block></div><div><article-code-block _nghost-c61=""><pre _ngcontent-c61="">  <code _ngcontent-c61="">declare
</code><code _ngcontent-c61="">  @LongRunningJobThreshold int=300,
</code><code _ngcontent-c61="">  @runningtime int=0
</code><code _ngcontent-c61=""> 
</code><code _ngcontent-c61="">select @runningtime=max(er.total_elapsed_time)/1000
</code><code _ngcontent-c61="">from sys.dm_exec_requests er
</code><code _ngcontent-c61="">inner join sys.sysprocesses p on er.session_id = p.spid
</code><code _ngcontent-c61="">where p.program_name like &#39;%SQLAgent%&#39;
</code><code _ngcontent-c61=""> 
</code><code _ngcontent-c61="">if @runningtime &gt; @LongRunningJobThreshold
</code><code _ngcontent-c61="">RAISERROR (&#39;long running sql agent job&#39;,16,1) with LOG</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c60=""><p>Then schedule this to run every couple of minutes. This will produce an error log  message and textPayload as in the image below. The same steps can be used for alerting and monitoring as described above for notifications</p></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c59=""></article-up-1to3-block></section></div></article></main></article-page></dynamic-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Bryan Hamilton&lt;/name&gt;&lt;title&gt;Database Engineer, SQL Server, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 16 May 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing a high-usage tier for Managed Service for Prometheus</title>
      <link>https://cloud.google.com/blog/products/devops-sre/managed-service-for-prometheus-offers-new-pricing-tier/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Prometheus is considered the de facto standard for Kubernetes application metrics, but running it yourself can strain engineering time and infrastructure resources when your usage grows. In March, we announced the general availability of Google Cloud &lt;a href=&#34;https://cloud.google.com/managed-prometheus&#34;&gt;Managed Service for Prometheus&lt;/a&gt; to help you offload that burden, and today, we’re excited to announce a new low-cost, high-usage pricing tier designed for customers who are moving large volumes of Kubernetes metrics over to the service. Furthermore, we’re lowering the price of the current usage tiers.&lt;/p&gt;&lt;p&gt;Whether you are a mature enterprise, a rapidly scaling digital native startup, or somewhere in between, this new pricing structure makes it an easier decision to scale your production Kubernetes metrics with Managed Service for Prometheus. Free up your engineering resources to concentrate on building your next big application, not your metrics infrastructure.&lt;/p&gt;&lt;h3&gt;Introducing a high-usage tier and lower prices&lt;/h3&gt;&lt;p&gt;Since it launched &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes&#34;&gt;just two months ago&lt;/a&gt;, companies of all shapes and sizes have begun to adopt Managed Service for Prometheus, but we heard that cost at high data volumes was an issue for customers that move their entire Kubernetes metrics operation to our service. That’s why we created a new tier, which is priced 50% lower than the previous highest-usage tier. You get scale and ease of use at a price that works. &lt;/p&gt;&lt;p&gt;We didn’t forget about customers with lower metric volumes, however. Whether you’re just getting started with Managed Service for Prometheus or you haven’t yet migrated all your Kubernetes metrics to our service, cost is always top of mind. As part of the new pricing, list prices for the lower-usage tiers are now 25% lower than they were originally.&lt;/p&gt;&lt;p&gt;See the table below for comparisons:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_H8hBh7w.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Prometheus.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_H8hBh7w.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Note that samples are counted per billing account. See full pricing details on the Google Cloud&#39;s operations suite &lt;a href=&#34;https://cloud.google.com/stackdriver/pricing&#34;&gt;pricing page&lt;/a&gt;.&lt;/i&gt;&lt;br/&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Early success at enterprise scale&lt;/h3&gt;&lt;p&gt;Maisons du Monde, a French furniture and home decor giant, &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/maisons-du-monde-improved-their-kubernetes-observability&#34; target=&#34;_blank&#34;&gt;recently published a story&lt;/a&gt; about their experience with Google Kubernetes Engine (GKE) application metrics. They started with open source Prometheus, then added Thanos to deal with Prometheus scaling issues, before finally choosing Managed Service for Prometheus.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-pull_quote&#34;&gt;&lt;div class=&#34;uni-pull-quote h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;div class=&#34;uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;div class=&#34;uni-pull-quote__inner-wrapper h-c-copy h-c-copy&#34;&gt;&lt;q class=&#34;uni-pull-quote__text&#34;&gt;Our engineers’ time is very valuable and we would rather spend it developing new features instead of maintaining a state-of-the-art metrics system.&lt;/q&gt; &lt;cite class=&#34;uni-pull-quote__author&#34;&gt;&lt;span class=&#34;uni-pull-quote__author-meta&#34;&gt;&lt;strong class=&#34;h-u-font-weight-medium&#34;&gt;Victor Ladouceur&lt;/strong&gt;&lt;br/&gt; SRE, Maisons du Monde&lt;/span&gt;&lt;/cite&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Why Managed Service for Prometheus&lt;/h3&gt;&lt;p&gt;The service is designed to be a drop-in replacement for running your own Prometheus stack, so you can gather, store, and alert on your metrics. Some of the benefits include: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Two-year retention of all metrics, included in the price&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cost-effective monitoring on a per-sample basis&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Easy &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources&#34;&gt;cost identification and attribution&lt;/a&gt; using Cloud Monitoring&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;No changes needed to existing Prometheus querying or alerting workflows, with &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed&#34;&gt;managed&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged&#34;&gt;self-deployed&lt;/a&gt; collection options&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ability to view Prometheus and &lt;a href=&#34;https://cloud.google.com/blog/products/operations/in-depth-explanation-of-operational-metrics-at-google-cloud&#34;&gt;Google Cloud system metrics&lt;/a&gt; together&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Get ready to scale your Kubernetes metrics&lt;/h3&gt;&lt;p&gt;If you’re ready to learn more about how managed metrics can help you improve your Kubernetes monitoring, or if you’re ready to get hands on, check out the following resources:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Try out our &lt;a href=&#34;https://go.qwiklabs.com/Google-Cloud-Workshops-Prometheus&#34; target=&#34;_blank&#34;&gt;new Managed Service for Prometheus Qwiklab&lt;/a&gt; at &lt;b&gt;no charge&lt;/b&gt; now through June 15. Walk through 4.5 hours of content covering migration, metrics collection, analysis, and cost saving strategies.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Join us tomorrow (May 17th) for our session, &lt;a href=&#34;https://cloudonair.withgoogle.com/events/kubecon-eu-2022&#34; target=&#34;_blank&#34;&gt;Easy, scalable metrics for Kubernetes with Managed Service for Prometheus&lt;/a&gt;, during our Day Zero event for &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/google-cloud-at-kubecon-eu-2022&#34;&gt;KubeCon EMEA 2022&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloudonair.withgoogle.com/events/managed-service-prometheus&#34; target=&#34;_blank&#34;&gt;Register to attend a webinar&lt;/a&gt; on June 1, where we will talk about the service, the price cut, and how easy it is to migrate your metrics collection. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://g.co/cloud/managedprometheus&#34; target=&#34;_blank&#34;&gt;Start using the service today&lt;/a&gt; by visiting the onboarding documentation.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Google Cloud Managed Service for Prometheus is now generally available&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Announcing the GA of Google Cloud Managed Service for Prometheus for the collection, storage, and querying of Kubernetes metrics.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;Prometheus is considered the de facto standard for Kubernetes application metrics, but running it yourself can strain engineering time and infrastructure resources when your usage grows. In March, we announced the general availability of Google Cloud &lt;a href=&#34;https://cloud.google.com/managed-prometheus&#34;&gt;Managed Service for Prometheus&lt;/a&gt; to help you offload that burden, and today, we&amp;#8217;re excited to announce a new low-cost, high-usage pricing tier designed for customers who are moving large volumes of Kubernetes metrics over to the service. Furthermore, we&amp;#8217;re lowering the price of the current usage tiers.&lt;/p&gt;&lt;p&gt;Whether you are a mature enterprise, a rapidly scaling digital native startup, or somewhere in between, this new pricing structure makes it an easier decision to scale your production Kubernetes metrics with Managed Service for Prometheus. Free up your engineering resources to concentrate on building your next big application, not your metrics infrastructure.&lt;/p&gt;&lt;h3&gt;Introducing a high-usage tier and lower prices&lt;/h3&gt;&lt;p&gt;Since it launched &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes&#34;&gt;just two months ago&lt;/a&gt;, companies of all shapes and sizes have begun to adopt Managed Service for Prometheus, but we heard that cost at high data volumes was an issue for customers that move their entire Kubernetes metrics operation to our service. That&amp;#8217;s why we created a new tier, which is priced 50% lower than the previous highest-usage tier. You get scale and ease of use at a price that works.&amp;#160;&lt;/p&gt;&lt;p&gt;We didn&amp;#8217;t forget about customers with lower metric volumes, however. Whether you&amp;#8217;re just getting started with Managed Service for Prometheus or you haven&amp;#8217;t yet migrated all your Kubernetes metrics to our service, cost is always top of mind. As part of the new pricing, list prices for the lower-usage tiers are now 25% lower than they were originally.&lt;/p&gt;&lt;p&gt;See the table below for comparisons:&lt;/p&gt;"><p>Prometheus is considered the de facto standard for Kubernetes application metrics, but running it yourself can strain engineering time and infrastructure resources when your usage grows. In March, we announced the general availability of Google Cloud <a href="https://cloud.google.com/managed-prometheus" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/managed-prometheus" track-metadata-module="post">Managed Service for Prometheus</a> to help you offload that burden, and today, we’re excited to announce a new low-cost, high-usage pricing tier designed for customers who are moving large volumes of Kubernetes metrics over to the service. Furthermore, we’re lowering the price of the current usage tiers.</p><p>Whether you are a mature enterprise, a rapidly scaling digital native startup, or somewhere in between, this new pricing structure makes it an easier decision to scale your production Kubernetes metrics with Managed Service for Prometheus. Free up your engineering resources to concentrate on building your next big application, not your metrics infrastructure.</p><h3>Introducing a high-usage tier and lower prices</h3><p>Since it launched <a href="https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes" track-metadata-module="post">just two months ago</a>, companies of all shapes and sizes have begun to adopt Managed Service for Prometheus, but we heard that cost at high data volumes was an issue for customers that move their entire Kubernetes metrics operation to our service. That’s why we created a new tier, which is priced 50% lower than the previous highest-usage tier. You get scale and ease of use at a price that works. </p><p>We didn’t forget about customers with lower metric volumes, however. Whether you’re just getting started with Managed Service for Prometheus or you haven’t yet migrated all your Kubernetes metrics to our service, cost is always top of mind. As part of the new pricing, list prices for the lower-usage tiers are now 25% lower than they were originally.</p><p>See the table below for comparisons:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Lee Yanco&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 16 May 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>New observability features for your Splunk Dataflow streaming pipelines</title>
      <link>https://cloud.google.com/blog/products/data-analytics/simplify-your-splunk-dataflow-ops-with-improved-pipeline-observability/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We’re thrilled to announce several new observability features for the &lt;a href=&#34;https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk&#34;&gt;Pub/Sub to Splunk Dataflow template&lt;/a&gt; to help operators keep a tab on their streaming pipeline performance. &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-enterprise-gcp&#34;&gt;Splunk Enterprise&lt;/a&gt; and &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-cloud&#34;&gt;Splunk Cloud&lt;/a&gt; customers use the Splunk Dataflow template to &lt;a href=&#34;https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow&#34;&gt;reliably export Google Cloud logs&lt;/a&gt; for in-depth analytics for security, IT or business use cases. With newly added metrics and improved logging for Splunk IO sink, it’s now easier to answer operational questions such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Is the Dataflow pipeline keeping up with the volume of logs generated?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;What is the latency and throughput (Event Per Second or EPS) when writing to Splunk?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;What is the response status breakdown of downstream Splunk HTTP Event Collector (HEC) and potential error messages?&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This critical visibility helps you derive your log export &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#defn-sli&#34;&gt;service-level indicators (SLIs)&lt;/a&gt; and monitor for any pipeline performance regressions. You can also more easily root cause potential downstream failures between Dataflow &amp;amp; Splunk such as Splunk HEC network connections or server issues, and fix the problem before it cascades. &lt;br/&gt;&lt;/p&gt;&lt;p&gt;To help you quickly chart these new metrics, we’ve included them in the custom dashboard as part of the updated &lt;a href=&#34;https://github.com/GoogleCloudPlatform/terraform-splunk-log-export&#34; target=&#34;_blank&#34;&gt;Terraform module for Splunk Dataflow&lt;/a&gt;. You can use those Terraform templates to deploy the entire infrastructure for log export to Splunk, or just the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf&#34; target=&#34;_blank&#34;&gt;Monitoring dashboard&lt;/a&gt; alone.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Log_Export_Ops_Dashboard_for_Splunk_Data.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Log Export Ops Dashboard for Splunk Dataflow.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Log_Export_Ops_Dashboard_for_Splunk_Data.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Log Export Ops Dashboard for Splunk Dataflow&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h2&gt;More metrics&lt;/h2&gt;&lt;p&gt;In your Dataflow Console, you may have noticed several new custom metrics (highlighted below) for launched jobs as of template version &lt;code&gt;2022-03-21-00_RC01&lt;/code&gt;, that is &lt;code&gt;gs://dataflow-templates/2022-03-21-00_RC01/Cloud_PubSub_to_Splunk&lt;/code&gt; or later:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_More_metrics.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 More metrics.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_More_metrics.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Pipeline instrumentation&lt;/h3&gt;&lt;p&gt;Before we dive into the new metrics, let’s take a step back and go over the Splunk Dataflow job steps. The following flowchart represents the different stages that comprise a Splunk Dataflow job along with corresponding custom metrics:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Pipeline_instrumentation.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;3 Pipeline instrumentation.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Pipeline_instrumentation.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In this pipeline, we utilize two types of &lt;a href=&#34;https://beam.apache.org/documentation/programming-guide/#types-of-metrics&#34; target=&#34;_blank&#34;&gt;Apache Beam custom metrics&lt;/a&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Counter metrics, labeled 1 through 10 above, used to count messages and requests (both successful and failed).&lt;/li&gt;&lt;li&gt;Distribution metrics, labeled A through C above, used to report on distribution of request latency (both successful and failed) and batch size. &lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Downstream request visibility&lt;/h3&gt;&lt;p&gt;Splunk Dataflow operators have relied on some of these pre-built custom metrics to monitor log messages progress through the different pipeline stages, particularly in the last stage &lt;code&gt;Write To Splunk&lt;/code&gt;, with metrics &lt;code&gt;outbound-successful-events&lt;/code&gt; (counter #6 above) and &lt;code&gt;outbound-failed-events&lt;/code&gt; (counter #7 above) to track the number of messages that were successfully exported (or not) to Splunk. While operators had visibility of the outbound message success rate, they lacked visibility at the HEC request level. Splunk Dataflow operators can now monitor not only the number of successful and failed HEC requests over time, but also the response status breakdown to determine if request failed due to a client request issue (e.g. invalid Splunk index or HEC token), or a transient network or Splunk issue (e.g. server busy or down) all from Dataflow Console with the addition of counters #7-10 above, that is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;http-valid-requests&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;http-invalid-requests&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;http-server-error-requests&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Splunk Dataflow operators can also now track average latency of downstream requests to Splunk HEC, as well as average request batch size, by using the new distribution metrics #A-C, that is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;successful_write_to_splunk_latency_ms&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;unsuccessful_write_to_splunk_latency_ms&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;write_to_splunk_batch&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Note that a Distribution metric in Beam is reported by Dataflow as four sub-metrics suffixed with _MAX, _MIN, _MEAN and _COUNT. That is why those 3 new distribution metrics translate to 12 new metrics in Cloud Monitoring, as you can see in the earlier job info screenshot from Dataflow Console. Dataflow &lt;a href=&#34;https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#custom_metrics&#34;&gt;currently does not support creating a histogram&lt;/a&gt; to visualize the breakdown of these metrics’ values. Therefore, _MEAN metric is the only useful sub-metric for our purposes. As an all-time average value, _MEAN cannot be used to track changes over arbitrary time intervals (e.g. hourly), but it is useful to capture baseline, track trend or to compare different pipelines.&lt;/p&gt;&lt;p&gt;Dataflow custom metrics, including aforementioned metrics reported by Splunk Dataflow template, are a chargeable feature of Cloud Monitoring. For more information on metrics pricing, see &lt;a href=&#34;https://cloud.google.com/stackdriver/pricing#monitoring-costs&#34;&gt;Pricing for Cloud Monitoring&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;Improved logging&lt;/h2&gt;&lt;h3&gt;Logging HEC errors&lt;/h3&gt;&lt;p&gt;To further root cause downstream issues, HEC request errors are now adequately logged, including both response status code and message:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/4_Logging_HEC_errors.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;4 Logging HEC errors.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/4_Logging_HEC_errors.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;You can retrieve them directly in Worker Logs from Dataflow Console by setting log severity to Error.&lt;/p&gt;&lt;p&gt;Alternatively, for those who prefer using &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Logs Explorer&lt;/a&gt;, you can use the following query.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;log_id(&#34;dataflow.googleapis.com/worker&#34;)\r\nresource.type=&#34;dataflow_step&#34;\r\nresource.labels.step_id=&#34;WriteToSplunk/Write Splunk events&#34;\r\nseverity=ERROR&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h2&gt;Disabling batch logs&lt;/h2&gt;By default, Splunk Dataflow workers log every HEC request as follows:&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/5_Disabling_batch_logs.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;5 Disabling batch logs.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/5_Disabling_batch_logs.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Even though these requests are often batched events, these ‘batch logs’ are chatty as they add 2 log messages for every HEC request. With the addition of request-level counters (&lt;code&gt;http-*-requests&lt;/code&gt;), latency &amp;amp; batch size distributions, and HEC error logging mentioned above, these batch logs are generally redundant. To control worker log volume, you can now disable these batch logs by setting the new optional template parameter &lt;code&gt;enableBatchLogs&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;, when deploying the Splunk Dataflow job. For more details on latest template parameters, refer to &lt;a href=&#34;https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk&#34;&gt;template user documentation&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Enabling debug level logs&lt;/h3&gt;&lt;p&gt;The default logging level for Google provided templates written using the Apache Beam Java SDK is INFO, which means all messages of INFO and higher i.e. WARN and ERROR will be logged. If you’d like to enable lower log levels like DEBUG, you can do so by setting the -&lt;i&gt;-defaultWorkerLogLevel&lt;/i&gt; flag to DEBUG while starting the pipeline using gcloud command-line tool. &lt;/p&gt;&lt;p&gt;You can also override log levels for specific packages or classes with the &lt;i&gt;--workerLogLevelOverrides&lt;/i&gt;flag. For example, the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/src/main/java/com/google/cloud/teleport/splunk/HttpEventPublisher.java&#34; target=&#34;_blank&#34;&gt;HttpEventPublisher&lt;/a&gt; class logs the final payload sent to Splunk at the DEBUG level. You can set the &lt;i&gt;--workerLogLevelOverrides&lt;/i&gt;flag to {&#34;com.google.cloud.teleport.splunk.HttpEventPublisher&#34;:&#34;DEBUG&#34;} to view the final message in the logs before it is sent to Splunk, and keep the log level at INFO for other classes. Exercise caution while using this as it will log &lt;b&gt;all&lt;/b&gt; messages sent to Splunk under the &lt;i&gt;Worker Logs&lt;/i&gt; tab in the console, which might lead to &lt;a href=&#34;https://cloud.google.com/dataflow/docs/guides/logging#LogLimits&#34;&gt;log throttling&lt;/a&gt; or reveal sensitive information.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Putting it all together&lt;/h2&gt;We put all this together in a single &lt;a href=&#34;https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf&#34; target=&#34;_blank&#34;&gt;Monitoring dashboard&lt;/a&gt; that you can readily use to monitor your log export operations:&lt;br/&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/6_Pipeline_Throughput.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;6 Pipeline Throughput.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/6_Pipeline_Throughput.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Pipeline Throughput, Latency &amp;amp; Errors&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;This dashboard is a single pane of glass for monitoring your Pub/Sub to Splunk Dataflow pipeline. Use it to ensure your log export is meeting your dynamic log volume requirements, by scaling to adequate throughput (EPS) rate, while keeping latency and backlog to a minimum. There’s also a panel to track pipeline resource usage and utilization, to help you validate that the pipeline is running cost-efficiently during steady-state.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/7_Pipeline_Utilization.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;7 Pipeline Utilization.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/7_Pipeline_Utilization.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Pipeline Utilization and Worker Logs&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;For specific guidance on handling and replaying failed messages, refer to &lt;a href=&#34;https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow#troubleshoot_failed_messages&#34;&gt;Troubleshoot failed messages&lt;/a&gt; as part of the Splunk Dataflow reference guide. For general information on troubleshooting any Dataflow pipeline, check out the &lt;a href=&#34;https://cloud.google.com/dataflow/docs/guides/troubleshooting-your-pipeline&#34;&gt;Troubleshooting and debugging&lt;/a&gt; documentation, and for a list of common errors and their resolutions look through the &lt;a href=&#34;https://cloud.google.com/dataflow/docs/guides/common-errors&#34;&gt;Common error guidance&lt;/a&gt; documentation. If you encounter any issue, please &lt;a href=&#34;https://github.com/GoogleCloudPlatform/DataflowTemplates/issues&#34; target=&#34;_blank&#34;&gt;open an issue&lt;/a&gt; in the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/DataflowTemplates&#34; target=&#34;_blank&#34;&gt;Dataflow templates GitHub repository&lt;/a&gt;, or &lt;a href=&#34;https://console.cloud.google.com/support/cases&#34;&gt;open a support case&lt;/a&gt; directly in your Google Cloud Console.&lt;/p&gt;&lt;p&gt;For a step-by-step guide on how to export GCP logs to Splunk, check out the &lt;a href=&#34;https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow&#34;&gt;Deploy production-ready log exports to Splunk using Dataflow&lt;/a&gt; tutorial, or use the accompanying &lt;a href=&#34;https://github.com/GoogleCloudPlatform/terraform-splunk-log-export&#34; target=&#34;_blank&#34;&gt;Terraform scripts&lt;/a&gt; to automate the setup of your log export infrastructure along with the associated operational dashboard.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/data-analytics/whats-new-splunk-dataflow-template/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DataAnalytics_B_1_Kp1Qjnf.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;What’s new with Splunk Dataflow template: Automatic log parsing, UDF support, and more&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Announcing new features for Splunk Dataflow template with improved compatibility with Splunk Add-on for GCP, more extensibility using use...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><router-outlet></router-outlet><dynamic-page><article-page><main id="jump-content"><promo-banner-block _nghost-c49=""></promo-banner-block><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>splunk.jpg</span></p></figure></article-aspect-image-block><div><article-cta _nghost-c60=""><div _ngcontent-c60=""><h4 _ngcontent-c60=""><span _ngcontent-c60="">Try Google Cloud</span></h4><p _ngcontent-c60=""><span _ngcontent-c60="">Start building on Google Cloud with $300 in free credits and 20+ always free products.</span></p><p><a _ngcontent-c60="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="free trial" track-metadata-eventdetail="https://cloud.google.com/free/" href="https://cloud.google.com/free/"><span _ngcontent-c60="">Free Trial</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c62=""><div _ngcontent-c62="" innerhtml="&lt;p&gt;We&amp;#8217;re thrilled to announce several new observability features for the &lt;a href=&#34;https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk&#34;&gt;Pub/Sub to Splunk Dataflow template&lt;/a&gt; to help operators keep a tab on their streaming pipeline performance. &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-enterprise-gcp&#34;&gt;Splunk Enterprise&lt;/a&gt; and &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-cloud&#34;&gt;Splunk Cloud&lt;/a&gt; customers use the Splunk Dataflow template to &lt;a href=&#34;https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow&#34;&gt;reliably export Google Cloud logs&lt;/a&gt; for in-depth analytics for security, IT or business use cases. With newly added metrics and improved logging for Splunk IO sink, it&amp;#8217;s now easier to answer operational questions such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Is the Dataflow pipeline keeping up with the volume of logs generated?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;What is the latency and throughput (Event Per Second or EPS) when writing to Splunk?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;What is the response status breakdown of downstream Splunk HTTP Event Collector (HEC) and potential error messages?&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This critical visibility helps you derive your log export &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#defn-sli&#34;&gt;service-level indicators (SLIs)&lt;/a&gt; and monitor for any pipeline performance regressions. You can also more easily root cause potential downstream failures between Dataflow &amp;amp; Splunk such as Splunk HEC network connections or server issues, and fix the problem before it cascades. &lt;br&gt;&lt;/p&gt;&lt;p&gt;To help you quickly chart these new metrics, we&amp;#8217;ve included them in the custom dashboard as part of the updated &lt;a href=&#34;https://github.com/GoogleCloudPlatform/terraform-splunk-log-export&#34; target=&#34;_blank&#34;&gt;Terraform module for Splunk Dataflow&lt;/a&gt;. You can use those Terraform templates to deploy the entire infrastructure for log export to Splunk, or just the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf&#34; target=&#34;_blank&#34;&gt;Monitoring dashboard&lt;/a&gt; alone.&lt;/p&gt;"><p>We’re thrilled to announce several new observability features for the <a href="https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk" track-metadata-module="post">Pub/Sub to Splunk Dataflow template</a> to help operators keep a tab on their streaming pipeline performance. <a href="https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-enterprise-gcp" track-type="inline link" track-name="2" track-metadata-eventdetail="https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-enterprise-gcp" track-metadata-module="post">Splunk Enterprise</a> and <a href="https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-cloud" track-type="inline link" track-name="3" track-metadata-eventdetail="https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-cloud" track-metadata-module="post">Splunk Cloud</a> customers use the Splunk Dataflow template to <a href="https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow" track-metadata-module="post">reliably export Google Cloud logs</a> for in-depth analytics for security, IT or business use cases. With newly added metrics and improved logging for Splunk IO sink, it’s now easier to answer operational questions such as:</p><ul><li><p>Is the Dataflow pipeline keeping up with the volume of logs generated?</p></li><li><p>What is the latency and throughput (Event Per Second or EPS) when writing to Splunk?</p></li><li><p>What is the response status breakdown of downstream Splunk HTTP Event Collector (HEC) and potential error messages?</p></li></ul><p>This critical visibility helps you derive your log export <a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#defn-sli" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#defn-sli" track-metadata-module="post">service-level indicators (SLIs)</a> and monitor for any pipeline performance regressions. You can also more easily root cause potential downstream failures between Dataflow &amp; Splunk such as Splunk HEC network connections or server issues, and fix the problem before it cascades. <br/></p><p>To help you quickly chart these new metrics, we’ve included them in the custom dashboard as part of the updated <a href="https://github.com/GoogleCloudPlatform/terraform-splunk-log-export" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://github.com" track-metadata-module="post">Terraform module for Splunk Dataflow</a>. You can use those Terraform templates to deploy the entire infrastructure for log export to Splunk, or just the <a href="https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://github.com" track-metadata-module="post">Monitoring dashboard</a> alone.</p></div></paragraph-block></div><div><paragraph-block _nghost-c62=""><div _ngcontent-c62="" innerhtml="&lt;h2&gt;More metrics&lt;/h2&gt;&lt;p&gt;In your Dataflow Console, you may have noticed several new custom metrics (highlighted below) for launched jobs as of template version &lt;code&gt;2022-03-21-00_RC01&lt;/code&gt;, that is &lt;code&gt;gs://dataflow-templates/2022-03-21-00_RC01/Cloud_PubSub_to_Splunk&lt;/code&gt; or later:&lt;/p&gt;"><h2>More metrics</h2><p>In your Dataflow Console, you may have noticed several new custom metrics (highlighted below) for launched jobs as of template version <code>2022-03-21-00_RC01</code>, that is <code>gs://dataflow-templates/2022-03-21-00_RC01/Cloud_PubSub_to_Splunk</code> or later:</p></div></paragraph-block></div><div><paragraph-block _nghost-c62=""><div _ngcontent-c62="" innerhtml="&lt;h3&gt;Pipeline instrumentation&lt;/h3&gt;&lt;p&gt;Before we dive into the new metrics, let&amp;#8217;s take a step back and go over the Splunk Dataflow job steps. The following flowchart represents the different stages that comprise a Splunk Dataflow job along with corresponding custom metrics:&lt;/p&gt;"><h3>Pipeline instrumentation</h3><p>Before we dive into the new metrics, let’s take a step back and go over the Splunk Dataflow job steps. The following flowchart represents the different stages that comprise a Splunk Dataflow job along with corresponding custom metrics:</p></div></paragraph-block></div><div><paragraph-block _nghost-c62=""><div _ngcontent-c62="" innerhtml="&lt;p&gt;In this pipeline, we utilize two types of &lt;a href=&#34;https://beam.apache.org/documentation/programming-guide/#types-of-metrics&#34; target=&#34;_blank&#34;&gt;Apache Beam custom metrics&lt;/a&gt;:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Counter metrics, labeled 1 through 10 above, used to count messages and requests (both successful and failed).&lt;/li&gt;&lt;li&gt;Distribution metrics, labeled A through C above, used to report on distribution of request latency (both successful and failed) and batch size.&amp;#160;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Downstream request visibility&lt;/h3&gt;&lt;p&gt;Splunk Dataflow operators have relied on some of these pre-built custom metrics to monitor log messages progress through the different pipeline stages, particularly in the last stage &lt;code&gt;Write To Splunk&lt;/code&gt;, with metrics &lt;code&gt;outbound-successful-events&lt;/code&gt; (counter #6 above) and &lt;code&gt;outbound-failed-events&lt;/code&gt; (counter #7 above) to track the number of messages that were successfully exported (or not) to Splunk. While operators had visibility of the outbound message success rate, they lacked visibility at the HEC request level. Splunk Dataflow operators can now monitor not only the number of successful and failed HEC requests over time, but also the response status breakdown to determine if request failed due to a client request issue (e.g. invalid Splunk index or HEC token), or a transient network or Splunk issue (e.g. server busy or down) all from Dataflow Console with the addition of counters #7-10 above, that is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;http-valid-requests&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;http-invalid-requests&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;http-server-error-requests&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Splunk Dataflow operators can also now track average latency of downstream requests to Splunk HEC, as well as average request batch size, by using the new distribution metrics #A-C, that is:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;successful_write_to_splunk_latency_ms&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;unsuccessful_write_to_splunk_latency_ms&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;write_to_splunk_batch&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Note that a Distribution metric in Beam is reported by Dataflow as four sub-metrics suffixed with _MAX, _MIN, _MEAN and _COUNT. That is why those 3 new distribution metrics translate to 12 new metrics in Cloud Monitoring, as you can see in the earlier job info screenshot from Dataflow Console. Dataflow &lt;a href=&#34;https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#custom_metrics&#34;&gt;currently does not support creating a histogram&lt;/a&gt; to visualize the breakdown of these metrics&amp;#8217; values. Therefore, _MEAN metric is the only useful sub-metric for our purposes. As an all-time average value, _MEAN cannot be used to track changes over arbitrary time intervals (e.g. hourly), but it is useful to capture baseline, track trend or to compare different pipelines.&lt;/p&gt;&lt;p&gt;Dataflow custom metrics, including aforementioned metrics reported by Splunk Dataflow template, are a chargeable feature of Cloud Monitoring. For more information on metrics pricing, see &lt;a href=&#34;https://cloud.google.com/stackdriver/pricing#monitoring-costs&#34;&gt;Pricing for Cloud Monitoring&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;Improved logging&lt;/h2&gt;&lt;h3&gt;Logging HEC errors&lt;/h3&gt;&lt;p&gt;To further root cause downstream issues, HEC request errors are now adequately logged, including both response status code and message:&lt;/p&gt;"><p>In this pipeline, we utilize two types of <a href="https://beam.apache.org/documentation/programming-guide/#types-of-metrics" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://beam.apache.org" track-metadata-module="post">Apache Beam custom metrics</a>:</p><ul><li>Counter metrics, labeled 1 through 10 above, used to count messages and requests (both successful and failed).</li><li>Distribution metrics, labeled A through C above, used to report on distribution of request latency (both successful and failed) and batch size. </li></ul><h3>Downstream request visibility</h3><p>Splunk Dataflow operators have relied on some of these pre-built custom metrics to monitor log messages progress through the different pipeline stages, particularly in the last stage <code>Write To Splunk</code>, with metrics <code>outbound-successful-events</code> (counter #6 above) and <code>outbound-failed-events</code> (counter #7 above) to track the number of messages that were successfully exported (or not) to Splunk. While operators had visibility of the outbound message success rate, they lacked visibility at the HEC request level. Splunk Dataflow operators can now monitor not only the number of successful and failed HEC requests over time, but also the response status breakdown to determine if request failed due to a client request issue (e.g. invalid Splunk index or HEC token), or a transient network or Splunk issue (e.g. server busy or down) all from Dataflow Console with the addition of counters #7-10 above, that is:</p><ul><li><code>http-valid-requests</code></li><li><code>http-invalid-requests</code></li><li><code>http-server-error-requests</code></li></ul><p>Splunk Dataflow operators can also now track average latency of downstream requests to Splunk HEC, as well as average request batch size, by using the new distribution metrics #A-C, that is:</p><ul><li><code>successful_write_to_splunk_latency_ms</code></li><li><code>unsuccessful_write_to_splunk_latency_ms</code></li><li><code>write_to_splunk_batch</code></li></ul><p>Note that a Distribution metric in Beam is reported by Dataflow as four sub-metrics suffixed with _MAX, _MIN, _MEAN and _COUNT. That is why those 3 new distribution metrics translate to 12 new metrics in Cloud Monitoring, as you can see in the earlier job info screenshot from Dataflow Console. Dataflow <a href="https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#custom_metrics" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#custom_metrics" track-metadata-module="post">currently does not support creating a histogram</a> to visualize the breakdown of these metrics’ values. Therefore, _MEAN metric is the only useful sub-metric for our purposes. As an all-time average value, _MEAN cannot be used to track changes over arbitrary time intervals (e.g. hourly), but it is useful to capture baseline, track trend or to compare different pipelines.</p><p>Dataflow custom metrics, including aforementioned metrics reported by Splunk Dataflow template, are a chargeable feature of Cloud Monitoring. For more information on metrics pricing, see <a href="https://cloud.google.com/stackdriver/pricing#monitoring-costs" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/stackdriver/pricing#monitoring-costs" track-metadata-module="post">Pricing for Cloud Monitoring</a>.</p><h2>Improved logging</h2><h3>Logging HEC errors</h3><p>To further root cause downstream issues, HEC request errors are now adequately logged, including both response status code and message:</p></div></paragraph-block></div><div><paragraph-block _nghost-c62=""><div _ngcontent-c62="" innerhtml="&lt;p&gt;You can retrieve them directly in Worker Logs from Dataflow Console by setting log severity to Error.&lt;/p&gt;&lt;p&gt;Alternatively, for those who prefer using &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Logs Explorer&lt;/a&gt;, you can use the following query.&lt;/p&gt;"><p>You can retrieve them directly in Worker Logs from Dataflow Console by setting log severity to Error.</p><p>Alternatively, for those who prefer using <a href="https://cloud.google.com/logging/docs/view/logs-explorer-interface" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/logs-explorer-interface" track-metadata-module="post">Logs Explorer</a>, you can use the following query.</p></div></paragraph-block></div><div><article-code-block _nghost-c63=""><pre _ngcontent-c63="">  <code _ngcontent-c63="">log_id(&#34;dataflow.googleapis.com/worker&#34;)
</code><code _ngcontent-c63="">resource.type=&#34;dataflow_step&#34;
</code><code _ngcontent-c63="">resource.labels.step_id=&#34;WriteToSplunk/Write Splunk events&#34;
</code><code _ngcontent-c63="">severity=ERROR</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c62=""><div _ngcontent-c62="" innerhtml="&lt;h2&gt;Disabling batch logs&lt;/h2&gt;By default, Splunk Dataflow workers log every HEC request as follows:"><h2>Disabling batch logs</h2><p>By default, Splunk Dataflow workers log every HEC request as follows:</p></div></paragraph-block></div><div><paragraph-block _nghost-c62=""><div _ngcontent-c62="" innerhtml="&lt;p&gt;Even though these requests are often batched events, these &amp;#8216;batch logs&amp;#8217; are chatty as they add 2 log messages for every HEC request. With the addition of request-level counters (&lt;code&gt;http-*-requests&lt;/code&gt;), latency &amp;amp; batch size distributions, and HEC error logging mentioned above, these batch logs are generally redundant. To control worker log volume, you can now disable these batch logs by setting the new optional template parameter &lt;code&gt;enableBatchLogs&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;, when deploying the Splunk Dataflow job. For more details on latest template parameters, refer to &lt;a href=&#34;https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk&#34;&gt;template user documentation&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Enabling debug level logs&lt;/h3&gt;&lt;p&gt;The default logging level for Google provided templates written using the Apache Beam Java SDK is INFO, which means all messages of INFO and higher i.e. WARN and ERROR will be logged. If you&amp;#8217;d like to enable lower log levels like DEBUG, you can do so by setting the -&lt;i&gt;-defaultWorkerLogLevel&lt;/i&gt; flag to DEBUG while starting the pipeline using gcloud command-line tool.&amp;#160;&lt;/p&gt;&lt;p&gt;You can also override log levels for specific packages or classes with the &lt;i&gt;--workerLogLevelOverrides &lt;/i&gt;flag. For example, the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/src/main/java/com/google/cloud/teleport/splunk/HttpEventPublisher.java&#34; target=&#34;_blank&#34;&gt;HttpEventPublisher&lt;/a&gt; class logs the final payload sent to Splunk at the DEBUG level. You can set the &lt;i&gt;--workerLogLevelOverrides &lt;/i&gt;flag to {&amp;#34;com.google.cloud.teleport.splunk.HttpEventPublisher&amp;#34;:&amp;#34;DEBUG&amp;#34;} to view the final message in the logs before it is sent to Splunk, and keep the log level at INFO for other classes. Exercise caution while using this as it will log &lt;b&gt;all&lt;/b&gt; messages sent to Splunk under the &lt;i&gt;Worker Logs&lt;/i&gt; tab in the console, which might lead to &lt;a href=&#34;https://cloud.google.com/dataflow/docs/guides/logging#LogLimits&#34;&gt;log throttling&lt;/a&gt; or reveal sensitive information.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Putting it all together&lt;/h2&gt;We put all this together in a single &lt;a href=&#34;https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf&#34; target=&#34;_blank&#34;&gt;Monitoring dashboard&lt;/a&gt; that you can readily use to monitor your log export operations:&lt;br&gt;&lt;p&gt;&lt;/p&gt;"><p>Even though these requests are often batched events, these ‘batch logs’ are chatty as they add 2 log messages for every HEC request. With the addition of request-level counters (<code>http-*-requests</code>), latency &amp; batch size distributions, and HEC error logging mentioned above, these batch logs are generally redundant. To control worker log volume, you can now disable these batch logs by setting the new optional template parameter <code>enableBatchLogs</code> to <code>false</code>, when deploying the Splunk Dataflow job. For more details on latest template parameters, refer to <a href="https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk" track-metadata-module="post">template user documentation</a>.</p><h3>Enabling debug level logs</h3><p>The default logging level for Google provided templates written using the Apache Beam Java SDK is INFO, which means all messages of INFO and higher i.e. WARN and ERROR will be logged. If you’d like to enable lower log levels like DEBUG, you can do so by setting the -<i>-defaultWorkerLogLevel</i> flag to DEBUG while starting the pipeline using gcloud command-line tool. </p><p>You can also override log levels for specific packages or classes with the <i>--workerLogLevelOverrides </i>flag. For example, the <a href="https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/src/main/java/com/google/cloud/teleport/splunk/HttpEventPublisher.java" target="_blank" track-type="inline link" track-name="13" track-metadata-eventdetail="https://github.com" track-metadata-module="post">HttpEventPublisher</a> class logs the final payload sent to Splunk at the DEBUG level. You can set the <i>--workerLogLevelOverrides </i>flag to {&#34;com.google.cloud.teleport.splunk.HttpEventPublisher&#34;:&#34;DEBUG&#34;} to view the final message in the logs before it is sent to Splunk, and keep the log level at INFO for other classes. Exercise caution while using this as it will log <b>all</b> messages sent to Splunk under the <i>Worker Logs</i> tab in the console, which might lead to <a href="https://cloud.google.com/dataflow/docs/guides/logging#LogLimits" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/dataflow/docs/guides/logging#LogLimits" track-metadata-module="post">log throttling</a> or reveal sensitive information.</p><h2>Putting it all together</h2><p>We put all this together in a single <a href="https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf" target="_blank" track-type="inline link" track-name="15" track-metadata-eventdetail="https://github.com" track-metadata-module="post">Monitoring dashboard</a> that you can readily use to monitor your log export operations:</p></div></paragraph-block></div><div><paragraph-block _nghost-c62=""><p>This dashboard is a single pane of glass for monitoring your Pub/Sub to Splunk Dataflow pipeline. Use it to ensure your log export is meeting your dynamic log volume requirements, by scaling to adequate throughput (EPS) rate, while keeping latency and backlog to a minimum. There’s also a panel to track pipeline resource usage and utilization, to help you validate that the pipeline is running cost-efficiently during steady-state.</p></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c61=""></article-up-1to3-block></section></div></article></main></article-page></dynamic-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Roy Arsan&lt;/name&gt;&lt;title&gt;Solutions Architect&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/splunk_ISRQYWX.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 13 May 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Are your SLOs realistic? How to analyze your risks like an SRE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/how-sres-analyze-risks-to-evaluate-slos/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Setting up Service Level Objectives (SLOs) is one of the foundational tasks of Site Reliability Engineering (SRE) practices, giving the SRE team a target against which to evaluate whether or not a service is running reliably enough. The inverse of your SLO is your &lt;a href=&#34;https://sre.google/sre-book/embracing-risk/&#34; target=&#34;_blank&#34;&gt;error budget&lt;/a&gt; — how much unreliability you are willing to tolerate. Once you’ve identified those targets and &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos&#34;&gt;learned how to set SLOs&lt;/a&gt;, the next question you should ask yourself is whether your SLOs are realistic, given your application architecture and team practices? Are you sure that you can meet them? And what’s most likely to spend the error budget?&lt;/p&gt;&lt;p&gt;At Google, SREs answer these questions up front when they take on a new service, as part of &lt;a href=&#34;https://sre.google/sre-book/evolving-sre-engagement-model/&#34; target=&#34;_blank&#34;&gt;a Production Readiness Review (PRR)&lt;/a&gt;. The intention of this risk analysis is not to prompt you to change your SLOs, but rather to &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons&#34;&gt;prioritize and communicate the risks&lt;/a&gt; to a given service, so you can evaluate whether you’ll be able to actually meet your SLOs, with or without any changes to the service. In addition, it can help you identify which risks are the most important to prioritize and mitigate, using the best available data.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-pull_quote&#34;&gt;&lt;div class=&#34;uni-pull-quote h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;div class=&#34;uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;div class=&#34;uni-pull-quote__inner-wrapper h-c-copy h-c-copy&#34;&gt;&lt;q class=&#34;uni-pull-quote__text&#34;&gt;You can make your service more reliable by identifying and mitigating risks.&lt;/q&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Risk analysis basics&lt;/h3&gt;&lt;p&gt;Before you can evaluate and prioritize your risks, though, you need to come up with a comprehensive list of things to watch out for. In this post, we’ll provide some guidelines for teams tasked with brainstorming all the potential risks to an application. Then, with that list in hand, we’ll show you how to actually analyze and prioritize the risks you’ve identified. &lt;/p&gt;&lt;h3&gt;What risks do you want to consider?&lt;/h3&gt;&lt;p&gt;When brainstorming risks, it’s important to try to map risks in different categories — risks that are related to your dependencies, monitoring, capacity, operations, and release process. And for each of those, imagine what will happen if specific failures happen, for example, if a third party is down, or if you introduce an application or configuration bug. Thus, when thinking about your measurements, ask yourself: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Are there any observability gaps? &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you have alerts for this specific SLI? &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you even currently collect those metrics? &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Also be sure to also map any monitoring and alerting dependencies. For example, what happens if a managed system that you use goes down?&lt;/p&gt;&lt;p&gt;Ideally, you want to identify the risks associated with each failure point for each critical component in a critical user journey, or CUJ. And after identifying those risks, you will want to quantify them:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;What percentage of users was affected by the failure?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;How often do you estimate that failure will occur?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;How long did it take to detect the failure? &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It’s also helpful to gather information about any incidents that happened in the last year that affected CUJs. Compared with gut feelings, relying on historical data can provide more accurate estimates and a good starting point for actual incidents. For example, you may want to consider incidents such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A configuration mishap that reduces capacity, causing overload and dropped requests&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A new release that breaks a small set of requests; the failure is not detected for a day; quick rollback when detected.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A cloud provider’s single-zone VM/network outage&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A cloud provider’s regional VM/network outage&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The operator accidentally deletes a database, requiring a restore from backup&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Another aspect to think about is risk factors; these are global factors that affect the overall time to detection (TTD) and time to repair (TTR). These tend to be operational factors that can increase the time needed to detect outages (for example when using log-based metrics) or alert the on-call engineers. Another example could be a lack of playbooks/documentation or lack of automatic procedures. For example, you have:  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Estimated time to detection (ETTD) of +30m due to operational overload such as noisy alerting&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A 10% greater frequency of a possible failure, due to lack of postmortems or action item follow-up&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Brainstorming guidelines: Recommendation for the facilitator&lt;/h3&gt;&lt;p&gt;Beyond the technical aspects of what to look for in a potential risk to your service, there are some best practices to consider when holding a brainstorming session with your team. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Start the discussion with a high-level block diagram of the service, its users, and its dependencies. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Get a set of diverse opinions in the room — different roles that intersect with the product differently than you do. Also, avoid having only one party speak. Ask participants for the ways in which each element of the diagram could cause an error to be served to the user. Group similar root causes together into a single risk category, such as &#34;database outage&#34;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Try to avoid spending too long discussing things where the estimated time between a given failure is longer than a couple of years, or where the impact is limited to a very small subset of users.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Creating your risk catalog &lt;/h3&gt;&lt;p&gt;You don&#39;t need to capture an endless list of risks; seven to 12 risks per Service Level Indicator (SLI) are sufficient. The important thing is that the data capture high probability and critical risks. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Starting with real outages is best. Those can be as simple as unavailability of &amp;lt;depended service or network&amp;gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Capture both infrastructure- and software-related issues. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Think about risks that can affect the SLI, the time-to-detect and time-to-resolve, and frequency — more on those metrics below.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Capture both risks in the risk catalog and risk factors (global factors). For example, the risk of not having a playbook adds to your time-to-repair; not having alerts for the CUJ adds to the time-to-detection; the risk of a log sync delay of x minutes increases your time-to-detection by the same amount. Then, catalog all these risks and their associated impacts to a global impacts tab.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Here are a few examples of risks: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A new release breaks a small set of requests; not detected for a day; quick rollback when detected.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A new release breaks a sizable subset of requests; and no automatic rollback.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A configuration mishap reduces capacity / Unnoticed growth in usage hits max.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Recommendation&lt;/b&gt;: Examining the data/result of &lt;b&gt;implementing&lt;/b&gt; the SLI will give you a good indication of where you stand in regard to achieving your targets. I recommend starting with creating one dashboard for each CUJ — ideally a dashboard that includes metrics that will also allow us to troubleshoot and debug problems in achieving the SLOs.&lt;/p&gt;&lt;h3&gt;Analyzing the risks&lt;/h3&gt;&lt;p&gt;Now that you’ve generated a list of potential risks, it’s time to analyze them, in order to prioritize their likelihood, and potentially find ways to mitigate against them. It’s time, in other words, to do a risk analysis. &lt;/p&gt;&lt;p&gt;Risk analysis provides a data-driven approach to address and prioritize the needed risks, by estimating four key dimensions: the above-mentioned TTD and TTR, as well as time-between failures (TBF), and their impact on users.&lt;/p&gt;&lt;p&gt;In &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/shrinking-the-impact-of-production-incidents-using-sre-principles-cre-life-lessons&#34;&gt;Shrinking the impact of production incidents using SRE principles&lt;/a&gt;, we introduced a diagram of the production incident cycle. Blue represents when users are happy, and red represents when users are unhappy. &lt;/p&gt;&lt;p&gt;The time that your services are unreliable and your users are unhappy consists of the time-to-detect and the time-to-repair, and is affected by the frequency of incidents (which can be translated to time-between-failures).&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/SRE_e5I0CIV.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;SRE.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/SRE_e5I0CIV.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;br/&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Therefore, &lt;b&gt;we can improve reliability&lt;/b&gt; by increasing the &lt;b&gt;time between failures&lt;/b&gt;, decreasing the &lt;b&gt;time-to-detect&lt;/b&gt; or &lt;b&gt;time-to-repair&lt;/b&gt;, and of course, &lt;b&gt;reducing the impact of the outages&lt;/b&gt; in the first place.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/compute/docs/tutorials/robustsystems#distribute&#34;&gt;Engineering your service for resiliency&lt;/a&gt; can reduce the frequency of total failures. You should avoid single points of failure in your architecture, whether it be an individual instance, availability zone, or even an entire region, which can prevent a smaller, localized outage from snowballing into global downtime.&lt;/p&gt;&lt;p&gt;You can reduce the impact on your users by reducing the percentage of infrastructure or users affected or the requests (e.g., throttling part of the requests vs. all of them). In order to reduce the blast radius of outages, avoid global changes and adopt advanced deployments strategies that allow you to gradually deploy changes. Consider progressive and canary rollouts over the course of hours, days, or weeks, which allow you to reduce the risk and to identify an issue before all your users are affected.&lt;/p&gt;&lt;p&gt;Further, having robust Continuous Integration and Continuous Delivery (CI/CD) pipelines allows you to deploy and roll back with confidence and reduce customer impact (See: SRE Book: &lt;a href=&#34;https://sre.google/sre-book/release-engineering/&#34; target=&#34;_blank&#34;&gt;Chapter 8 - Release Engineering&lt;/a&gt;). Creating an integrated process of code review and testing will help you find the issues early on before users are affected. &lt;/p&gt;&lt;p&gt;Improving the &lt;b&gt;time to detect&lt;/b&gt; means that you catch outages faster. As a reminder, having an &lt;b&gt;estimated TTD&lt;/b&gt; expresses how long until a human being is informed of the problem. For example, imagine someone receives and acts upon a page. TTD also includes any delays until the &#39;detection&#39; like data processing. For example, if I&#39;m using a log-based alert, and my log system has an ingestion time of 5 minutes, this increases the TTD for every alert by 5 minutes.&lt;/p&gt;&lt;p&gt;&lt;b&gt;ETTR&lt;/b&gt; (estimated time-to-repair) is the time between the time a human sees the alert and the time your users are happy. Improving &lt;b&gt;time-to-repair&lt;/b&gt; means that we fix outages quicker, in principle. That said, our focus should still be &#34;does this incident still affect our users?&#34; In most cases &lt;a href=&#34;https://www.oreilly.com/content/generic-mitigations/&#34; target=&#34;_blank&#34;&gt;mitigations&lt;/a&gt; like rolling back new releases or diverting traffic to unaffected regions can reduce or eliminate the impact of an ongoing outage on users much faster than trying to roll forward to a new, patched build. The root cause isn&#39;t yet fixed, but the users don&#39;t know or care — all they see is that the service is working again. &lt;/p&gt;&lt;p&gt;While it takes the human out of the loop, using automation can reduce the TTR and can be crucial to achieving higher reliability targets. However, it doesn&#39;t eliminate the TTR altogether, because even if a mitigation such as failing over to a different region is automated, it still takes time for it to have an impact.&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;A note about “estimated” values&lt;/b&gt;: At the beginning of a risk analysis, you might start with rough estimates for these metrics. But as you collect more data from incidents data you can update these estimates based on data from prior outages. &lt;/i&gt;&lt;/p&gt;&lt;h3&gt;Risk analysis process at a high level &lt;/h3&gt;&lt;p&gt;The risk analysis process starts by brainstorming risks for each of your SLOs, and more correctly for each one of your SLIs, as different SLIs will be exposed to different risks. In the next phase, build a risk catalog and iterate on it.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Create a risk analysis sheet for two or three SLIs, using this &lt;a href=&#34;http://goo.gl/bnsPj7&#34; target=&#34;_blank&#34;&gt;template&lt;/a&gt;. Read more at &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons&#34;&gt;How to prioritize and communicate risks&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Brainstorm risks internally, considering the things that can affect your SLOs, and gathering some initial data. Do this first with the engineering team and then include the product team.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The risk analysis sheets for each of your SLIs should include ETTD, ETTR, impact, and frequency. Include global factors and suggested risks and whether these risks are acceptable or not.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Collect historical data and consult with the product team regarding the SLO-business needs. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Iterate and update data based on incidents in production.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Accepting risks&lt;/h3&gt;&lt;p&gt;After building the risk catalog and capturing the risk factors, finalize the SLOs according to business need and risk analysis. This step means you need to evaluate whether your SLO is achievable given the risks, and if it isn’t — what do you need to do to achieve your targets? It is crucial that PMs be part of this review process especially as they might need to prioritize engineering work that mitigates or eliminates any unacceptable risks.&lt;/p&gt;&lt;p&gt;In &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons&#34;&gt;how to prioritize and communicate risks&lt;/a&gt;, we introduce how to use the &#39;Risk Stack Rank&#39; sheet to see how much a given risk may “cost” you, and which risks you can accept (or not) for a given SLO. For example, in the &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1XTsPG79XCCiaOEMj8K4mgPg39ZWB1l5fzDc1aDjLW2Y/view#gid=1494250520&#34; target=&#34;_blank&#34;&gt;template sheet&lt;/a&gt;, you could accept all risks and achieve 99.5% reliability, some of the risks to achieve 99.9% and none of them to achieve 99.99%. If you can&#39;t accept a risk because you estimate that it will burn more error budget than your SLO affords you, that is a clear argument for dedicating engineering time to either fixing the root cause or building some sort of mitigation.&lt;/p&gt;&lt;p&gt;One final note: similar to SLOs, you will want to iterate on your risk refining your ETTD based on actual TTD observed during outages, and similarly for ETTR. After incidents, you need to update the data and see where you stand regarding those estimates. In addition, revisit those estimates periodically to evaluate whether your risks are still relevant, if your estimates are correct, or if there are any additional risks that you need to account for. Like the SRE principle of &lt;a href=&#34;https://sre.google/sre-book/evolving-sre-engagement-model/&#34; target=&#34;_blank&#34;&gt;continuous improvement&lt;/a&gt;, it’s work that’s never truly done, but that is well worth the effort!&lt;/p&gt;&lt;p&gt;For more on this topic, check out my upcoming &lt;a href=&#34;http://devopsdays.org/&#34; target=&#34;_blank&#34;&gt;DevOpsDays 2022&lt;/a&gt; talk, taking place in &lt;a href=&#34;https://devopsdays.org/events/2022-birmingham-uk/&#34; target=&#34;_blank&#34;&gt;Birmingham&lt;/a&gt; on May 6 and in &lt;a href=&#34;https://devopsdays.org/events/2022-prague/welcome/&#34; target=&#34;_blank&#34;&gt;Prague&lt;/a&gt; on May 24.   &lt;/p&gt;&lt;h3&gt;Further reading and resources&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.coursera.org/learn/site-reliability-engineering-slos&#34; target=&#34;_blank&#34;&gt;&lt;b&gt;Site Reliability Engineering: Measuring and Managing Reliability&lt;/b&gt;&lt;/a&gt; (Coursera course)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/architecture/framework/reliability&#34;&gt;Google Cloud Architecture Framework: Reliability&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://queue.acm.org/detail.cfm?id=3096459&#34; target=&#34;_blank&#34;&gt;The Calculus of Service Availability&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons&#34;&gt;Know thy enemy: how to prioritize and communicate risks—CRE life lessons&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://sre.google/resources/practices-and-processes/incident-metrics-in-sre/&#34; target=&#34;_blank&#34;&gt;Incident Metrics in SRE - Google - Site Reliability Engineering&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/sre&#34;&gt;SRE on Google Cloud&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_aWHZoxD.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Know thy enemy: How to prioritize and communicate risks—CRE life lessons&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;How to effectively communicate and stack-rank risks in your system.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Ayelet Sachto&lt;/name&gt;&lt;title&gt;Strategic Cloud Engineer, Infra, AppMod, SRE&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 04 May 2022 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Announcing new simple query options in Cloud Logging</title>
      <link>https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;When you’re troubleshooting an issue, finding the root cause often involves finding specific logs generated by infrastructure and application code. The faster you can find logs, the faster you can confirm or refute your hypothesis about the root cause and resolve the issue! Today, we’re pleased to announce a dramatically simpler way to find logs in Logs Explorer. &lt;/p&gt;&lt;h3&gt;Making querying even easier!&lt;/h3&gt;&lt;p&gt;Over the past 2 years, we heard feedback that many users needed simple free text search to find their logs. We also heard that users wanted to build a query using the dropdown selectors. We took all that feedback to heart and made many critical changes to the Logs Explorer to address this feedback and make searching logs even easier.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Simple text search&lt;/b&gt; – a new simple text search box for global text searches&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Advanced query&lt;/b&gt; – a new toggle to show/hide the Logging query language for the query&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Date/time picker&lt;/b&gt;– the date/time range picker is now a part of the query builder&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Date/time preferences&lt;/b&gt; – the date/time display now respects date/time preferences set in the Cloud Console settings&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Dropdown selectors&lt;/b&gt; – prominently display the resource, logName and severity dropdown selectors&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Dropdown selector state&lt;/b&gt; – maintain the state in the resource, logName, severity and free text search boxes whether building query via dropdown or by editing the Logging query language&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Default summary fields&lt;/b&gt; – a new option to disable default summary fields for a more basic log view&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;01.logs_explorer_simple_mode_query.gif&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/01.logs_explorer_simple_mode_query.gif&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Simple text search&lt;/h3&gt;&lt;p&gt;The new text search box performs global free text searches across your logs for the strings added to the text search box. For example, a simple “POST OR GET” will find any logs including the text “POST” or “GET” in any log field.  &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;02.simple_text_search.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/02.simple_text_search.1000066220000402.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Additionally, you’ll see your query results highlighted both in the log summary line and the individual log entry itself. &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;03.log_results_text_highlight.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/03.log_results_text_highlight.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Show/hide query toggle&lt;/h3&gt;&lt;p&gt;The new features simplify the query experience for many users, but the Logs Explorer still needs to allow users to to write complex queries for advanced use cases. That’s why we added the &lt;i&gt;Show/hide&lt;/i&gt; query toggle which expands and closes the Logging query language behind the query. &lt;/p&gt;&lt;p&gt;You can use the &lt;i&gt;Show/hide query&lt;/i&gt; toggle when the dropdowns just don’t cut it for your use case and you need to build conditional logic or regexes into your queries. You can update the Logging query language directly by selecting the &lt;i&gt;Show/hide query&lt;/i&gt; toggle.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;04.logs_explorer_simple_query_mode_show_query.gif&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/04.logs_explorer_simple_query_mode_show_query.gif&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Date/time picker&lt;/h3&gt;&lt;p&gt;We moved the date/time picker location to be featured prominently as the first item in the query builder which makes it easier to find. While this move represents a relocation in the Logs Explorer user interface, we have a series of improvements that the team is actively developing to make it even easier to find the right logs for a date/time range.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;05.date_time_range_selector.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/05.date_time_range_selector.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Date/time display based on Cloud Console settings&lt;/h3&gt;&lt;p&gt;Whether you’re a developer, DevOps engineer, SRE, or anywhere in between, working with dates can be difficult because of the different representations. With this change, the &lt;i&gt;Jump to time and Enter custom range&lt;/i&gt; options in the date/time range selector will now respect your date and time format preferences set in the Cloud Console settings. This means, if you select mm/dd/yyyy or dd/mm/yyyy in your Cloud Console settings, you’ll see the date/time options in that format. When you select a 24-hour time format or an AM/PM time format, you’ll see the time options in your selected format.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;06.date_time_preferences.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/06.date_time_preferences.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Resource, logName and severity dropdown selectors&lt;/h3&gt;&lt;p&gt;The Logs Explorer now prominently displays the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus&#34;&gt;dropdown selectors&lt;/a&gt; for resources, logNames and severity. These dropdown selectors have also been improved so that they run the query each time a selection is made. This makes it easier to narrow logs quickly with each dropdown selection. Together, these changes make the dropdown selectors easier to find and more responsive.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;07.logs_explorer_simple_query_mode_resource_selectors.gif&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/07.logs_explorer_simple_query_mode_resource_selectors.gif&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Keeping the Logging query language and dropdowns in sync&lt;/h3&gt;&lt;p&gt;When you use the resource, logName, severity dropdowns or add search text, Logs Explorer now builds a Logging query language query for you. For basic queries you don’t even need to look at the Logging query language. For more complex queries, you can edit the query language directly. &lt;/p&gt;&lt;p&gt;Here’s the interesting part: when you edit the query directly, the resource, logName, severity dropdowns and search text will be updated to match the Logging query language terms if they can be parsed and don’t include complex logical conditions. &lt;/p&gt;&lt;p&gt;For example, if you use the &lt;i&gt;Show logs&lt;/i&gt; toggle to show the Logging query language and add the &lt;code&gt;severity=ERROR&lt;/code&gt;, the severity dropdown is updated to show that ERROR is selected.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;08.show_query_severity_selector.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/08.show_query_severity_selector.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Next, if you select  “DEBUG” from the severity dropdown, the query is updated to &lt;code&gt;severity=(ERROR OR DEBUG)&lt;/code&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;09.severity_selectors.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/09.severity_selectors.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Maintaining the query state regardless of whether you’re selecting from the dropdowns or typing in queries means there is one less detail to remember when you’re querying your logs.&lt;/p&gt;&lt;h3&gt;Disabling default summary fields for a basic log view&lt;/h3&gt;&lt;p&gt;The Logs Explorer adds default &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields&#34;&gt;summary fields&lt;/a&gt; to the log results to highlight useful information and make it easy to take action directly from the log line. For example, on App Engine logs, the default summary field chips highlight the latency which can help you more easily filter logs.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;10.custom_summary_fields.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/10.custom_summary_fields.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Logs Explorer also enables you to add your own &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields&#34;&gt;custom summary fields&lt;/a&gt; to the log lines so you can view what’s most important to you about the log lines. &lt;/p&gt;&lt;p&gt;We’ve heard feedback that sometimes all that’s needed is the raw text of logs. To show raw text logs, we’ve added a toggle to turn off/on the default summary fields for your log results for the duration of your session. By turning off the default summary fields, you’ll see only the raw text logs summary. To turn the summary fields back on, simply enable the toggle or start a new Logs Explorer session in a new tab.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;11.logs_explorer_simple_query_mode_hide_default_summary_fields.gif&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/11.logs_explorer_simple_query_mode_hide_default_summary_fields.gif&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;The road ahead&lt;/h3&gt;&lt;p&gt;We’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Logs Explorer&lt;/a&gt; and join the discussion in our &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Cloud Operations page&lt;/a&gt; on the Google Cloud Community site.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/operations/faster-debugging-with-traces-and-logs-together/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/cloud_logging_OUrfE4R.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Enabling SRE best practices: new contextual traces in Cloud Logging&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Developers can now view trace information for applications directly in Google Cloud Logging for faster debugging.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><promo-banner-block _nghost-c60=""></promo-banner-block><article><article-header-block></article-header-block><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c68=""><div _ngcontent-c68="" innerhtml="&lt;p&gt;When you&amp;#8217;re troubleshooting an issue, finding the root cause often involves finding specific logs generated by infrastructure and application code. The faster you can find logs, the faster you can confirm or refute your hypothesis about the root cause and resolve the issue! Today, we&amp;#8217;re pleased to announce a dramatically simpler way to find logs in Logs Explorer.&amp;#160;&lt;/p&gt;&lt;h3&gt;Making querying even easier!&lt;/h3&gt;&lt;p&gt;Over the past 2 years, we heard feedback that many users needed simple free text search to find their logs. We also heard that users wanted to build a query using the dropdown selectors. We took all that feedback to heart and made many critical changes to the Logs Explorer to address this feedback and make searching logs even easier.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Simple text search&lt;/b&gt; &amp;#8211;&amp;#160;a new simple text search box for global text searches&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Advanced query&lt;/b&gt; &amp;#8211;&amp;#160;a new toggle to show/hide the Logging query language for the query&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Date/time picker &lt;/b&gt;&amp;#8211; the date/time range picker is now a part of the query builder&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Date/time preferences&lt;/b&gt; &amp;#8211; the date/time display now respects date/time preferences set in the Cloud Console settings&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Dropdown selectors&lt;/b&gt; &amp;#8211; prominently display the resource, logName and severity dropdown selectors&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Dropdown selector state&lt;/b&gt; &amp;#8211;&amp;#160;maintain the state in the resource, logName, severity and free text search boxes whether building query via dropdown or by editing the Logging query language&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Default summary fields&lt;/b&gt; &amp;#8211; a new option to disable default summary fields for a more basic log view&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;"><p>When you’re troubleshooting an issue, finding the root cause often involves finding specific logs generated by infrastructure and application code. The faster you can find logs, the faster you can confirm or refute your hypothesis about the root cause and resolve the issue! Today, we’re pleased to announce a dramatically simpler way to find logs in Logs Explorer. </p><h3>Making querying even easier!</h3><p>Over the past 2 years, we heard feedback that many users needed simple free text search to find their logs. We also heard that users wanted to build a query using the dropdown selectors. We took all that feedback to heart and made many critical changes to the Logs Explorer to address this feedback and make searching logs even easier.</p><ol><li><p><b>Simple text search</b> – a new simple text search box for global text searches</p></li><li><p><b>Advanced query</b> – a new toggle to show/hide the Logging query language for the query</p></li><li><p><b>Date/time picker </b>– the date/time range picker is now a part of the query builder</p></li><li><p><b>Date/time preferences</b> – the date/time display now respects date/time preferences set in the Cloud Console settings</p></li><li><p><b>Dropdown selectors</b> – prominently display the resource, logName and severity dropdown selectors</p></li><li><p><b>Dropdown selector state</b> – maintain the state in the resource, logName, severity and free text search boxes whether building query via dropdown or by editing the Logging query language</p></li><li><p><b>Default summary fields</b> – a new option to disable default summary fields for a more basic log view</p></li></ol></div></paragraph-block></div><div><paragraph-block _nghost-c68=""><div _ngcontent-c68="" innerhtml="&lt;h3&gt;Simple text search&lt;/h3&gt;&lt;p&gt;The new text search box performs global free text searches across your logs for the strings added to the text search box. For example, a simple &amp;#8220;POST OR GET&amp;#8221; will find any logs including the text &amp;#8220;POST&amp;#8221; or &amp;#8220;GET&amp;#8221; in any log field.&amp;#160;&amp;#160;&lt;/p&gt;"><h3>Simple text search</h3><p>The new text search box performs global free text searches across your logs for the strings added to the text search box. For example, a simple “POST OR GET” will find any logs including the text “POST” or “GET” in any log field.  </p></div></paragraph-block></div><div><paragraph-block _nghost-c68=""><p>Additionally, you’ll see your query results highlighted both in the log summary line and the individual log entry itself.  </p></paragraph-block></div><div><paragraph-block _nghost-c68=""><div _ngcontent-c68="" innerhtml="&lt;h3&gt;Show/hide query toggle&lt;/h3&gt;&lt;p&gt;The new features simplify the query experience for many users, but the Logs Explorer still needs to allow users to to write complex queries for advanced use cases. That&amp;#8217;s why we added the &lt;i&gt;Show/hide&lt;/i&gt; query toggle which expands and closes the Logging query language behind the query.&amp;#160;&lt;/p&gt;&lt;p&gt;You can use the &lt;i&gt;Show/hide query&lt;/i&gt; toggle when the dropdowns just don&amp;#8217;t cut it for your use case and you need to build conditional logic or regexes into your queries. You can update the Logging query language directly by selecting the &lt;i&gt;Show/hide query&lt;/i&gt; toggle.&lt;/p&gt;"><h3>Show/hide query toggle</h3><p>The new features simplify the query experience for many users, but the Logs Explorer still needs to allow users to to write complex queries for advanced use cases. That’s why we added the <i>Show/hide</i> query toggle which expands and closes the Logging query language behind the query. </p><p>You can use the <i>Show/hide query</i> toggle when the dropdowns just don’t cut it for your use case and you need to build conditional logic or regexes into your queries. You can update the Logging query language directly by selecting the <i>Show/hide query</i> toggle.</p></div></paragraph-block></div><div><paragraph-block _nghost-c68=""><div _ngcontent-c68="" innerhtml="&lt;h3&gt;Date/time picker&lt;/h3&gt;&lt;p&gt;We moved the date/time picker location to be featured prominently as the first item in the query builder which makes it easier to find. While this move represents a relocation in the Logs Explorer user interface, we have a series of improvements that the team is actively developing to make it even easier to find the right logs for a date/time range.&lt;/p&gt;"><h3>Date/time picker</h3><p>We moved the date/time picker location to be featured prominently as the first item in the query builder which makes it easier to find. While this move represents a relocation in the Logs Explorer user interface, we have a series of improvements that the team is actively developing to make it even easier to find the right logs for a date/time range.</p></div></paragraph-block></div><div><paragraph-block _nghost-c68=""><div _ngcontent-c68="" innerhtml="&lt;h3&gt;Date/time display based on Cloud Console settings&lt;/h3&gt;&lt;p&gt;Whether you&amp;#8217;re a developer, DevOps engineer, SRE, or anywhere in between, working with dates can be difficult because of the different representations. With this change, the &lt;i&gt;Jump to time and Enter custom range&lt;/i&gt; options in the date/time range selector will now respect your date and time format preferences set in the Cloud Console settings. This means, if you select mm/dd/yyyy or dd/mm/yyyy in your Cloud Console settings, you&amp;#8217;ll see the date/time options in that format. When you select a 24-hour time format or an AM/PM time format, you&amp;#8217;ll see the time options in your selected format.&lt;/p&gt;"><h3>Date/time display based on Cloud Console settings</h3><p>Whether you’re a developer, DevOps engineer, SRE, or anywhere in between, working with dates can be difficult because of the different representations. With this change, the <i>Jump to time and Enter custom range</i> options in the date/time range selector will now respect your date and time format preferences set in the Cloud Console settings. This means, if you select mm/dd/yyyy or dd/mm/yyyy in your Cloud Console settings, you’ll see the date/time options in that format. When you select a 24-hour time format or an AM/PM time format, you’ll see the time options in your selected format.</p></div></paragraph-block></div><div><paragraph-block _nghost-c68=""><div _ngcontent-c68="" innerhtml="&lt;h3&gt;Resource, logName and severity dropdown selectors&lt;/h3&gt;&lt;p&gt;The Logs Explorer now prominently displays the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus&#34;&gt;dropdown selectors&lt;/a&gt; for resources, logNames and severity. These dropdown selectors have also been improved so that they run the query each time a selection is made. This makes it easier to narrow logs quickly with each dropdown selection. Together, these changes make the dropdown selectors easier to find and more responsive.&lt;/p&gt;"><h3>Resource, logName and severity dropdown selectors</h3><p>The Logs Explorer now prominently displays the <a href="https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus" track-metadata-module="post">dropdown selectors</a> for resources, logNames and severity. These dropdown selectors have also been improved so that they run the query each time a selection is made. This makes it easier to narrow logs quickly with each dropdown selection. Together, these changes make the dropdown selectors easier to find and more responsive.</p></div></paragraph-block></div><div><paragraph-block _nghost-c68=""><div _ngcontent-c68="" innerhtml="&lt;h3&gt;Keeping the Logging query language and dropdowns in sync&lt;/h3&gt;&lt;p&gt;When you use the resource, logName, severity dropdowns or add search text, Logs Explorer now builds a Logging query language query for you. For basic queries you don&amp;#8217;t even need to look at the Logging query language. For more complex queries, you can edit the query language directly.&amp;#160;&lt;/p&gt;&lt;p&gt;Here&amp;#8217;s the interesting part: when you edit the query directly, the resource, logName, severity dropdowns and search text will be updated to match the Logging query language terms if they can be parsed and don&amp;#8217;t include complex logical conditions.&amp;#160;&lt;/p&gt;&lt;p&gt;For example, if you use the &lt;i&gt;Show logs&lt;/i&gt; toggle to show the Logging query language and add the &lt;code&gt;severity=ERROR&lt;/code&gt;, the severity dropdown is updated to show that ERROR is selected.&lt;/p&gt;"><h3>Keeping the Logging query language and dropdowns in sync</h3><p>When you use the resource, logName, severity dropdowns or add search text, Logs Explorer now builds a Logging query language query for you. For basic queries you don’t even need to look at the Logging query language. For more complex queries, you can edit the query language directly. </p><p>Here’s the interesting part: when you edit the query directly, the resource, logName, severity dropdowns and search text will be updated to match the Logging query language terms if they can be parsed and don’t include complex logical conditions. </p><p>For example, if you use the <i>Show logs</i> toggle to show the Logging query language and add the <code>severity=ERROR</code>, the severity dropdown is updated to show that ERROR is selected.</p></div></paragraph-block></div><div><paragraph-block _nghost-c68=""><p>Next, if you select  “DEBUG” from the severity dropdown, the query is updated to <code>severity=(ERROR OR DEBUG)</code>. </p></paragraph-block></div><div><paragraph-block _nghost-c68=""><div _ngcontent-c68="" innerhtml="&lt;p&gt;Maintaining the query state regardless of whether you&amp;#8217;re selecting from the dropdowns or typing in queries means there is one less detail to remember when you&amp;#8217;re querying your logs.&lt;/p&gt;&lt;h3&gt;Disabling default summary fields for a basic log view&lt;/h3&gt;&lt;p&gt;The Logs Explorer adds default &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields&#34;&gt;summary fields&lt;/a&gt; to the log results to highlight useful information and make it easy to take action directly from the log line. For example, on App Engine logs, the default summary field chips highlight the latency which can help you more easily filter logs.&lt;/p&gt;"><p>Maintaining the query state regardless of whether you’re selecting from the dropdowns or typing in queries means there is one less detail to remember when you’re querying your logs.</p><h3>Disabling default summary fields for a basic log view</h3><p>The Logs Explorer adds default <a href="https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields" track-metadata-module="post">summary fields</a> to the log results to highlight useful information and make it easy to take action directly from the log line. For example, on App Engine logs, the default summary field chips highlight the latency which can help you more easily filter logs.</p></div></paragraph-block></div><div><paragraph-block _nghost-c68=""><div _ngcontent-c68="" innerhtml="&lt;p&gt;Logs Explorer also enables you to add your own &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields&#34;&gt;custom summary fields&lt;/a&gt; to the log lines so you can view what&amp;#8217;s most important to you about the log lines.&amp;#160;&lt;/p&gt;&lt;p&gt;We&amp;#8217;ve heard feedback that sometimes all that&amp;#8217;s needed is the raw text of logs. To show raw text logs, we&amp;#8217;ve added a toggle to turn off/on the default summary fields for your log results for the duration of your session. By turning off the default summary fields, you&amp;#8217;ll see only the raw text logs summary. To turn the summary fields back on, simply enable the toggle or start a new Logs Explorer session in a new tab.&lt;/p&gt;"><p>Logs Explorer also enables you to add your own <a href="https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields" track-metadata-module="post">custom summary fields</a> to the log lines so you can view what’s most important to you about the log lines. </p><p>We’ve heard feedback that sometimes all that’s needed is the raw text of logs. To show raw text logs, we’ve added a toggle to turn off/on the default summary fields for your log results for the duration of your session. By turning off the default summary fields, you’ll see only the raw text logs summary. To turn the summary fields back on, simply enable the toggle or start a new Logs Explorer session in a new tab.</p></div></paragraph-block></div><div><paragraph-block _nghost-c68=""><div _ngcontent-c68="" innerhtml="&lt;h3&gt;The road ahead&lt;/h3&gt;&lt;p&gt;We&amp;#8217;re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven&amp;#8217;t already, get started with the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface&#34;&gt;Logs Explorer&lt;/a&gt; and join the discussion in our &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Cloud Operations page&lt;/a&gt; on the Google Cloud Community site.&lt;/p&gt;"><h3>The road ahead</h3><p>We’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the <a href="https://cloud.google.com/logging/docs/view/logs-explorer-interface" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/logs-explorer-interface" track-metadata-module="post">Logs Explorer</a> and join the discussion in our <a href="https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">Cloud Operations page</a> on the Google Cloud Community site.</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c67=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Charles Baer&lt;/name&gt;&lt;title&gt;Product Manager, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 25 Apr 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Google Cloud and MongoDB Atlas expand their partnership</title>
      <link>https://cloud.google.com/blog/products/databases/making-mongodb-on-google-cloud-even-more-flexible-with-pay-go/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;As the volume and velocity of data grows daily, business success depends on the ability to manage it effectively and transform it into actionable insights. Since 2019, &lt;a href=&#34;https://cloud.google.com/blog/topics/partners/mongodb-and-google-extend-partnership-drive-enterprise-cloud-modernization&#34;&gt;Google Cloud and MongoDB have worked together&lt;/a&gt; to give businesses the secure, global, and highly performant infrastructure, the sophisticated data intelligence, and the developer-centric tools they need to power modern, data-driven cloud applications. Our partnership with MongoDB continues to deliver richer yet simpler ways to lead with software. &lt;/p&gt;&lt;p&gt;With MongoDB Atlas on Google Cloud, developers can build upon a solid foundation that enables them to work with data the way they want in support of global-scale applications. For example, &lt;a href=&#34;https://www.youtube.com/watch?v=vvBZ-LFOHko&#34; target=&#34;_blank&#34;&gt;Forbes&lt;/a&gt;, a large business media brand, migrated its platform to Google Cloud and MongoDB Atlas in just six months. The company’s new cloud infrastructure helped the website scale to accommodate record-breaking growth even while making development more nimble. MongoDB’s document model meant developers could build new features quickly, easily incorporate changes, and better handle a growing diversity of data types. It also allowed for more powerful tools, such a machine-language trending story recommendation engine for journalists. Results have been impressive, including:&lt;br/&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;58% faster build time for new products and fixes&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Accelerated release cycle by 4x&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Reduced total cost of ownership by 25%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;28% increase in subscriptions from new newsletters&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Forbes team is already looking ahead, with plans for improved personalization, loyalty, and management of first-party data. “Our decision to migrate to Google Cloud was made based on the toolset that it offers, scalability, and developer friendliness,” says Vadim Supitsky, Forbes CTO.&lt;/p&gt;&lt;p&gt;Over the course of the partnership between MongoDB and Google Cloud, we have continually added new benefits for our mutual customers, such as the ability to to integrate &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/mongodb/atlas-pro?utm_campaign=gcpatlaspro&amp;amp;utm_source=linkedin&amp;amp;utm_medium=organic_social&#34;&gt;MongoDB Atlas with Google Cloud&lt;/a&gt; products; leveraging &lt;a href=&#34;https://cloud.google.com/bigquery&#34;&gt;BigQuery&lt;/a&gt; to create a managed, serverless, scalable architecture; rich data connectivity; and flexible scaling. We’ve also made it easier to migrate MongoDB on-premises instances to MongoDB Atlas on Google Cloud. &lt;/p&gt;&lt;h3&gt;Making MongoDB on Google Cloud even more flexible with Pay-Go&lt;/h3&gt;&lt;p&gt;That’s why we’re excited to see  yet another reason for companies to choose MongoDB Atlas on Google Cloud: A new pay-as-you-go option, available on the &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service&#34;&gt;Google Cloud Marketplace&lt;/a&gt;. With this new offering, developers now have a simplified subscription experience, and enterprises have a simplified way to procure MongoDB in addition to privately negotiated offers already supported on the Google Cloud Marketplace. There are no up-front commitments required to use MongoDB Atlas on Google Cloud, and customers pay only for the resources they use and scale based on their needs. Here are just a few of the benefits of this new service:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Spin up a MongoDB Atlas cluster on the Google Cloud Console within a few minutes&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;No need for a separate credit card payment: You can use your Google Cloud Billing account for your MongoDB Atlas environment&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Receive a single bill for Google Cloud and MongoDB Atlas &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Apply Google Cloud committed spend to MongoDB transactions through Google Cloud Marketplace&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Get started with MongoDB Atlas with 512 MB of storage for free. Atlas free tier clusters are perfect for learning MongoDB or prototyping applications&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We’re excited to see even further growth in our continuing partnership with three new developments: First, because resellers are such an important force, we are pleased to share that reseller partners can now make MongoDB Atlas available via the Google Cloud Marketplace. Second, MongoDB and Google Cloud have expanded our joint reach to 28 global regions after adding Toronto, Canada, and Santiago, Chile. And third, we’ve made a joint commitment to early stage companies through &lt;a href=&#34;https://www.mongodb.com/startups&#34; target=&#34;_blank&#34;&gt;MongoDB for Startups&lt;/a&gt; and the &lt;a href=&#34;https://cloud.google.com/startup&#34;&gt;Google for Startups Cloud Program&lt;/a&gt;. Pay-Go is great for startups because it’s easy to scale and you only pay for what you use. Similarly, MongoDB and Google are helping startups get off the ground with our respective programs, which both include credits and support.  &lt;/p&gt;&lt;h3&gt;Better together&lt;/h3&gt;&lt;p&gt;Your company’s transformation hinges on new cloud database capabilities. The first step to building all-new digital experiences is to select the operational database that will power your application and, in essence, run your business.The partnership between MongoDB and Google Cloud gives you the benefits of a modern database service in a tightly integrated, cloud-native way. Since the beginning of our collaborative journey, we have made significant enhancements to the experience for our mutual customers, and we continue to work toward providing a rich developer experience for MongoDB Atlas on Google Cloud.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/mongodb?utm_source=youtube&amp;amp;utm_medium=unpaidsoc&amp;amp;utm_campaign=fy21q3-googlecloud-web-data-description-no-brand-global&amp;amp;utm_content=skyvine1016918617&amp;amp;utm_term=-&#34;&gt;Discover how customers&lt;/a&gt; choose MongoDB and Google Cloud to power the future of customer innovation. For more information on how to get started, visit &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service/&#34;&gt;MongoDB Atlas on Google Cloud Marketplace.&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/databases/ochk-used-cloud-spanner-to-build-covid-19-vaccine-app/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_Databases.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Vaccinating a nation: Vaccination app delivery in 30 days with Cloud Spanner&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;OChK used Cloud Spanner to design and deploy an application to help vaccinate every citizen in Poland against COVID-19.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c74=""><div _ngcontent-c74="" innerhtml="&lt;p&gt;As the volume and velocity of data grows daily, business success depends on the ability to manage it effectively and transform it into actionable insights. Since 2019, &lt;a href=&#34;https://cloud.google.com/blog/topics/partners/mongodb-and-google-extend-partnership-drive-enterprise-cloud-modernization&#34;&gt;Google Cloud and MongoDB have worked together&lt;/a&gt; to give businesses the secure, global, and highly performant infrastructure, the sophisticated data intelligence, and the developer-centric tools they need to power modern, data-driven cloud applications. Our partnership with MongoDB continues to deliver richer yet simpler ways to lead with software.&amp;#160;&lt;/p&gt;&lt;p&gt;With MongoDB Atlas on Google Cloud, developers can build upon a solid foundation that enables them to work with data the way they want in support of global-scale applications. For example, &lt;a href=&#34;https://www.youtube.com/watch?v=vvBZ-LFOHko&#34; target=&#34;_blank&#34;&gt;Forbes&lt;/a&gt;, a large business media brand, migrated its platform to Google Cloud and MongoDB Atlas in just six months. The company&amp;#8217;s new cloud infrastructure helped the website scale to accommodate record-breaking growth even while making development more nimble. MongoDB&amp;#8217;s document model meant developers could build new features quickly, easily incorporate changes, and better handle a growing diversity of data types. It also allowed for more powerful tools, such a machine-language trending story recommendation engine for journalists. Results have been impressive, including:&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;58% faster build time for new products and fixes&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Accelerated release cycle by 4x&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Reduced total cost of ownership by 25%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;28% increase in subscriptions from new newsletters&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Forbes team is already looking ahead, with plans for improved personalization, loyalty, and management of first-party data. &amp;#8220;Our decision to migrate to Google Cloud was made based on the toolset that it offers, scalability, and developer friendliness,&amp;#8221; says Vadim Supitsky, Forbes CTO.&lt;/p&gt;&lt;p&gt;Over the course of the partnership between MongoDB and Google Cloud, we have continually added new benefits for our mutual customers, such as the ability to to integrate &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/mongodb/atlas-pro?utm_campaign=gcpatlaspro&amp;amp;utm_source=linkedin&amp;amp;utm_medium=organic_social&#34;&gt;MongoDB Atlas with Google Cloud&lt;/a&gt; products; leveraging &lt;a href=&#34;https://cloud.google.com/bigquery&#34;&gt;BigQuery&lt;/a&gt; to create a managed, serverless, scalable architecture; rich data connectivity; and flexible scaling. We&amp;#8217;ve also made it easier to migrate MongoDB on-premises instances to MongoDB Atlas on Google Cloud.&amp;#160;&lt;/p&gt;&lt;h3&gt;Making MongoDB on Google Cloud even more flexible with Pay-Go&lt;/h3&gt;&lt;p&gt;That&amp;#8217;s why we&amp;#8217;re excited to see&amp;#160; yet another reason for companies to choose MongoDB Atlas on Google Cloud: A new pay-as-you-go option, available on the &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service&#34;&gt;Google Cloud Marketplace&lt;/a&gt;. With this new offering, developers now have a simplified subscription experience, and enterprises have a simplified way to procure MongoDB in addition to privately negotiated offers already supported on the Google Cloud Marketplace. There are no up-front commitments required to use MongoDB Atlas on Google Cloud, and customers pay only for the resources they use and scale based on their needs. Here are just a few of the benefits of this new service:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Spin up a MongoDB Atlas cluster on the Google Cloud Console within a few minutes&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;No need for a separate credit card payment: You can use your Google Cloud Billing account for your MongoDB Atlas environment&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Receive a single bill for Google Cloud and MongoDB Atlas&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Apply Google Cloud committed spend to MongoDB transactions through Google Cloud Marketplace&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Get started with MongoDB Atlas with 512 MB of storage for free. Atlas free tier clusters are perfect for learning MongoDB or prototyping applications&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We&amp;#8217;re excited to see even further growth in our continuing partnership with three new developments: First, because resellers are such an important force, we are pleased to share that reseller partners can now make MongoDB Atlas available via the Google Cloud Marketplace. Second, MongoDB and Google Cloud have expanded our joint reach to 28 global regions after adding Toronto, Canada, and Santiago, Chile. And third, we&amp;#8217;ve made a joint commitment to early stage companies through &lt;a href=&#34;https://www.mongodb.com/startups&#34; target=&#34;_blank&#34;&gt;MongoDB for Startups&lt;/a&gt; and the &lt;a href=&#34;https://cloud.google.com/startup&#34;&gt;Google for Startups Cloud Program&lt;/a&gt;. Pay-Go is great for startups because it&amp;#8217;s easy to scale and you only pay for what you use. Similarly, MongoDB and Google are helping startups get off the ground with our respective programs, which both include credits and support.&amp;#160;&amp;#160;&lt;/p&gt;&lt;h3&gt;Better together&lt;/h3&gt;&lt;p&gt;Your company&amp;#8217;s transformation hinges on new cloud database capabilities. The first step to building all-new digital experiences is to select the operational database that will power your application and, in essence, run your business.The partnership between MongoDB and Google Cloud gives you the benefits of a modern database service in a tightly integrated, cloud-native way. Since the beginning of our collaborative journey, we have made significant enhancements to the experience for our mutual customers, and we continue to work toward providing a rich developer experience for MongoDB Atlas on Google Cloud.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/mongodb?utm_source=youtube&amp;amp;utm_medium=unpaidsoc&amp;amp;utm_campaign=fy21q3-googlecloud-web-data-description-no-brand-global&amp;amp;utm_content=skyvine1016918617&amp;amp;utm_term=-&#34;&gt;Discover how customers&lt;/a&gt; choose MongoDB and Google Cloud to power the future of customer innovation. For more information on how to get started, visit &lt;a href=&#34;https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service/&#34;&gt;MongoDB Atlas on Google Cloud Marketplace.&lt;/a&gt;&lt;/p&gt;"><p>As the volume and velocity of data grows daily, business success depends on the ability to manage it effectively and transform it into actionable insights. Since 2019, <a href="https://cloud.google.com/blog/topics/partners/mongodb-and-google-extend-partnership-drive-enterprise-cloud-modernization" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/topics/partners/mongodb-and-google-extend-partnership-drive-enterprise-cloud-modernization" track-metadata-module="post">Google Cloud and MongoDB have worked together</a> to give businesses the secure, global, and highly performant infrastructure, the sophisticated data intelligence, and the developer-centric tools they need to power modern, data-driven cloud applications. Our partnership with MongoDB continues to deliver richer yet simpler ways to lead with software. </p><p>With MongoDB Atlas on Google Cloud, developers can build upon a solid foundation that enables them to work with data the way they want in support of global-scale applications. For example, <a href="https://www.youtube.com/watch?v=vvBZ-LFOHko" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://www.youtube.com" track-metadata-module="post">Forbes</a>, a large business media brand, migrated its platform to Google Cloud and MongoDB Atlas in just six months. The company’s new cloud infrastructure helped the website scale to accommodate record-breaking growth even while making development more nimble. MongoDB’s document model meant developers could build new features quickly, easily incorporate changes, and better handle a growing diversity of data types. It also allowed for more powerful tools, such a machine-language trending story recommendation engine for journalists. Results have been impressive, including:<br/></p><ul><li><p>58% faster build time for new products and fixes</p></li><li><p>Accelerated release cycle by 4x</p></li><li><p>Reduced total cost of ownership by 25%</p></li><li><p>28% increase in subscriptions from new newsletters</p></li></ul><p>The Forbes team is already looking ahead, with plans for improved personalization, loyalty, and management of first-party data. “Our decision to migrate to Google Cloud was made based on the toolset that it offers, scalability, and developer friendliness,” says Vadim Supitsky, Forbes CTO.</p><p>Over the course of the partnership between MongoDB and Google Cloud, we have continually added new benefits for our mutual customers, such as the ability to to integrate <a href="https://console.cloud.google.com/marketplace/product/mongodb/atlas-pro?utm_campaign=gcpatlaspro&amp;utm_source=linkedin&amp;utm_medium=organic_social" track-type="inline link" track-name="3" track-metadata-eventdetail="https://console.cloud.google.com/marketplace/product/mongodb/atlas-pro?utm_campaign=gcpatlaspro&amp;utm_source=linkedin&amp;utm_medium=organic_social" track-metadata-module="post">MongoDB Atlas with Google Cloud</a> products; leveraging <a href="https://cloud.google.com/bigquery" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/bigquery" track-metadata-module="post">BigQuery</a> to create a managed, serverless, scalable architecture; rich data connectivity; and flexible scaling. We’ve also made it easier to migrate MongoDB on-premises instances to MongoDB Atlas on Google Cloud. </p><h3>Making MongoDB on Google Cloud even more flexible with Pay-Go</h3><p>That’s why we’re excited to see  yet another reason for companies to choose MongoDB Atlas on Google Cloud: A new pay-as-you-go option, available on the <a href="https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service" track-type="inline link" track-name="5" track-metadata-eventdetail="https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service" track-metadata-module="post">Google Cloud Marketplace</a>. With this new offering, developers now have a simplified subscription experience, and enterprises have a simplified way to procure MongoDB in addition to privately negotiated offers already supported on the Google Cloud Marketplace. There are no up-front commitments required to use MongoDB Atlas on Google Cloud, and customers pay only for the resources they use and scale based on their needs. Here are just a few of the benefits of this new service:</p><ul><li><p>Spin up a MongoDB Atlas cluster on the Google Cloud Console within a few minutes</p></li><li><p>No need for a separate credit card payment: You can use your Google Cloud Billing account for your MongoDB Atlas environment</p></li><li><p>Receive a single bill for Google Cloud and MongoDB Atlas </p></li><li><p>Apply Google Cloud committed spend to MongoDB transactions through Google Cloud Marketplace</p></li><li><p>Get started with MongoDB Atlas with 512 MB of storage for free. Atlas free tier clusters are perfect for learning MongoDB or prototyping applications</p></li></ul><p>We’re excited to see even further growth in our continuing partnership with three new developments: First, because resellers are such an important force, we are pleased to share that reseller partners can now make MongoDB Atlas available via the Google Cloud Marketplace. Second, MongoDB and Google Cloud have expanded our joint reach to 28 global regions after adding Toronto, Canada, and Santiago, Chile. And third, we’ve made a joint commitment to early stage companies through <a href="https://www.mongodb.com/startups" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://www.mongodb.com" track-metadata-module="post">MongoDB for Startups</a> and the <a href="https://cloud.google.com/startup" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/startup" track-metadata-module="post">Google for Startups Cloud Program</a>. Pay-Go is great for startups because it’s easy to scale and you only pay for what you use. Similarly, MongoDB and Google are helping startups get off the ground with our respective programs, which both include credits and support.  </p><h3>Better together</h3><p>Your company’s transformation hinges on new cloud database capabilities. The first step to building all-new digital experiences is to select the operational database that will power your application and, in essence, run your business.The partnership between MongoDB and Google Cloud gives you the benefits of a modern database service in a tightly integrated, cloud-native way. Since the beginning of our collaborative journey, we have made significant enhancements to the experience for our mutual customers, and we continue to work toward providing a rich developer experience for MongoDB Atlas on Google Cloud.</p><p><a href="https://cloud.google.com/mongodb?utm_source=youtube&amp;utm_medium=unpaidsoc&amp;utm_campaign=fy21q3-googlecloud-web-data-description-no-brand-global&amp;utm_content=skyvine1016918617&amp;utm_term=-" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/mongodb?utm_source=youtube&amp;utm_medium=unpaidsoc&amp;utm_campaign=fy21q3-googlecloud-web-data-description-no-brand-global&amp;utm_content=skyvine1016918617&amp;utm_term=-" track-metadata-module="post">Discover how customers</a> choose MongoDB and Google Cloud to power the future of customer innovation. For more information on how to get started, visit <a href="https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service/" track-type="inline link" track-name="9" track-metadata-eventdetail="https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service/" track-metadata-module="post">MongoDB Atlas on Google Cloud Marketplace.</a></p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Ritika Suri&lt;/name&gt;&lt;title&gt;Director Data Technology Partnerships, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 22 Apr 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The SRE book turns 6!</title>
      <link>https://cloud.google.com/blog/products/devops-sre/the-sre-book-turns-6/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;It&#39;s hard to believe that it&#39;s already been six years since we published &lt;a href=&#34;https://sre.google/sre-book/table-of-contents/&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering: How Google Runs Production Systems&lt;/a&gt; with O&#39;Reilly Media. We&#39;ve been both humbled and pleasantly surprised by how popular the book has been, and continues to be. You may already be familiar with the two related books Google published after the SRE Book became a bestseller: &lt;a href=&#34;https://sre.google/workbook/table-of-contents/&#34; target=&#34;_blank&#34;&gt;The Site Reliability Workbook&lt;/a&gt; and &lt;a href=&#34;https://static.googleusercontent.com/media/sre.google/en//static/pdf/building_secure_and_reliable_systems.pdf&#34; target=&#34;_blank&#34;&gt;Building Secure and Reliable Systems&lt;/a&gt;. All three books are available for free at &lt;a href=&#34;http://sre.google/books&#34; target=&#34;_blank&#34;&gt;sre.google/books&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;It&#39;s perhaps harder to find and explore the numerous journal articles, longer format reports, blog posts, and trainings that Google SREs have published since 2016. Google SREs have also given dozens of talks at conferences about the topics covered in the SRE Book in the intervening years. While the content in the book remains largely evergreen, SRE is a dynamic field, and we&#39;ve had a lot more to say as our practices have evolved and gained depth. &lt;/p&gt;&lt;p&gt;To make this body of work more discoverable, we&#39;ve put together a compendium of this material, mapped by topic to each chapter of the book on sre.google: &lt;a href=&#34;https://sre.google/resources/book-update/&#34; target=&#34;_blank&#34;&gt;SRE Book Updates, by Topic&lt;/a&gt;. Here you&#39;ll find dozens more resources on some of our most popular topics, such as SLOs, Monitoring and Alerting, Canarying, Incident Management and Postmortem Culture, and Training SREs. Please explore away!&lt;/p&gt;&lt;p&gt;Of course, SREs have also spoken and written about topics beyond what&#39;s covered in the SRE Book (for example: Machine Learning, Capacity Planning, Innovations, and Security and Privacy); stay tuned for a catalog of those resources.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/prodcast.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Introducing the Google SRE Prodcast&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Discover Prodcast, Google’s Site Reliability Engineering Podcast. This limited-edition series explores fundamental topics in reliability ...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c52=""><div _ngcontent-c52="" innerhtml="&lt;p&gt;It&#39;s hard to believe that it&#39;s already been six years since we published &lt;a href=&#34;https://sre.google/sre-book/table-of-contents/&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering: How Google Runs Production Systems&lt;/a&gt; with O&#39;Reilly Media. We&#39;ve been both humbled and pleasantly surprised by how popular the book has been, and continues to be. You may already be familiar with the two related books Google published after the SRE Book became a bestseller: &lt;a href=&#34;https://sre.google/workbook/table-of-contents/&#34; target=&#34;_blank&#34;&gt;The Site Reliability Workbook&lt;/a&gt; and &lt;a href=&#34;https://static.googleusercontent.com/media/sre.google/en//static/pdf/building_secure_and_reliable_systems.pdf&#34; target=&#34;_blank&#34;&gt;Building Secure and Reliable Systems&lt;/a&gt;. All three books are available for free at &lt;a href=&#34;http://sre.google/books&#34; target=&#34;_blank&#34;&gt;sre.google/books&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;It&#39;s perhaps harder to find and explore the numerous journal articles, longer format reports, blog posts, and trainings that Google SREs have published since 2016. Google SREs have also given dozens of talks at conferences about the topics covered in the SRE Book in the intervening years. While the content in the book remains largely evergreen, SRE is a dynamic field, and we&#39;ve had a lot more to say as our practices have evolved and gained depth.&amp;#160;&lt;/p&gt;&lt;p&gt;To make this body of work more discoverable, we&#39;ve put together a compendium of this material, mapped by topic to each chapter of the book on sre.google: &lt;a href=&#34;https://sre.google/resources/book-update/&#34; target=&#34;_blank&#34;&gt;SRE Book Updates, by Topic&lt;/a&gt;. Here you&#39;ll find dozens more resources on some of our most popular topics, such as SLOs, Monitoring and Alerting, Canarying, Incident Management and Postmortem Culture, and Training SREs. Please explore away!&lt;/p&gt;&lt;p&gt;Of course, SREs have also spoken and written about topics beyond what&#39;s covered in the SRE Book (for example: Machine Learning, Capacity Planning, Innovations, and Security and Privacy); stay tuned for a catalog of those resources.&lt;/p&gt;"><p>It&#39;s hard to believe that it&#39;s already been six years since we published <a href="https://sre.google/sre-book/table-of-contents/" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">Site Reliability Engineering: How Google Runs Production Systems</a> with O&#39;Reilly Media. We&#39;ve been both humbled and pleasantly surprised by how popular the book has been, and continues to be. You may already be familiar with the two related books Google published after the SRE Book became a bestseller: <a href="https://sre.google/workbook/table-of-contents/" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">The Site Reliability Workbook</a> and <a href="https://static.googleusercontent.com/media/sre.google/en//static/pdf/building_secure_and_reliable_systems.pdf" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://static.googleusercontent.com" track-metadata-module="post">Building Secure and Reliable Systems</a>. All three books are available for free at <a href="http://sre.google/books" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="http://sre.google" track-metadata-module="post">sre.google/books</a>. </p><p>It&#39;s perhaps harder to find and explore the numerous journal articles, longer format reports, blog posts, and trainings that Google SREs have published since 2016. Google SREs have also given dozens of talks at conferences about the topics covered in the SRE Book in the intervening years. While the content in the book remains largely evergreen, SRE is a dynamic field, and we&#39;ve had a lot more to say as our practices have evolved and gained depth. </p><p>To make this body of work more discoverable, we&#39;ve put together a compendium of this material, mapped by topic to each chapter of the book on sre.google: <a href="https://sre.google/resources/book-update/" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">SRE Book Updates, by Topic</a>. Here you&#39;ll find dozens more resources on some of our most popular topics, such as SLOs, Monitoring and Alerting, Canarying, Incident Management and Postmortem Culture, and Training SREs. Please explore away!</p><p>Of course, SREs have also spoken and written about topics beyond what&#39;s covered in the SRE Book (for example: Machine Learning, Capacity Planning, Innovations, and Security and Privacy); stay tuned for a catalog of those resources.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Betsy Beyer&lt;/name&gt;&lt;title&gt;Technical Writer for SRE&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 19 Apr 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What’s new in cloud-native apps?</title>
      <link>https://cloud.google.com/blog/topics/cloud-first/whats-new-cloud-native-apps/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Developers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Apr 11 - Apr 15, 2022&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Listen to a Prodcast&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Google’s SRE team has launched a “&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast&#34;&gt;Prodcast&lt;/a&gt;” focusing on concepts from its SRE book. Available from wherever you get your podcasts. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Run Apache Spark on a modern container base&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Dataproc, our managed version of Apache Spark, is now generally available on Google Kubernetes Engine (GKE), allowing you to create a Dataproc cluster and submit Spark jobs on a self-managed GKE cluster. &lt;a href=&#34;https://cloud.google.com/blog/products/infrastructure-modernization/running-spark-on-kubernetes-with-dataproc&#34;&gt;Read all about it&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Loads of new runtimes in App Engine and Cloud Functions&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Java, Ruby, Python and PHP developers, rejoice! You can now update or develop new App Engine apps and Cloud Functions using &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/new-java-ruby-python-php-runtimes&#34;&gt;Java 17, Ruby 3, Python 3.10 and PHP 8.1&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;b&gt;BeReal shows you how modern app development is done&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Social media company BeReal discusses how it uses Google Cloud services including Firebase, Cloud Functions and GKE to &lt;a href=&#34;https://cloud.google.com/blog/topics/startups/bereal-creates-reality-based-social-media-using-google-cloud&#34;&gt;build its app&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Build fast without breaking things&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;In this three-part series, learn about the Supply-chain Levels for Software Artifacts (SLSA) framework designed to improve the integrity of your software packages and infrastructure. Start with, &lt;a href=&#34;https://security.googleblog.com/2022/04/how-to-slsa-part-1-basics.html&#34; target=&#34;_blank&#34;&gt;How to SLSA Part 1 - The Basics&lt;/a&gt;, then move on to &lt;a href=&#34;https://security.googleblog.com/2022/04/how-to-slsa-part-2-details.html&#34; target=&#34;_blank&#34;&gt;part 2&lt;/a&gt; and &lt;a href=&#34;https://security.googleblog.com/2022/04/how-to-slsa-part-3-putting-it-all.html&#34; target=&#34;_blank&#34;&gt;part 3&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/topics/sustainability/how-the-us-forest-service-uses-google-cloud/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/USFS.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Picture this: How the U.S. Forest Service uses Google Cloud tools to analyze a changing planet&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;For over a decade, the U.S. Forest Service has been using Google Earth Engine and other Google Cloud tools to study our changing planet.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Apr 4 - Apr 8, 2022&lt;/h3&gt;&lt;p&gt;&lt;b&gt;How to migrate a container from a VM to Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;With Cloud Run, you can migrate a legacy VM to a container and save money – even if you don’t know Kubernetes. This &lt;a href=&#34;https://youtu.be/HKuUmzSpljU&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt; shows you how. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Receive Error Reporting notifications through Slack and Webhooks&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Error Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications&#34;&gt;blog&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud-native architecture is in the cards at NCR&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Earlier this year, NCR Authentic Cards talked about &lt;a href=&#34;https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud&#34;&gt;how it built&lt;/a&gt; a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for &lt;a href=&#34;https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2&#34;&gt;part two of the migration story&lt;/a&gt;, taking a detailed look at all the components that went into the cloud-based architecture. &lt;/p&gt;&lt;p&gt;&lt;b&gt;How to easily share a service with Cloud Run &lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Have you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates &lt;a href=&#34;https://coloring-page.lolo.dev/&#34; target=&#34;_blank&#34;&gt;an image processing service&lt;/a&gt; that generates coloring pages, then makes it available to others — all in under 200 lines of Python and JavaScript. &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run&#34;&gt;Follow along in this tutorial&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/infrastructure/topaz-subsea-cable-connects-canada-and-asia/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Banner_Topaz_map_hero_Banner.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Introducing Topaz — the first subsea cable to connect Canada and Asia&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The Topaz subsea cable is the first fiber cable to connect Canada and Asia, and will provide better resiliency and lower latency for Goog...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 28 - Apr 1, 2022&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Another cool thing you can do with Cloud Functions&lt;br/&gt;&lt;/b&gt;Got data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial &lt;a href=&#34;https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark&#34;&gt;shows you how&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Add custom severity levels to Cloud Monitoring alert policies&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Not all alerts are created equal. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts&#34;&gt;In this blog post&lt;/a&gt;, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Learn how to use CPU allocation controls in Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Last fall, &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation&#34;&gt;we added&lt;/a&gt; “always-on CPU” capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work&#34;&gt;In this post&lt;/a&gt;, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app’s average user response latency by over 80%.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_App_Dev_4.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Go 1.18 and Google Cloud: Go now with Google Cloud&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Go 1.18 release and Google Cloud working better together.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 21 - Mar 25, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Get Going with latest Go 1.18 release&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;With the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud&#34;&gt;Learn more here&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 14 - Mar 18, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Create EventArc triggers with Terraform&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;In addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform&#34;&gt;shows you how&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Scaling to new markets with Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;French publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms&#34;&gt;website architecture&lt;/a&gt; here. &lt;/p&gt;&lt;p&gt;&lt;b&gt;The serverless way to celebrate Pi Day&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;In honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions&#34;&gt;calculate π&lt;/a&gt; — serverlessly.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 07 - Mar 11, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Rhode Island moves to Google Cloud-based job board&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center&#34;&gt;how they did it&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Containerized microservices at Lowe’s&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Lowe’s already told us &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices&#34;&gt;how they use SRE&lt;/a&gt;. They’re at it again, describing how they built an e-commerce website using a &lt;a href=&#34;https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce&#34;&gt;containerized microservices architecture and Kubernetes&lt;/a&gt;, with Istio for service mesh and Cloud Operations for good measure.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cruise AVs hit the road with Google Cloud services&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Autonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform&#34;&gt;Read the guest post&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;L’Oréal’s data analytics gets a makeover with serverless&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;We’re hurtling toward a &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud&#34;&gt;programmable cloud&lt;/a&gt; — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings&#34;&gt;L’Oréal is a great example&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Better telemetry for your Anthos clusters&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/service-mesh/docs/observability/explore-dashboard&#34;&gt;Anthos Service Mesh Dashboard&lt;/a&gt; is now available (public preview) on the &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/latest&#34;&gt;Anthos clusters on Bare Metal&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/on-prem/1.10&#34;&gt;Anthos clusters on VMware&lt;/a&gt;. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Instrument your Java apps&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;With the new version of the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features&#34;&gt;Google Cloud Logging Java library&lt;/a&gt;, you can wire your application logs with more information — without adding a single line of code.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Visualize metrics from Cloud Spanner&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Building an app on top of Cloud Spanner but can’t assess how well it’s operating? The new &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver&#34; target=&#34;_blank&#34;&gt;OpenTelemetery receiver for Cloud Spanner&lt;/a&gt; provides an easy way for you to process and visualize metrics from Cloud Spanner &lt;a href=&#34;https://cloud.google.com/spanner/docs/introspection&#34;&gt;System tables&lt;/a&gt;, and export these to the APM tool of your choice. &lt;a href=&#34;https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery&#34;&gt;Read more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Feb 28 - Mar 4, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Introducing Cloud SDK&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;The rebranded &lt;a href=&#34;https://cloud.google.com/sdk&#34;&gt;Cloud SDK&lt;/a&gt; is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development&#34;&gt;here&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud CLI, meet Terraform&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Google Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview&#34;&gt;available in preview&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Knative graduates to incubating project &lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Congratulations to Knative, which has been &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project&#34;&gt;accepted by the Cloud Native Computing Foundation&lt;/a&gt;, or CNCF, as an incubating project, enabling the next phase of serverless architecture. &lt;/p&gt;&lt;p&gt;&lt;b&gt;We manage Prometheus so you don’t have to&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Google Cloud Managed Service for Prometheus&lt;/a&gt; is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes&#34;&gt;Learn more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><promo-banner-block _nghost-c45=""></promo-banner-block><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>inframod living.jpg</span></p></figure></article-aspect-image-block><div><article-cta _nghost-c47=""><div _ngcontent-c47=""><h4 _ngcontent-c47=""><span _ngcontent-c47="">Anchoring on Containers</span></h4><p _ngcontent-c47=""><span _ngcontent-c47="">Learn why Google Cloud’s container offerings lead the market</span></p><p><a _ngcontent-c47="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="forrester_container_feb2022" track-metadata-eventdetail="https://cloud.google.com/resources/forrester-wave-container-platforms-report" href="https://cloud.google.com/resources/forrester-wave-container-platforms-report"><span _ngcontent-c47="">Download</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c49=""><p>Developers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place. </p></paragraph-block></div><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;h3&gt;Week of Apr 11 - Apr 15, 2022&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Listen to a Prodcast&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Google&amp;#8217;s SRE team has launched a &amp;#8220;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast&#34;&gt;Prodcast&lt;/a&gt;&amp;#8221; focusing on concepts from its SRE book. Available from wherever you get your podcasts.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Run Apache Spark on a modern container base&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Dataproc, our managed version of Apache Spark, is now generally available on Google Kubernetes Engine (GKE), allowing you to create a Dataproc cluster and submit Spark jobs on a self-managed GKE cluster. &lt;a href=&#34;https://cloud.google.com/blog/products/infrastructure-modernization/running-spark-on-kubernetes-with-dataproc&#34;&gt;Read all about it&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Loads of new runtimes in App Engine and Cloud Functions&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Java, Ruby, Python and PHP developers, rejoice! You can now update or develop new App Engine apps and Cloud Functions using &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/new-java-ruby-python-php-runtimes&#34;&gt;Java 17, Ruby 3, Python 3.10 and PHP 8.1&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;b&gt;BeReal shows you how modern app development is done&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Social media company BeReal discusses how it uses Google Cloud services including Firebase, Cloud Functions and GKE to &lt;a href=&#34;https://cloud.google.com/blog/topics/startups/bereal-creates-reality-based-social-media-using-google-cloud&#34;&gt;build its app&lt;/a&gt;.&amp;#160;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Build fast without breaking things&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;In this three-part series, learn about the Supply-chain Levels for Software Artifacts (SLSA) framework designed to improve the integrity of your software packages and infrastructure. Start with, &lt;a href=&#34;https://security.googleblog.com/2022/04/how-to-slsa-part-1-basics.html&#34; target=&#34;_blank&#34;&gt;How to SLSA Part 1 - The Basics&lt;/a&gt;, then move on to &lt;a href=&#34;https://security.googleblog.com/2022/04/how-to-slsa-part-2-details.html&#34; target=&#34;_blank&#34;&gt;part 2&lt;/a&gt; and &lt;a href=&#34;https://security.googleblog.com/2022/04/how-to-slsa-part-3-putting-it-all.html&#34; target=&#34;_blank&#34;&gt;part 3&lt;/a&gt;.&lt;/p&gt;"><h3>Week of Apr 11 - Apr 15, 2022</h3><p><b>Listen to a Prodcast</b><b><br/></b>Google’s SRE team has launched a “<a href="https://cloud.google.com/blog/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast" track-metadata-module="post">Prodcast</a>” focusing on concepts from its SRE book. Available from wherever you get your podcasts. </p><p><b>Run Apache Spark on a modern container base</b><b><br/></b>Dataproc, our managed version of Apache Spark, is now generally available on Google Kubernetes Engine (GKE), allowing you to create a Dataproc cluster and submit Spark jobs on a self-managed GKE cluster. <a href="https://cloud.google.com/blog/products/infrastructure-modernization/running-spark-on-kubernetes-with-dataproc" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/blog/products/infrastructure-modernization/running-spark-on-kubernetes-with-dataproc" track-metadata-module="post">Read all about it</a>. </p><p><b>Loads of new runtimes in App Engine and Cloud Functions</b><b><br/></b>Java, Ruby, Python and PHP developers, rejoice! You can now update or develop new App Engine apps and Cloud Functions using <a href="https://cloud.google.com/blog/topics/developers-practitioners/new-java-ruby-python-php-runtimes" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/new-java-ruby-python-php-runtimes" track-metadata-module="post">Java 17, Ruby 3, Python 3.10 and PHP 8.1</a>.</p><p><b>BeReal shows you how modern app development is done</b><b><br/></b>Social media company BeReal discusses how it uses Google Cloud services including Firebase, Cloud Functions and GKE to <a href="https://cloud.google.com/blog/topics/startups/bereal-creates-reality-based-social-media-using-google-cloud" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/blog/topics/startups/bereal-creates-reality-based-social-media-using-google-cloud" track-metadata-module="post">build its app</a>.  </p><p><b>Build fast without breaking things</b><b><br/></b>In this three-part series, learn about the Supply-chain Levels for Software Artifacts (SLSA) framework designed to improve the integrity of your software packages and infrastructure. Start with, <a href="https://security.googleblog.com/2022/04/how-to-slsa-part-1-basics.html" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://security.googleblog.com" track-metadata-module="post">How to SLSA Part 1 - The Basics</a>, then move on to <a href="https://security.googleblog.com/2022/04/how-to-slsa-part-2-details.html" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://security.googleblog.com" track-metadata-module="post">part 2</a> and <a href="https://security.googleblog.com/2022/04/how-to-slsa-part-3-putting-it-all.html" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://security.googleblog.com" track-metadata-module="post">part 3</a>.</p></div></paragraph-block></div><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;h3&gt;Week of Apr 4 - Apr 8, 2022&lt;/h3&gt;&lt;p&gt;&lt;b&gt;How to migrate a container from a VM to Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;With Cloud Run, you can migrate a legacy VM to a container and save money &amp;#8211; even if you don&amp;#8217;t know Kubernetes. This &lt;a href=&#34;https://youtu.be/HKuUmzSpljU&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt; shows you how.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Receive Error Reporting notifications through Slack and Webhooks&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Error Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications&#34;&gt;blog&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud-native architecture is in the cards at NCR&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Earlier this year, NCR Authentic Cards talked about &lt;a href=&#34;https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud&#34;&gt;how it built&lt;/a&gt; a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for &lt;a href=&#34;https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2&#34;&gt;part two of the migration story&lt;/a&gt;, taking a detailed look at all the components that went into the cloud-based architecture.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;How to easily share a service with Cloud Run&amp;#160;&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Have you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates &lt;a href=&#34;https://coloring-page.lolo.dev/&#34; target=&#34;_blank&#34;&gt;an image processing service&lt;/a&gt; that generates coloring pages, then makes it available to others &amp;#8212; all in under 200 lines of Python and JavaScript. &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run&#34;&gt;Follow along in this tutorial&lt;/a&gt;.&lt;/p&gt;"><h3>Week of Apr 4 - Apr 8, 2022</h3><p><b>How to migrate a container from a VM to Cloud Run</b><b><br/></b>With Cloud Run, you can migrate a legacy VM to a container and save money – even if you don’t know Kubernetes. This <a href="https://youtu.be/HKuUmzSpljU" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://youtu.be" track-metadata-module="post">video</a> shows you how. </p><p><b>Receive Error Reporting notifications through Slack and Webhooks</b><b><br/></b>Error Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this <a href="https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications" track-metadata-module="post">blog</a>. </p><p><b>Cloud-native architecture is in the cards at NCR</b><b><br/></b>Earlier this year, NCR Authentic Cards talked about <a href="https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud" track-metadata-module="post">how it built</a> a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for <a href="https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2" track-metadata-module="post">part two of the migration story</a>, taking a detailed look at all the components that went into the cloud-based architecture. </p><p><b>How to easily share a service with Cloud Run </b><b><br/></b>Have you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates <a href="https://coloring-page.lolo.dev/" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="https://coloring-page.lolo.dev" track-metadata-module="post">an image processing service</a> that generates coloring pages, then makes it available to others — all in under 200 lines of Python and JavaScript. <a href="https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run" track-metadata-module="post">Follow along in this tutorial</a>.</p></div></paragraph-block></div><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;h3&gt;Week of Mar 28 - Apr 1, 2022&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Another cool thing you can do with Cloud Functions&lt;br&gt;&lt;/b&gt;Got data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial &lt;a href=&#34;https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark&#34;&gt;shows you how&lt;/a&gt;.&amp;#160;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Add custom severity levels to Cloud Monitoring alert policies&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Not all alerts are created equal. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts&#34;&gt;In this blog post&lt;/a&gt;, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Learn how to use CPU allocation controls in Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Last fall, &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation&#34;&gt;we added&lt;/a&gt; &amp;#8220;always-on CPU&amp;#8221; capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work&#34;&gt;In this post&lt;/a&gt;, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app&amp;#8217;s average user response latency by over 80%.&lt;/p&gt;"><h3>Week of Mar 28 - Apr 1, 2022</h3><p><b>Another cool thing you can do with Cloud Functions<br/></b>Got data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial <a href="https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark" track-metadata-module="post">shows you how</a>.  </p><p><b>Add custom severity levels to Cloud Monitoring alert policies</b><b><br/></b>Not all alerts are created equal. <a href="https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts" track-metadata-module="post">In this blog post</a>, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty. </p><p><b>Learn how to use CPU allocation controls in Cloud Run</b><b><br/></b>Last fall, <a href="https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation" track-metadata-module="post">we added</a> “always-on CPU” capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. <a href="https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work" track-type="inline link" track-name="17" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work" track-metadata-module="post">In this post</a>, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app’s average user response latency by over 80%.</p></div></paragraph-block></div><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;h3&gt;Week of Mar 21 - Mar 25, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Get Going with latest Go 1.18 release&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;With the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud&#34;&gt;Learn more here&lt;/a&gt;.&lt;/p&gt;"><h3>Week of Mar 21 - Mar 25, 2022</h3><p><b>Get Going with latest Go 1.18 release</b><b><br/></b>With the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. <a href="https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud" track-metadata-module="post">Learn more here</a>.</p></div></paragraph-block></div><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;h3&gt;Week of Mar 14 - Mar 18, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Create EventArc triggers with Terraform&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;In addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform&#34;&gt;shows you how&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Scaling to new markets with Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;French publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms&#34;&gt;website architecture&lt;/a&gt; here.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;The serverless way to celebrate Pi Day&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;In honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions&#34;&gt;calculate &amp;#960;&lt;/a&gt; &amp;#8212; serverlessly.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Week of Mar 14 - Mar 18, 2022</h3><p><b>Create EventArc triggers with Terraform</b><b><br/></b>In addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel <a href="https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform" track-type="inline link" track-name="19" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform" track-metadata-module="post">shows you how</a>. </p><p><b>Scaling to new markets with Cloud Run</b><b><br/></b>French publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its <a href="https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms" track-type="inline link" track-name="20" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms" track-metadata-module="post">website architecture</a> here. </p><p><b>The serverless way to celebrate Pi Day</b><b><br/></b>In honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to <a href="https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions" track-type="inline link" track-name="21" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions" track-metadata-module="post">calculate π</a> — serverlessly.</p></div></paragraph-block></div><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;h3&gt;Week of Mar 07 - Mar 11, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Rhode Island moves to Google Cloud-based job board&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center&#34;&gt;how they did it&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Containerized microservices at Lowe&amp;#8217;s&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Lowe&amp;#8217;s already told us &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices&#34;&gt;how they use SRE&lt;/a&gt;. They&amp;#8217;re at it again, describing how they built an e-commerce website using a &lt;a href=&#34;https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce&#34;&gt;containerized microservices architecture and Kubernetes&lt;/a&gt;, with Istio for service mesh and Cloud Operations for good measure.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cruise AVs hit the road with Google Cloud services&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Autonomous Vehicle (AV) startup Cruise detailed how it&amp;#8217;s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform&#34;&gt;Read the guest post&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;L&amp;#8217;Or&amp;#233;al&amp;#8217;s data analytics gets a makeover with serverless&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;We&amp;#8217;re hurtling toward a &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud&#34;&gt;programmable cloud&lt;/a&gt; &amp;#8212; a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings&#34;&gt;L&amp;#8217;Or&amp;#233;al is a great example&lt;/a&gt;.&amp;#160;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Better telemetry for your Anthos clusters&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/service-mesh/docs/observability/explore-dashboard&#34;&gt;Anthos Service Mesh Dashboard&lt;/a&gt; is now available (public preview) on the &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/latest&#34;&gt;Anthos clusters on Bare Metal&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/on-prem/1.10&#34;&gt;Anthos clusters on VMware&lt;/a&gt;. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Instrument your Java apps&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;With the new version of the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features&#34;&gt;Google Cloud Logging Java library&lt;/a&gt;, you can wire your application logs with more information &amp;#8212; without adding a single line of code.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Visualize metrics from Cloud Spanner&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Building an app on top of Cloud Spanner but can&amp;#8217;t assess how well it&amp;#8217;s operating? The new &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver&#34; target=&#34;_blank&#34;&gt;OpenTelemetery receiver for Cloud Spanner&lt;/a&gt; provides an easy way for you to process and visualize metrics from Cloud Spanner &lt;a href=&#34;https://cloud.google.com/spanner/docs/introspection&#34;&gt;System tables&lt;/a&gt;, and export these to the APM tool of your choice. &lt;a href=&#34;https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery&#34;&gt;Read more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Week of Mar 07 - Mar 11, 2022</h3><p><b>Rhode Island moves to Google Cloud-based job board</b><b><br/></b>When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out <a href="https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center" track-metadata-module="post">how they did it</a>. </p><p><b>Containerized microservices at Lowe’s</b><b><br/></b>Lowe’s already told us <a href="https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices" track-metadata-module="post">how they use SRE</a>. They’re at it again, describing how they built an e-commerce website using a <a href="https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce" track-metadata-module="post">containerized microservices architecture and Kubernetes</a>, with Istio for service mesh and Cloud Operations for good measure.</p><p><b>Cruise AVs hit the road with Google Cloud services</b><b><br/></b>Autonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. <a href="https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform" track-metadata-module="post">Read the guest post</a>. </p><p><b>L’Oréal’s data analytics gets a makeover with serverless</b><b><br/></b>We’re hurtling toward a <a href="https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud" track-metadata-module="post">programmable cloud</a> — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. <a href="https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings" track-metadata-module="post">L’Oréal is a great example</a>.  </p><p><b>Better telemetry for your Anthos clusters</b><b><br/></b><a href="https://cloud.google.com/service-mesh/docs/observability/explore-dashboard" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/service-mesh/docs/observability/explore-dashboard" track-metadata-module="post">Anthos Service Mesh Dashboard</a> is now available (public preview) on the <a href="https://cloud.google.com/anthos/clusters/docs/bare-metal/latest" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/anthos/clusters/docs/bare-metal/latest" track-metadata-module="post">Anthos clusters on Bare Metal</a> and <a href="https://cloud.google.com/anthos/clusters/docs/on-prem/1.10" track-type="inline link" track-name="30" track-metadata-eventdetail="https://cloud.google.com/anthos/clusters/docs/on-prem/1.10" track-metadata-module="post">Anthos clusters on VMware</a>. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.</p><p><b>Instrument your Java apps</b><b><br/></b>With the new version of the <a href="https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features" track-type="inline link" track-name="31" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features" track-metadata-module="post">Google Cloud Logging Java library</a>, you can wire your application logs with more information — without adding a single line of code.</p><p><b>Visualize metrics from Cloud Spanner</b><b><br/></b>Building an app on top of Cloud Spanner but can’t assess how well it’s operating? The new <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver" target="_blank" track-type="inline link" track-name="32" track-metadata-eventdetail="https://github.com" track-metadata-module="post">OpenTelemetery receiver for Cloud Spanner</a> provides an easy way for you to process and visualize metrics from Cloud Spanner <a href="https://cloud.google.com/spanner/docs/introspection" track-type="inline link" track-name="33" track-metadata-eventdetail="https://cloud.google.com/spanner/docs/introspection" track-metadata-module="post">System tables</a>, and export these to the APM tool of your choice. <a href="https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery" track-type="inline link" track-name="34" track-metadata-eventdetail="https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery" track-metadata-module="post">Read more here</a>.</p></div></paragraph-block></div><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;h3&gt;Week of Feb 28 - Mar 4, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Introducing Cloud SDK&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;The rebranded &lt;a href=&#34;https://cloud.google.com/sdk&#34;&gt;Cloud SDK&lt;/a&gt; is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development&#34;&gt;here&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud CLI, meet Terraform&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Google Cloud CLI&amp;#8217;s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google&amp;#8217;s KRM declarative tooling, and is now &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview&#34;&gt;available in preview&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Knative graduates to incubating project&amp;#160;&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Congratulations to Knative, which has been &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project&#34;&gt;accepted by the Cloud Native Computing Foundation&lt;/a&gt;, or CNCF, as an incubating project, enabling the next phase of serverless architecture.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;We manage Prometheus so you don&amp;#8217;t have to&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Google Cloud Managed Service for Prometheus&lt;/a&gt; is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes&#34;&gt;Learn more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Week of Feb 28 - Mar 4, 2022</h3><p><b>Introducing Cloud SDK</b><b><br/></b>The rebranded <a href="https://cloud.google.com/sdk" track-type="inline link" track-name="35" track-metadata-eventdetail="https://cloud.google.com/sdk" track-metadata-module="post">Cloud SDK</a> is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more <a href="https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development" track-type="inline link" track-name="36" track-metadata-eventdetail="https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development" track-metadata-module="post">here</a>. </p><p><b>Cloud CLI, meet Terraform</b><b><br/></b>Google Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now <a href="https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview" track-type="inline link" track-name="37" track-metadata-eventdetail="https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview" track-metadata-module="post">available in preview</a>. </p><p><b>Knative graduates to incubating project </b><b><br/></b>Congratulations to Knative, which has been <a href="https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project" track-type="inline link" track-name="38" track-metadata-eventdetail="https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project" track-metadata-module="post">accepted by the Cloud Native Computing Foundation</a>, or CNCF, as an incubating project, enabling the next phase of serverless architecture. </p><p><b>We manage Prometheus so you don’t have to</b><b><br/></b><a href="https://cloud.google.com/stackdriver/docs/managed-prometheus" track-type="inline link" track-name="39" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/managed-prometheus" track-metadata-module="post">Google Cloud Managed Service for Prometheus</a> is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. <a href="https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes" track-type="inline link" track-name="40" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes" track-metadata-module="post">Learn more here</a>.</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c48=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Google Cloud Content &amp; Editorial &lt;/name&gt;&lt;title&gt;&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/inframod_living_3.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 15 Apr 2022 20:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing the Google SRE Prodcast</title>
      <link>https://cloud.google.com/blog/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We&#39;re excited to announce the launch of the Google SRE Prodcast, an interview-style podcast where Site Reliability Engineers at Google discuss key SRE concepts and share their experiences, advice, and strong opinions along the way.&lt;/p&gt;&lt;p&gt;Since early 2016, SREs at Google have been producing and listening to an internal version of the Prodcast. This year, we decided to think bigger and share insights from Google SREs with the wider SRE community.&lt;/p&gt;&lt;p&gt;The resulting Prodcast that we&#39;re launching today is a series of conversations inspired by concepts from the &lt;a href=&#34;https://sre.google/sre-book/table-of-contents/&#34; target=&#34;_blank&#34;&gt;SRE Book&lt;/a&gt;. Over the course of nine episodes, domain experts explain, challenge, and reframe everything from SLOs to what it means to be an excellent SRE.&lt;/p&gt;&lt;p&gt;We had so much fun creating this series with our guests, and we hope you enjoy it. Check the SRE Prodcast out at &lt;a href=&#34;https://sre.google/prodcast&#34; target=&#34;_blank&#34;&gt;https://sre.google/prodcast&lt;/a&gt;!&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_aWHZoxD.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Add severity levels to your alert policies in Cloud Monitoring&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Add static and dynamic severity levels to your alert policies for easier triaging and include these in notifications when sent to 3rd par...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;p&gt;We&#39;re excited to announce the launch of the Google SRE Prodcast, an interview-style podcast where Site Reliability Engineers at Google discuss key SRE concepts and share their experiences, advice, and strong opinions along the way.&lt;/p&gt;&lt;p&gt;Since early 2016, SREs at Google have been producing and listening to an internal version of the Prodcast. This year, we decided to think bigger and share insights from Google SREs with the wider SRE community.&lt;/p&gt;&lt;p&gt;The resulting Prodcast that we&#39;re launching today is a series of conversations inspired by concepts from the &lt;a href=&#34;https://sre.google/sre-book/table-of-contents/&#34; target=&#34;_blank&#34;&gt;SRE Book&lt;/a&gt;. Over the course of nine episodes, domain experts explain, challenge, and reframe everything from SLOs to what it means to be an excellent SRE.&lt;/p&gt;&lt;p&gt;We had so much fun creating this series with our guests, and we hope you enjoy it. Check the SRE Prodcast out at &lt;a href=&#34;https://sre.google/prodcast&#34; target=&#34;_blank&#34;&gt;https://sre.google/prodcast&lt;/a&gt;!&lt;/p&gt;"><p>We&#39;re excited to announce the launch of the Google SRE Prodcast, an interview-style podcast where Site Reliability Engineers at Google discuss key SRE concepts and share their experiences, advice, and strong opinions along the way.</p><p>Since early 2016, SREs at Google have been producing and listening to an internal version of the Prodcast. This year, we decided to think bigger and share insights from Google SREs with the wider SRE community.</p><p>The resulting Prodcast that we&#39;re launching today is a series of conversations inspired by concepts from the <a href="https://sre.google/sre-book/table-of-contents/" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">SRE Book</a>. Over the course of nine episodes, domain experts explain, challenge, and reframe everything from SLOs to what it means to be an excellent SRE.</p><p>We had so much fun creating this series with our guests, and we hope you enjoy it. Check the SRE Prodcast out at <a href="https://sre.google/prodcast" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">https://sre.google/prodcast</a>!</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;SRE Prodcast Team &lt;/name&gt;&lt;title&gt;&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/prodcast.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 14 Apr 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What’s new in cloud-native apps?</title>
      <link>https://cloud.google.com/blog/topics/cloud-first/whats-new-cloud-native-apps/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Developers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Apr 4 - Apr 8, 2022&lt;/h3&gt;&lt;p&gt;&lt;b&gt;How to migrate a container from a VM to Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;With Cloud Run, you can migrate a legacy VM to a container and save money – even if you don’t know Kubernetes. This &lt;a href=&#34;https://youtu.be/HKuUmzSpljU&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt; shows you how. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Receive Error Reporting notifications through Slack and Webhooks&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Error Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications&#34;&gt;blog&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud-native architecture is in the cards at NCR&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Earlier this year, NCR Authentic Cards talked about &lt;a href=&#34;https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud&#34;&gt;how it built&lt;/a&gt; a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for &lt;a href=&#34;https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2&#34;&gt;part two of the migration story&lt;/a&gt;, taking a detailed look at all the components that went into the cloud-based architecture. &lt;/p&gt;&lt;p&gt;&lt;b&gt;How to easily share a service with Cloud Run &lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Have you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates &lt;a href=&#34;https://coloring-page.lolo.dev/&#34; target=&#34;_blank&#34;&gt;an image processing service&lt;/a&gt; that generates coloring pages, then makes it available to others — all in under 200 lines of Python and JavaScript. &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run&#34;&gt;Follow along in this tutorial&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/infrastructure/topaz-subsea-cable-connects-canada-and-asia/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Banner_Topaz_map_hero_Banner.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Introducing Topaz — the first subsea cable to connect Canada and Asia&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The Topaz subsea cable is the first fiber cable to connect Canada and Asia, and will provide better resiliency and lower latency for Goog...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 28 - Apr 1, 2022&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Another cool thing you can do with Cloud Functions&lt;br/&gt;&lt;/b&gt;Got data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial &lt;a href=&#34;https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark&#34;&gt;shows you how&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Add custom severity levels to Cloud Monitoring alert policies&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Not all alerts are created equal. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts&#34;&gt;In this blog post&lt;/a&gt;, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Learn how to use CPU allocation controls in Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Last fall, &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation&#34;&gt;we added&lt;/a&gt; “always-on CPU” capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work&#34;&gt;In this post&lt;/a&gt;, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app’s average user response latency by over 80%.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_App_Dev_4.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Go 1.18 and Google Cloud: Go now with Google Cloud&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Go 1.18 release and Google Cloud working better together.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 21 - Mar 25, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Get Going with latest Go 1.18 release&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;With the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud&#34;&gt;Learn more here&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/serverless_2.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Scaling quickly to new markets with Cloud Run—a web modernization story&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Moving from on-prem to cloud using serverless containers and PHP, a French news outlet more easily expands to reach new markets.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 14 - Mar 18, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Create EventArc triggers with Terraform&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;In addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform&#34;&gt;shows you how&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Scaling to new markets with Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;French publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms&#34;&gt;website architecture&lt;/a&gt; here. &lt;/p&gt;&lt;p&gt;&lt;b&gt;The serverless way to celebrate Pi Day&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;In honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions&#34;&gt;calculate π&lt;/a&gt; — serverlessly.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 07 - Mar 11, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Rhode Island moves to Google Cloud-based job board&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center&#34;&gt;how they did it&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Containerized microservices at Lowe’s&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Lowe’s already told us &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices&#34;&gt;how they use SRE&lt;/a&gt;. They’re at it again, describing how they built an e-commerce website using a &lt;a href=&#34;https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce&#34;&gt;containerized microservices architecture and Kubernetes&lt;/a&gt;, with Istio for service mesh and Cloud Operations for good measure.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cruise AVs hit the road with Google Cloud services&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Autonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform&#34;&gt;Read the guest post&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;L’Oréal’s data analytics gets a makeover with serverless&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;We’re hurtling toward a &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud&#34;&gt;programmable cloud&lt;/a&gt; — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings&#34;&gt;L’Oréal is a great example&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Better telemetry for your Anthos clusters&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/service-mesh/docs/observability/explore-dashboard&#34;&gt;Anthos Service Mesh Dashboard&lt;/a&gt; is now available (public preview) on the &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/latest&#34;&gt;Anthos clusters on Bare Metal&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/on-prem/1.10&#34;&gt;Anthos clusters on VMware&lt;/a&gt;. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Instrument your Java apps&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;With the new version of the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features&#34;&gt;Google Cloud Logging Java library&lt;/a&gt;, you can wire your application logs with more information — without adding a single line of code.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Visualize metrics from Cloud Spanner&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Building an app on top of Cloud Spanner but can’t assess how well it’s operating? The new &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver&#34; target=&#34;_blank&#34;&gt;OpenTelemetery receiver for Cloud Spanner&lt;/a&gt; provides an easy way for you to process and visualize metrics from Cloud Spanner &lt;a href=&#34;https://cloud.google.com/spanner/docs/introspection&#34;&gt;System tables&lt;/a&gt;, and export these to the APM tool of your choice. &lt;a href=&#34;https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery&#34;&gt;Read more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Feb 28 - Mar 4, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Introducing Cloud SDK&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;The rebranded &lt;a href=&#34;https://cloud.google.com/sdk&#34;&gt;Cloud SDK&lt;/a&gt; is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development&#34;&gt;here&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud CLI, meet Terraform&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Google Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview&#34;&gt;available in preview&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Knative graduates to incubating project &lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Congratulations to Knative, which has been &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project&#34;&gt;accepted by the Cloud Native Computing Foundation&lt;/a&gt;, or CNCF, as an incubating project, enabling the next phase of serverless architecture. &lt;/p&gt;&lt;p&gt;&lt;b&gt;We manage Prometheus so you don’t have to&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Google Cloud Managed Service for Prometheus&lt;/a&gt; is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes&#34;&gt;Learn more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><promo-banner-block _nghost-c61=""></promo-banner-block><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>inframod living.jpg</span></p></figure></article-aspect-image-block><div><article-cta _nghost-c67=""><div _ngcontent-c67=""><h4 _ngcontent-c67=""><span _ngcontent-c67="">Anchoring on Containers</span></h4><p _ngcontent-c67=""><span _ngcontent-c67="">Learn why Google Cloud’s container offerings lead the market</span></p><p><a _ngcontent-c67="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="forrester_container_feb2022" track-metadata-eventdetail="https://cloud.google.com/resources/forrester-wave-container-platforms-report" href="https://cloud.google.com/resources/forrester-wave-container-platforms-report"><span _ngcontent-c67="">Download</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c69=""><p>Developers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place. </p></paragraph-block></div><div><paragraph-block _nghost-c69=""><div _ngcontent-c69="" innerhtml="&lt;h3&gt;Week of Apr 4 - Apr 8, 2022&lt;/h3&gt;&lt;p&gt;&lt;b&gt;How to migrate a container from a VM to Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;With Cloud Run, you can migrate a legacy VM to a container and save money &amp;#8211; even if you don&amp;#8217;t know Kubernetes. This &lt;a href=&#34;https://youtu.be/HKuUmzSpljU&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt; shows you how.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Receive Error Reporting notifications through Slack and Webhooks&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Error Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications&#34;&gt;blog&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud-native architecture is in the cards at NCR&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Earlier this year, NCR Authentic Cards talked about &lt;a href=&#34;https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud&#34;&gt;how it built&lt;/a&gt; a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for &lt;a href=&#34;https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2&#34;&gt;part two of the migration story&lt;/a&gt;, taking a detailed look at all the components that went into the cloud-based architecture.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;How to easily share a service with Cloud Run&amp;#160;&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Have you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates &lt;a href=&#34;https://coloring-page.lolo.dev/&#34; target=&#34;_blank&#34;&gt;an image processing service&lt;/a&gt; that generates coloring pages, then makes it available to others &amp;#8212; all in under 200 lines of Python and JavaScript. &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run&#34;&gt;Follow along in this tutorial&lt;/a&gt;.&lt;/p&gt;"><h3>Week of Apr 4 - Apr 8, 2022</h3><p><b>How to migrate a container from a VM to Cloud Run</b><b><br/></b>With Cloud Run, you can migrate a legacy VM to a container and save money – even if you don’t know Kubernetes. This <a href="https://youtu.be/HKuUmzSpljU" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://youtu.be" track-metadata-module="post">video</a> shows you how. </p><p><b>Receive Error Reporting notifications through Slack and Webhooks</b><b><br/></b>Error Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this <a href="https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications" track-metadata-module="post">blog</a>. </p><p><b>Cloud-native architecture is in the cards at NCR</b><b><br/></b>Earlier this year, NCR Authentic Cards talked about <a href="https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud" track-metadata-module="post">how it built</a> a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for <a href="https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2" track-metadata-module="post">part two of the migration story</a>, taking a detailed look at all the components that went into the cloud-based architecture. </p><p><b>How to easily share a service with Cloud Run </b><b><br/></b>Have you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates <a href="https://coloring-page.lolo.dev/" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://coloring-page.lolo.dev" track-metadata-module="post">an image processing service</a> that generates coloring pages, then makes it available to others — all in under 200 lines of Python and JavaScript. <a href="https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run" track-metadata-module="post">Follow along in this tutorial</a>.</p></div></paragraph-block></div><div><paragraph-block _nghost-c69=""><div _ngcontent-c69="" innerhtml="&lt;h3&gt;Week of Mar 28 - Apr 1, 2022&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Another cool thing you can do with Cloud Functions&lt;br&gt;&lt;/b&gt;Got data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial &lt;a href=&#34;https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark&#34;&gt;shows you how&lt;/a&gt;.&amp;#160;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Add custom severity levels to Cloud Monitoring alert policies&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Not all alerts are created equal. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts&#34;&gt;In this blog post&lt;/a&gt;, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Learn how to use CPU allocation controls in Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Last fall, &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation&#34;&gt;we added&lt;/a&gt; &amp;#8220;always-on CPU&amp;#8221; capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work&#34;&gt;In this post&lt;/a&gt;, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app&amp;#8217;s average user response latency by over 80%.&lt;/p&gt;"><h3>Week of Mar 28 - Apr 1, 2022</h3><p><b>Another cool thing you can do with Cloud Functions<br/></b>Got data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial <a href="https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark" track-metadata-module="post">shows you how</a>.  </p><p><b>Add custom severity levels to Cloud Monitoring alert policies</b><b><br/></b>Not all alerts are created equal. <a href="https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts" track-metadata-module="post">In this blog post</a>, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty. </p><p><b>Learn how to use CPU allocation controls in Cloud Run</b><b><br/></b>Last fall, <a href="https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation" track-metadata-module="post">we added</a> “always-on CPU” capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. <a href="https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work" track-metadata-module="post">In this post</a>, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app’s average user response latency by over 80%.</p></div></paragraph-block></div><div><paragraph-block _nghost-c69=""><div _ngcontent-c69="" innerhtml="&lt;h3&gt;Week of Mar 21 - Mar 25, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Get Going with latest Go 1.18 release&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;With the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud&#34;&gt;Learn more here&lt;/a&gt;.&lt;/p&gt;"><h3>Week of Mar 21 - Mar 25, 2022</h3><p><b>Get Going with latest Go 1.18 release</b><b><br/></b>With the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. <a href="https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud" track-metadata-module="post">Learn more here</a>.</p></div></paragraph-block></div><div><paragraph-block _nghost-c69=""><div _ngcontent-c69="" innerhtml="&lt;h3&gt;Week of Mar 14 - Mar 18, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Create EventArc triggers with Terraform&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;In addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform&#34;&gt;shows you how&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Scaling to new markets with Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;French publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms&#34;&gt;website architecture&lt;/a&gt; here.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;The serverless way to celebrate Pi Day&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;In honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions&#34;&gt;calculate &amp;#960;&lt;/a&gt; &amp;#8212; serverlessly.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Week of Mar 14 - Mar 18, 2022</h3><p><b>Create EventArc triggers with Terraform</b><b><br/></b>In addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel <a href="https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform" track-metadata-module="post">shows you how</a>. </p><p><b>Scaling to new markets with Cloud Run</b><b><br/></b>French publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its <a href="https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms" track-metadata-module="post">website architecture</a> here. </p><p><b>The serverless way to celebrate Pi Day</b><b><br/></b>In honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to <a href="https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions" track-metadata-module="post">calculate π</a> — serverlessly.</p></div></paragraph-block></div><div><paragraph-block _nghost-c69=""><div _ngcontent-c69="" innerhtml="&lt;h3&gt;Week of Mar 07 - Mar 11, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Rhode Island moves to Google Cloud-based job board&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center&#34;&gt;how they did it&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Containerized microservices at Lowe&amp;#8217;s&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Lowe&amp;#8217;s already told us &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices&#34;&gt;how they use SRE&lt;/a&gt;. They&amp;#8217;re at it again, describing how they built an e-commerce website using a &lt;a href=&#34;https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce&#34;&gt;containerized microservices architecture and Kubernetes&lt;/a&gt;, with Istio for service mesh and Cloud Operations for good measure.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cruise AVs hit the road with Google Cloud services&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Autonomous Vehicle (AV) startup Cruise detailed how it&amp;#8217;s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform&#34;&gt;Read the guest post&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;L&amp;#8217;Or&amp;#233;al&amp;#8217;s data analytics gets a makeover with serverless&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;We&amp;#8217;re hurtling toward a &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud&#34;&gt;programmable cloud&lt;/a&gt; &amp;#8212; a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings&#34;&gt;L&amp;#8217;Or&amp;#233;al is a great example&lt;/a&gt;.&amp;#160;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Better telemetry for your Anthos clusters&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/service-mesh/docs/observability/explore-dashboard&#34;&gt;Anthos Service Mesh Dashboard&lt;/a&gt; is now available (public preview) on the &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/latest&#34;&gt;Anthos clusters on Bare Metal&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/on-prem/1.10&#34;&gt;Anthos clusters on VMware&lt;/a&gt;. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Instrument your Java apps&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;With the new version of the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features&#34;&gt;Google Cloud Logging Java library&lt;/a&gt;, you can wire your application logs with more information &amp;#8212; without adding a single line of code.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Visualize metrics from Cloud Spanner&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Building an app on top of Cloud Spanner but can&amp;#8217;t assess how well it&amp;#8217;s operating? The new &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver&#34; target=&#34;_blank&#34;&gt;OpenTelemetery receiver for Cloud Spanner&lt;/a&gt; provides an easy way for you to process and visualize metrics from Cloud Spanner &lt;a href=&#34;https://cloud.google.com/spanner/docs/introspection&#34;&gt;System tables&lt;/a&gt;, and export these to the APM tool of your choice. &lt;a href=&#34;https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery&#34;&gt;Read more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Week of Mar 07 - Mar 11, 2022</h3><p><b>Rhode Island moves to Google Cloud-based job board</b><b><br/></b>When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out <a href="https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center" track-metadata-module="post">how they did it</a>. </p><p><b>Containerized microservices at Lowe’s</b><b><br/></b>Lowe’s already told us <a href="https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices" track-metadata-module="post">how they use SRE</a>. They’re at it again, describing how they built an e-commerce website using a <a href="https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce" track-type="inline link" track-name="17" track-metadata-eventdetail="https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce" track-metadata-module="post">containerized microservices architecture and Kubernetes</a>, with Istio for service mesh and Cloud Operations for good measure.</p><p><b>Cruise AVs hit the road with Google Cloud services</b><b><br/></b>Autonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. <a href="https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform" track-metadata-module="post">Read the guest post</a>. </p><p><b>L’Oréal’s data analytics gets a makeover with serverless</b><b><br/></b>We’re hurtling toward a <a href="https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud" track-type="inline link" track-name="19" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud" track-metadata-module="post">programmable cloud</a> — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. <a href="https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings" track-type="inline link" track-name="20" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings" track-metadata-module="post">L’Oréal is a great example</a>.  </p><p><b>Better telemetry for your Anthos clusters</b><b><br/></b><a href="https://cloud.google.com/service-mesh/docs/observability/explore-dashboard" track-type="inline link" track-name="21" track-metadata-eventdetail="https://cloud.google.com/service-mesh/docs/observability/explore-dashboard" track-metadata-module="post">Anthos Service Mesh Dashboard</a> is now available (public preview) on the <a href="https://cloud.google.com/anthos/clusters/docs/bare-metal/latest" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/anthos/clusters/docs/bare-metal/latest" track-metadata-module="post">Anthos clusters on Bare Metal</a> and <a href="https://cloud.google.com/anthos/clusters/docs/on-prem/1.10" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/anthos/clusters/docs/on-prem/1.10" track-metadata-module="post">Anthos clusters on VMware</a>. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.</p><p><b>Instrument your Java apps</b><b><br/></b>With the new version of the <a href="https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features" track-metadata-module="post">Google Cloud Logging Java library</a>, you can wire your application logs with more information — without adding a single line of code.</p><p><b>Visualize metrics from Cloud Spanner</b><b><br/></b>Building an app on top of Cloud Spanner but can’t assess how well it’s operating? The new <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver" target="_blank" track-type="inline link" track-name="25" track-metadata-eventdetail="https://github.com" track-metadata-module="post">OpenTelemetery receiver for Cloud Spanner</a> provides an easy way for you to process and visualize metrics from Cloud Spanner <a href="https://cloud.google.com/spanner/docs/introspection" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/spanner/docs/introspection" track-metadata-module="post">System tables</a>, and export these to the APM tool of your choice. <a href="https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery" track-metadata-module="post">Read more here</a>.</p></div></paragraph-block></div><div><paragraph-block _nghost-c69=""><div _ngcontent-c69="" innerhtml="&lt;h3&gt;Week of Feb 28 - Mar 4, 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Introducing Cloud SDK&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;The rebranded &lt;a href=&#34;https://cloud.google.com/sdk&#34;&gt;Cloud SDK&lt;/a&gt; is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development&#34;&gt;here&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud CLI, meet Terraform&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Google Cloud CLI&amp;#8217;s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google&amp;#8217;s KRM declarative tooling, and is now &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview&#34;&gt;available in preview&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Knative graduates to incubating project&amp;#160;&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Congratulations to Knative, which has been &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project&#34;&gt;accepted by the Cloud Native Computing Foundation&lt;/a&gt;, or CNCF, as an incubating project, enabling the next phase of serverless architecture.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;We manage Prometheus so you don&amp;#8217;t have to&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Google Cloud Managed Service for Prometheus&lt;/a&gt; is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes&#34;&gt;Learn more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Week of Feb 28 - Mar 4, 2022</h3><p><b>Introducing Cloud SDK</b><b><br/></b>The rebranded <a href="https://cloud.google.com/sdk" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/sdk" track-metadata-module="post">Cloud SDK</a> is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more <a href="https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development" track-metadata-module="post">here</a>. </p><p><b>Cloud CLI, meet Terraform</b><b><br/></b>Google Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now <a href="https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview" track-type="inline link" track-name="30" track-metadata-eventdetail="https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview" track-metadata-module="post">available in preview</a>. </p><p><b>Knative graduates to incubating project </b><b><br/></b>Congratulations to Knative, which has been <a href="https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project" track-type="inline link" track-name="31" track-metadata-eventdetail="https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project" track-metadata-module="post">accepted by the Cloud Native Computing Foundation</a>, or CNCF, as an incubating project, enabling the next phase of serverless architecture. </p><p><b>We manage Prometheus so you don’t have to</b><b><br/></b><a href="https://cloud.google.com/stackdriver/docs/managed-prometheus" track-type="inline link" track-name="32" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/managed-prometheus" track-metadata-module="post">Google Cloud Managed Service for Prometheus</a> is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. <a href="https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes" track-type="inline link" track-name="33" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes" track-metadata-module="post">Learn more here</a>.</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c68=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Google Cloud Content &amp; Editorial &lt;/name&gt;&lt;title&gt;&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/inframod_living_3.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 08 Apr 2022 20:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Deliver exception messages through Slack and Webhooks for fast resolution</title>
      <link>https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Building new applications is a lot of fun, but troubleshooting and fixing the crashes that can come with app development is not. While many organizations are fast adopting the DevOps model, there are still some legacy frameworks where developers and operations teams are separate. Developers build and submit apps to their ops team, who in turn deploy and maintain the production stack. &lt;/p&gt;&lt;p&gt;A common issue that arises due to this workflow is the time it takes to find and resolve crashes. To help reduce the time it takes to find crashes, we recently introduced new &lt;a href=&#34;https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available&#34;&gt;notification&lt;/a&gt; channels for our Alerting product. Building on that release, we’re happy to announce today that you can send Error Reporting notifications through both &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications#slack&#34;&gt;Slack&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications#webhooks&#34;&gt;Webhooks&lt;/a&gt;. &lt;/p&gt;&lt;h3&gt;What is Error Reporting?&lt;/h3&gt;&lt;p&gt;Error Reporting can analyze, aggregate, and notify you about crashes in your running cloud services. It can synthesize this information from ingested logs in &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt; and has a &lt;a href=&#34;http://console.cloud.google.com/errors&#34;&gt;&lt;i&gt;&lt;b&gt;dedicated page&lt;/b&gt;&lt;/i&gt;&lt;/a&gt; that displays the details of the errors, including a histogram of occurrences, list of affected versions, request URL, and links to the request log.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Error_Reporting_interface.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Error Reporting interface.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Error_Reporting_interface.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;What are we launching?&lt;/h3&gt;&lt;p&gt;Building on our recent launch of &lt;a href=&#34;https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available&#34;&gt;alerting notification&lt;/a&gt; channels, today we are announcing an extension of &lt;b&gt;Error Reporting’s notification capabilities to include Slack and Webhooks&lt;/b&gt;. &lt;/p&gt;&lt;p&gt;With this launch, your teams can receive notifications about crashes directly into their configured Slack channel or preferred collaboration platform using webhooks. &lt;/p&gt;&lt;p&gt;Crash information can then be quickly swarmed, discussed, and resolved.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Slack_and_Webhooks.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 Slack and Webhooks.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Slack_and_Webhooks.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;These new channels  join our existing Error Reporting notification capabilities of email and the Google Cloud Console mobile app.&lt;/p&gt;&lt;h3&gt;What do I have to do to enable Error Reporting and these notifications?&lt;/h3&gt;&lt;p&gt;Error Reporting is automatically enabled as soon as logs that contain error events like stack traces are ingested into Cloud Logging. Alternatively,  you can &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/how-to&#34;&gt;self configure&lt;/a&gt; Error Reporting for a new project if you won’t be using Cloud Logging.&lt;/p&gt;&lt;p&gt;To configure the new notification channels, see documentation &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications#create-channel&#34;&gt;here&lt;/a&gt; for &lt;b&gt;&lt;i&gt;Slack&lt;/i&gt;&lt;/b&gt; and &lt;b&gt;&lt;i&gt;Webhooks&lt;/i&gt;&lt;/b&gt;, or keep reading below:&lt;/p&gt;&lt;h3&gt;Enabling Slack&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;In Slack&lt;/b&gt;: Create a Slack workspace and channel at the &lt;a href=&#34;https://www.slack.com/&#34; target=&#34;_blank&#34;&gt;Slack site&lt;/a&gt;. Record the channel URL.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In the Cloud Console, select &lt;a href=&#34;https://console.cloud.google.com/monitoring&#34;&gt;&lt;b&gt;Monitoring&lt;/b&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click &lt;b&gt;Alerting&lt;/b&gt; and then click &lt;b&gt;Edit notification channels&lt;/b&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In the &lt;b&gt;Slack&lt;/b&gt; section, click &lt;b&gt;Add new&lt;/b&gt; to open the Slack sign-in page:&lt;/p&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Select your Slack workspace.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click &lt;b&gt;Allow&lt;/b&gt; to enable Cloud Monitoring access to your Slack workspace. This action takes you back to the Monitoring configuration page for your notification channel.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Enter the name of the Slack channel you want to use for notifications.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Enter a display name for the Slack notification channel.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;(Optional) To test the connection between Cloud Monitoring and your Slack workspace, click &lt;b&gt;Send test notification&lt;/b&gt;. If the connection is successful, then you see a message This is a test alert notification... in the Slack notification channel that you specified. Check the notification channel to confirm receipt.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;&lt;p&gt;If the Slack channel you want to use for notifications is a private channel, then you must manually invite the Monitoring app to the channel:&lt;/p&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Open Slack.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Go to the channel you specified as your Monitoring notification channel.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Invite the Monitoring app to the channel by entering and sending the following message in the channel:&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;/invite @Google Cloud Monitoring&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Be sure you invite the Monitoring app to the private channel you specified when creating the notification channel in Monitoring. Inviting the Monitoring app to public channels is optional.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;h3&gt;Enabling Webhooks&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;The webhook handler&lt;/b&gt;: Identify the public endpoint URL to receive webhook data from Monitoring.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In the Cloud Console, select &lt;a href=&#34;https://console.cloud.google.com/monitoring&#34;&gt;&lt;b&gt;Monitoring&lt;/b&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click &lt;b&gt;Alerting&lt;/b&gt; and then click &lt;b&gt;Edit notification channels&lt;/b&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In the Webhook section, click Add new.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Complete the dialog.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click &lt;b&gt;Test Connection&lt;/b&gt; to send a test payload to the Webhook endpoint. You can go to the receiving endpoint to verify delivery.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click &lt;b&gt;Save&lt;/b&gt;.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Webhook schema&lt;/h3&gt;&lt;p&gt;The Webhook schema structure for Error Reporting, is as follows:&lt;/p&gt;&lt;p&gt;&lt;b&gt;Schema structure, version 1.0&lt;/b&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;{\r\n &#34;version&#34;: &#34;1.0&#34;,\r\n &#34;subject&#34;: string, description of the new or reopened error group.\r\n &#34;group_info&#34;: {\r\n &#34;project_id&#34;: string, project that owns the error group.\r\n &#34;detail_link&#34;: string, link to the Error Reporting Details page for the error group.\r\n },\r\n &#34;exception_info&#34;: {\r\n &#34;type&#34;: string, type of the exception logged in the event.\r\n &#34;message&#34;: string, exception message for the event.\r\n },\r\n &#34;event_info&#34;: {\r\n &#34;log_message&#34;: string\r\n &#34;request_method&#34;: string\r\n &#34;request_url&#34;: string\r\n &#34;referrer&#34;: string\r\n &#34;user_agent&#34;: string\r\n &#34;service&#34;: string\r\n &#34;version&#34;: string\r\n &#34;response_status&#34;: string\r\n },\r\n}&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Basic authentication&lt;/b&gt;&lt;/p&gt;&lt;p&gt;In addition to the webhook request sent by Cloud Monitoring, basic authentication utilizes the HTTP specification for the username and password. Cloud Monitoring requires your server to return a 401 response with the proper WWW-Authenticate header. For more information about basic authentication, see the following:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.ietf.org/rfc/rfc2617.txt&#34; target=&#34;_blank&#34;&gt;RFC Specification&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Basic_access_authentication&#34; target=&#34;_blank&#34;&gt;Basic authentication&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Token authentication&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Token Authentication requires a query string parameter in the endpoint URL and a key that the server expects to be secret between itself and Monitoring. The following is a sample URL that includes a token:&lt;/p&gt;&lt;p&gt;https://www.myserver.com/stackdriver-hook?auth_token=1234-abcd&lt;/p&gt;&lt;p&gt;If Monitoring posts an incident to the endpoint URL, your server can validate the attached token. This method of authentication can be more effective when used with SSL/TLS to encrypt the HTTP request, which can prevent snoopers from learning the token.&lt;/p&gt;&lt;p&gt;For an example server in Python, see this &lt;a href=&#34;https://gist.github.com/tschieggm/7604940&#34; target=&#34;_blank&#34;&gt;sample server&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Get Started Today&lt;/h3&gt;&lt;p&gt;If you haven’t visited your Error Reporting console yet, &lt;a href=&#34;http://console.cloud.google.com/errors&#34;&gt;give it a try today&lt;/a&gt; and learn more about it in &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/viewing-errors&#34;&gt;documentation&lt;/a&gt;. When you’re ready to configure your notifications, consider &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications#slack&#34;&gt;Slack&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications#webhooks&#34;&gt;Webhooks&lt;/a&gt; if they fit into your current alerting and notification strategy.&lt;/p&gt;&lt;p&gt;If you have any questions or want to start a discussion with other Error Reporting users, visit the Cloud Operations section of the &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Google Cloud Community&lt;/a&gt; and post a discussion topic.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/operations/pub-sub-webook-and-slack-notifications-are-now-available/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Genric_GCP_upA1oyz.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Webhook, Pub/Sub, and Slack Alerting notification channels launched&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Announcing the general availability of the new Pub/Sub, Webhook, and Slack Notification channels.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><promo-banner-block _nghost-c56=""></promo-banner-block><article><article-header-block></article-header-block><div><div><article-author-block><div><div><p> Alek Szilagyi </p><p> Software Engineer, Google Cloud </p></div><p><span> April 5, 2022 </span></p></div></article-author-block></div><article-cta _nghost-c58=""></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;Building new applications is a lot of fun, but troubleshooting and fixing the crashes that can come with app development is not. While many organizations are fast adopting the DevOps model, there are still some legacy frameworks where developers and operations teams are separate. Developers build and submit apps to their ops team, who in turn deploy and maintain the production stack.&amp;#160;&lt;/p&gt;&lt;p&gt;A common issue that arises due to this workflow is the time it takes to find and resolve crashes. To help reduce the time it takes to find crashes, we recently introduced new &lt;a href=&#34;https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available&#34;&gt;notification&lt;/a&gt; channels for our Alerting product. Building on that release, we&amp;#8217;re happy to announce today that you can send Error Reporting notifications through both &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications#slack&#34;&gt;Slack&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications#webhooks&#34;&gt;Webhooks&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;h3&gt;What is Error Reporting?&lt;/h3&gt;&lt;p&gt;Error Reporting can analyze, aggregate, and notify you about crashes in your running cloud services. It can synthesize this information from ingested logs in &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt; and has a &lt;a href=&#34;http://console.cloud.google.com/errors&#34;&gt;&lt;i&gt;&lt;b&gt;dedicated page&lt;/b&gt;&lt;/i&gt;&lt;/a&gt; that displays the details of the errors, including a histogram of occurrences, list of affected versions, request URL, and links to the request log.&lt;/p&gt;"><p>Building new applications is a lot of fun, but troubleshooting and fixing the crashes that can come with app development is not. While many organizations are fast adopting the DevOps model, there are still some legacy frameworks where developers and operations teams are separate. Developers build and submit apps to their ops team, who in turn deploy and maintain the production stack. </p><p>A common issue that arises due to this workflow is the time it takes to find and resolve crashes. To help reduce the time it takes to find crashes, we recently introduced new <a href="https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available" track-metadata-module="post">notification</a> channels for our Alerting product. Building on that release, we’re happy to announce today that you can send Error Reporting notifications through both <a href="https://cloud.google.com/error-reporting/docs/notifications#slack" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/notifications#slack" track-metadata-module="post">Slack</a> and <a href="https://cloud.google.com/error-reporting/docs/notifications#webhooks" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/notifications#webhooks" track-metadata-module="post">Webhooks</a>. </p><h3>What is Error Reporting?</h3><p>Error Reporting can analyze, aggregate, and notify you about crashes in your running cloud services. It can synthesize this information from ingested logs in <a href="https://cloud.google.com/logging" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/logging" track-metadata-module="post">Cloud Logging</a> and has a <a href="http://console.cloud.google.com/errors" track-type="inline link" track-name="5" track-metadata-eventdetail="http://console.cloud.google.com/errors" track-metadata-module="post"><i><b>dedicated page</b></i></a> that displays the details of the errors, including a histogram of occurrences, list of affected versions, request URL, and links to the request log.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;h3&gt;What are we launching?&lt;/h3&gt;&lt;p&gt;Building on our recent launch of &lt;a href=&#34;https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available&#34;&gt;alerting notification&lt;/a&gt; channels, today we are announcing an extension of &lt;b&gt;Error Reporting&amp;#8217;s notification capabilities to include Slack and Webhooks&lt;/b&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;With this launch, your teams can receive notifications about crashes directly into their configured Slack channel or preferred collaboration platform using webhooks.&amp;#160;&lt;/p&gt;&lt;p&gt;Crash information can then be quickly swarmed, discussed, and resolved.&lt;/p&gt;"><h3>What are we launching?</h3><p>Building on our recent launch of <a href="https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available" track-metadata-module="post">alerting notification</a> channels, today we are announcing an extension of <b>Error Reporting’s notification capabilities to include Slack and Webhooks</b>. </p><p>With this launch, your teams can receive notifications about crashes directly into their configured Slack channel or preferred collaboration platform using webhooks. </p><p>Crash information can then be quickly swarmed, discussed, and resolved.</p></div></paragraph-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;These new channels&amp;#160; join our existing Error Reporting notification capabilities of email and the Google Cloud Console mobile app.&lt;/p&gt;&lt;h3&gt;What do I have to do to enable Error Reporting and these notifications?&lt;/h3&gt;&lt;p&gt;Error Reporting is automatically enabled as soon as logs that contain error events like stack traces are ingested into Cloud Logging. Alternatively,&amp;#160; you can &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/how-to&#34;&gt;self configure&lt;/a&gt; Error Reporting for a new project if you won&amp;#8217;t be using Cloud Logging.&lt;/p&gt;&lt;p&gt;To configure the new notification channels, see documentation &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications#create-channel&#34;&gt;here&lt;/a&gt; for &lt;b&gt;&lt;i&gt;Slack&lt;/i&gt;&lt;/b&gt; and &lt;b&gt;&lt;i&gt;Webhooks&lt;/i&gt;&lt;/b&gt;, or keep reading below:&lt;/p&gt;&lt;h3&gt;Enabling Slack&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;In Slack&lt;/b&gt;: Create a Slack workspace and channel at the &lt;a href=&#34;https://www.slack.com/&#34; target=&#34;_blank&#34;&gt;Slack site&lt;/a&gt;. Record the channel URL.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In the Cloud Console, select &lt;a href=&#34;https://console.cloud.google.com/monitoring&#34;&gt;&lt;b&gt;Monitoring&lt;/b&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click &lt;b&gt;Alerting&lt;/b&gt; and then click &lt;b&gt;Edit notification channels&lt;/b&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In the &lt;b&gt;Slack&lt;/b&gt; section, click &lt;b&gt;Add new&lt;/b&gt; to open the Slack sign-in page:&lt;/p&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Select your Slack workspace.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click &lt;b&gt;Allow&lt;/b&gt; to enable Cloud Monitoring access to your Slack workspace. This action takes you back to the Monitoring configuration page for your notification channel.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Enter the name of the Slack channel you want to use for notifications.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Enter a display name for the Slack notification channel.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;(Optional) To test the connection between Cloud Monitoring and your Slack workspace, click &lt;b&gt;Send test notification&lt;/b&gt;. If the connection is successful, then you see a message This is a test alert notification... in the Slack notification channel that you specified. Check the notification channel to confirm receipt.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;&lt;p&gt;If the Slack channel you want to use for notifications is a private channel, then you must manually invite the Monitoring app to the channel:&lt;/p&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Open Slack.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Go to the channel you specified as your Monitoring notification channel.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Invite the Monitoring app to the channel by entering and sending the following message in the channel:&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;/invite @Google Cloud Monitoring&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Be sure you invite the Monitoring app to the private channel you specified when creating the notification channel in Monitoring. Inviting the Monitoring app to public channels is optional.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;h3&gt;Enabling Webhooks&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;The webhook handler&lt;/b&gt;: Identify the public endpoint URL to receive webhook data from Monitoring.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In the Cloud Console, select &lt;a href=&#34;https://console.cloud.google.com/monitoring&#34;&gt;&lt;b&gt;Monitoring&lt;/b&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click &lt;b&gt;Alerting&lt;/b&gt; and then click &lt;b&gt;Edit notification channels&lt;/b&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In the Webhook section, click Add new.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Complete the dialog.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click &lt;b&gt;Test Connection&lt;/b&gt; to send a test payload to the Webhook endpoint. You can go to the receiving endpoint to verify delivery.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click &lt;b&gt;Save&lt;/b&gt;.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;Webhook schema&lt;/h3&gt;&lt;p&gt;The Webhook schema structure for Error Reporting, is as follows:&lt;/p&gt;&lt;p&gt;&lt;b&gt;Schema structure, version 1.0&lt;/b&gt;&lt;/p&gt;"><p>These new channels  join our existing Error Reporting notification capabilities of email and the Google Cloud Console mobile app.</p><h3>What do I have to do to enable Error Reporting and these notifications?</h3><p>Error Reporting is automatically enabled as soon as logs that contain error events like stack traces are ingested into Cloud Logging. Alternatively,  you can <a href="https://cloud.google.com/error-reporting/docs/how-to" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/how-to" track-metadata-module="post">self configure</a> Error Reporting for a new project if you won’t be using Cloud Logging.</p><p>To configure the new notification channels, see documentation <a href="https://cloud.google.com/error-reporting/docs/notifications#create-channel" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/notifications#create-channel" track-metadata-module="post">here</a> for <b><i>Slack</i></b> and <b><i>Webhooks</i></b>, or keep reading below:</p><h3>Enabling Slack</h3><ol><li><p><b>In Slack</b>: Create a Slack workspace and channel at the <a href="https://www.slack.com/" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://www.slack.com" track-metadata-module="post">Slack site</a>. Record the channel URL.</p></li><li><p>In the Cloud Console, select <a href="https://console.cloud.google.com/monitoring" track-type="inline link" track-name="10" track-metadata-eventdetail="https://console.cloud.google.com/monitoring" track-metadata-module="post"><b>Monitoring</b></a>.</p></li><li><p>Click <b>Alerting</b> and then click <b>Edit notification channels</b>.</p></li><li><p>In the <b>Slack</b> section, click <b>Add new</b> to open the Slack sign-in page:</p></li><ol><li><p>Select your Slack workspace.</p></li><li><p>Click <b>Allow</b> to enable Cloud Monitoring access to your Slack workspace. This action takes you back to the Monitoring configuration page for your notification channel.</p></li><li><p>Enter the name of the Slack channel you want to use for notifications.</p></li><li><p>Enter a display name for the Slack notification channel.</p></li><li><p>(Optional) To test the connection between Cloud Monitoring and your Slack workspace, click <b>Send test notification</b>. If the connection is successful, then you see a message This is a test alert notification... in the Slack notification channel that you specified. Check the notification channel to confirm receipt.</p></li></ol><li><p>If the Slack channel you want to use for notifications is a private channel, then you must manually invite the Monitoring app to the channel:</p></li><ol><li><p>Open Slack.</p></li><li><p>Go to the channel you specified as your Monitoring notification channel.</p></li><li><p>Invite the Monitoring app to the channel by entering and sending the following message in the channel:</p></li><li><p>/invite @Google Cloud Monitoring</p></li><li><p>Be sure you invite the Monitoring app to the private channel you specified when creating the notification channel in Monitoring. Inviting the Monitoring app to public channels is optional.</p></li></ol></ol><h3>Enabling Webhooks</h3><ol><li><p><b>The webhook handler</b>: Identify the public endpoint URL to receive webhook data from Monitoring.</p></li><li><p>In the Cloud Console, select <a href="https://console.cloud.google.com/monitoring" track-type="inline link" track-name="11" track-metadata-eventdetail="https://console.cloud.google.com/monitoring" track-metadata-module="post"><b>Monitoring</b></a>.</p></li><li><p>Click <b>Alerting</b> and then click <b>Edit notification channels</b>.</p></li><li><p>In the Webhook section, click Add new.</p></li><li><p>Complete the dialog.</p></li><li><p>Click <b>Test Connection</b> to send a test payload to the Webhook endpoint. You can go to the receiving endpoint to verify delivery.</p></li><li><p>Click <b>Save</b>.</p></li></ol><h3>Webhook schema</h3><p>The Webhook schema structure for Error Reporting, is as follows:</p><p><b>Schema structure, version 1.0</b></p></div></paragraph-block></div><div><article-code-block _nghost-c61=""><pre _ngcontent-c61="">  <code _ngcontent-c61="">{
</code><code _ngcontent-c61="">  &#34;version&#34;: &#34;1.0&#34;,
</code><code _ngcontent-c61="">  &#34;subject&#34;: string, description of the new or reopened error group.
</code><code _ngcontent-c61="">  &#34;group_info&#34;: {
</code><code _ngcontent-c61="">    &#34;project_id&#34;: string, project that owns the error group.
</code><code _ngcontent-c61="">    &#34;detail_link&#34;: string, link to the Error Reporting Details page for the error group.
</code><code _ngcontent-c61="">  },
</code><code _ngcontent-c61="">  &#34;exception_info&#34;: {
</code><code _ngcontent-c61="">    &#34;type&#34;: string, type of the exception logged in the event.
</code><code _ngcontent-c61="">    &#34;message&#34;: string, exception message for the event.
</code><code _ngcontent-c61="">  },
</code><code _ngcontent-c61="">  &#34;event_info&#34;: {
</code><code _ngcontent-c61="">    &#34;log_message&#34;: string
</code><code _ngcontent-c61="">    &#34;request_method&#34;: string
</code><code _ngcontent-c61="">    &#34;request_url&#34;: string
</code><code _ngcontent-c61="">    &#34;referrer&#34;: string
</code><code _ngcontent-c61="">    &#34;user_agent&#34;: string
</code><code _ngcontent-c61="">    &#34;service&#34;: string
</code><code _ngcontent-c61="">    &#34;version&#34;: string
</code><code _ngcontent-c61="">    &#34;response_status&#34;: string
</code><code _ngcontent-c61="">  },
</code><code _ngcontent-c61="">}</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;&lt;b&gt;Basic authentication&lt;/b&gt;&lt;/p&gt;&lt;p&gt;In addition to the webhook request sent by Cloud Monitoring, basic authentication utilizes the HTTP specification for the username and password. Cloud Monitoring requires your server to return a 401 response with the proper WWW-Authenticate header. For more information about basic authentication, see the following:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.ietf.org/rfc/rfc2617.txt&#34; target=&#34;_blank&#34;&gt;RFC Specification&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Basic_access_authentication&#34; target=&#34;_blank&#34;&gt;Basic authentication&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Token authentication&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Token Authentication requires a query string parameter in the endpoint URL and a key that the server expects to be secret between itself and Monitoring. The following is a sample URL that includes a token:&lt;/p&gt;&lt;p&gt;https://www.myserver.com/stackdriver-hook?auth_token=1234-abcd&lt;/p&gt;&lt;p&gt;If Monitoring posts an incident to the endpoint URL, your server can validate the attached token. This method of authentication can be more effective when used with SSL/TLS to encrypt the HTTP request, which can prevent snoopers from learning the token.&lt;/p&gt;&lt;p&gt;For an example server in Python, see this &lt;a href=&#34;https://gist.github.com/tschieggm/7604940&#34; target=&#34;_blank&#34;&gt;sample server&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Get Started Today&lt;/h3&gt;&lt;p&gt;If you haven&amp;#8217;t visited your Error Reporting console yet, &lt;a href=&#34;http://console.cloud.google.com/errors&#34;&gt;give it a try today&lt;/a&gt; and learn more about it in &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/viewing-errors&#34;&gt;documentation&lt;/a&gt;. When you&amp;#8217;re ready to configure your notifications, consider &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications#slack&#34;&gt;Slack&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications#webhooks&#34;&gt;Webhooks&lt;/a&gt; if they fit into your current alerting and notification strategy.&lt;/p&gt;&lt;p&gt;If you have any questions or want to start a discussion with other Error Reporting users, visit the Cloud Operations section of the &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Google Cloud Community&lt;/a&gt; and post a discussion topic.&lt;/p&gt;"><p><b>Basic authentication</b></p><p>In addition to the webhook request sent by Cloud Monitoring, basic authentication utilizes the HTTP specification for the username and password. Cloud Monitoring requires your server to return a 401 response with the proper WWW-Authenticate header. For more information about basic authentication, see the following:</p><ul><li><p><a href="https://www.ietf.org/rfc/rfc2617.txt" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="https://www.ietf.org" track-metadata-module="post">RFC Specification</a></p></li><li><p><a href="https://en.wikipedia.org/wiki/Basic_access_authentication" target="_blank" track-type="inline link" track-name="13" track-metadata-eventdetail="https://en.wikipedia.org" track-metadata-module="post">Basic authentication</a></p></li></ul><p><b>Token authentication</b></p><p>Token Authentication requires a query string parameter in the endpoint URL and a key that the server expects to be secret between itself and Monitoring. The following is a sample URL that includes a token:</p><p>https://www.myserver.com/stackdriver-hook?auth_token=1234-abcd</p><p>If Monitoring posts an incident to the endpoint URL, your server can validate the attached token. This method of authentication can be more effective when used with SSL/TLS to encrypt the HTTP request, which can prevent snoopers from learning the token.</p><p>For an example server in Python, see this <a href="https://gist.github.com/tschieggm/7604940" target="_blank" track-type="inline link" track-name="14" track-metadata-eventdetail="https://gist.github.com" track-metadata-module="post">sample server</a>.</p><h3>Get Started Today</h3><p>If you haven’t visited your Error Reporting console yet, <a href="http://console.cloud.google.com/errors" track-type="inline link" track-name="15" track-metadata-eventdetail="http://console.cloud.google.com/errors" track-metadata-module="post">give it a try today</a> and learn more about it in <a href="https://cloud.google.com/error-reporting/docs/viewing-errors" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/viewing-errors" track-metadata-module="post">documentation</a>. When you’re ready to configure your notifications, consider <a href="https://cloud.google.com/error-reporting/docs/notifications#slack" track-type="inline link" track-name="17" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/notifications#slack" track-metadata-module="post">Slack</a> and <a href="https://cloud.google.com/error-reporting/docs/notifications#webhooks" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/notifications#webhooks" track-metadata-module="post">Webhooks</a> if they fit into your current alerting and notification strategy.</p><p>If you have any questions or want to start a discussion with other Error Reporting users, visit the Cloud Operations section of the <a href="https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">Google Cloud Community</a> and post a discussion topic.</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c59=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Alek Szilagyi&lt;/name&gt;&lt;title&gt;Software Engineer, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 05 Apr 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Application observability made easier for Compute Engine</title>
      <link>https://cloud.google.com/blog/products/devops-sre/vm-best-practices-app-observability-with-the-ops-agent/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;When IT operators and architects begin their journey with Google Cloud, Day 0 observability needs tend to focus on infrastructure and aim to address questions about resource needs, a plan for scaling, and similar considerations. During this phase, developers and DevOps engineers also make a plan for how to get deep observability into the performance of third-party and open-source applications running on their &lt;a href=&#34;https://cloud.google.com/compute&#34;&gt;Compute Engine VMs&lt;/a&gt;. That’s why we recently launched a &lt;a href=&#34;https://cloud.google.com/monitoring/agent/integrations&#34;&gt;dedicated UI&lt;/a&gt; under &lt;b&gt;Monitoring → Integrations&lt;/b&gt; that guides you to the available integrations that may be a good fit for your fleet, automatically installs relevant dashboards to help you get value faster, and shows you the integrations that are live on your VMs.   &lt;/p&gt;&lt;h3&gt;Getting up and running faster with application observability &lt;/h3&gt;&lt;p&gt;Since the Ops Agent &lt;a href=&#34;https://cloud.google.com/blog/products/operations/ops-agent-now-ga-and-it-includes-opentelemetry&#34;&gt;entered General Availability&lt;/a&gt;, we have added out of the box integrations with dozens of &lt;a href=&#34;https://cloud.google.com/monitoring/agent/ops-agent/third-party&#34;&gt;popular open source and licensed applications&lt;/a&gt; like databases, web servers, in-memory caches, event streaming, and application runtimes.  &lt;/p&gt;&lt;p&gt;These integrations can be easily turned on within the Ops Agent on any VM, usually with a simple YAML configuration file update. Application-specific metrics and logs are sent to &lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;cloud operations&lt;/a&gt;, providing deep visibility not just into infrastructure health indicators, but also into the application-specific telemetry that drives underlying infrastructure utilization. This data populates the analysis and query tools of &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/monitoring&#34;&gt;Cloud Monitoring&lt;/a&gt; like dashboards, Logs Explorer, alerts, SLOs, and more.    &lt;/p&gt;&lt;h3&gt; A single location for all integrations&lt;/h3&gt;&lt;p&gt;Even with easy configuration instructions, setting up an observability toolset with a view of dozens of third-party applications can be tricky. That’s why we created one UI that lists out all application integrations that we offer, along with instructions on how to get up and running quickly. With this UI, you can:  &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Filter by integrations already installed on one or more VMs in your fleet, or those that are available but not yet installed on any VMs.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Check at a glance which of your VMs have the Ops Agent installed already, and go through quick installation for VMs that will need the Ops Agent for application monitoring.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Quickly identify lists of the specific metrics and log types collected for each integration so you know the telemetry collected.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Link directly to the setup instructions for each integration to save time searching through documentation.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;View samples and screenshots of “out of the box” dashboards and visualizations available with zero configuration, and have the dashboards automatically added to your ‘Integration Dashboards’ list based on what is live on your fleet. Links to the dashboards will be available in the Integrations UI, or by going to &lt;b&gt;Monitoring → Dashboards → Integrations&lt;/b&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;With a single click, choose “Copy Dashboard” to clone a given dashboard into your Custom list, and make modifications and edits specific to your applications and use cases.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Application_observability.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Application observability.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Application_observability.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;The integrations UI makes it easy to browse available application integrations and learn more&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Application_observability.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 Application observability.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Application_observability.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Each integration lists telemetry collected and helps you get up and running with intuitive dashboards that you can later customize&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We also heard from our customers that you want a feedback loop that clearly shows which VMs have which integrations live. So, we added the following to our VM Instances Dashboard (located under &lt;b&gt;Monitoring → Dashboards → GCP → VM Instances&lt;/b&gt;):&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Click into any VM to see whether the Ops Agent is installed, and the exact version running.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;View the “Integrations” chips to check which Integrations are currently collecting metrics and logs for each VM.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;View other infrastructure metrics and logs per VM, and quickly spot if there are any alerts or uptime checks set up for the VM or live events and incidents that require attention.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Application_observability.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;3 Application observability.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Application_observability.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Get started today&lt;/h3&gt;&lt;p&gt;In the coming months, we will add more integrations to the Ops Agent for more open source and licensed applications that you have told us are priorities. &lt;/p&gt;&lt;p&gt;To get started with application observability today, make sure you are running the latest version of the &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent&#34;&gt;Ops Agent&lt;/a&gt; on your GCE VMs, check out the instructions to add &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/third-party&#34;&gt;third-party application integrations&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/monitoring/agent/integrations&#34;&gt;check out the Integrations UI&lt;/a&gt; in the Monitoring section of the Google Cloud Console. &lt;/p&gt;&lt;p&gt;Lastly, if you have feedback, want to ask us questions, or request another application integration, drop us a line on the &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Google Cloud Community Cloud Ops&lt;/a&gt; area!&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/operations/ops-agent-now-ga-and-it-includes-opentelemetry/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Blog_CloudMigration_D.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;The Ops Agent is now GA and it leverages OpenTelemetry&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Today, we’re happy to announce the General Availability of the new Ops Agent, which replaces both the Logging and Monitoring agents and s...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;JJ Roepke&lt;/name&gt;&lt;title&gt;Software Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 04 Apr 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What’s new in cloud-native apps?</title>
      <link>https://cloud.google.com/blog/topics/cloud-first/whats-new-cloud-native-apps/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Developers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 28 - Apr 1 2022&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Another cool thing you can do with Cloud Functions&lt;br/&gt;&lt;/b&gt;Got data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial &lt;a href=&#34;https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark&#34;&gt;shows you how&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Add custom severity levels to Cloud Monitoring alert policies&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Not all alerts are created equal. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts&#34;&gt;In this blog post&lt;/a&gt;, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Learn how to use CPU allocation controls in Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Last fall, &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation&#34;&gt;we added&lt;/a&gt; “always-on CPU” capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work&#34;&gt;In this post&lt;/a&gt;, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app’s average user response latency by over 80%.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_App_Dev_4.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Go 1.18 and Google Cloud: Go now with Google Cloud&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Go 1.18 release and Google Cloud working better together.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 21 - Mar 25 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Get Going with latest Go 1.18 release&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;With the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud&#34;&gt;Learn more here&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/serverless_2.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Scaling quickly to new markets with Cloud Run—a web modernization story&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Moving from on-prem to cloud using serverless containers and PHP, a French news outlet more easily expands to reach new markets.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 14 - Mar 18 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Create EventArc triggers with Terraform&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;In addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform&#34;&gt;shows you how&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Scaling to new markets with Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;French publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms&#34;&gt;website architecture&lt;/a&gt; here. &lt;/p&gt;&lt;p&gt;&lt;b&gt;The serverless way to celebrate Pi Day&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;In honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions&#34;&gt;calculate π&lt;/a&gt; — serverlessly.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/automotive.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Building continuous integration &amp;amp; continuous delivery for autonomous vehicles on Google Cloud&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Cruise relies on a whole host of Google Cloud technologies to develop and test the tech that goes into its autonomous vehicles, or AVs.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 07 - Mar 11 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Rhode Island moves to Google Cloud-based job board&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center&#34;&gt;how they did it&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Containerized microservices at Lowe’s&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Lowe’s already told us &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices&#34;&gt;how they use SRE&lt;/a&gt;. They’re at it again, describing how they built an e-commerce website using a &lt;a href=&#34;https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce&#34;&gt;containerized microservices architecture and Kubernetes&lt;/a&gt;, with Istio for service mesh and Cloud Operations for good measure.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cruise AVs hit the road with Google Cloud services&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Autonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform&#34;&gt;Read the guest post&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;L’Oréal’s data analytics gets a makeover with serverless&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;We’re hurtling toward a &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud&#34;&gt;programmable cloud&lt;/a&gt; — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings&#34;&gt;L’Oréal is a great example&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Better telemetry for your Anthos clusters&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/service-mesh/docs/observability/explore-dashboard&#34;&gt;Anthos Service Mesh Dashboard&lt;/a&gt; is now available (public preview) on the &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/latest&#34;&gt;Anthos clusters on Bare Metal&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/on-prem/1.10&#34;&gt;Anthos clusters on VMware&lt;/a&gt;. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Instrument your Java apps&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;With the new version of the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features&#34;&gt;Google Cloud Logging Java library&lt;/a&gt;, you can wire your application logs with more information — without adding a single line of code.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Visualize metrics from Cloud Spanner&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Building an app on top of Cloud Spanner but can’t assess how well it’s operating? The new &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver&#34; target=&#34;_blank&#34;&gt;OpenTelemetery receiver for Cloud Spanner&lt;/a&gt; provides an easy way for you to process and visualize metrics from Cloud Spanner &lt;a href=&#34;https://cloud.google.com/spanner/docs/introspection&#34;&gt;System tables&lt;/a&gt;, and export these to the APM tool of your choice. &lt;a href=&#34;https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery&#34;&gt;Read more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Feb 28 - Mar 4 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Introducing Cloud SDK&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;The rebranded &lt;a href=&#34;https://cloud.google.com/sdk&#34;&gt;Cloud SDK&lt;/a&gt; is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development&#34;&gt;here&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud CLI, meet Terraform&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Google Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview&#34;&gt;available in preview&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Knative graduates to incubating project &lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Congratulations to Knative, which has been &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project&#34;&gt;accepted by the Cloud Native Computing Foundation&lt;/a&gt;, or CNCF, as an incubating project, enabling the next phase of serverless architecture. &lt;/p&gt;&lt;p&gt;&lt;b&gt;We manage Prometheus so you don’t have to&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Google Cloud Managed Service for Prometheus&lt;/a&gt; is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes&#34;&gt;Learn more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c53=""><div _ngcontent-c53="" innerhtml="&lt;h3&gt;Week of Mar 07 - Mar 11 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Rhode Island moves to Google Cloud-based job board&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center&#34;&gt;how they did it&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Containerized microservices at Lowe&amp;#8217;s&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Lowe&amp;#8217;s already told us &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices&#34;&gt;how they use SRE&lt;/a&gt;. They&amp;#8217;re at it again, describing how they built an e-commerce website using a &lt;a href=&#34;https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce&#34;&gt;containerized microservices architecture and Kubernetes&lt;/a&gt;, with Istio for service mesh and Cloud Operations for good measure.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cruise AVs hit the road with Google Cloud services&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Autonomous Vehicle (AV) startup Cruise detailed how it&amp;#8217;s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform&#34;&gt;Read the guest post&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;L&amp;#8217;Or&amp;#233;al&amp;#8217;s data analytics gets a makeover with serverless&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;We&amp;#8217;re hurtling toward a &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud&#34;&gt;programmable cloud&lt;/a&gt; &amp;#8212; a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings&#34;&gt;L&amp;#8217;Or&amp;#233;al is a great example&lt;/a&gt;.&amp;#160;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Better telemetry for your Anthos clusters&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/service-mesh/docs/observability/explore-dashboard&#34;&gt;Anthos Service Mesh Dashboard&lt;/a&gt; is now available (public preview) on the &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/latest&#34;&gt;Anthos clusters on Bare Metal&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/on-prem/1.10&#34;&gt;Anthos clusters on VMware&lt;/a&gt;. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Instrument your Java apps&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;With the new version of the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features&#34;&gt;Google Cloud Logging Java library&lt;/a&gt;, you can wire your application logs with more information &amp;#8212; without adding a single line of code.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Visualize metrics from Cloud Spanner&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Building an app on top of Cloud Spanner but can&amp;#8217;t assess how well it&amp;#8217;s operating? The new &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver&#34; target=&#34;_blank&#34;&gt;OpenTelemetery receiver for Cloud Spanner&lt;/a&gt; provides an easy way for you to process and visualize metrics from Cloud Spanner &lt;a href=&#34;https://cloud.google.com/spanner/docs/introspection&#34;&gt;System tables&lt;/a&gt;, and export these to the APM tool of your choice. &lt;a href=&#34;https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery&#34;&gt;Read more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Week of Mar 07 - Mar 11 2022</h3><p><b>Rhode Island moves to Google Cloud-based job board</b><b><br/></b>When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out <a href="https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center" track-metadata-module="post">how they did it</a>. </p><p><b>Containerized microservices at Lowe’s</b><b><br/></b>Lowe’s already told us <a href="https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices" track-metadata-module="post">how they use SRE</a>. They’re at it again, describing how they built an e-commerce website using a <a href="https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce" track-metadata-module="post">containerized microservices architecture and Kubernetes</a>, with Istio for service mesh and Cloud Operations for good measure.</p><p><b>Cruise AVs hit the road with Google Cloud services</b><b><br/></b>Autonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. <a href="https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform" track-metadata-module="post">Read the guest post</a>. </p><p><b>L’Oréal’s data analytics gets a makeover with serverless</b><b><br/></b>We’re hurtling toward a <a href="https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud" track-metadata-module="post">programmable cloud</a> — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. <a href="https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings" track-metadata-module="post">L’Oréal is a great example</a>.  </p><p><b>Better telemetry for your Anthos clusters</b><b><br/></b><a href="https://cloud.google.com/service-mesh/docs/observability/explore-dashboard" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/service-mesh/docs/observability/explore-dashboard" track-metadata-module="post">Anthos Service Mesh Dashboard</a> is now available (public preview) on the <a href="https://cloud.google.com/anthos/clusters/docs/bare-metal/latest" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/anthos/clusters/docs/bare-metal/latest" track-metadata-module="post">Anthos clusters on Bare Metal</a> and <a href="https://cloud.google.com/anthos/clusters/docs/on-prem/1.10" track-type="inline link" track-name="17" track-metadata-eventdetail="https://cloud.google.com/anthos/clusters/docs/on-prem/1.10" track-metadata-module="post">Anthos clusters on VMware</a>. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.</p><p><b>Instrument your Java apps</b><b><br/></b>With the new version of the <a href="https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features" track-metadata-module="post">Google Cloud Logging Java library</a>, you can wire your application logs with more information — without adding a single line of code.</p><p><b>Visualize metrics from Cloud Spanner</b><b><br/></b>Building an app on top of Cloud Spanner but can’t assess how well it’s operating? The new <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://github.com" track-metadata-module="post">OpenTelemetery receiver for Cloud Spanner</a> provides an easy way for you to process and visualize metrics from Cloud Spanner <a href="https://cloud.google.com/spanner/docs/introspection" track-type="inline link" track-name="20" track-metadata-eventdetail="https://cloud.google.com/spanner/docs/introspection" track-metadata-module="post">System tables</a>, and export these to the APM tool of your choice. <a href="https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery" track-type="inline link" track-name="21" track-metadata-eventdetail="https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery" track-metadata-module="post">Read more here</a>.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Google Cloud Content &amp; Editorial &lt;/name&gt;&lt;title&gt;&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/inframod_living_3.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 31 Mar 2022 20:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Automate Public Certificates Lifecycle Management via RFC 8555 (ACME)</title>
      <link>https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We’re excited to announce an enhancement of our preview of Certificate Manager which allows Google Cloud customers to acquire public certificates for their workloads that terminate TLS directly or for their cross-cloud and on-premise workloads. This is accomplished via the Automatic Certificate Management Environment (&lt;a href=&#34;https://datatracker.ietf.org/doc/html/rfc8555&#34; target=&#34;_blank&#34;&gt;ACME&lt;/a&gt;) protocol which is the same protocol used by Certificate Authorities  to enable seamless automatic lifecycle management of TLS certificates.&lt;/p&gt;&lt;p&gt;These certificates come from Google Trust Services, the same Certificate Authority (CA) we use by default when we manage certificates on your behalf with the Global External HTTPS Load Balancer. By using the same CA for managed certificates and unmanaged certificates you are assured that both scenarios work equally well across the entire spectrum of devices that use your services.&lt;/p&gt;&lt;p&gt;This also enables Cloud Customers to get a more reliable TLS deployment. It does so by enabling one common certificate lifecycle management story based on ACME to be used without a single point of failure (relying just on one certificate authority). In other words, it is now possible to freely load balance or fail over to multiple ACME CAs with confidence. &lt;/p&gt;&lt;h3&gt;How do I use it ? &lt;/h3&gt;&lt;p&gt;To use this feature you will need an API key so you can use a feature in ACME called &lt;a href=&#34;https://tools.ietf.org/html/rfc8555#section-7.3.4&#34; target=&#34;_blank&#34;&gt;External Account Binding&lt;/a&gt;. This enables us to associate your certificate requests to your Google Cloud account and allows us to impose rate limits on a per customer basis. You may easily get an API key using the following commands:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;$ gcloud config set project &amp;lt;project ID&amp;gt;\r\n$ gcloud projects add-iam-policy-binding project-foo \\\r\n --member=user:ca-manager@example.net \\\r\n --role=roles/publicca.externalAccountKeyCreator\r\n# Request a key:\r\n$ gcloud alpha publicca external-account-keys create&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Each ACME implementation differs slightly on how you specify this API key but as an example with the popular&lt;a href=&#34;https://certbot.eff.org/&#34; target=&#34;_blank&#34;&gt;Certbot ACME&lt;/a&gt; client the configuration looks something like this:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;certbot -d &amp;lt;domain.com&amp;gt; --server https://dv.acme-v02.api.pki.goog/directory --eab-kid &amp;lt;EAB_KEY_ID&amp;gt; --eab-hmac-key &amp;lt;EAB_HMAC_KEY&amp;gt; --standalone --preferred-challenges dns certonly&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;It is that simple. Just schedule this task to run periodically and you will now be automatically acquiring and maintaining the TLS certificates for the associated workload.&lt;/p&gt;&lt;h3&gt;For Kubernetes based workloads&lt;/h3&gt;&lt;p&gt;If you are using Kubernetes, thanks to &lt;a href=&#34;https://cert-manager.io/&#34; target=&#34;_blank&#34;&gt;cert-manager&lt;/a&gt; (another ACME client), it is just as easy. Simply specify the ACME url and &lt;a href=&#34;https://cert-manager.io/docs/configuration/acme/#external-account-bindings&#34; target=&#34;_blank&#34;&gt;External Account Binding&lt;/a&gt; details in your configuration. Your ACME client will ensure you always have an up to date certificate for your Kubernetes deployment.&lt;/p&gt;&lt;h3&gt;Announcing the Private Preview&lt;/h3&gt;&lt;p&gt;We have heard loud and clear that our customers want to use a unified solution for managing their HTTPS certificates which is why we have launched this offering today.&lt;/p&gt;&lt;p&gt;Using this service and Google Trust Services means you will get the same industry leading &lt;a href=&#34;https://security.googleblog.com/2021/03/google-https-and-device-compatibility.html&#34; target=&#34;_blank&#34;&gt;device compatibility&lt;/a&gt; we use for services like YouTube and Google search for your own products and services.&lt;/p&gt;&lt;h3&gt;FAQ&lt;/h3&gt;&lt;p&gt;We know you might have some questions about this release so here are our answers to the most frequent questions we hear:&lt;/p&gt;&lt;p&gt;&lt;b&gt;How can I get access?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;You can request access to this Private Preview using &lt;a href=&#34;https://docs.google.com/forms/d/1Euhflb5CXpuLik8czElhyAloTZJZobar4086dmlPqXA/viewform?ts=620a6854&#34; target=&#34;_blank&#34;&gt;this sign up form&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;b&gt;How long are the certificates you issue good for?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;By default all certificates issued by Google Trust Services are good for up to 90 days; however, ACME allows for clients to request certificates with different validity periods. Using this capability we allow the requestor to get certificates that are good for as little as 1 day, though we would not recommend using anything less than 3 days due to concerns over &lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46359.pdf&#34; target=&#34;_blank&#34;&gt;clock skew&lt;/a&gt; and certificate validity overlap.&lt;/p&gt;&lt;p&gt;&lt;b&gt;What forms of domain control verification do you support?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;The ACME protocol defines several mechanisms for domain control verification and we support three of them, they include : TLS-ALPN-01, HTTP-01, and DNS-01. &lt;br/&gt;&lt;/p&gt;&lt;p&gt;Each of these have different scenarios where their use makes the most sense, for example TLS-ALPN-01 might make sense in cases where HTTPS is not used and the requestor does not have access to dynamically update DNS records. Choose the mechanism that fits your use case best.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Do you support email based domain control verification?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;No we do not.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Do you issue wildcard certificates?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Yes we do. Please note, as with other Certificate Authorities you must currently use  DNS based domain control verification to get a wildcard certificate.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Do you issue certificates for punycode encoded Unicode domain names?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Not at this time.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Do you issue certificates containing IP addresses?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Yes we do; however, this is currently limited to customers who control an IANA assigned IP address block. Contact your sales representative for more information.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Can I use ACME to get private certificates from Cloud CA Service?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Yes, but not directly. Our partner &lt;a href=&#34;https://smallstep.com/blog/private-acme-server/&#34; target=&#34;_blank&#34;&gt;SmallStep&lt;/a&gt; created an ACME Registration Authority (RA) that can be used to get certificates from the &lt;a href=&#34;https://cloud.google.com/certificate-authority-service&#34;&gt;Cloud CA Service&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;b&gt;What algorithms and key lengths do you support?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;We support issuing both ECC and RSA certificates. For more information see our &lt;a href=&#34;https://pki.goog/repository/&#34; target=&#34;_blank&#34;&gt;Certificate Practice Statement&lt;/a&gt; and &lt;a href=&#34;https://pki.goog/repository/&#34; target=&#34;_blank&#34;&gt;CA Certificate Repository&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Do you offer certificates from a pure ECC based certificate chain?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Not at this time.&lt;/p&gt;&lt;p&gt;&lt;b&gt;What root certificates do you use?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;We list all of our root certificates and intermediate certificates &lt;a href=&#34;https://pki.goog/repository/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and we do change which ones we use from time to time. &lt;/p&gt;&lt;p&gt;It is important to also note that we send the appropriate intermediate certificates with every certificate request via the &lt;a href=&#34;https://tools.ietf.org/html/rfc8555#section-7.4.2&#34; target=&#34;_blank&#34;&gt;ACME protocol.&lt;/a&gt; &lt;/p&gt;&lt;p&gt;&lt;b&gt;Why should I use Google Trust Services instead of another certificate authority?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;There are multiple good ACME CAs you may use. &lt;/p&gt;&lt;p&gt;We envision a world where those that deploy SSL use a number of ACME based certificate authorities to enable sites to continue to operate without downtime when one provider has availability issues. If you need a large number of certificates or guarantees on geographic diversity, the GTS CA may be an especially good fit.&lt;/p&gt;&lt;p&gt;It is our hope that by making this service available to cloud customers they will be able to get the benefit of that robustness, and reduce latency for workloads terminating TLS within Google Cloud. &lt;/p&gt;&lt;p&gt;&lt;b&gt;You recently announced Certificate Manager, is this an alternative to that offering?&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;No it is not. This extends Certificate Manager so that workloads that choose to terminate TLS on their own are able to get certificates from the same CA we use when we manage your certificates for you.&lt;/p&gt;&lt;p&gt;It is our hope that with this ACME API, you will be able to simplify your HTTPS certificate lifecycle management for your workloads.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/identity-security/simplify-saas-scale-tls-certificate-management/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/google_cloud_security.0999040819990817.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Introducing Certificate Manager to simplify SaaS scale TLS and certificate management&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Cloud Certificate Manager lets our users acquire and manage TLS certificates for use with Cloud Load Balancing.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;It is that simple. Just schedule this task to run periodically and you will now be automatically acquiring and maintaining the TLS certificates for the associated workload.&lt;/p&gt;&lt;h3&gt;For Kubernetes based workloads&lt;/h3&gt;&lt;p&gt;If you are using Kubernetes, thanks to &lt;a href=&#34;https://cert-manager.io/&#34; target=&#34;_blank&#34;&gt;cert-manager&lt;/a&gt; (another ACME client), it is just as easy. Simply specify the ACME url and &lt;a href=&#34;https://cert-manager.io/docs/configuration/acme/#external-account-bindings&#34; target=&#34;_blank&#34;&gt;External Account Binding&lt;/a&gt; details in your configuration. Your ACME client will ensure you always have an up to date certificate for your Kubernetes deployment.&lt;/p&gt;&lt;h3&gt;Announcing the Private Preview&lt;/h3&gt;&lt;p&gt;We have heard loud and clear that our customers want to use a unified solution for managing their HTTPS certificates which is why we have launched this offering today.&lt;/p&gt;&lt;p&gt;Using this service and Google Trust Services means you will get the same industry leading &lt;a href=&#34;https://security.googleblog.com/2021/03/google-https-and-device-compatibility.html&#34; target=&#34;_blank&#34;&gt;device compatibility&lt;/a&gt; we use for services like YouTube and Google search for your own products and services.&lt;/p&gt;&lt;h3&gt;FAQ&lt;/h3&gt;&lt;p&gt;We know you might have some questions about this release so here are our answers to the most frequent questions we hear:&lt;/p&gt;&lt;p&gt;&lt;b&gt;How can I get access?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;You can request access to this Private Preview using &lt;a href=&#34;https://docs.google.com/forms/d/1Euhflb5CXpuLik8czElhyAloTZJZobar4086dmlPqXA/viewform?ts=620a6854&#34; target=&#34;_blank&#34;&gt;this sign up form&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;b&gt;How long are the certificates you issue good for?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;By default all certificates issued by Google Trust Services are good for up to 90 days; however, ACME allows for clients to request certificates with different validity periods. Using this capability we allow the requestor to get certificates that are good for as little as 1 day, though we would not recommend using anything less than 3 days due to concerns over &lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46359.pdf&#34; target=&#34;_blank&#34;&gt;clock skew&lt;/a&gt; and certificate validity overlap.&lt;/p&gt;&lt;p&gt;&lt;b&gt;What forms of domain control verification do you support?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;The ACME protocol defines several mechanisms for domain control verification and we support three of them, they include : TLS-ALPN-01, HTTP-01, and DNS-01. &lt;br&gt;&lt;/p&gt;&lt;p&gt;Each of these have different scenarios where their use makes the most sense, for example TLS-ALPN-01 might make sense in cases where HTTPS is not used and the requestor does not have access to dynamically update DNS records. Choose the mechanism that fits your use case best.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Do you support email based domain control verification?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;No we do not.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Do you issue wildcard certificates?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Yes we do. Please note, as with other Certificate Authorities you must currently use&amp;#160; DNS based domain control verification to get a wildcard certificate.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Do you issue certificates for punycode encoded Unicode domain names?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Not at this time.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Do you issue certificates containing IP addresses?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Yes we do; however, this is currently limited to customers who control an IANA assigned IP address block. Contact your sales representative for more information.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Can I use ACME to get private certificates from Cloud CA Service?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Yes, but not directly. Our partner &lt;a href=&#34;https://smallstep.com/blog/private-acme-server/&#34; target=&#34;_blank&#34;&gt;SmallStep&lt;/a&gt; created an ACME Registration Authority (RA) that can be used to get certificates from the &lt;a href=&#34;https://cloud.google.com/certificate-authority-service&#34;&gt;Cloud CA Service&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;b&gt;What algorithms and key lengths do you support?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;We support issuing both ECC and RSA certificates. For more information see our &lt;a href=&#34;https://pki.goog/repository/&#34; target=&#34;_blank&#34;&gt;Certificate Practice Statement&lt;/a&gt; and &lt;a href=&#34;https://pki.goog/repository/&#34; target=&#34;_blank&#34;&gt;CA Certificate Repository&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Do you offer certificates from a pure ECC based certificate chain?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;Not at this time.&lt;/p&gt;&lt;p&gt;&lt;b&gt;What root certificates do you use?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;We list all of our root certificates and intermediate certificates &lt;a href=&#34;https://pki.goog/repository/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and we do change which ones we use from time to time.&amp;#160;&lt;/p&gt;&lt;p&gt;It is important to also note that we send the appropriate intermediate certificates with every certificate request via the &lt;a href=&#34;https://tools.ietf.org/html/rfc8555#section-7.4.2&#34; target=&#34;_blank&#34;&gt;ACME protocol. &lt;/a&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Why should I use Google Trust Services instead of another certificate authority?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;There are multiple good ACME CAs you may use.&amp;#160;&lt;/p&gt;&lt;p&gt;We envision a world where those that deploy SSL use a number of ACME based certificate authorities to enable sites to continue to operate without downtime when one provider has availability issues. If you need a large number of certificates or guarantees on geographic diversity, the GTS CA may be an especially good fit.&lt;/p&gt;&lt;p&gt;It is our hope that by making this service available to cloud customers they will be able to get the benefit of that robustness, and reduce latency for workloads terminating TLS within Google Cloud.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;You recently announced Certificate Manager, is this an alternative to that offering?&lt;/b&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;No it is not. This extends Certificate Manager so that workloads that choose to terminate TLS on their own are able to get certificates from the same CA we use when we manage your certificates for you.&lt;/p&gt;&lt;p&gt;It is our hope that with this ACME API, you will be able to simplify your HTTPS certificate lifecycle management for your workloads.&lt;/p&gt;"><p>It is that simple. Just schedule this task to run periodically and you will now be automatically acquiring and maintaining the TLS certificates for the associated workload.</p><h3>For Kubernetes based workloads</h3><p>If you are using Kubernetes, thanks to <a href="https://cert-manager.io/" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cert-manager.io" track-metadata-module="post">cert-manager</a> (another ACME client), it is just as easy. Simply specify the ACME url and <a href="https://cert-manager.io/docs/configuration/acme/#external-account-bindings" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cert-manager.io" track-metadata-module="post">External Account Binding</a> details in your configuration. Your ACME client will ensure you always have an up to date certificate for your Kubernetes deployment.</p><h3>Announcing the Private Preview</h3><p>We have heard loud and clear that our customers want to use a unified solution for managing their HTTPS certificates which is why we have launched this offering today.</p><p>Using this service and Google Trust Services means you will get the same industry leading <a href="https://security.googleblog.com/2021/03/google-https-and-device-compatibility.html" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://security.googleblog.com" track-metadata-module="post">device compatibility</a> we use for services like YouTube and Google search for your own products and services.</p><h3>FAQ</h3><p>We know you might have some questions about this release so here are our answers to the most frequent questions we hear:</p><p><b>How can I get access?</b><b><br/></b>You can request access to this Private Preview using <a href="https://docs.google.com/forms/d/1Euhflb5CXpuLik8czElhyAloTZJZobar4086dmlPqXA/viewform?ts=620a6854" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://docs.google.com" track-metadata-module="post">this sign up form</a>.</p><p><b>How long are the certificates you issue good for?</b><b><br/></b>By default all certificates issued by Google Trust Services are good for up to 90 days; however, ACME allows for clients to request certificates with different validity periods. Using this capability we allow the requestor to get certificates that are good for as little as 1 day, though we would not recommend using anything less than 3 days due to concerns over <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46359.pdf" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://static.googleusercontent.com" track-metadata-module="post">clock skew</a> and certificate validity overlap.</p><p><b>What forms of domain control verification do you support?</b><b><br/></b>The ACME protocol defines several mechanisms for domain control verification and we support three of them, they include : TLS-ALPN-01, HTTP-01, and DNS-01. <br/></p><p>Each of these have different scenarios where their use makes the most sense, for example TLS-ALPN-01 might make sense in cases where HTTPS is not used and the requestor does not have access to dynamically update DNS records. Choose the mechanism that fits your use case best.</p><p><b>Do you support email based domain control verification?</b><b><br/></b>No we do not.</p><p><b>Do you issue wildcard certificates?</b><b><br/></b>Yes we do. Please note, as with other Certificate Authorities you must currently use  DNS based domain control verification to get a wildcard certificate.</p><p><b>Do you issue certificates for punycode encoded Unicode domain names?</b><b><br/></b>Not at this time.</p><p><b>Do you issue certificates containing IP addresses?</b><b><br/></b>Yes we do; however, this is currently limited to customers who control an IANA assigned IP address block. Contact your sales representative for more information.</p><p><b>Can I use ACME to get private certificates from Cloud CA Service?</b><b><br/></b>Yes, but not directly. Our partner <a href="https://smallstep.com/blog/private-acme-server/" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://smallstep.com" track-metadata-module="post">SmallStep</a> created an ACME Registration Authority (RA) that can be used to get certificates from the <a href="https://cloud.google.com/certificate-authority-service" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/certificate-authority-service" track-metadata-module="post">Cloud CA Service</a>.</p><p><b>What algorithms and key lengths do you support?</b><b><br/></b>We support issuing both ECC and RSA certificates. For more information see our <a href="https://pki.goog/repository/" target="_blank" track-type="inline link" track-name="11" track-metadata-eventdetail="https://pki.goog" track-metadata-module="post">Certificate Practice Statement</a> and <a href="https://pki.goog/repository/" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="https://pki.goog" track-metadata-module="post">CA Certificate Repository</a></p><p><b>Do you offer certificates from a pure ECC based certificate chain?</b><b><br/></b>Not at this time.</p><p><b>What root certificates do you use?</b><b><br/></b>We list all of our root certificates and intermediate certificates <a href="https://pki.goog/repository/" target="_blank" track-type="inline link" track-name="13" track-metadata-eventdetail="https://pki.goog" track-metadata-module="post">here</a> and we do change which ones we use from time to time. </p><p>It is important to also note that we send the appropriate intermediate certificates with every certificate request via the <a href="https://tools.ietf.org/html/rfc8555#section-7.4.2" target="_blank" track-type="inline link" track-name="14" track-metadata-eventdetail="https://tools.ietf.org" track-metadata-module="post">ACME protocol. </a> </p><p><b>Why should I use Google Trust Services instead of another certificate authority?</b><b><br/></b>There are multiple good ACME CAs you may use. </p><p>We envision a world where those that deploy SSL use a number of ACME based certificate authorities to enable sites to continue to operate without downtime when one provider has availability issues. If you need a large number of certificates or guarantees on geographic diversity, the GTS CA may be an especially good fit.</p><p>It is our hope that by making this service available to cloud customers they will be able to get the benefit of that robustness, and reduce latency for workloads terminating TLS within Google Cloud. </p><p><b>You recently announced Certificate Manager, is this an alternative to that offering?</b><b><br/></b>No it is not. This extends Certificate Manager so that workloads that choose to terminate TLS on their own are able to get certificates from the same CA we use when we manage your certificates for you.</p><p>It is our hope that with this ACME API, you will be able to simplify your HTTPS certificate lifecycle management for your workloads.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Babi Seal&lt;/name&gt;&lt;title&gt;Product Manager, Load Balancing&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 29 Mar 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Add severity levels to your alert policies in Cloud Monitoring</title>
      <link>https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;When you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we’re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty). &lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/monitoring/support/notification-options&#34;&gt;notification channels&lt;/a&gt; have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.&lt;/p&gt;&lt;p&gt;Below, we&#39;ll walk through examples of &lt;a href=&#34;https://cloud.google.com/monitoring/alerts/labels&#34;&gt;how to add&lt;/a&gt; static and dynamic severity levels to an Alert Policy. &lt;/p&gt;&lt;h3&gt;Create user labels to support static severity levels&lt;/h3&gt;&lt;p&gt;When you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the &lt;a href=&#34;https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies&#34;&gt;documentation&lt;/a&gt; to see how to add user labels to alert policies via the Alert Policy API. &lt;/p&gt;&lt;p&gt;Let’s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;INFO&lt;/code&gt; when CPU utilization is between 70% and 80%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;WARNING&lt;/code&gt; when CPU utilization is between 80% and 90%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;CRITICAL&lt;/code&gt; when CPU utilization is above 90%&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;To accomplish this, you can create three separate alert policies with user labels defined as below:&lt;/p&gt;&lt;p&gt;Create alert policy (&lt;code&gt;A&lt;/code&gt;) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;CRITICAL&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;&#34;userLabels&#34;: {&lt;/code&gt;&lt;br/&gt;    &lt;code&gt;“Severity”: “CRITICAL”,&lt;/code&gt;&lt;br/&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Create a second policy (&lt;code&gt;B&lt;/code&gt;) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;WARNING&lt;/code&gt;. &lt;/p&gt;&lt;p&gt;&lt;code&gt;&#34;userLabels&#34;: {&lt;/code&gt;&lt;br/&gt;    &lt;code&gt;“Severity”: “WARNING”,&lt;/code&gt;&lt;br/&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Create a third policy (&lt;code&gt;C&lt;/code&gt;) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;INFO&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;&#34;userLabels&#34;: {&lt;/code&gt;&lt;br/&gt;    &lt;code&gt;“Severity”: “INFO”,&lt;/code&gt;&lt;br/&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;In this scenario, when the CPU utilization crosses a threshold of 90% policies &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt; will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy &lt;code&gt;A&lt;/code&gt; will close, but the incidents from policies &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; will remain open. If the CPU utilization falls even further down to 75%, the incident from policy &lt;code&gt;B&lt;/code&gt; will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.&lt;/p&gt;&lt;h3&gt;Use MQL to create dynamic severity levels&lt;/h3&gt;&lt;p&gt;Alert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label &lt;code&gt;Severity&lt;/code&gt; with value:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;INFO&lt;/code&gt; below a threshold of 80%,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;WARNING&lt;/code&gt; below a threshold of 90%, and&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;CRITICAL&lt;/code&gt; above a threshold of 90%.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If you&#39;d like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can &lt;a href=&#34;https://cloud.google.com/monitoring/mql/alerts&#34;&gt;utilize MQL to create alert policies&lt;/a&gt; with dynamic custom metric labels that will be embedded in the incident. Via MQL &lt;a href=&#34;https://cloud.google.com/monitoring/mql/reference#map&#34;&gt;map&lt;/a&gt;, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.&lt;/p&gt;&lt;p&gt;Take the sample MQL query below:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;fetch gce_instance\r\n| metric &#39;compute.googleapis.com/instance/cpu/utilization&#39;\r\n| filter (metadata.user_labels.env == &#39;prod&#39;) &amp;amp;&amp;amp; (resource.zone =~ &#39;asia.*&#39;)\r\n| group_by sliding(5m), [value_utilization_mean: mean(value.utilization)]\r\n| map\r\n add[\r\n Severity:\r\n if(val() &amp;gt; 90 &#39;%&#39;, &#39;CRITICAL&#39;,\r\n if(val() &amp;gt;= 80 &#39;%&#39; &amp;amp;&amp;amp; val() &amp;lt;= 90 &#39;%&#39;, &#39;WARNING&#39;, &#39;INFO&#39;))]\r\n| condition val() &amp;gt; 70 &#39;%&#39;&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In this example, an incident will be created any time CPU utilization is above a threshold of 70%. If the value is between 70-80%, the incident will contain a metric label called &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;INFO&lt;/code&gt;. If the value is between 80-90%, the metric label &lt;code&gt;Severity&lt;/code&gt; will have value WARNING, and if the value is above 90%, the label &lt;code&gt;Severity&lt;/code&gt; will have value &lt;code&gt;CRITICAL&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In the above scenario, if the CPU utilization value starts at 92%, incident A will be created with severity level &lt;code&gt;CRITICAL&lt;/code&gt;. If the utilization value then drops down to 73%, a new incident &lt;code&gt;B&lt;/code&gt; will be opened with severity level &lt;code&gt;INFO&lt;/code&gt;. Incident &lt;code&gt;A&lt;/code&gt;, however, will remain open. If the value jumps to 82%, a new incident &lt;code&gt;C&lt;/code&gt; will open with severity level &lt;code&gt;WARNING&lt;/code&gt; and incidents &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; will remain open. If &lt;a href=&#34;https://cloud.google.com/monitoring/alerts/incidents-events#closing&#34;&gt;auto-close&lt;/a&gt; is configured in your policy with a duration of 30 minutes, incident `A` will auto-close 30 minutes after incident `B` starts, and incident `B` will auto-close 30 minutes after incident `C` starts.  If the value drops below 70%, all incidents will close. &lt;/p&gt;&lt;p&gt;In order to ensure the alert policy only has one incident open at a time with the correct corresponding label, and to avoid waiting for incidents to auto-close as in the example above, set &lt;a href=&#34;https://cloud.google.com/monitoring/alerts/concepts-indepth#partial-metric-data&#34;&gt;evaluationMissingData&lt;/a&gt; to &lt;code&gt;EVALUATION_MISSING_DATA_INACTIVE&lt;/code&gt; in your API request. This field tells the Alert Policy how to handle situations when the metric stream has sparse or missing data, so the incident can be closed appropriately as needed. If you are making your MQL alert policy in the UI, select the &lt;code&gt;Missing data points treated as values that do not violate the policy condition&lt;/code&gt; button in the &lt;code&gt;Advanced Options&lt;/code&gt; dropdown in the &lt;code&gt;Configure Trigger&lt;/code&gt; section:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_trigger.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Monitoring trigger.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_trigger.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;When &lt;code&gt;EVALUATION_MISSING_DATA_INACTIVE&lt;/code&gt; is specified in the above scenario, incident &lt;code&gt;A&lt;/code&gt; will close once incident&lt;code&gt;B&lt;/code&gt; is created, and incident &lt;code&gt;B&lt;/code&gt; will close once incident &lt;code&gt;C&lt;/code&gt; is created.&lt;/p&gt;&lt;h3&gt;Severity Labels in Notification Channels&lt;/h3&gt;&lt;p&gt;If you send &lt;a href=&#34;https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.notificationChannels#resource:-notificationchannel&#34;&gt;notifications&lt;/a&gt; to a third-party service like &lt;a href=&#34;https://cloud.google.com/monitoring/support/notification-options#creating_channels&#34;&gt;PagerDuty, Webhooks, or Pub/Sub&lt;/a&gt; then you can parse the JSON payload and route the notification according to its severity so that critical information is not missed by your team. &lt;/p&gt;&lt;p&gt;If you utilize alert policy user labels, these will appear as an object on the notification with the key &lt;code&gt;policy_user_labels&lt;/code&gt; i.e.:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;&#34;policy_user_labels&#34;: {\r\n &#34;Severity&#34;: &#34;CRITICAL&#34;,\r\n}&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;If you utilize metric labels via MQL, these will appear as an object with key &lt;code&gt;labels&lt;/code&gt; nested in an object with key &lt;code&gt;metric&lt;/code&gt; i.e.:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;&#34;metric&#34;: {\r\n &#34;displayName&#34;: &#34;Some Display Name&#34;,\r\n &#34;labels&#34;: {\r\n &#34;instance_name&#34;: &#34;some_instance_name&#34;,\r\n &#34;Severity&#34;: &#34;CRITICAL&#34;\r\n },\r\n }&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Get Started Today&lt;/h3&gt;&lt;p&gt;Alerts can be configured on nearly any metric, log, or trace (or the absence of that data) that is captured in &lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;Google Cloud’s operations suite&lt;/a&gt;. Severity levels give you and your teams an additional way to cut through noise to find the issues that you know will have the most positive impact when resolved. Check out this video on &lt;a href=&#34;https://youtu.be/4RgJjx4IxMs&#34; target=&#34;_blank&#34;&gt;log alerts&lt;/a&gt; as part of our Observability in-depth video series and if you have questions, feature requests, or just want to read topics from other customers who are using Cloud Alerting, visit our &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Google Cloud Community site&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/operations/pub-sub-webook-and-slack-notifications-are-now-available/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Genric_GCP_upA1oyz.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Webhook, Pub/Sub, and Slack Alerting notification channels launched&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Announcing the general availability of the new Pub/Sub, Webhook, and Slack Notification channels.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c77=""><div _ngcontent-c77="" innerhtml="&lt;p&gt;When you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we&amp;#8217;re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty).&amp;#160;&lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/monitoring/support/notification-options&#34;&gt;notification channels&lt;/a&gt; have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.&lt;/p&gt;&lt;p&gt;Below, we&#39;ll walk through examples of &lt;a href=&#34;https://cloud.google.com/monitoring/alerts/labels&#34;&gt;how to add&lt;/a&gt; static and dynamic severity levels to an Alert Policy.&amp;#160;&lt;/p&gt;&lt;h3&gt;Create user labels to support static severity levels&lt;/h3&gt;&lt;p&gt;When you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the &lt;a href=&#34;https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies&#34;&gt;documentation&lt;/a&gt; to see how to add user labels to alert policies via the Alert Policy API.&amp;#160;&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;INFO&lt;/code&gt; when CPU utilization is between 70% and 80%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;WARNING&lt;/code&gt; when CPU utilization is between 80% and 90%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;CRITICAL&lt;/code&gt; when CPU utilization is above 90%&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;To accomplish this, you can create three separate alert policies with user labels defined as below:&lt;/p&gt;&lt;p&gt;Create alert policy (&lt;code&gt;A&lt;/code&gt;) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;CRITICAL&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;&amp;#34;userLabels&amp;#34;: {&lt;/code&gt;&lt;br&gt;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&lt;code&gt;&amp;#8220;Severity&amp;#8221;: &amp;#8220;CRITICAL&amp;#8221;,&lt;/code&gt;&lt;br&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Create a second policy (&lt;code&gt;B&lt;/code&gt;) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;WARNING&lt;/code&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;code&gt;&amp;#34;userLabels&amp;#34;: {&lt;/code&gt;&lt;br&gt;&amp;#160; &amp;#160;&amp;#160;&lt;code&gt;&amp;#8220;Severity&amp;#8221;: &amp;#8220;WARNING&amp;#8221;,&lt;/code&gt;&lt;br&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Create a third policy (&lt;code&gt;C&lt;/code&gt;) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;INFO&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;&amp;#34;userLabels&amp;#34;: {&lt;/code&gt;&lt;br&gt;&amp;#160; &amp;#160;&amp;#160;&lt;code&gt;&amp;#8220;Severity&amp;#8221;: &amp;#8220;INFO&amp;#8221;,&lt;/code&gt;&lt;br&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;In this scenario, when the CPU utilization crosses a threshold of 90% policies &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt; will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy &lt;code&gt;A&lt;/code&gt; will close, but the incidents from policies &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; will remain open. If the CPU utilization falls even further down to 75%, the incident from policy &lt;code&gt;B&lt;/code&gt; will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.&lt;/p&gt;&lt;h3&gt;Use MQL to create dynamic severity levels&lt;/h3&gt;&lt;p&gt;Alert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label &lt;code&gt;Severity&lt;/code&gt; with value:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;INFO&lt;/code&gt; below a threshold of 80%,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;WARNING&lt;/code&gt; below a threshold of 90%, and&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;CRITICAL&lt;/code&gt; above a threshold of 90%.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If you&#39;d like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can &lt;a href=&#34;https://cloud.google.com/monitoring/mql/alerts&#34;&gt;utilize MQL to create alert policies&lt;/a&gt; with dynamic custom metric labels that will be embedded in the incident. Via MQL &lt;a href=&#34;https://cloud.google.com/monitoring/mql/reference#map&#34;&gt;map&lt;/a&gt;, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.&lt;/p&gt;&lt;p&gt;Take the sample MQL query below:&lt;/p&gt;"><p>When you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we’re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty). </p><p>The <a href="https://cloud.google.com/monitoring/support/notification-options" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/monitoring/support/notification-options" track-metadata-module="post">notification channels</a> have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.</p><p>Below, we&#39;ll walk through examples of <a href="https://cloud.google.com/monitoring/alerts/labels" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/monitoring/alerts/labels" track-metadata-module="post">how to add</a> static and dynamic severity levels to an Alert Policy. </p><h3>Create user labels to support static severity levels</h3><p>When you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the <a href="https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies" track-metadata-module="post">documentation</a> to see how to add user labels to alert policies via the Alert Policy API. </p><p>Let’s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:</p><ul><li><p><code>INFO</code> when CPU utilization is between 70% and 80%</p></li><li><p><code>WARNING</code> when CPU utilization is between 80% and 90%</p></li><li><p><code>CRITICAL</code> when CPU utilization is above 90%</p></li></ul><p>To accomplish this, you can create three separate alert policies with user labels defined as below:</p><p>Create alert policy (<code>A</code>) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label <code>Severity</code> with value <code>CRITICAL</code>.</p><p><code>&#34;userLabels&#34;: {</code><br/>    <code>“Severity”: “CRITICAL”,</code><br/><code>}</code></p><p>Create a second policy (<code>B</code>) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label <code>Severity</code> with value <code>WARNING</code>. </p><p><code>&#34;userLabels&#34;: {</code><br/>    <code>“Severity”: “WARNING”,</code><br/><code>}</code></p><p>Create a third policy (<code>C</code>) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label <code>Severity</code> with value <code>INFO</code>.</p><p><code>&#34;userLabels&#34;: {</code><br/>    <code>“Severity”: “INFO”,</code><br/><code>}</code></p><p>In this scenario, when the CPU utilization crosses a threshold of 90% policies <code>A</code>, <code>B</code>, and <code>C</code> will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy <code>A</code> will close, but the incidents from policies <code>B</code> and <code>C</code> will remain open. If the CPU utilization falls even further down to 75%, the incident from policy <code>B</code> will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.</p><h3>Use MQL to create dynamic severity levels</h3><p>Alert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label <code>Severity</code> with value:</p><ul><li><p><code>INFO</code> below a threshold of 80%,</p></li><li><p><code>WARNING</code> below a threshold of 90%, and</p></li><li><p><code>CRITICAL</code> above a threshold of 90%.</p></li></ul><p>If you&#39;d like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can <a href="https://cloud.google.com/monitoring/mql/alerts" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/monitoring/mql/alerts" track-metadata-module="post">utilize MQL to create alert policies</a> with dynamic custom metric labels that will be embedded in the incident. Via MQL <a href="https://cloud.google.com/monitoring/mql/reference#map" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/monitoring/mql/reference#map" track-metadata-module="post">map</a>, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.</p><p>Take the sample MQL query below:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Alizah Lalani&lt;/name&gt;&lt;title&gt;Software Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 29 Mar 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Add severity levels to your alert policies in Cloud Monitoring</title>
      <link>https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;When you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we’re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty). &lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/monitoring/support/notification-options&#34;&gt;notification channels&lt;/a&gt; have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.&lt;/p&gt;&lt;p&gt;Below, we&#39;ll walk through examples of &lt;a href=&#34;https://cloud.google.com/monitoring/alerts/labels&#34;&gt;how to add&lt;/a&gt; static and dynamic severity levels to an Alert Policy. &lt;/p&gt;&lt;h3&gt;Create user labels to support static severity levels&lt;/h3&gt;&lt;p&gt;When you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the &lt;a href=&#34;https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies&#34;&gt;documentation&lt;/a&gt; to see how to add user labels to alert policies via the Alert Policy API. &lt;/p&gt;&lt;p&gt;Let’s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;INFO&lt;/code&gt; when CPU utilization is between 70% and 80%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;WARNING&lt;/code&gt; when CPU utilization is between 80% and 90%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;CRITICAL&lt;/code&gt; when CPU utilization is above 90%&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;To accomplish this, you can create three separate alert policies with user labels defined as below:&lt;/p&gt;&lt;p&gt;Create alert policy (&lt;code&gt;A&lt;/code&gt;) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;CRITICAL&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;&#34;userLabels&#34;: {&lt;/code&gt;&lt;br/&gt;    &lt;code&gt;“Severity”: “CRITICAL”,&lt;/code&gt;&lt;br/&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Create a second policy (&lt;code&gt;B&lt;/code&gt;) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;WARNING&lt;/code&gt;. &lt;/p&gt;&lt;p&gt;&lt;code&gt;&#34;userLabels&#34;: {&lt;/code&gt;&lt;br/&gt;    &lt;code&gt;“Severity”: “WARNING”,&lt;/code&gt;&lt;br/&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Create a third policy (&lt;code&gt;C&lt;/code&gt;) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;INFO&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;&#34;userLabels&#34;: {&lt;/code&gt;&lt;br/&gt;    &lt;code&gt;“Severity”: “INFO”,&lt;/code&gt;&lt;br/&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;In this scenario, when the CPU utilization crosses a threshold of 90% policies &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt; will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy &lt;code&gt;A&lt;/code&gt; will close, but the incidents from policies &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; will remain open. If the CPU utilization falls even further down to 75%, the incident from policy &lt;code&gt;B&lt;/code&gt; will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.&lt;/p&gt;&lt;h3&gt;Use MQL to create dynamic severity levels&lt;/h3&gt;&lt;p&gt;Alert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label &lt;code&gt;Severity&lt;/code&gt; with value:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;INFO&lt;/code&gt; below a threshold of 80%,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;WARNING&lt;/code&gt; below a threshold of 90%, and&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;CRITICAL&lt;/code&gt; above a threshold of 90%.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If you&#39;d like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can &lt;a href=&#34;https://cloud.google.com/monitoring/mql/alerts&#34;&gt;utilize MQL to create alert policies&lt;/a&gt; with dynamic custom metric labels that will be embedded in the incident. Via MQL &lt;a href=&#34;https://cloud.google.com/monitoring/mql/reference#map&#34;&gt;map&lt;/a&gt;, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.&lt;/p&gt;&lt;p&gt;Take the sample MQL query below:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;fetch gce_instance\r\n| metric &#39;compute.googleapis.com/instance/cpu/utilization&#39;\r\n| filter (metadata.user_labels.env == &#39;prod&#39;) &amp;amp;&amp;amp; (resource.zone =~ &#39;asia.*&#39;)\r\n| group_by sliding(5m), [value_utilization_mean: mean(value.utilization)]\r\n| map\r\n add[\r\n Severity:\r\n if(val() &amp;gt; 90 &#39;%&#39;, &#39;CRITICAL&#39;,\r\n if(val() &amp;gt;= 80 &#39;%&#39; &amp;amp;&amp;amp; val() &amp;lt;= 90 &#39;%&#39;, &#39;WARNING&#39;, &#39;INFO&#39;))]\r\n| condition val() &amp;gt; 70 &#39;%&#39;&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In this example, an incident will be created any time CPU utilization is above a threshold of 70%. If the value is between 70-80%, the incident will contain a metric label called &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;INFO&lt;/code&gt;. If the value is between 80-90%, the metric label &lt;code&gt;Severity&lt;/code&gt; will have value WARNING, and if the value is above 90%, the label &lt;code&gt;Severity&lt;/code&gt; will have value &lt;code&gt;CRITICAL&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In the above scenario, if the CPU utilization value starts at 92%, incident A will be created with severity level &lt;code&gt;CRITICAL&lt;/code&gt;. If the utilization value then drops down to 73%, a new incident &lt;code&gt;B&lt;/code&gt; will be opened with severity level &lt;code&gt;INFO&lt;/code&gt;. Incident &lt;code&gt;A&lt;/code&gt;, however, will remain open. If the value jumps to 82%, a new incident &lt;code&gt;C&lt;/code&gt; will open with severity level &lt;code&gt;WARNING&lt;/code&gt; and incidents &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; will remain open. If &lt;a href=&#34;https://cloud.google.com/monitoring/alerts/incidents-events#closing&#34;&gt;auto-close&lt;/a&gt; is configured in your policy with a duration of 30 minutes, incident `A` will auto-close 30 minutes after incident `B` starts, and incident `B` will auto-close 30 minutes after incident `C` starts.  If the value drops below 70%, all incidents will close. &lt;/p&gt;&lt;p&gt;In order to ensure the alert policy only has one incident open at a time with the correct corresponding label, and to avoid waiting for incidents to auto-close as in the example above, set &lt;a href=&#34;https://cloud.google.com/monitoring/alerts/concepts-indepth#partial-metric-data&#34;&gt;evaluationMissingData&lt;/a&gt; to &lt;code&gt;EVALUATION_MISSING_DATA_INACTIVE&lt;/code&gt; in your API request. This field tells the Alert Policy how to handle situations when the metric stream has sparse or missing data, so the incident can be closed appropriately as needed. If you are making your MQL alert policy in the UI, select the &lt;code&gt;Missing data points treated as values that do not violate the policy condition&lt;/code&gt; button in the &lt;code&gt;Advanced Options&lt;/code&gt; dropdown in the &lt;code&gt;Configure Trigger&lt;/code&gt; section:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_trigger.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Monitoring trigger.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_trigger.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;When &lt;code&gt;EVALUATION_MISSING_DATA_INACTIVE&lt;/code&gt; is specified in the above scenario, incident &lt;code&gt;A&lt;/code&gt; will close once incident&lt;code&gt;B&lt;/code&gt; is created, and incident &lt;code&gt;B&lt;/code&gt; will close once incident &lt;code&gt;C&lt;/code&gt; is created.&lt;/p&gt;&lt;h3&gt;Severity Labels in Notification Channels&lt;/h3&gt;&lt;p&gt;If you send &lt;a href=&#34;https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.notificationChannels#resource:-notificationchannel&#34;&gt;notifications&lt;/a&gt; to a third-party service like &lt;a href=&#34;https://cloud.google.com/monitoring/support/notification-options#creating_channels&#34;&gt;PagerDuty, Webhooks, or Pub/Sub&lt;/a&gt; then you can parse the JSON payload and route the notification according to its severity so that critical information is not missed by your team. &lt;/p&gt;&lt;p&gt;If you utilize alert policy user labels, these will appear as an object on the notification with the key &lt;code&gt;policy_user_labels&lt;/code&gt; i.e.:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;&#34;policy_user_labels&#34;: {\r\n &#34;Severity&#34;: &#34;CRITICAL&#34;,\r\n}&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;If you utilize metric labels via MQL, these will appear as an object with key &lt;code&gt;labels&lt;/code&gt; nested in an object with key &lt;code&gt;metric&lt;/code&gt; i.e.:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;&#34;metric&#34;: {\r\n &#34;displayName&#34;: &#34;Some Display Name&#34;,\r\n &#34;labels&#34;: {\r\n &#34;instance_name&#34;: &#34;some_instance_name&#34;,\r\n &#34;Severity&#34;: &#34;CRITICAL&#34;\r\n },\r\n }&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Get Started Today&lt;/h3&gt;&lt;p&gt;Alerts can be configured on nearly any metric, log, or trace (or the absence of that data) that is captured in &lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;Google Cloud’s operations suite&lt;/a&gt;. Severity levels give you and your teams an additional way to cut through noise to find the issues that you know will have the most positive impact when resolved. Check out this video on &lt;a href=&#34;https://youtu.be/4RgJjx4IxMs&#34; target=&#34;_blank&#34;&gt;log alerts&lt;/a&gt; as part of our Observability in-depth video series and if you have questions, feature requests, or just want to read topics from other customers who are using Cloud Alerting, visit our &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Google Cloud Community site&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/operations/pub-sub-webook-and-slack-notifications-are-now-available/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Genric_GCP_upA1oyz.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Webhook, Pub/Sub, and Slack Alerting notification channels launched&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Announcing the general availability of the new Pub/Sub, Webhook, and Slack Notification channels.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;When you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we&amp;#8217;re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty).&amp;#160;&lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/monitoring/support/notification-options&#34;&gt;notification channels&lt;/a&gt; have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.&lt;/p&gt;&lt;p&gt;Below, we&#39;ll walk through examples of &lt;a href=&#34;https://cloud.google.com/monitoring/alerts/labels&#34;&gt;how to add&lt;/a&gt; static and dynamic severity levels to an Alert Policy.&amp;#160;&lt;/p&gt;&lt;h3&gt;Create user labels to support static severity levels&lt;/h3&gt;&lt;p&gt;When you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the &lt;a href=&#34;https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies&#34;&gt;documentation&lt;/a&gt; to see how to add user labels to alert policies via the Alert Policy API.&amp;#160;&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;INFO&lt;/code&gt; when CPU utilization is between 70% and 80%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;WARNING&lt;/code&gt; when CPU utilization is between 80% and 90%&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;CRITICAL&lt;/code&gt; when CPU utilization is above 90%&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;To accomplish this, you can create three separate alert policies with user labels defined as below:&lt;/p&gt;&lt;p&gt;Create alert policy (&lt;code&gt;A&lt;/code&gt;) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;CRITICAL&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;&amp;#34;userLabels&amp;#34;: {&lt;/code&gt;&lt;br&gt;&amp;#160;&amp;#160;&amp;#160;&amp;#160;&lt;code&gt;&amp;#8220;Severity&amp;#8221;: &amp;#8220;CRITICAL&amp;#8221;,&lt;/code&gt;&lt;br&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Create a second policy (&lt;code&gt;B&lt;/code&gt;) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;WARNING&lt;/code&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;code&gt;&amp;#34;userLabels&amp;#34;: {&lt;/code&gt;&lt;br&gt;&amp;#160; &amp;#160;&amp;#160;&lt;code&gt;&amp;#8220;Severity&amp;#8221;: &amp;#8220;WARNING&amp;#8221;,&lt;/code&gt;&lt;br&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Create a third policy (&lt;code&gt;C&lt;/code&gt;) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label &lt;code&gt;Severity&lt;/code&gt; with value &lt;code&gt;INFO&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;&amp;#34;userLabels&amp;#34;: {&lt;/code&gt;&lt;br&gt;&amp;#160; &amp;#160;&amp;#160;&lt;code&gt;&amp;#8220;Severity&amp;#8221;: &amp;#8220;INFO&amp;#8221;,&lt;/code&gt;&lt;br&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;In this scenario, when the CPU utilization crosses a threshold of 90% policies &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, and &lt;code&gt;C&lt;/code&gt; will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy &lt;code&gt;A&lt;/code&gt; will close, but the incidents from policies &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;C&lt;/code&gt; will remain open. If the CPU utilization falls even further down to 75%, the incident from policy &lt;code&gt;B&lt;/code&gt; will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.&lt;/p&gt;&lt;h3&gt;Use MQL to create dynamic severity levels&lt;/h3&gt;&lt;p&gt;Alert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label &lt;code&gt;Severity&lt;/code&gt; with value:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;INFO&lt;/code&gt; below a threshold of 80%,&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;WARNING&lt;/code&gt; below a threshold of 90%, and&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;code&gt;CRITICAL&lt;/code&gt; above a threshold of 90%.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If you&#39;d like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can &lt;a href=&#34;https://cloud.google.com/monitoring/mql/alerts&#34;&gt;utilize MQL to create alert policies&lt;/a&gt; with dynamic custom metric labels that will be embedded in the incident. Via MQL &lt;a href=&#34;https://cloud.google.com/monitoring/mql/reference#map&#34;&gt;map&lt;/a&gt;, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.&lt;/p&gt;&lt;p&gt;Take the sample MQL query below:&lt;/p&gt;"><p>When you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we’re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty). </p><p>The <a href="https://cloud.google.com/monitoring/support/notification-options" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/monitoring/support/notification-options" track-metadata-module="post">notification channels</a> have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.</p><p>Below, we&#39;ll walk through examples of <a href="https://cloud.google.com/monitoring/alerts/labels" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/monitoring/alerts/labels" track-metadata-module="post">how to add</a> static and dynamic severity levels to an Alert Policy. </p><h3>Create user labels to support static severity levels</h3><p>When you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the <a href="https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies" track-metadata-module="post">documentation</a> to see how to add user labels to alert policies via the Alert Policy API. </p><p>Let’s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:</p><ul><li><p><code>INFO</code> when CPU utilization is between 70% and 80%</p></li><li><p><code>WARNING</code> when CPU utilization is between 80% and 90%</p></li><li><p><code>CRITICAL</code> when CPU utilization is above 90%</p></li></ul><p>To accomplish this, you can create three separate alert policies with user labels defined as below:</p><p>Create alert policy (<code>A</code>) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label <code>Severity</code> with value <code>CRITICAL</code>.</p><p><code>&#34;userLabels&#34;: {</code><br/>    <code>“Severity”: “CRITICAL”,</code><br/><code>}</code></p><p>Create a second policy (<code>B</code>) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label <code>Severity</code> with value <code>WARNING</code>. </p><p><code>&#34;userLabels&#34;: {</code><br/>    <code>“Severity”: “WARNING”,</code><br/><code>}</code></p><p>Create a third policy (<code>C</code>) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label <code>Severity</code> with value <code>INFO</code>.</p><p><code>&#34;userLabels&#34;: {</code><br/>    <code>“Severity”: “INFO”,</code><br/><code>}</code></p><p>In this scenario, when the CPU utilization crosses a threshold of 90% policies <code>A</code>, <code>B</code>, and <code>C</code> will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy <code>A</code> will close, but the incidents from policies <code>B</code> and <code>C</code> will remain open. If the CPU utilization falls even further down to 75%, the incident from policy <code>B</code> will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.</p><h3>Use MQL to create dynamic severity levels</h3><p>Alert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label <code>Severity</code> with value:</p><ul><li><p><code>INFO</code> below a threshold of 80%,</p></li><li><p><code>WARNING</code> below a threshold of 90%, and</p></li><li><p><code>CRITICAL</code> above a threshold of 90%.</p></li></ul><p>If you&#39;d like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can <a href="https://cloud.google.com/monitoring/mql/alerts" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/monitoring/mql/alerts" track-metadata-module="post">utilize MQL to create alert policies</a> with dynamic custom metric labels that will be embedded in the incident. Via MQL <a href="https://cloud.google.com/monitoring/mql/reference#map" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/monitoring/mql/reference#map" track-metadata-module="post">map</a>, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.</p><p>Take the sample MQL query below:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Alizah Lalani&lt;/name&gt;&lt;title&gt;Software Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 29 Mar 2022 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What’s new in cloud-native apps?</title>
      <link>https://cloud.google.com/blog/topics/cloud-first/whats-new-cloud-native-apps/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Developers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 14 - Mar 18 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Create EventArc triggers with Terraform&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;In addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform&#34;&gt;shows you how&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Scaling to new markets with Cloud Run&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;French publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms&#34;&gt;website architecture&lt;/a&gt; here. &lt;/p&gt;&lt;p&gt;&lt;b&gt;The serverless way to celebrate Pi Day&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;In honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions&#34;&gt;calculate π&lt;/a&gt; — serverlessly.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/automotive.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Building continuous integration &amp;amp; continuous delivery for autonomous vehicles on Google Cloud&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Cruise relies on a whole host of Google Cloud technologies to develop and test the tech that goes into its autonomous vehicles, or AVs.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Mar 07 - Mar 11 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Rhode Island moves to Google Cloud-based job board&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center&#34;&gt;how they did it&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Containerized microservices at Lowe’s&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Lowe’s already told us &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices&#34;&gt;how they use SRE&lt;/a&gt;. They’re at it again, describing how they built an e-commerce website using a &lt;a href=&#34;https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce&#34;&gt;containerized microservices architecture and Kubernetes&lt;/a&gt;, with Istio for service mesh and Cloud Operations for good measure.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cruise AVs hit the road with Google Cloud services&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Autonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform&#34;&gt;Read the guest post&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;L’Oréal’s data analytics gets a makeover with serverless&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;We’re hurtling toward a &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud&#34;&gt;programmable cloud&lt;/a&gt; — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings&#34;&gt;L’Oréal is a great example&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Better telemetry for your Anthos clusters&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/service-mesh/docs/observability/explore-dashboard&#34;&gt;Anthos Service Mesh Dashboard&lt;/a&gt; is now available (public preview) on the &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/latest&#34;&gt;Anthos clusters on Bare Metal&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/on-prem/1.10&#34;&gt;Anthos clusters on VMware&lt;/a&gt;. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Instrument your Java apps&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;With the new version of the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features&#34;&gt;Google Cloud Logging Java library&lt;/a&gt;, you can wire your application logs with more information — without adding a single line of code.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Visualize metrics from Cloud Spanner&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Building an app on top of Cloud Spanner but can’t assess how well it’s operating? The new &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver&#34; target=&#34;_blank&#34;&gt;OpenTelemetery receiver for Cloud Spanner&lt;/a&gt; provides an easy way for you to process and visualize metrics from Cloud Spanner &lt;a href=&#34;https://cloud.google.com/spanner/docs/introspection&#34;&gt;System tables&lt;/a&gt;, and export these to the APM tool of your choice. &lt;a href=&#34;https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery&#34;&gt;Read more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/knative.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Congratulations Knative on becoming part of the CNCF&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Knative enters the CNCF as an incubating project&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Week of Feb 28 - Mar 4 2022&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Introducing Cloud SDK&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;The rebranded &lt;a href=&#34;https://cloud.google.com/sdk&#34;&gt;Cloud SDK&lt;/a&gt; is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development&#34;&gt;here&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud CLI, meet Terraform&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Google Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview&#34;&gt;available in preview&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Knative graduates to incubating project &lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;Congratulations to Knative, which has been &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project&#34;&gt;accepted by the Cloud Native Computing Foundation&lt;/a&gt;, or CNCF, as an incubating project, enabling the next phase of serverless architecture. &lt;/p&gt;&lt;p&gt;&lt;b&gt;We manage Prometheus so you don’t have to&lt;/b&gt;&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Google Cloud Managed Service for Prometheus&lt;/a&gt; is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes&#34;&gt;Learn more here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Google Cloud Content &amp; Editorial &lt;/name&gt;&lt;title&gt;&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/inframod_living_3.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 18 Mar 2022 20:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Get more insights from your Java applications logs</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Today it is even easier to capture logs in your Java applications. Developers can get more data with their application logs using a new version of the Cloud Logging client library for Java. The library populates the current executing context implicitly with every ingested log entry. Read this if you want to learn how to get HTTP requests and tracing information and additional metadata in your logs without writing a single line of code.&lt;/p&gt;&lt;p&gt;There are three ways to ingest log data into Google Cloud Logging:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Develop a proprietary solution that directly calls the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/api-overview&#34;&gt;Logging API&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Leverage logging capabilities of the Google Cloud managed environments like &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#overview&#34;&gt;GKE&lt;/a&gt; or install Google Cloud &lt;a href=&#34;https://cloud.google.com/monitoring/agent&#34;&gt;Ops agent&lt;/a&gt; and print your application logs to stdout and stderr.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use Google Cloud &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/libraries&#34;&gt;Logging client library&lt;/a&gt; in one of many supported programming languages.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The library provides you with ready to use boilerplate constructs built following the best practices of using Logging API. Java applications can use the Google Cloud Logging library to ingest logs using the integrations with &lt;a href=&#34;https://docs.oracle.com/javase/10/core/java-logging-overview.htm&#34; target=&#34;_blank&#34;&gt;Java Logging&lt;/a&gt; and &lt;a href=&#34;https://logback.qos.ch/&#34; target=&#34;_blank&#34;&gt;Logback framework&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;If you are new to using Google Logging client libraries for Java, follow the steps to &lt;a href=&#34;https://cloud.google.com/logging/docs/setup/java&#34;&gt;set up Cloud Logging for Java&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/libraries&#34;&gt;get started&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In the version 3.6 release of the the Logging client library for Java you get many long demanding features including automatic population of the metadata about the environment&#39;s resource supporting Cloud Run and Cloud Functions, HTTP request contextual information, tracing correlation that enables displaying grouped log entries in Logs Explorer and more. This release of the library is composed of the three packages:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/googleapis/java-logging&#34; target=&#34;_blank&#34;&gt;google-cloud-logging&lt;/a&gt; -- provides the hand-written layer above Cloud Logging API and the integration with legacy &lt;a href=&#34;https://docs.oracle.com/javase/10/core/java-logging-overview.htm&#34; target=&#34;_blank&#34;&gt;Java Logging&lt;/a&gt; solution.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/googleapis/java-logging-logback&#34; target=&#34;_blank&#34;&gt;google-cloud-logging-logback&lt;/a&gt; is the integration with the Logback framework and ingests logs using the google-cloud-logging package.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/googleapis/java-logging-servlet-initializer&#34; target=&#34;_blank&#34;&gt;google-cloud-logging-servlet-initializer&lt;/a&gt; is a new addition to the library; it provides integration with servlet-based Web applications.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The features are available in the versions ≥3.6.3 and ≥0.123.3-alpha of the google-cloud-logging and google-cloud-logging-logback packages respectively.&lt;/p&gt;&lt;p&gt;If you are using &lt;a href=&#34;https://maven.apache.org/&#34; target=&#34;_blank&#34;&gt;Maven&lt;/a&gt;, update the packages&#39; versions in the pom.xml:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;&amp;lt;dependency&amp;gt;\r\n &amp;lt;groupId&amp;gt;com.google.cloud&amp;lt;/groupId&amp;gt;\r\n &amp;lt;artifactId&amp;gt;google-cloud-logging&amp;lt;/artifactId&amp;gt;\r\n &amp;lt;version&amp;gt;3.6.3&amp;lt;/version&amp;gt;\r\n&amp;lt;/dependency&amp;gt;\r\n&amp;lt;dependency&amp;gt;\r\n &amp;lt;groupId&amp;gt;com.google.cloud&amp;lt;/groupId&amp;gt;\r\n &amp;lt;artifactId&amp;gt;google-cloud-logging-logback&amp;lt;/artifactId&amp;gt;\r\n &amp;lt;version&amp;gt;0.123.3-alpha&amp;lt;/version&amp;gt;\r\n&amp;lt;/dependency&amp;gt;&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;If you are using &lt;a href=&#34;https://gradle.org/&#34; target=&#34;_blank&#34;&gt;Gradle&lt;/a&gt;, , update your dependencies:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;implementation &#39;com.google.cloud:google-cloud-logging:3.6.3&#39;\r\nimplementation &#39;com.google.cloud:google-cloud-logging-logback:0.123.3-alpha&#39;&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;You can use the official Google Cloud BOM version 0.167.0 that includes the new releases of the packages.&lt;/p&gt;&lt;h3&gt;What is new&lt;/h3&gt;&lt;p&gt;The Java library inserts structured information about the executing environment including resource types, HTTP request metadata, tracing and more. Using the library you can write your payloads in one of the three formats:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A text provided as a Java string&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A JSON object provided as an instance of &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/util/Map.html&#34; target=&#34;_blank&#34;&gt;Map&amp;lt;String, ?&amp;gt;&lt;/a&gt; or &lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Struct&#34; target=&#34;_blank&#34;&gt;Struct&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A protobuf object provided as an instance of &lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Any&#34; target=&#34;_blank&#34;&gt;Any&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;You can use the structured logs with enhanced filtering in Logs Explorer to observe and troubleshoot their applications. The Logs Explorer uses structured logs to establish &lt;a href=&#34;https://cloud.google.com/trace/docs/trace-log-integration&#34;&gt;correlations&lt;/a&gt; between traces and logs and to group together logs that belong to the same transaction. The correlated &#34;child&#34; logs are displayed &#34;under&#34; the entry of the &#34;parent&#34; log:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;Grouped logs display in Logs Explorer.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Grouped_logs_display_in_Logs_Explorer.max-1000x1000.jpg&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Grouped logs display in Logs Explorer&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;With the previous versions of the Logging library you had to write code to explicitly populate these fields. For example, developers that use Logback framework had to write a code like below to populate the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace&#34;&gt;trace&lt;/a&gt; field of the ingested logs:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;// . . .\r\nString traceInfo = request.getHeader(&#34;x-cloud-trace-context&#34;);\r\nTraceLoggingEventEnhancer.setCurrentTraceId(traceInfo);\r\n// . . .&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;And to invoke this code at the beginning of each transaction.&lt;/p&gt;&lt;p&gt;The new features of the Logging library makes implementing the population logic unnecessary. The new version of the library supports automatic population of following log entry fields:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource&#34;&gt;resource&lt;/a&gt; ‒ describes the resource type and its attributes where the application is running. Along with GCE instances, it supports Google Cloud managed services such as GKE, AppEngine (both Standard and Flexible), Cloud Run and Cloud Functions.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.http_request&#34;&gt;httpRequest&lt;/a&gt; ‒ captures info about HTTP requests from the current application&#39;s context. The context is defined per-thread and can be populated both explicitly in the application code or implicitly from the &lt;a href=&#34;https://docs.oracle.com/cd/E26180_01/Platform.94/ATGProgGuide/html/s0801requesthandlingwithservletpipeli01.html&#34; target=&#34;_blank&#34;&gt;Jakarta servlet requests pipeline&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace&#34;&gt;trace&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.span_id&#34;&gt;spanId&lt;/a&gt; ‒ reads the tracing data from the HTTP request header. The tracing data assists in correlating multiple logs that belong to the same transaction.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.source_location&#34;&gt;sourceLocation&lt;/a&gt; ‒ stores info about the class and method names as well as the line of code where the application called the log ingestion method. The library retrieves the data by traversing the trace stack up until the first entry that is not part of the Logging library code or the system package.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;What is left to you is to set the payload and relevant payload&#39;s metadata labels. The only field in the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry&#34;&gt;log entry&lt;/a&gt; that the library does not automatically populate now is the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.operation&#34;&gt;operation&lt;/a&gt; field.&lt;/p&gt;&lt;h3&gt;Disable information auto-population in log entries&lt;/h3&gt;&lt;p&gt;You have full control over the auto-population functionality. The auto-population is enabled by default for your convenience. But in certain scenarios it can be desirable to disable it. For example, if your application is log intensive and has a narrow bandwidth, you may want to disable the auto-population in order to save the connection&#39;s bandwidth for the application communication.&lt;/p&gt;&lt;p&gt;If you are ingesting logs using the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/libraries#write_standard_logs&#34;&gt;write()&lt;/a&gt; method of the Logging interface&lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Logging.java#L1209&#34; target=&#34;_blank&#34;&gt;,&lt;/a&gt; you can configure the LoggingOptions argument to disable the auto-population:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;LoggingOptions options = LoggingOptions.newBuilder()\r\n .setAutoPopulateMetadata(false).build();\r\nLogging logging = options.getService();&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;If you are using Java Logging, you can disable auto population by adding the following to your logging.properties file:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;com.google.cloud.logging.LoggingHandler.autoPopulateMetadata=false&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;If you are using Logback framework, you can disable auto population by adding the following to your Logback configuration:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;&amp;lt;autoPopulateMetadata&amp;gt;false&amp;lt;/autoPopulateMetadata&amp;gt;&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;How the current context is populated&lt;/h3&gt;&lt;p&gt;Rich query and display capabilities of Log Explorer such as displaying correlated logs use the log entries&#39; fields such as httpRequest and trace. The new version of the library uses the &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java&#34; target=&#34;_blank&#34;&gt;Context&lt;/a&gt; class to store the information about the HTTP request and tracing data in the current application context. The context&#39;s scope is per thread. Before the library ingests logs into Cloud Logging, it reads the HTTP request and tracing information from the current context and sets the respective fields in the log entries. The fields are populated only if the caller did not explicitly provide values in these fields. Using the &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/ContextHandler.java&#34; target=&#34;_blank&#34;&gt;ContextHandler&lt;/a&gt; class you can setup the HTTP request and tracing data of the current context:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;import com.google.cloud.logging.HttpRequest;\r\n// . . .\r\nHttpRequest request;\r\n// . . .\r\nContextHandler ctxHandler = new ContextHandler();\r\nContext ctx = Context.newBuilder()\r\n .setRequest(request)\r\n .setTraceId(traceId)\r\n .setSpanId(spanId)\r\n .build();\r\nctxHandler.setCurrentContext(ctx);&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;After the context is set all logs that will be ingested in the same scope as the context will be populated with the HTTP request and tracing information that was set in the current context. The &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java&#34; target=&#34;_blank&#34;&gt;Context&lt;/a&gt; class can setup the HTTP request using partial data such as URL or request method:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;import com.google.cloud.logging.HttpRequest.RequestMethod;\r\n// . . .\r\nContextHandler ctxHandler = new ContextHandler();\r\nContext ctx = Context.newBuilder()\r\n .setRequestUrl(&#34;https://example.com/info&#34;)\r\n .setRequestMethod(RequestMethod.GET);\r\n .build();\r\nctxHandler.setCurrentContext(ctx);&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The builder of the Context class also supports setting the tracing information from the parsed values of the &lt;a href=&#34;https://cloud.google.com/trace/docs/setup#force-trace&#34;&gt;Google tracing context&lt;/a&gt; and  &lt;a href=&#34;https://www.w3.org/TR/trace-context/&#34; target=&#34;_blank&#34;&gt;W3C tracing context&lt;/a&gt; strings using the methods &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L116&#34; target=&#34;_blank&#34;&gt;loadCloudTraceContext()&lt;/a&gt; and &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L149&#34; target=&#34;_blank&#34;&gt;loadW3CTraceParentContext()&lt;/a&gt; respectively.&lt;/p&gt;&lt;p&gt;Implementation of the context population can be a complex task. Java Web servers support asynchronous execution of the request handlers. To manage the context in the right scope may require in-depth knowledge of specific implementation details about each Web server. The new version of the Logging library provides a simple way to automate the process of the current context management, saving you the effort of implementing the code by themselves. The automation supports all Web servers that are based on the Jakarta servlets such as &lt;a href=&#34;https://tomcat.apache.org/&#34; target=&#34;_blank&#34;&gt;Tomcat&lt;/a&gt;, &lt;a href=&#34;https://www.eclipse.org/jetty/&#34; target=&#34;_blank&#34;&gt;Jetty&lt;/a&gt; or &lt;a href=&#34;https://undertow.io/&#34; target=&#34;_blank&#34;&gt;Undertow&lt;/a&gt;. The current implementation supports Jakarta servlets version ≥ 4.0.4. The implementation is added to the new &lt;a href=&#34;https://github.com/googleapis/java-logging-servlet-initializer&#34; target=&#34;_blank&#34;&gt;google-cloud-logging-servlet-initializer&lt;/a&gt; package. All that you have to do to enable automatic capturing of the current context is to add the package to your application.&lt;/p&gt;If you are using &lt;a href=&#34;https://maven.apache.org/&#34; target=&#34;_blank&#34;&gt;Maven&lt;/a&gt; add the following to your pom.xml:&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;&amp;lt;dependency&amp;gt;\r\n &amp;lt;groupId&amp;gt;com.google.cloud&amp;lt;/groupId&amp;gt;\r\n &amp;lt;artifactId&amp;gt;google-cloud-logging-servlet-initializer&amp;lt;/artifactId&amp;gt;\r\n &amp;lt;version&amp;gt;0.1.7-alpha&amp;lt;/version&amp;gt;\r\n &amp;lt;type&amp;gt;pom&amp;lt;/type&amp;gt;\r\n&amp;lt;/dependency&amp;gt;&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;If you are using &lt;a href=&#34;https://gradle.org/&#34; target=&#34;_blank&#34;&gt;Gradle&lt;/a&gt;, add the following to your dependencies:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#34;implementation &#39;com.google.cloud:google-cloud-logging-servlet-initializer:0.1.7-alpha&#39;&#34;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The added package uses the Java&#39;s &lt;a href=&#34;https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html&#34; target=&#34;_blank&#34;&gt;Service Provider Interface&lt;/a&gt; to register the &lt;a href=&#34;https://github.com/googleapis/java-logging-servlet-initializer/blob/5076b0cc81fd1c0c3b39c6add17a0c25c38c7ece/src/main/java/com/google/cloud/logging/servlet/ContextCaptureInitializer.java&#34; target=&#34;_blank&#34;&gt;ContextCaptureInitializer&lt;/a&gt; class which integrates into the servlet pipeline to capture information about current HTTP requests. The information is parsed to populate the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#HttpRequest&#34;&gt;HttpRequest&lt;/a&gt; structure. It also parses the request&#39;s headers to retrieve tracing information. It supports &#34;&lt;code&gt;x-cloud-trace-context&lt;/code&gt;&#34; (&lt;a href=&#34;https://cloud.google.com/trace/docs/setup#force-trace&#34;&gt;Google tracing context&lt;/a&gt;) and &#34;traceparent&#34; (&lt;a href=&#34;https://www.w3.org/TR/trace-context/&#34; target=&#34;_blank&#34;&gt;W3C tracing context&lt;/a&gt;) headers.&lt;/p&gt;&lt;h3&gt;Use Logging library with logging agents&lt;/h3&gt;&lt;p&gt;Many applications utilize logging capabilities of the Google Cloud managed services. The applications output their logs to &lt;code&gt;stdout&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt;, and the logs are ingested into Cloud Logging by &lt;a href=&#34;https://cloud.google.com/logging/docs/agent&#34;&gt;Logging agents&lt;/a&gt; or the Cloud managed services with the logging agent capabilities. This approach benefits from asynchronous log processing that does not consume application resources. The drawback of the approach is that if you want to populate fields in the structured logs or provide the structured payload, they have to format their output following the special &lt;a href=&#34;https://cloud.google.com/logging/docs/structured-logging#special-payload-fields&#34;&gt;Json format&lt;/a&gt; that the logging agents can parse. Also, while the logging agents can detect and populate the resource information about the managed environment, they cannot help with auto population of other fields of the log entry such as traceId or sourceLocation.&lt;/p&gt;&lt;p&gt;The new release of the Logging library for Java introduces the support for logging agents in both of its Java Logging and Logback integrations. Now the library&#39;s users can instruct the appropriate handler to redirect the log writing to stdout instead of Logging API.&lt;/p&gt;&lt;p&gt;If you are using Java Logging, add the following to your logging.properties file:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;com.google.cloud.logging.LoggingHandler.redirectToStdout=true&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;If you are using Logback, add the following to the Logback configuration:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;dl&gt;&lt;dt&gt;code_block&lt;/dt&gt;&lt;dd&gt;[StructValue([(u&#39;code&#39;, u&#39;&amp;lt;redirectToStdout&amp;gt;true&amp;lt;/redirectToStdout&amp;gt;&#39;), (u&#39;language&#39;, u&#39;&#39;)])]&lt;/dd&gt;&lt;/dl&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;By default, both &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LoggingHandler.java&#34; target=&#34;_blank&#34;&gt;LoggingHandler&lt;/a&gt; and &lt;a href=&#34;https://github.com/googleapis/java-logging-logback/blob/6ce7950d4fe380c6f4f785660af80373c136352a/src/main/java/com/google/cloud/logging/logback/LoggingAppender.java&#34; target=&#34;_blank&#34;&gt;LoggingAppender&lt;/a&gt; write logs by calling the Logging API. You have to add the above configurations to make them utilize the logging agents for the log ingestion.&lt;/p&gt;&lt;h3&gt;Some limitations of using Logging Agents&lt;/h3&gt;&lt;p&gt;When configuring the library&#39;s Java Logging handler or Logback adapter to redirect log writing to stdout, you should be aware of the constraints that the use of logging agents implies.&lt;/p&gt;&lt;p&gt;Google Cloud managed services (e.g. GKE) automatically install logging agents in the resources that they provision. For example, a GKE cluster has a logging agent installed in each worker node (GCE instance) of the cluster. As a result, logging agents are constrained with the resource they run and do not support customization of the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource&#34;&gt;resource&lt;/a&gt; field of the ingested log entries.&lt;/p&gt;&lt;p&gt;Additionally, the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.log_name&#34;&gt;logName&lt;/a&gt; of all ingested logs is defined by the agent and cannot be changed*. It means that the application cannot define the log name or where the log entry will be stored (a.k.a. &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LogDestinationName.java&#34; target=&#34;_blank&#34;&gt;log&#39;s destination name&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;If it is essential for you to define a custom resource type or to control to which project the logs will be routed and/or the log name, you should not redirect the log writing to standard output.&lt;/p&gt;&lt;p&gt;* It is possible to customize the log name (but not the destination) by &lt;a href=&#34;https://cloud.google.com/logging/docs/agent/logging/configuration#configure&#34;&gt;customizing the Logging agent&#39;s configuration&lt;/a&gt; in GCE instances by defining the name as the &#34;tag&#34;.&lt;/p&gt;&lt;h3&gt;What is next&lt;/h3&gt;&lt;p&gt;Let&#39;s recap the benefits of upgrading your logging client to the latest version.&lt;/p&gt;&lt;p&gt;Use the new Logging library if you need log correlation capabilities of Log Explorer or forward Cloud Logging structured logs to external solutions and use the data in the auto-populated fields.&lt;/p&gt;&lt;p&gt;Use the &lt;a href=&#34;https://github.com/googleapis/java-logging-servlet-initializer&#34; target=&#34;_blank&#34;&gt;google-cloud-logging-servlet-initializer&lt;/a&gt; package to automate the context management if you run a request based application that uses Jakarta servlets. Note that it will not work with legacy Java EE servlets or Web servers that are not based on Java servlets such as &lt;a href=&#34;https://netty.io/&#34; target=&#34;_blank&#34;&gt;Netty&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;If you run your application in the Google Cloud serverless environments like Cloud Run or Cloud Functions, consider using Java Logging or Logback with the configuration that redirects formatted logs to standard output like it is described in the previous section. Leveraging logging agents for ingesting logs resolves some reliability problems about asynchronous log ingestion such as &lt;a href=&#34;https://cloud.google.com/run/docs/configuring/cpu-allocation&#34;&gt;CPU throttling&lt;/a&gt; on Cloud Run or no grace period in Cloud Functions.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-logging-python-client-library-v3-0-0-release/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/logging.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Getting Started with Google Cloud Logging Python v3.0.0&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Learn how to manage your app&#39;s Python logs and related metadata using Google Cloud client libraries.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-content-stream-block><div><paragraph-block _nghost-c74=""><div _ngcontent-c74="" innerhtml="&lt;p&gt;Today it is even easier to capture logs in your Java applications. Developers can get more data with their application logs using a new version of the Cloud Logging client library for Java. The library populates the current executing context implicitly with every ingested log entry. Read this if you want to learn how to get HTTP requests and tracing information and additional metadata in your logs without writing a single line of code.&lt;/p&gt;&lt;p&gt;There are three ways to ingest log data into Google Cloud Logging:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Develop a proprietary solution that directly calls the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/api-overview&#34;&gt;Logging API&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Leverage logging capabilities of the Google Cloud managed environments like &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#overview&#34;&gt;GKE&lt;/a&gt; or install Google Cloud &lt;a href=&#34;https://cloud.google.com/monitoring/agent&#34;&gt;Ops agent&lt;/a&gt; and print your application logs to stdout and stderr.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use Google Cloud &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/libraries&#34;&gt;Logging client library&lt;/a&gt; in one of many supported programming languages.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The library provides you with ready to use boilerplate constructs built following the best practices of using Logging API. Java applications can use the Google Cloud Logging library to ingest logs using the integrations with &lt;a href=&#34;https://docs.oracle.com/javase/10/core/java-logging-overview.htm&#34; target=&#34;_blank&#34;&gt;Java Logging&lt;/a&gt; and &lt;a href=&#34;https://logback.qos.ch/&#34; target=&#34;_blank&#34;&gt;Logback framework&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;If you are new to using Google Logging client libraries for Java, follow the steps to &lt;a href=&#34;https://cloud.google.com/logging/docs/setup/java&#34;&gt;set up Cloud Logging for Java&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/libraries&#34;&gt;get started&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In the version 3.6 release of the the Logging client library for Java you get many long demanding features including automatic population of the metadata about the environment&#39;s resource supporting Cloud Run and Cloud Functions, HTTP request contextual information, tracing correlation that enables displaying grouped log entries in Logs Explorer and more. This release of the library is composed of the three packages:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/googleapis/java-logging&#34; target=&#34;_blank&#34;&gt;google-cloud-logging&lt;/a&gt; -- provides the hand-written layer above Cloud Logging API and the integration with legacy &lt;a href=&#34;https://docs.oracle.com/javase/10/core/java-logging-overview.htm&#34; target=&#34;_blank&#34;&gt;Java Logging&lt;/a&gt; solution.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/googleapis/java-logging-logback&#34; target=&#34;_blank&#34;&gt;google-cloud-logging-logback&lt;/a&gt; is the integration with the Logback framework and ingests logs using the google-cloud-logging package.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/googleapis/java-logging-servlet-initializer&#34; target=&#34;_blank&#34;&gt;google-cloud-logging-servlet-initializer&lt;/a&gt; is a new addition to the library; it provides integration with servlet-based Web applications.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The features are available in the versions &amp;#8805;3.6.3 and &amp;#8805;0.123.3-alpha of the google-cloud-logging and google-cloud-logging-logback packages respectively.&lt;/p&gt;&lt;p&gt;If you are using &lt;a href=&#34;https://maven.apache.org/&#34; target=&#34;_blank&#34;&gt;Maven&lt;/a&gt;, update the packages&#39; versions in the pom.xml:&lt;/p&gt;"><p>Today it is even easier to capture logs in your Java applications. Developers can get more data with their application logs using a new version of the Cloud Logging client library for Java. The library populates the current executing context implicitly with every ingested log entry. Read this if you want to learn how to get HTTP requests and tracing information and additional metadata in your logs without writing a single line of code.</p><p>There are three ways to ingest log data into Google Cloud Logging:</p><ul><li><p>Develop a proprietary solution that directly calls the <a href="https://cloud.google.com/logging/docs/reference/api-overview" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/api-overview" track-metadata-module="post">Logging API</a>.</p></li><li><p>Leverage logging capabilities of the Google Cloud managed environments like <a href="https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#overview" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#overview" track-metadata-module="post">GKE</a> or install Google Cloud <a href="https://cloud.google.com/monitoring/agent" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/monitoring/agent" track-metadata-module="post">Ops agent</a> and print your application logs to stdout and stderr.</p></li><li><p>Use Google Cloud <a href="https://cloud.google.com/logging/docs/reference/libraries" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/libraries" track-metadata-module="post">Logging client library</a> in one of many supported programming languages.</p></li></ul><p>The library provides you with ready to use boilerplate constructs built following the best practices of using Logging API. Java applications can use the Google Cloud Logging library to ingest logs using the integrations with <a href="https://docs.oracle.com/javase/10/core/java-logging-overview.htm" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://docs.oracle.com" track-metadata-module="post">Java Logging</a> and <a href="https://logback.qos.ch/" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://logback.qos.ch" track-metadata-module="post">Logback framework</a>.</p><p>If you are new to using Google Logging client libraries for Java, follow the steps to <a href="https://cloud.google.com/logging/docs/setup/java" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/logging/docs/setup/java" track-metadata-module="post">set up Cloud Logging for Java</a> and <a href="https://cloud.google.com/logging/docs/reference/libraries" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/libraries" track-metadata-module="post">get started</a>.</p><p>In the version 3.6 release of the the Logging client library for Java you get many long demanding features including automatic population of the metadata about the environment&#39;s resource supporting Cloud Run and Cloud Functions, HTTP request contextual information, tracing correlation that enables displaying grouped log entries in Logs Explorer and more. This release of the library is composed of the three packages:</p><ul><li><p><a href="https://github.com/googleapis/java-logging" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://github.com" track-metadata-module="post">google-cloud-logging</a> -- provides the hand-written layer above Cloud Logging API and the integration with legacy <a href="https://docs.oracle.com/javase/10/core/java-logging-overview.htm" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://docs.oracle.com" track-metadata-module="post">Java Logging</a> solution.</p></li><li><p><a href="https://github.com/googleapis/java-logging-logback" target="_blank" track-type="inline link" track-name="11" track-metadata-eventdetail="https://github.com" track-metadata-module="post">google-cloud-logging-logback</a> is the integration with the Logback framework and ingests logs using the google-cloud-logging package.</p></li><li><p><a href="https://github.com/googleapis/java-logging-servlet-initializer" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="https://github.com" track-metadata-module="post">google-cloud-logging-servlet-initializer</a> is a new addition to the library; it provides integration with servlet-based Web applications.</p></li></ul><p>The features are available in the versions ≥3.6.3 and ≥0.123.3-alpha of the google-cloud-logging and google-cloud-logging-logback packages respectively.</p><p>If you are using <a href="https://maven.apache.org/" target="_blank" track-type="inline link" track-name="13" track-metadata-eventdetail="https://maven.apache.org" track-metadata-module="post">Maven</a>, update the packages&#39; versions in the pom.xml:</p></div></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">&lt;dependency&gt;
</code><code _ngcontent-c75="">  &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
</code><code _ngcontent-c75="">  &lt;artifactId&gt;google-cloud-logging&lt;/artifactId&gt;
</code><code _ngcontent-c75="">  &lt;version&gt;3.6.3&lt;/version&gt;
</code><code _ngcontent-c75="">&lt;/dependency&gt;
</code><code _ngcontent-c75="">&lt;dependency&gt;
</code><code _ngcontent-c75="">  &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
</code><code _ngcontent-c75="">  &lt;artifactId&gt;google-cloud-logging-logback&lt;/artifactId&gt;
</code><code _ngcontent-c75="">  &lt;version&gt;0.123.3-alpha&lt;/version&gt;
</code><code _ngcontent-c75="">&lt;/dependency&gt;</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><p>If you are using <a href="https://gradle.org/" target="_blank" track-type="inline link" track-name="14" track-metadata-eventdetail="https://gradle.org" track-metadata-module="post">Gradle</a>, , update your dependencies:</p></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">implementation &#39;com.google.cloud:google-cloud-logging:3.6.3&#39;
</code><code _ngcontent-c75="">implementation &#39;com.google.cloud:google-cloud-logging-logback:0.123.3-alpha&#39;</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><div _ngcontent-c74="" innerhtml="&lt;p&gt;You can use the official Google Cloud BOM version 0.167.0 that includes the new releases of the packages.&lt;/p&gt;&lt;h3&gt;What is new&lt;/h3&gt;&lt;p&gt;The Java library inserts structured information about the executing environment including resource types, HTTP request metadata, tracing and more. Using the library you can write your payloads in one of the three formats:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A text provided as a Java string&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A JSON object provided as an instance of &lt;a href=&#34;https://docs.oracle.com/javase/8/docs/api/java/util/Map.html&#34; target=&#34;_blank&#34;&gt;Map&amp;lt;String, ?&amp;gt;&lt;/a&gt; or &lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Struct&#34; target=&#34;_blank&#34;&gt;Struct&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A protobuf object provided as an instance of &lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Any&#34; target=&#34;_blank&#34;&gt;Any&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;You can use the structured logs with enhanced filtering in Logs Explorer to observe and troubleshoot their applications. The Logs Explorer uses structured logs to establish &lt;a href=&#34;https://cloud.google.com/trace/docs/trace-log-integration&#34;&gt;correlations&lt;/a&gt; between traces and logs and to group together logs that belong to the same transaction. The correlated &amp;#34;child&amp;#34; logs are displayed &amp;#34;under&amp;#34; the entry of the &amp;#34;parent&amp;#34; log:&lt;/p&gt;"><p>You can use the official Google Cloud BOM version 0.167.0 that includes the new releases of the packages.</p><h3>What is new</h3><p>The Java library inserts structured information about the executing environment including resource types, HTTP request metadata, tracing and more. Using the library you can write your payloads in one of the three formats:</p><ul><li><p>A text provided as a Java string</p></li><li><p>A JSON object provided as an instance of <a href="https://docs.oracle.com/javase/8/docs/api/java/util/Map.html" target="_blank" track-type="inline link" track-name="15" track-metadata-eventdetail="https://docs.oracle.com" track-metadata-module="post">Map&lt;String, ?&gt;</a> or <a href="https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Struct" target="_blank" track-type="inline link" track-name="16" track-metadata-eventdetail="https://developers.google.com" track-metadata-module="post">Struct</a></p></li><li><p>A protobuf object provided as an instance of <a href="https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Any" target="_blank" track-type="inline link" track-name="17" track-metadata-eventdetail="https://developers.google.com" track-metadata-module="post">Any</a></p></li></ul><p>You can use the structured logs with enhanced filtering in Logs Explorer to observe and troubleshoot their applications. The Logs Explorer uses structured logs to establish <a href="https://cloud.google.com/trace/docs/trace-log-integration" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/trace/docs/trace-log-integration" track-metadata-module="post">correlations</a> between traces and logs and to group together logs that belong to the same transaction. The correlated &#34;child&#34; logs are displayed &#34;under&#34; the entry of the &#34;parent&#34; log:</p></div></paragraph-block></div><div><paragraph-block _nghost-c74=""><p>With the previous versions of the Logging library you had to write code to explicitly populate these fields. For example, developers that use Logback framework had to write a code like below to populate the <a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace" track-type="inline link" track-name="19" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace" track-metadata-module="post">trace</a> field of the ingested logs:</p></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">// . . .
</code><code _ngcontent-c75="">String traceInfo = request.getHeader(&#34;x-cloud-trace-context&#34;);
</code><code _ngcontent-c75="">TraceLoggingEventEnhancer.setCurrentTraceId(traceInfo);
</code><code _ngcontent-c75="">// . . .</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><div _ngcontent-c74="" innerhtml="&lt;p&gt;And to invoke this code at the beginning of each transaction.&lt;/p&gt;&lt;p&gt;The new features of the Logging library makes implementing the population logic unnecessary. The new version of the library supports automatic population of following log entry fields:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource&#34;&gt;resource&lt;/a&gt; &amp;#8210; describes the resource type and its attributes where the application is running. Along with GCE instances, it supports Google Cloud managed services such as GKE, AppEngine (both Standard and Flexible), Cloud Run and Cloud Functions.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.http_request&#34;&gt;httpRequest&lt;/a&gt; &amp;#8210; captures info about HTTP requests from the current application&#39;s context. The context is defined per-thread and can be populated both explicitly in the application code or implicitly from the &lt;a href=&#34;https://docs.oracle.com/cd/E26180_01/Platform.94/ATGProgGuide/html/s0801requesthandlingwithservletpipeli01.html&#34; target=&#34;_blank&#34;&gt;Jakarta servlet requests pipeline&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace&#34;&gt;trace&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.span_id&#34;&gt;spanId&lt;/a&gt; &amp;#8210; reads the tracing data from the HTTP request header. The tracing data assists in correlating multiple logs that belong to the same transaction.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.source_location&#34;&gt;sourceLocation&lt;/a&gt; &amp;#8210; stores info about the class and method names as well as the line of code where the application called the log ingestion method. The library retrieves the data by traversing the trace stack up until the first entry that is not part of the Logging library code or the system package.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;What is left to you is to set the payload and relevant payload&#39;s metadata labels. The only field in the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry&#34;&gt;log entry&lt;/a&gt; that the library does not automatically populate now is the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.operation&#34;&gt;operation&lt;/a&gt; field.&lt;/p&gt;&lt;h3&gt;Disable information auto-population in log entries&lt;/h3&gt;&lt;p&gt;You have full control over the auto-population functionality. The auto-population is enabled by default for your convenience. But in certain scenarios it can be desirable to disable it. For example, if your application is log intensive and has a narrow bandwidth, you may want to disable the auto-population in order to save the connection&#39;s bandwidth for the application communication.&lt;/p&gt;&lt;p&gt;If you are ingesting logs using the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/libraries#write_standard_logs&#34;&gt;write()&lt;/a&gt; method of the Logging interface&lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Logging.java#L1209&#34; target=&#34;_blank&#34;&gt;,&lt;/a&gt; you can configure the LoggingOptions argument to disable the auto-population:&lt;/p&gt;"><p>And to invoke this code at the beginning of each transaction.</p><p>The new features of the Logging library makes implementing the population logic unnecessary. The new version of the library supports automatic population of following log entry fields:</p><ul><li><p><a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource" track-type="inline link" track-name="20" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource" track-metadata-module="post">resource</a> ‒ describes the resource type and its attributes where the application is running. Along with GCE instances, it supports Google Cloud managed services such as GKE, AppEngine (both Standard and Flexible), Cloud Run and Cloud Functions.</p></li><li><p><a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.http_request" track-type="inline link" track-name="21" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.http_request" track-metadata-module="post">httpRequest</a> ‒ captures info about HTTP requests from the current application&#39;s context. The context is defined per-thread and can be populated both explicitly in the application code or implicitly from the <a href="https://docs.oracle.com/cd/E26180_01/Platform.94/ATGProgGuide/html/s0801requesthandlingwithservletpipeli01.html" target="_blank" track-type="inline link" track-name="22" track-metadata-eventdetail="https://docs.oracle.com" track-metadata-module="post">Jakarta servlet requests pipeline</a>.</p></li><li><p><a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace" track-metadata-module="post">trace</a> and <a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.span_id" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.span_id" track-metadata-module="post">spanId</a> ‒ reads the tracing data from the HTTP request header. The tracing data assists in correlating multiple logs that belong to the same transaction.</p></li><li><p><a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.source_location" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.source_location" track-metadata-module="post">sourceLocation</a> ‒ stores info about the class and method names as well as the line of code where the application called the log ingestion method. The library retrieves the data by traversing the trace stack up until the first entry that is not part of the Logging library code or the system package.</p></li></ul><p>What is left to you is to set the payload and relevant payload&#39;s metadata labels. The only field in the <a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry" track-metadata-module="post">log entry</a> that the library does not automatically populate now is the <a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.operation" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.operation" track-metadata-module="post">operation</a> field.</p><h3>Disable information auto-population in log entries</h3><p>You have full control over the auto-population functionality. The auto-population is enabled by default for your convenience. But in certain scenarios it can be desirable to disable it. For example, if your application is log intensive and has a narrow bandwidth, you may want to disable the auto-population in order to save the connection&#39;s bandwidth for the application communication.</p><p>If you are ingesting logs using the <a href="https://cloud.google.com/logging/docs/reference/libraries#write_standard_logs" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/libraries#write_standard_logs" track-metadata-module="post">write()</a> method of the Logging interface<a href="https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Logging.java#L1209" target="_blank" track-type="inline link" track-name="29" track-metadata-eventdetail="https://github.com" track-metadata-module="post">,</a> you can configure the LoggingOptions argument to disable the auto-population:</p></div></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">LoggingOptions options = LoggingOptions.newBuilder()
</code><code _ngcontent-c75="">    .setAutoPopulateMetadata(false).build();
</code><code _ngcontent-c75="">Logging logging = options.getService();</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><p>If you are using Java Logging, you can disable auto population by adding the following to your logging.properties file:</p></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">com.google.cloud.logging.LoggingHandler.autoPopulateMetadata=false</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><p>If you are using Logback framework, you can disable auto population by adding the following to your Logback configuration:</p></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">&lt;autoPopulateMetadata&gt;false&lt;/autoPopulateMetadata&gt;</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><div _ngcontent-c74="" innerhtml="&lt;h3&gt;How the current context is populated&lt;/h3&gt;&lt;p&gt;Rich query and display capabilities of Log Explorer such as displaying correlated logs use the log entries&#39; fields such as httpRequest and trace. The new version of the library uses the &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java&#34; target=&#34;_blank&#34;&gt;Context&lt;/a&gt; class to store the information about the HTTP request and tracing data in the current application context. The context&#39;s scope is per thread. Before the library ingests logs into Cloud Logging, it reads the HTTP request and tracing information from the current context and sets the respective fields in the log entries. The fields are populated only if the caller did not explicitly provide values in these fields. Using the &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/ContextHandler.java&#34; target=&#34;_blank&#34;&gt;ContextHandler&lt;/a&gt; class you can setup the HTTP request and tracing data of the current context:&lt;/p&gt;"><h3>How the current context is populated</h3><p>Rich query and display capabilities of Log Explorer such as displaying correlated logs use the log entries&#39; fields such as httpRequest and trace. The new version of the library uses the <a href="https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java" target="_blank" track-type="inline link" track-name="30" track-metadata-eventdetail="https://github.com" track-metadata-module="post">Context</a> class to store the information about the HTTP request and tracing data in the current application context. The context&#39;s scope is per thread. Before the library ingests logs into Cloud Logging, it reads the HTTP request and tracing information from the current context and sets the respective fields in the log entries. The fields are populated only if the caller did not explicitly provide values in these fields. Using the <a href="https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/ContextHandler.java" target="_blank" track-type="inline link" track-name="31" track-metadata-eventdetail="https://github.com" track-metadata-module="post">ContextHandler</a> class you can setup the HTTP request and tracing data of the current context:</p></div></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">import com.google.cloud.logging.HttpRequest;
</code><code _ngcontent-c75="">// . . .
</code><code _ngcontent-c75="">HttpRequest request;
</code><code _ngcontent-c75="">// . . .
</code><code _ngcontent-c75="">ContextHandler ctxHandler = new ContextHandler();
</code><code _ngcontent-c75="">Context ctx = Context.newBuilder()
</code><code _ngcontent-c75="">        .setRequest(request)
</code><code _ngcontent-c75="">        .setTraceId(traceId)
</code><code _ngcontent-c75="">        .setSpanId(spanId)
</code><code _ngcontent-c75="">        .build();
</code><code _ngcontent-c75="">ctxHandler.setCurrentContext(ctx);</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><p>After the context is set all logs that will be ingested in the same scope as the context will be populated with the HTTP request and tracing information that was set in the current context. The <a href="https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java" target="_blank" track-type="inline link" track-name="32" track-metadata-eventdetail="https://github.com" track-metadata-module="post">Context</a> class can setup the HTTP request using partial data such as URL or request method:</p></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">import com.google.cloud.logging.HttpRequest.RequestMethod;
</code><code _ngcontent-c75="">// . . .
</code><code _ngcontent-c75="">ContextHandler ctxHandler = new ContextHandler();
</code><code _ngcontent-c75="">Context ctx = Context.newBuilder()
</code><code _ngcontent-c75="">        .setRequestUrl(&#34;https://example.com/info&#34;)
</code><code _ngcontent-c75="">        .setRequestMethod(RequestMethod.GET);
</code><code _ngcontent-c75="">        .build();
</code><code _ngcontent-c75="">ctxHandler.setCurrentContext(ctx);</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><div _ngcontent-c74="" innerhtml="&lt;p&gt;The builder of the Context class also supports setting the tracing information from the parsed values of the &lt;a href=&#34;https://cloud.google.com/trace/docs/setup#force-trace&#34;&gt;Google tracing context&lt;/a&gt; and&amp;#160; &lt;a href=&#34;https://www.w3.org/TR/trace-context/&#34; target=&#34;_blank&#34;&gt;W3C tracing context&lt;/a&gt; strings using the methods &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L116&#34; target=&#34;_blank&#34;&gt;loadCloudTraceContext()&lt;/a&gt; and &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L149&#34; target=&#34;_blank&#34;&gt;loadW3CTraceParentContext()&lt;/a&gt; respectively.&lt;/p&gt;&lt;p&gt;Implementation of the context population can be a complex task. Java Web servers support asynchronous execution of the request handlers. To manage the context in the right scope may require in-depth knowledge of specific implementation details about each Web server. The new version of the Logging library provides a simple way to automate the process of the current context management, saving you the effort of implementing the code by themselves. The automation supports all Web servers that are based on the Jakarta servlets such as &lt;a href=&#34;https://tomcat.apache.org/&#34; target=&#34;_blank&#34;&gt;Tomcat&lt;/a&gt;, &lt;a href=&#34;https://www.eclipse.org/jetty/&#34; target=&#34;_blank&#34;&gt;Jetty&lt;/a&gt; or &lt;a href=&#34;https://undertow.io/&#34; target=&#34;_blank&#34;&gt;Undertow&lt;/a&gt;. The current implementation supports Jakarta servlets version &amp;#8805; 4.0.4. The implementation is added to the new &lt;a href=&#34;https://github.com/googleapis/java-logging-servlet-initializer&#34; target=&#34;_blank&#34;&gt;google-cloud-logging-servlet-initializer&lt;/a&gt; package. All that you have to do to enable automatic capturing of the current context is to add the package to your application.&lt;/p&gt;If you are using &lt;a href=&#34;https://maven.apache.org/&#34; target=&#34;_blank&#34;&gt;Maven&lt;/a&gt; add the following to your pom.xml:"><p>The builder of the Context class also supports setting the tracing information from the parsed values of the <a href="https://cloud.google.com/trace/docs/setup#force-trace" track-type="inline link" track-name="33" track-metadata-eventdetail="https://cloud.google.com/trace/docs/setup#force-trace" track-metadata-module="post">Google tracing context</a> and  <a href="https://www.w3.org/TR/trace-context/" target="_blank" track-type="inline link" track-name="34" track-metadata-eventdetail="https://www.w3.org" track-metadata-module="post">W3C tracing context</a> strings using the methods <a href="https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L116" target="_blank" track-type="inline link" track-name="35" track-metadata-eventdetail="https://github.com" track-metadata-module="post">loadCloudTraceContext()</a> and <a href="https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L149" target="_blank" track-type="inline link" track-name="36" track-metadata-eventdetail="https://github.com" track-metadata-module="post">loadW3CTraceParentContext()</a> respectively.</p><p>Implementation of the context population can be a complex task. Java Web servers support asynchronous execution of the request handlers. To manage the context in the right scope may require in-depth knowledge of specific implementation details about each Web server. The new version of the Logging library provides a simple way to automate the process of the current context management, saving you the effort of implementing the code by themselves. The automation supports all Web servers that are based on the Jakarta servlets such as <a href="https://tomcat.apache.org/" target="_blank" track-type="inline link" track-name="37" track-metadata-eventdetail="https://tomcat.apache.org" track-metadata-module="post">Tomcat</a>, <a href="https://www.eclipse.org/jetty/" target="_blank" track-type="inline link" track-name="38" track-metadata-eventdetail="https://www.eclipse.org" track-metadata-module="post">Jetty</a> or <a href="https://undertow.io/" target="_blank" track-type="inline link" track-name="39" track-metadata-eventdetail="https://undertow.io" track-metadata-module="post">Undertow</a>. The current implementation supports Jakarta servlets version ≥ 4.0.4. The implementation is added to the new <a href="https://github.com/googleapis/java-logging-servlet-initializer" target="_blank" track-type="inline link" track-name="40" track-metadata-eventdetail="https://github.com" track-metadata-module="post">google-cloud-logging-servlet-initializer</a> package. All that you have to do to enable automatic capturing of the current context is to add the package to your application.</p><p>If you are using <a href="https://maven.apache.org/" target="_blank" track-type="inline link" track-name="41" track-metadata-eventdetail="https://maven.apache.org" track-metadata-module="post">Maven</a> add the following to your pom.xml:</p></div></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">&lt;dependency&gt;
</code><code _ngcontent-c75="">  &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;
</code><code _ngcontent-c75="">  &lt;artifactId&gt;google-cloud-logging-servlet-initializer&lt;/artifactId&gt;
</code><code _ngcontent-c75="">  &lt;version&gt;0.1.7-alpha&lt;/version&gt;
</code><code _ngcontent-c75="">  &lt;type&gt;pom&lt;/type&gt;
</code><code _ngcontent-c75="">&lt;/dependency&gt;</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><p>If you are using <a href="https://gradle.org/" target="_blank" track-type="inline link" track-name="42" track-metadata-eventdetail="https://gradle.org" track-metadata-module="post">Gradle</a>, add the following to your dependencies:</p></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">implementation &#39;com.google.cloud:google-cloud-logging-servlet-initializer:0.1.7-alpha&#39;</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><div _ngcontent-c74="" innerhtml="&lt;p&gt;The added package uses the Java&#39;s &lt;a href=&#34;https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html&#34; target=&#34;_blank&#34;&gt;Service Provider Interface&lt;/a&gt; to register the &lt;a href=&#34;https://github.com/googleapis/java-logging-servlet-initializer/blob/5076b0cc81fd1c0c3b39c6add17a0c25c38c7ece/src/main/java/com/google/cloud/logging/servlet/ContextCaptureInitializer.java&#34; target=&#34;_blank&#34;&gt;ContextCaptureInitializer&lt;/a&gt; class which integrates into the servlet pipeline to capture information about current HTTP requests. The information is parsed to populate the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#HttpRequest&#34;&gt;HttpRequest&lt;/a&gt; structure. It also parses the request&#39;s headers to retrieve tracing information. It supports &amp;#34;&lt;code&gt;x-cloud-trace-context&lt;/code&gt;&amp;#34; (&lt;a href=&#34;https://cloud.google.com/trace/docs/setup#force-trace&#34;&gt;Google tracing context&lt;/a&gt;) and &amp;#34;traceparent&amp;#34; (&lt;a href=&#34;https://www.w3.org/TR/trace-context/&#34; target=&#34;_blank&#34;&gt;W3C tracing context&lt;/a&gt;) headers.&lt;/p&gt;&lt;h3&gt;Use Logging library with logging agents&lt;/h3&gt;&lt;p&gt;Many applications utilize logging capabilities of the Google Cloud managed services. The applications output their logs to &lt;code&gt;stdout&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt;, and the logs are ingested into Cloud Logging by &lt;a href=&#34;https://cloud.google.com/logging/docs/agent&#34;&gt;Logging agents&lt;/a&gt; or the Cloud managed services with the logging agent capabilities. This approach benefits from asynchronous log processing that does not consume application resources. The drawback of the approach is that if you want to populate fields in the structured logs or provide the structured payload, they have to format their output following the special &lt;a href=&#34;https://cloud.google.com/logging/docs/structured-logging#special-payload-fields&#34;&gt;Json format&lt;/a&gt; that the logging agents can parse. Also, while the logging agents can detect and populate the resource information about the managed environment, they cannot help with auto population of other fields of the log entry such as traceId or sourceLocation.&lt;/p&gt;&lt;p&gt;The new release of the Logging library for Java introduces the support for logging agents in both of its Java Logging and Logback integrations. Now the library&#39;s users can instruct the appropriate handler to redirect the log writing to stdout instead of Logging API.&lt;/p&gt;&lt;p&gt;If you are using Java Logging, add the following to your logging.properties file:&lt;/p&gt;"><p>The added package uses the Java&#39;s <a href="https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html" target="_blank" track-type="inline link" track-name="43" track-metadata-eventdetail="https://docs.oracle.com" track-metadata-module="post">Service Provider Interface</a> to register the <a href="https://github.com/googleapis/java-logging-servlet-initializer/blob/5076b0cc81fd1c0c3b39c6add17a0c25c38c7ece/src/main/java/com/google/cloud/logging/servlet/ContextCaptureInitializer.java" target="_blank" track-type="inline link" track-name="44" track-metadata-eventdetail="https://github.com" track-metadata-module="post">ContextCaptureInitializer</a> class which integrates into the servlet pipeline to capture information about current HTTP requests. The information is parsed to populate the <a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#HttpRequest" track-type="inline link" track-name="45" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#HttpRequest" track-metadata-module="post">HttpRequest</a> structure. It also parses the request&#39;s headers to retrieve tracing information. It supports &#34;<code>x-cloud-trace-context</code>&#34; (<a href="https://cloud.google.com/trace/docs/setup#force-trace" track-type="inline link" track-name="46" track-metadata-eventdetail="https://cloud.google.com/trace/docs/setup#force-trace" track-metadata-module="post">Google tracing context</a>) and &#34;traceparent&#34; (<a href="https://www.w3.org/TR/trace-context/" target="_blank" track-type="inline link" track-name="47" track-metadata-eventdetail="https://www.w3.org" track-metadata-module="post">W3C tracing context</a>) headers.</p><h3>Use Logging library with logging agents</h3><p>Many applications utilize logging capabilities of the Google Cloud managed services. The applications output their logs to <code>stdout</code> and <code>stderr</code>, and the logs are ingested into Cloud Logging by <a href="https://cloud.google.com/logging/docs/agent" track-type="inline link" track-name="48" track-metadata-eventdetail="https://cloud.google.com/logging/docs/agent" track-metadata-module="post">Logging agents</a> or the Cloud managed services with the logging agent capabilities. This approach benefits from asynchronous log processing that does not consume application resources. The drawback of the approach is that if you want to populate fields in the structured logs or provide the structured payload, they have to format their output following the special <a href="https://cloud.google.com/logging/docs/structured-logging#special-payload-fields" track-type="inline link" track-name="49" track-metadata-eventdetail="https://cloud.google.com/logging/docs/structured-logging#special-payload-fields" track-metadata-module="post">Json format</a> that the logging agents can parse. Also, while the logging agents can detect and populate the resource information about the managed environment, they cannot help with auto population of other fields of the log entry such as traceId or sourceLocation.</p><p>The new release of the Logging library for Java introduces the support for logging agents in both of its Java Logging and Logback integrations. Now the library&#39;s users can instruct the appropriate handler to redirect the log writing to stdout instead of Logging API.</p><p>If you are using Java Logging, add the following to your logging.properties file:</p></div></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">com.google.cloud.logging.LoggingHandler.redirectToStdout=true</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><p>If you are using Logback, add the following to the Logback configuration:</p></paragraph-block></div><div><article-code-block _nghost-c75=""><pre _ngcontent-c75="">  <code _ngcontent-c75="">&lt;redirectToStdout&gt;true&lt;/redirectToStdout&gt;</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c74=""><div _ngcontent-c74="" innerhtml="&lt;p&gt;By default, both &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LoggingHandler.java&#34; target=&#34;_blank&#34;&gt;LoggingHandler&lt;/a&gt; and &lt;a href=&#34;https://github.com/googleapis/java-logging-logback/blob/6ce7950d4fe380c6f4f785660af80373c136352a/src/main/java/com/google/cloud/logging/logback/LoggingAppender.java&#34; target=&#34;_blank&#34;&gt;LoggingAppender&lt;/a&gt; write logs by calling the Logging API. You have to add the above configurations to make them utilize the logging agents for the log ingestion.&lt;/p&gt;&lt;h3&gt;Some limitations of using Logging Agents&lt;/h3&gt;&lt;p&gt;When configuring the library&#39;s Java Logging handler or Logback adapter to redirect log writing to stdout, you should be aware of the constraints that the use of logging agents implies.&lt;/p&gt;&lt;p&gt;Google Cloud managed services (e.g. GKE) automatically install logging agents in the resources that they provision. For example, a GKE cluster has a logging agent installed in each worker node (GCE instance) of the cluster. As a result, logging agents are constrained with the resource they run and do not support customization of the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource&#34;&gt;resource&lt;/a&gt; field of the ingested log entries.&lt;/p&gt;&lt;p&gt;Additionally, the &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.log_name&#34;&gt;logName&lt;/a&gt; of all ingested logs is defined by the agent and cannot be changed*. It means that the application cannot define the log name or where the log entry will be stored (a.k.a. &lt;a href=&#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LogDestinationName.java&#34; target=&#34;_blank&#34;&gt;log&#39;s destination name&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;If it is essential for you to define a custom resource type or to control to which project the logs will be routed and/or the log name, you should not redirect the log writing to standard output.&lt;/p&gt;&lt;p&gt;* It is possible to customize the log name (but not the destination) by &lt;a href=&#34;https://cloud.google.com/logging/docs/agent/logging/configuration#configure&#34;&gt;customizing the Logging agent&#39;s configuration&lt;/a&gt; in GCE instances by defining the name as the &amp;#34;tag&amp;#34;.&lt;/p&gt;&lt;h3&gt;What is next&lt;/h3&gt;&lt;p&gt;Let&#39;s recap the benefits of upgrading your logging client to the latest version.&lt;/p&gt;&lt;p&gt;Use the new Logging library if you need log correlation capabilities of Log Explorer or forward Cloud Logging structured logs to external solutions and use the data in the auto-populated fields.&lt;/p&gt;&lt;p&gt;Use the &lt;a href=&#34;https://github.com/googleapis/java-logging-servlet-initializer&#34; target=&#34;_blank&#34;&gt;google-cloud-logging-servlet-initializer&lt;/a&gt; package to automate the context management if you run a request based application that uses Jakarta servlets. Note that it will not work with legacy Java EE servlets or Web servers that are not based on Java servlets such as &lt;a href=&#34;https://netty.io/&#34; target=&#34;_blank&#34;&gt;Netty&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;If you run your application in the Google Cloud serverless environments like Cloud Run or Cloud Functions, consider using Java Logging or Logback with the configuration that redirects formatted logs to standard output like it is described in the previous section. Leveraging logging agents for ingesting logs resolves some reliability problems about asynchronous log ingestion such as &lt;a href=&#34;https://cloud.google.com/run/docs/configuring/cpu-allocation&#34;&gt;CPU throttling&lt;/a&gt; on Cloud Run or no grace period in Cloud Functions.&lt;/p&gt;"><p>By default, both <a href="https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LoggingHandler.java" target="_blank" track-type="inline link" track-name="50" track-metadata-eventdetail="https://github.com" track-metadata-module="post">LoggingHandler</a> and <a href="https://github.com/googleapis/java-logging-logback/blob/6ce7950d4fe380c6f4f785660af80373c136352a/src/main/java/com/google/cloud/logging/logback/LoggingAppender.java" target="_blank" track-type="inline link" track-name="51" track-metadata-eventdetail="https://github.com" track-metadata-module="post">LoggingAppender</a> write logs by calling the Logging API. You have to add the above configurations to make them utilize the logging agents for the log ingestion.</p><h3>Some limitations of using Logging Agents</h3><p>When configuring the library&#39;s Java Logging handler or Logback adapter to redirect log writing to stdout, you should be aware of the constraints that the use of logging agents implies.</p><p>Google Cloud managed services (e.g. GKE) automatically install logging agents in the resources that they provision. For example, a GKE cluster has a logging agent installed in each worker node (GCE instance) of the cluster. As a result, logging agents are constrained with the resource they run and do not support customization of the <a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource" track-type="inline link" track-name="52" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource" track-metadata-module="post">resource</a> field of the ingested log entries.</p><p>Additionally, the <a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.log_name" track-type="inline link" track-name="53" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.log_name" track-metadata-module="post">logName</a> of all ingested logs is defined by the agent and cannot be changed*. It means that the application cannot define the log name or where the log entry will be stored (a.k.a. <a href="https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LogDestinationName.java" target="_blank" track-type="inline link" track-name="54" track-metadata-eventdetail="https://github.com" track-metadata-module="post">log&#39;s destination name</a>).</p><p>If it is essential for you to define a custom resource type or to control to which project the logs will be routed and/or the log name, you should not redirect the log writing to standard output.</p><p>* It is possible to customize the log name (but not the destination) by <a href="https://cloud.google.com/logging/docs/agent/logging/configuration#configure" track-type="inline link" track-name="55" track-metadata-eventdetail="https://cloud.google.com/logging/docs/agent/logging/configuration#configure" track-metadata-module="post">customizing the Logging agent&#39;s configuration</a> in GCE instances by defining the name as the &#34;tag&#34;.</p><h3>What is next</h3><p>Let&#39;s recap the benefits of upgrading your logging client to the latest version.</p><p>Use the new Logging library if you need log correlation capabilities of Log Explorer or forward Cloud Logging structured logs to external solutions and use the data in the auto-populated fields.</p><p>Use the <a href="https://github.com/googleapis/java-logging-servlet-initializer" target="_blank" track-type="inline link" track-name="56" track-metadata-eventdetail="https://github.com" track-metadata-module="post">google-cloud-logging-servlet-initializer</a> package to automate the context management if you run a request based application that uses Jakarta servlets. Note that it will not work with legacy Java EE servlets or Web servers that are not based on Java servlets such as <a href="https://netty.io/" target="_blank" track-type="inline link" track-name="57" track-metadata-eventdetail="https://netty.io" track-metadata-module="post">Netty</a>.</p><p>If you run your application in the Google Cloud serverless environments like Cloud Run or Cloud Functions, consider using Java Logging or Logback with the configuration that redirects formatted logs to standard output like it is described in the previous section. Leveraging logging agents for ingesting logs resolves some reliability problems about asynchronous log ingestion such as <a href="https://cloud.google.com/run/docs/configuring/cpu-allocation" track-type="inline link" track-name="58" track-metadata-eventdetail="https://cloud.google.com/run/docs/configuring/cpu-allocation" track-metadata-module="post">CPU throttling</a> on Cloud Run or no grace period in Cloud Functions.</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Leonid Yankulin&lt;/name&gt;&lt;title&gt;Developer Relations Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Java_applications_logs.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 07 Mar 2022 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Google Cloud Managed Service for Prometheus is now generally available</title>
      <link>https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We are excited to announce that Google Cloud &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Managed Service for Prometheus&lt;/a&gt; is now generally available! Now you can get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services.&lt;/p&gt;&lt;p&gt;The rapid adoption of managed platforms and services across the cloud computing industry has shown that fewer and fewer organizations want to invest developer time into managing infrastructure. Google Cloud was recently recognized as a Leader in &lt;a href=&#34;https://cloud.google.com/resources/forrester-wave-container-platforms-report&#34;&gt;The Forrester Wave™: Public Cloud Container Platforms, Q1 2022&lt;/a&gt; and in the report the authors note: “Large firms are seeking enterprise container platforms that accelerate and simplify the development and operations of cloud-native apps with resiliency, manageability, and observability via full-stack cloud-native capabilities.” &lt;/p&gt;&lt;p&gt;Getting a better picture of the &lt;a href=&#34;https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals&#34; target=&#34;_blank&#34;&gt;four golden signals&lt;/a&gt; of monitoring, as laid out in Google’s SRE book, means you have to capture metrics. In both self-managed Kubernetes and Google Kubernetes Engine (GKE) environments, the de-facto standard monitoring technology is &lt;a href=&#34;http://prometheus.io&#34; target=&#34;_blank&#34;&gt;Prometheus&lt;/a&gt;, an open source metrics collection and alerting tool. While Prometheus works great out-of-the-box for smaller deployments, running Prometheus at scale creates some uniquely difficult challenges.&lt;/p&gt;&lt;p&gt;Much like you might use GKE because you prefer to not manage your own Kubernetes infrastructure, Managed Service for Prometheus is here for those who prefer to not manage their own Prometheus infrastructure. Focus your developers’ efforts on building features for your customers, as opposed to focusing efforts on operations toil that merely keeps the lights on.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-pull_quote&#34;&gt;&lt;div class=&#34;uni-pull-quote h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;div class=&#34;uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;div class=&#34;uni-pull-quote__inner-wrapper h-c-copy h-c-copy&#34;&gt;&lt;q class=&#34;uni-pull-quote__text&#34;&gt;Since adopting the service, we&#39;ve been able to really streamline our Prometheus management, and we&#39;ve highly enjoyed simplifying our operations by bringing together more data sources into a single pane of glass,&#34; says Jonathan Campos, CTO and VP of Engineering at Alto. &#34;And since we don&#39;t have to worry about managing historical data, we&#39;ve been able to reduce our cluster storage from 1TB down to 50GB while extending our Prometheus data retention period from 7 days to 2 years.&lt;/q&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Benefits of using Managed Service for Prometheus  &lt;/h3&gt;&lt;p&gt;&lt;b&gt;Two-year retention of all metrics, included in the price&lt;/b&gt;: Manually sharding long-term storage is a major pain point of running your own Prometheus-compatible aggregator. Thanks to the global scale and scalability of &lt;a href=&#34;https://research.google/pubs/pub50652/&#34; target=&#34;_blank&#34;&gt;Monarch&lt;/a&gt;, the system that powers not just &lt;a href=&#34;https://cloud.google.com/monitoring&#34;&gt;Cloud Monitoring&lt;/a&gt; but all monitoring at Google, long-term storage of Managed Service for Prometheus metrics is easy for us. That benefit gets passed along to you as a two-year retention policy, for all metrics, at no additional charge.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost-effective monitoring&lt;/b&gt;: Switching from open source to a managed service always brings the fear of increased costs, but the pricing model for this service is straightforward and predictable. Charging on a per-sample basis means you pay only while your containers are alive and sending metrics data, taking the worry out of using features like Horizontal Pod Autoscaling that frequently scale containers up and down. Managed Service for Prometheus also provides other &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls&#34;&gt;cost control and cost reducing measures&lt;/a&gt; such as a reduced charge for sparse histograms, a fee structure that charges less for longer sampling periods, and the ability to only send locally pre-aggregated data.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Easy cost identification and attribution&lt;/b&gt;: Within Cloud Monitoring, you can easily &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources&#34;&gt;separate out your Prometheus ingestion volume by metric name and namespace&lt;/a&gt;. This allows you to identify which metrics contribute the most to your bill, determine what team’s namespace is responsible for sending those metrics, and take action to reduce your costs.&lt;/p&gt;&lt;p&gt;&lt;b&gt;No changes needed to existing querying or alerting workflows&lt;/b&gt;: You can choose to &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged&#34;&gt;reuse your existing Prometheus collection deployment&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed&#34;&gt;switch to our managed collection&lt;/a&gt;. In either case you can keep using the same Grafana dashboards and alert configs you’re using today. PromQL compatibility for dashboarding and alerting means that your existing incident creation and investigation workflows will continue to work as before.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Viewing Prometheus metrics and&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/operations/in-depth-explanation-of-operational-metrics-at-google-cloud&#34;&gt;&lt;b&gt;Google Cloud system metrics&lt;/b&gt;&lt;/a&gt; &lt;b&gt;together&lt;/b&gt;: Many organizations try, but struggle to simplify their operations by building a “single pane of glass” for all their metric sources. Because our service is built on the same technology and backend as Cloud Monitoring, your Prometheus metrics can be used with the dashboarding, alerting, and SLO monitoring available within Cloud Monitoring. Chart your Prometheus metrics right alongside your &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics&#34;&gt;GKE metrics&lt;/a&gt;, your load balancer metrics, &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_gcp&#34;&gt;and more&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Google_Cloud_Managed_Service_for_Prometh.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Google Cloud Managed Service for Prometheus.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Google_Cloud_Managed_Service_for_Prometh.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Google Cloud Managed Service for Prometheus maintains compatibility with the rich ecosystem of open source tools and services used to analyze, query, and visualize Prometheus metrics&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Read more from customers who have used Managed Service for Prometheus on &lt;a href=&#34;https://cloud.google.com/managed-prometheus#section-3&#34;&gt;our website&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;What’s coming up next&lt;/h3&gt;&lt;p&gt;General availability is just the beginning! We have many more great Managed Service for Prometheus features on our roadmap, including:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;PromQL querying of &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_gcp&#34;&gt;free GCP system metrics&lt;/a&gt; available in Cloud Monitoring, including &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_kubernetes&#34;&gt;GKE&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_anthos&#34;&gt;Anthos&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/monitoring/api/metrics_istio&#34;&gt;Istio&lt;/a&gt; metrics, so you can chart these metrics right alongside your Prometheus metrics in Grafana&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Recommended default collection configs for commonly-used exporters, such as &lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics&#34; target=&#34;_blank&#34;&gt;kube-state-metrics&lt;/a&gt; and &lt;a href=&#34;https://github.com/prometheus/node_exporter&#34; target=&#34;_blank&#34;&gt;node-exporter&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Optimized network performance for on-prem clusters&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Debugging tools, such as a targets discovery and health page&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;And more!&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;How to get started&lt;/h3&gt;&lt;p&gt;Setting up Managed Service for Prometheus is straightforward. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;For those &lt;b&gt;starting from scratch&lt;/b&gt; or for those who want a &lt;b&gt;more fully-managed experience&lt;/b&gt;, you can deploy &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed&#34;&gt;managed collectors&lt;/a&gt; in any Kubernetes cluster by using the GKE UI in Cloud Console, the &lt;code&gt;gcloud&lt;/code&gt; CLI, or &lt;code&gt;kubectl&lt;/code&gt;. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;For those with &lt;b&gt;existing Prometheus deployments&lt;/b&gt;, you can &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged&#34;&gt;keep using your existing configuration&lt;/a&gt; by just swapping out your Prometheus binary with the Managed Service for Prometheus binary. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;See &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/managed-prometheus&#34;&gt;Managed Service for Prometheus documentation&lt;/a&gt; to get started. You can also check out &lt;a href=&#34;https://www.youtube.com/watch?v=X4qAEa8_JxQ&#34; target=&#34;_blank&#34;&gt;this video&lt;/a&gt; which walks you through a few different ways to set up the service:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_kubectl_commands.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 kubectl commands.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_kubectl_commands.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;A screenshot from the video, showing kubectl commands used when setting up Managed Service for Prometheus&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Lastly, if you have questions, feature requests, or just want to read topics from other customers who are using Google Cloud Managed Service for Prometheus and Google Cloud’s operations suite, visit our &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Google Cloud Community site&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/operations/introducing-google-cloud-managed-service-for-prometheus/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Get planet-scale monitoring with Managed Service for Prometheus&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Google Cloud&#39;s managed Prometheus monitoring solution, offering collection, storage, and global querying of Prometheus metrics at scale.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Lee Yanco&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/Prometheus_HCKF6h9.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 02 Mar 2022 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Quickly troubleshoot application errors with Error Reporting</title>
      <link>https://cloud.google.com/blog/products/devops-sre/application-exceptions-surfaced-automatically/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Are you familiar with the &lt;a href=&#34;https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals&#34; target=&#34;_blank&#34;&gt;four golden signals&lt;/a&gt; of Site Reliability Engineering (SRE): latency, traffic, errors, and saturation? Whether you’re a developer or an operator, you’ve likely been responsible for collecting, storing, or analyzing the data associated with these concepts. Much of this data is captured in application and infrastructure logs, which provide a rich history of what is happening behind the scenes in your workloads.  &lt;/p&gt;&lt;p&gt;Getting insights from your logs to track those four golden signals can become unruly very quickly as the application scales up, hindering the ability for your developers and operations teams to identify when and where errors are occurring. If you fail to set up your monitoring and logging systems correctly, your Mean Time to Recovery (MTTR) from service impacting events can be impacted.&lt;/p&gt;&lt;p&gt;Google Cloud provides guidance on what to think about when deciding how to set up your logging, monitoring, and alert systems in the operational excellence section of the &lt;a href=&#34;https://cloud.google.com/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging&#34;&gt;Cloud Architecture Framework&lt;/a&gt;. Google Cloud also provides managed services as part of the &lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;operations suite&lt;/a&gt; to automate collection, storage and analysis of the four golden signals. Cloud Error Reporting is one such service.&lt;/p&gt;&lt;h3&gt;Error Reporting - Speed up your MTTR with zero effort&lt;/h3&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/error-reporting/&#34;&gt;Error Reporting&lt;/a&gt; automatically captures exceptions found in logs ingested by Cloud Logging from the following languages: &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/go&#34;&gt;Go&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/nodejs&#34;&gt;Node.js&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/php&#34;&gt;PHP&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/python&#34;&gt;Python&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/ruby&#34;&gt;Ruby&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/dotnet&#34;&gt;.NET&lt;/a&gt;, aggregates them, and then &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications&#34;&gt;notifies you&lt;/a&gt; of their existence. The service intelligently groups together the errors that it finds and makes them available in a &lt;a href=&#34;http://console.cloud.google.com/errors&#34;&gt;dedicated dashboard&lt;/a&gt;. The dashboard displays the details of the exception including a histogram of occurrences, list of affected versions, request URL and links to the request log, meaning you can get to the affected resource immediately, with just one click!&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Homepage_of_the_Error_Reporting_service.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 Homepage of the Error Reporting service.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Homepage_of_the_Error_Reporting_service.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Homepage of the Error Reporting service&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;How can Error Reporting help your organization today?&lt;/p&gt;&lt;p&gt;Error Reporting helps focus your most valuable resource (i.e Developer attention) on the potential source of exceptions that are impacting your workloads. With the notifications and embedded links, exceptions can quickly be resolved before they impact your customers and bottom line.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Error_Reporting_value.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;2 Error Reporting value.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Error_Reporting_value.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Error Reporting value&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;What do you have to do to enable Error Reporting?&lt;/p&gt;&lt;p&gt;Error Reporting is automatically enabled as soon as logs that contain error events like stack traces are ingested into Cloud Logging or when you use the API to &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/how-to&#34;&gt;self configure&lt;/a&gt; a service to capture exceptions.&lt;/p&gt;&lt;p&gt;When you use Google Kubernetes Engine and our serverless offerings, application logs written to stdout or stderr will appear automatically in Cloud Logging, and therefore Error Reporting will automatically start analyzing them. To capture logs from applications running on VMs in Google Compute Engine, you will need to &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent&#34;&gt;install the Ops Agent&lt;/a&gt;. From there, app logs will be captured in Cloud Logging and exceptions will flow through to Error Reporting. &lt;/p&gt;&lt;h3&gt;Get started today&lt;/h3&gt;&lt;p&gt;To view available error events, visit the &lt;a href=&#34;http://console.cloud.google.com/errors&#34;&gt;Error Reporting page&lt;/a&gt; in the Google Cloud Console. You can find it in the left navigation panel or by searching in the search bar at the top of the console.&lt;/p&gt;&lt;p&gt;If you have any questions or want to start a discussion with other Error Reporting users, visit the Cloud Operations section of the &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations&#34; target=&#34;_blank&#34;&gt;Google Cloud Community&lt;/a&gt; and post a discussion topic.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/operations/faster-debugging-with-traces-and-logs-together/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/cloud_logging_OUrfE4R.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Enabling SRE best practices: new contextual traces in Cloud Logging&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Developers can now view trace information for applications directly in Google Cloud Logging for faster debugging.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c27=""><div _ngcontent-c27="" innerhtml="&lt;p&gt;Are you familiar with the &lt;a href=&#34;https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals&#34; target=&#34;_blank&#34;&gt;four golden signals&lt;/a&gt; of Site Reliability Engineering (SRE): latency, traffic, errors, and saturation? Whether you&amp;#8217;re a developer or an operator, you&amp;#8217;ve likely been responsible for collecting, storing, or analyzing the data associated with these concepts. Much of this data is captured in application and infrastructure logs, which provide a rich history of what is happening behind the scenes in your workloads.&amp;#160;&amp;#160;&lt;/p&gt;&lt;p&gt;Getting insights from your logs to track those four golden signals can become unruly very quickly as the application scales up, hindering the ability for your developers and operations teams to identify when and where errors are occurring. If you fail to set up your monitoring and logging systems correctly, your Mean Time to Recovery (MTTR) from service impacting events can be impacted.&lt;/p&gt;&lt;p&gt;Google Cloud provides guidance on what to think about when deciding how to set up your logging, monitoring, and alert systems in the operational excellence section of the &lt;a href=&#34;https://cloud.google.com/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging&#34;&gt;Cloud Architecture Framework&lt;/a&gt;. Google Cloud also provides managed services as part of the &lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;operations suite&lt;/a&gt; to automate collection, storage and analysis of the four golden signals. Cloud Error Reporting is one such service.&lt;/p&gt;&lt;h3&gt;Error Reporting - Speed up your MTTR with zero effort&lt;/h3&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/error-reporting/&#34;&gt;Error Reporting&lt;/a&gt; automatically captures exceptions found in logs ingested by Cloud Logging from the following languages: &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/go&#34;&gt;Go&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/nodejs&#34;&gt;Node.js&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/php&#34;&gt;PHP&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/python&#34;&gt;Python&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/ruby&#34;&gt;Ruby&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/setup/dotnet&#34;&gt;.NET&lt;/a&gt;, aggregates them, and then &lt;a href=&#34;https://cloud.google.com/error-reporting/docs/notifications&#34;&gt;notifies you&lt;/a&gt; of their existence. The service intelligently groups together the errors that it finds and makes them available in a &lt;a href=&#34;http://console.cloud.google.com/errors&#34;&gt;dedicated dashboard&lt;/a&gt;. The dashboard displays the details of the exception including a histogram of occurrences, list of affected versions, request URL and links to the request log, meaning you can get to the affected resource immediately, with just one click!&lt;/p&gt;"><p>Are you familiar with the <a href="https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">four golden signals</a> of Site Reliability Engineering (SRE): latency, traffic, errors, and saturation? Whether you’re a developer or an operator, you’ve likely been responsible for collecting, storing, or analyzing the data associated with these concepts. Much of this data is captured in application and infrastructure logs, which provide a rich history of what is happening behind the scenes in your workloads.  </p><p>Getting insights from your logs to track those four golden signals can become unruly very quickly as the application scales up, hindering the ability for your developers and operations teams to identify when and where errors are occurring. If you fail to set up your monitoring and logging systems correctly, your Mean Time to Recovery (MTTR) from service impacting events can be impacted.</p><p>Google Cloud provides guidance on what to think about when deciding how to set up your logging, monitoring, and alert systems in the operational excellence section of the <a href="https://cloud.google.com/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging" track-metadata-module="post">Cloud Architecture Framework</a>. Google Cloud also provides managed services as part of the <a href="https://cloud.google.com/products/operations" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/products/operations" track-metadata-module="post">operations suite</a> to automate collection, storage and analysis of the four golden signals. Cloud Error Reporting is one such service.</p><h3>Error Reporting - Speed up your MTTR with zero effort</h3><p><a href="https://cloud.google.com/error-reporting/" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/error-reporting/" track-metadata-module="post">Error Reporting</a> automatically captures exceptions found in logs ingested by Cloud Logging from the following languages: <a href="https://cloud.google.com/error-reporting/docs/setup/go" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/setup/go" track-metadata-module="post">Go</a>, <a href="https://cloud.google.com/error-reporting/docs/setup/java" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/setup/java" track-metadata-module="post">Java</a>, <a href="https://cloud.google.com/error-reporting/docs/setup/nodejs" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/setup/nodejs" track-metadata-module="post">Node.js</a>, <a href="https://cloud.google.com/error-reporting/docs/setup/php" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/setup/php" track-metadata-module="post">PHP</a>, <a href="https://cloud.google.com/error-reporting/docs/setup/python" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/setup/python" track-metadata-module="post">Python</a>, <a href="https://cloud.google.com/error-reporting/docs/setup/ruby" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/setup/ruby" track-metadata-module="post">Ruby</a>, and <a href="https://cloud.google.com/error-reporting/docs/setup/dotnet" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/setup/dotnet" track-metadata-module="post">.NET</a>, aggregates them, and then <a href="https://cloud.google.com/error-reporting/docs/notifications" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/error-reporting/docs/notifications" track-metadata-module="post">notifies you</a> of their existence. The service intelligently groups together the errors that it finds and makes them available in a <a href="http://console.cloud.google.com/errors" track-type="inline link" track-name="13" track-metadata-eventdetail="http://console.cloud.google.com/errors" track-metadata-module="post">dedicated dashboard</a>. The dashboard displays the details of the exception including a histogram of occurrences, list of affected versions, request URL and links to the request log, meaning you can get to the affected resource immediately, with just one click!</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;John Day&lt;/name&gt;&lt;title&gt;Product Marketing Manager, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/error_reporting.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 28 Feb 2022 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Getting Started with Google Cloud Logging Python v3.0.0</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-python-client-library-v3-0-0-release/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We’re excited to announce the release of a major update to the Google Cloud Python logging library. &lt;/p&gt;&lt;p&gt;v3.0.0 makes it even easier for Python developers to send and read logs from Google Cloud, providing real-time insights into what is happening in your application.  If you’re a Python developer working with Google Cloud, now is a great time to try out Cloud Logging!&lt;/p&gt;&lt;p&gt;If you&#39;re unfamiliar with the `google-cloud-logging` library, getting started is simple. First, download the library using pip:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Now, you can set up the client library to work with Python&#39;s built-in `logging` library. Doing this will make it so that all your standard Python log statements will start sending data to Google Cloud:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We recommend &lt;a href=&#34;https://googleapis.dev/python/logging/latest/std-lib-integration.html&#34; target=&#34;_blank&#34;&gt;using the standard Python `logging` interface&lt;/a&gt; for log creation, as demonstrated above. However, if you need access to other &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Google Cloud Logging features&lt;/a&gt; (reading logs, managing &lt;a href=&#34;https://cloud.google.com/logging/docs/export/configure_export_v2&#34;&gt;log sinks&lt;/a&gt;, etc), you can &lt;a href=&#34;https://googleapis.dev/python/logging/latest/direct-lib-usage.html&#34; target=&#34;_blank&#34;&gt;use `google.cloud.logging` directly&lt;/a&gt;:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Here are some of the main features of the &lt;a href=&#34;https://github.com/googleapis/python-logging/blob/eac5e2db83f83b24962524fd9e0d7afa09e2785b/UPGRADING.md&#34; target=&#34;_blank&#34;&gt;new release&lt;/a&gt;:&lt;/p&gt;&lt;h3&gt;Support More Cloud Environments&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_jAhSLdi.0999065319990470.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Previous versions of google-cloud-logging supported only&lt;a href=&#34;https://cloud.google.com/appengine&#34;&gt;App Engine&lt;/a&gt; and&lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Kubernetes Engine&lt;/a&gt;. Users reported that the library would occasionally drop logs on serverless environments like Cloud Run and Cloud Functions. This was because the library would send logs in batches over the network. When a serverless environment would spin down, unsent batches could be lost.&lt;/p&gt;&lt;p&gt;v3.0.0 fixes this issue by making use of GCP’s built in&lt;a href=&#34;https://cloud.google.com/logging/docs/structured-logging&#34;&gt;structured JSON logging functionality&lt;/a&gt; on supported environments (GKE, Cloud Run, or Cloud Functions). If the library detects it is running on an environment that supports structured logging, it will automatically make use of the new&lt;a href=&#34;https://github.com/googleapis/python-logging/blob/v3.0.0/google/cloud/logging_v2/handlers/structured_log.py&#34; target=&#34;_blank&#34;&gt;StructuredLogHandler&lt;/a&gt;, which writes logs as JSON strings printed to standard out. Google Cloud’s built-in agents will then parse the logs and deliver them to Cloud Logging, even if the code that produced the logs has spun down. &lt;/p&gt;&lt;p&gt;Structured Logging is more reliable on serverless environments, and it allows us to support all major GCP compute environments in v3.0.0. Still, if you would prefer to send logs over the network as before, you can manually set up the library with a &lt;a href=&#34;https://github.com/googleapis/python-logging/blob/d86be6cf83c3f3b91c4fc0b2e0666b0ca1d7e248/google/cloud/logging_v2/handlers/handlers.py#L118&#34; target=&#34;_blank&#34;&gt;CloudLoggingHandler&lt;/a&gt; instance:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Metadata Autodetection&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;2.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_om8Pxs0.0999064919990643.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;When you troubleshoot your application, it can be useful to have as much information about the environment as possible captured in your application logs. `google-cloud-logging` attempts to help in this process by detecting and attaching metadata about your environment to each log message. The following fields are currently supported:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;`&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource&#34;&gt;resource&lt;/a&gt;`: The Google Cloud resource the log originated from &lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;for example, Functions, GKE, or Cloud Run&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;`&lt;a href=&#34;http://httprequest&#34; target=&#34;_blank&#34;&gt;httpRequest&lt;/a&gt;`: Information about an HTTP request in the log’s context&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Flask and Django are currently supported&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;`&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogEntrySourceLocation&#34;&gt;sourceLocation&lt;/a&gt;` : File, line, and function names&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry&#34;&gt;trace&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry&#34;&gt;spanId&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry&#34;&gt;traceSampled&lt;/a&gt;: &lt;a href=&#34;https://medium.com/r/?url=https%3A%2F%2Fcloud.google.com%2Ftrace&#34;&gt;Cloud Trace&lt;/a&gt; metadata&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Supports &lt;a href=&#34;https://cloud.google.com/trace/docs/setup#force-trace&#34;&gt;X-Cloud-Trace-Context&lt;/a&gt; and &lt;a href=&#34;https://www.w3.org/TR/trace-context/#traceparent-header&#34; target=&#34;_blank&#34;&gt;w3c transparent&lt;/a&gt; trace formats&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;The library will make an attempt to populate this data whenever possible, but any of these fields can also be explicitly set by developers using the library.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;JSON Support in Standard Library Integration&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 &#34;&gt;&lt;img alt=&#34;3.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_wcTDIdW.0999064519990884.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Google Cloud Logging supports both&lt;a href=&#34;https://cloud.google.com/logging/docs/structured-logging&#34;&gt;string and JSON payloads&lt;/a&gt; for LogEntries, but up until now,&lt;a href=&#34;https://googleapis.dev/python/logging/latest/std-lib-integration.html&#34; target=&#34;_blank&#34;&gt;the Python standard library integration&lt;/a&gt; could only send logs with string payloads.&lt;/p&gt;&lt;p&gt;In `google-cloud-logging` v3,  you can log JSON data in two ways:&lt;/p&gt;&lt;p&gt;1. Log a JSON-parsable string:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;2. Pass a `json_fields` dictionary using Python logging&#39;s `extra` argument:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Next Steps&lt;/h3&gt;&lt;p&gt;With version v3.0.0, the Google Cloud Logging Python library now supports more compute environments, detects more helpful metadata, and provides more thorough support for JSON logs. Along with these major features, there are also user-experience improvements like a new &lt;a href=&#34;https://googleapis.dev/python/logging/latest/UPGRADING.html#new-logger-log-method-316&#34; target=&#34;_blank&#34;&gt;log method&lt;/a&gt; and more &lt;a href=&#34;https://googleapis.dev/python/logging/latest/UPGRADING.html#more-permissive-arguments-422&#34; target=&#34;_blank&#34;&gt;permissive argument parsing&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;If you want to learn more about the latest release, these changes and others are described in more detail in the &lt;a href=&#34;https://googleapis.dev/python/logging/latest/UPGRADING.html&#34; target=&#34;_blank&#34;&gt;v3.0.0 Migration Guide&lt;/a&gt;. If you’re new to the library, check out the &lt;a href=&#34;https://googleapis.dev/python/logging/latest/index.html&#34; target=&#34;_blank&#34;&gt;google-cloud-logging user guide&lt;/a&gt;. If you want to learn more about observability on GCP in general, you can spin up test environments using &lt;a href=&#34;https://cloud.google.com/blog/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox&#34;&gt;Cloud Ops Sandbox&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Finally, if you have any feedback about the latest release, have new feature requests, or would like to make any contributions, feel free to open issues on &lt;a href=&#34;https://github.com/googleapis/python-logging&#34; target=&#34;_blank&#34;&gt;our GitHub repo&lt;/a&gt;. The Google Cloud Logging libraries are open source software, and we welcome new contributors!&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Public-Sector-Momentum.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Take the first step toward SRE with Cloud Operations Sandbox&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Spin up the Cloud Operations Sandbox to see how Google’s logging, monitoring, tracing, profiling and debugging can kickstart your SRE pra...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><promo-banner-block _nghost-c34=""></promo-banner-block><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>logging.jpg</span></p></figure></article-aspect-image-block><div><article-cta _nghost-c55=""><div _ngcontent-c55=""><h4 _ngcontent-c55=""><span _ngcontent-c55="">Try Google Cloud</span></h4><p _ngcontent-c55=""><span _ngcontent-c55="">Start building on Google Cloud with $300 in free credits and 20+ always free products.</span></p><p><a _ngcontent-c55="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="free trial" track-metadata-eventdetail="https://cloud.google.com/free/" href="https://cloud.google.com/free/"><span _ngcontent-c55="">Free Trial</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c57=""><div _ngcontent-c57="" innerhtml="&lt;p&gt;We&amp;#8217;re excited to announce the release of a major update to the Google Cloud Python logging library.&amp;#160;&lt;/p&gt;&lt;p&gt;v3.0.0 makes it even easier for Python developers to send and read logs from Google Cloud, providing real-time insights into what is happening in your application.&amp;#160; If you&amp;#8217;re a Python developer working with Google Cloud, now is a great time to try out Cloud Logging!&lt;/p&gt;&lt;p&gt;If you&#39;re unfamiliar with the `google-cloud-logging` library, getting started is simple. First, download the library using pip:&lt;/p&gt;"><p>We’re excited to announce the release of a major update to the Google Cloud Python logging library. </p><p>v3.0.0 makes it even easier for Python developers to send and read logs from Google Cloud, providing real-time insights into what is happening in your application.  If you’re a Python developer working with Google Cloud, now is a great time to try out Cloud Logging!</p><p>If you&#39;re unfamiliar with the `google-cloud-logging` library, getting started is simple. First, download the library using pip:</p></div></paragraph-block></div><div><article-code-block _nghost-c58=""><pre _ngcontent-c58="">  <code _ngcontent-c58="">$ pip install &#34;google-cloud-logging&gt;=3.0.0&#34;</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c57=""><p>Now, you can set up the client library to work with Python&#39;s built-in `logging` library. Doing this will make it so that all your standard Python log statements will start sending data to Google Cloud:</p></paragraph-block></div><div><article-code-block _nghost-c58=""><pre _ngcontent-c58="">  <code _ngcontent-c58=""># set up the Google Cloud Logging python client library
</code><code _ngcontent-c58="">import google.cloud.logging
</code><code _ngcontent-c58="">client = google.cloud.logging.Client()
</code><code _ngcontent-c58="">client.setup_logging()
</code><code _ngcontent-c58="">
</code><code _ngcontent-c58=""># use Python&#39;s standard logging library to send logs to GCP
</code><code _ngcontent-c58="">
</code><code _ngcontent-c58="">import logging
</code><code _ngcontent-c58="">logging.warning(&#34;Hello World&#34;)</code>
</pre></article-code-block></div><div><article-code-block _nghost-c58=""><pre _ngcontent-c58="">  <code _ngcontent-c58="">import google.cloud.logging
</code><code _ngcontent-c58="">
</code><code _ngcontent-c58="">client = google.cloud.logging.Client()
</code><code _ngcontent-c58="">logger = client.logger(name=&#34;log_id&#34;)
</code><code _ngcontent-c58="">
</code><code _ngcontent-c58="">client.list_entries(max_size=5) # read logs from GCP
</code><code _ngcontent-c58="">logger.log(&#34;hello world&#34;, resource={&#34;type&#34;:&#34;global&#34;, &#34;labels&#34;:{}}) # write log to GCP</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c57=""><div _ngcontent-c57="" innerhtml="&lt;p&gt;Here are some of the main features of the &lt;a href=&#34;https://github.com/googleapis/python-logging/blob/eac5e2db83f83b24962524fd9e0d7afa09e2785b/UPGRADING.md&#34; target=&#34;_blank&#34;&gt;new release&lt;/a&gt;:&lt;/p&gt;&lt;h3&gt;Support More Cloud Environments&lt;/h3&gt;"><p>Here are some of the main features of the <a href="https://github.com/googleapis/python-logging/blob/eac5e2db83f83b24962524fd9e0d7afa09e2785b/UPGRADING.md" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://github.com" track-metadata-module="post">new release</a>:</p><h3>Support More Cloud Environments</h3></div></paragraph-block></div><div><paragraph-block _nghost-c57=""><div _ngcontent-c57="" innerhtml="&lt;p&gt;Previous versions of google-cloud-logging supported only&lt;a href=&#34;https://cloud.google.com/appengine&#34;&gt; App Engine&lt;/a&gt; and&lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt; Kubernetes Engine&lt;/a&gt;. Users reported that the library would occasionally drop logs on serverless environments like Cloud Run and Cloud Functions. This was because the library would send logs in batches over the network. When a serverless environment would spin down, unsent batches could be lost.&lt;/p&gt;&lt;p&gt;v3.0.0 fixes this issue by making use of GCP&amp;#8217;s built in&lt;a href=&#34;https://cloud.google.com/logging/docs/structured-logging&#34;&gt; structured JSON logging functionality&lt;/a&gt; on supported environments (GKE, Cloud Run, or Cloud Functions). If the library detects it is running on an environment that supports structured logging, it will automatically make use of the new&lt;a href=&#34;https://github.com/googleapis/python-logging/blob/v3.0.0/google/cloud/logging_v2/handlers/structured_log.py&#34; target=&#34;_blank&#34;&gt; StructuredLogHandler&lt;/a&gt;, which writes logs as JSON strings printed to standard out. Google Cloud&amp;#8217;s built-in agents will then parse the logs and deliver them to Cloud Logging, even if the code that produced the logs has spun down.&amp;#160;&lt;/p&gt;&lt;p&gt;Structured Logging is more reliable on serverless environments, and it allows us to support all major GCP compute environments in v3.0.0. Still, if you would prefer to send logs over the network as before, you can manually set up the library with a &lt;a href=&#34;https://github.com/googleapis/python-logging/blob/d86be6cf83c3f3b91c4fc0b2e0666b0ca1d7e248/google/cloud/logging_v2/handlers/handlers.py#L118&#34; target=&#34;_blank&#34;&gt;CloudLoggingHandler&lt;/a&gt; instance:&lt;/p&gt;"><p>Previous versions of google-cloud-logging supported only<a href="https://cloud.google.com/appengine" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/appengine" track-metadata-module="post"> App Engine</a> and<a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post"> Kubernetes Engine</a>. Users reported that the library would occasionally drop logs on serverless environments like Cloud Run and Cloud Functions. This was because the library would send logs in batches over the network. When a serverless environment would spin down, unsent batches could be lost.</p><p>v3.0.0 fixes this issue by making use of GCP’s built in<a href="https://cloud.google.com/logging/docs/structured-logging" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/logging/docs/structured-logging" track-metadata-module="post"> structured JSON logging functionality</a> on supported environments (GKE, Cloud Run, or Cloud Functions). If the library detects it is running on an environment that supports structured logging, it will automatically make use of the new<a href="https://github.com/googleapis/python-logging/blob/v3.0.0/google/cloud/logging_v2/handlers/structured_log.py" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://github.com" track-metadata-module="post"> StructuredLogHandler</a>, which writes logs as JSON strings printed to standard out. Google Cloud’s built-in agents will then parse the logs and deliver them to Cloud Logging, even if the code that produced the logs has spun down. </p><p>Structured Logging is more reliable on serverless environments, and it allows us to support all major GCP compute environments in v3.0.0. Still, if you would prefer to send logs over the network as before, you can manually set up the library with a <a href="https://github.com/googleapis/python-logging/blob/d86be6cf83c3f3b91c4fc0b2e0666b0ca1d7e248/google/cloud/logging_v2/handlers/handlers.py#L118" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://github.com" track-metadata-module="post">CloudLoggingHandler</a> instance:</p></div></paragraph-block></div><div><article-code-block _nghost-c58=""><pre _ngcontent-c58="">  <code _ngcontent-c58="">from google.cloud.logging.handlers import CloudLoggingHandler
</code><code _ngcontent-c58="">from google.cloud.logging_v2.handlers import setup_logging
</code><code _ngcontent-c58="">
</code><code _ngcontent-c58=""># explicitly set up a CloudLoggingHandler to send logs over the network
</code><code _ngcontent-c58="">handler = CloudLoggingHandler(client)
</code><code _ngcontent-c58="">setup_logging(handler)
</code><code _ngcontent-c58="">
</code><code _ngcontent-c58="">import logging
</code><code _ngcontent-c58="">logging.warning(“Hello World”)</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c57=""><div _ngcontent-c57="" innerhtml="&lt;p&gt;When you troubleshoot your application, it can be useful to have as much information about the environment as possible captured in your application logs. `google-cloud-logging` attempts to help in this process by detecting and attaching metadata about your environment to each log message. The following fields are currently supported:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;`&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource&#34;&gt;resource&lt;/a&gt;`: The Google Cloud resource the log originated from&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;for example, Functions, GKE, or Cloud Run&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;`&lt;a href=&#34;http://httprequest&#34; target=&#34;_blank&#34;&gt;httpRequest&lt;/a&gt;`: Information about an HTTP request in the log&amp;#8217;s context&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Flask and Django are currently supported&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;`&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogEntrySourceLocation&#34;&gt;sourceLocation&lt;/a&gt;` : File, line, and function names&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry&#34;&gt;trace&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry&#34;&gt;spanId&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry&#34;&gt;traceSampled&lt;/a&gt;: &lt;a href=&#34;https://medium.com/r/?url=https%3A%2F%2Fcloud.google.com%2Ftrace&#34;&gt;Cloud Trace&lt;/a&gt; metadata&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Supports &lt;a href=&#34;https://cloud.google.com/trace/docs/setup#force-trace&#34;&gt;X-Cloud-Trace-Context&lt;/a&gt; and &lt;a href=&#34;https://www.w3.org/TR/trace-context/#traceparent-header&#34; target=&#34;_blank&#34;&gt;w3c transparent&lt;/a&gt; trace formats&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;The library will make an attempt to populate this data whenever possible, but any of these fields can also be explicitly set by developers using the library.&lt;/p&gt;"><p>When you troubleshoot your application, it can be useful to have as much information about the environment as possible captured in your application logs. `google-cloud-logging` attempts to help in this process by detecting and attaching metadata about your environment to each log message. The following fields are currently supported:</p><ul><li><p>`<a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource" track-metadata-module="post">resource</a>`: The Google Cloud resource the log originated from </p></li><ul><li><p>for example, Functions, GKE, or Cloud Run</p></li></ul><li><p>`<a href="http://httprequest" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="http://httprequest" track-metadata-module="post">httpRequest</a>`: Information about an HTTP request in the log’s context</p></li><ul><li><p>Flask and Django are currently supported</p></li></ul><li><p>`<a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogEntrySourceLocation" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogEntrySourceLocation" track-metadata-module="post">sourceLocation</a>` : File, line, and function names</p></li><li><p><a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry" track-metadata-module="post">trace</a>, <a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry" track-metadata-module="post">spanId</a>, and <a href="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry" track-metadata-module="post">traceSampled</a>: <a href="https://medium.com/r/?url=https%3A%2F%2Fcloud.google.com%2Ftrace" track-type="inline link" track-name="17" track-metadata-eventdetail="https://medium.com" track-metadata-module="post">Cloud Trace</a> metadata</p></li><ul><li><p>Supports <a href="https://cloud.google.com/trace/docs/setup#force-trace" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/trace/docs/setup#force-trace" track-metadata-module="post">X-Cloud-Trace-Context</a> and <a href="https://www.w3.org/TR/trace-context/#traceparent-header" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://www.w3.org" track-metadata-module="post">w3c transparent</a> trace formats</p></li></ul></ul><p>The library will make an attempt to populate this data whenever possible, but any of these fields can also be explicitly set by developers using the library.</p></div></paragraph-block></div><div><article-code-block _nghost-c58=""><pre _ngcontent-c58="">  <code _ngcontent-c58="">logging.info(&#34;hello&#34;, extra={
</code><code _ngcontent-c58="">    &#34;labels&#34;: {&#34;foo&#34;: &#34;bar&#34;},
</code><code _ngcontent-c58="">    &#34;http_request&#34;: {&#34;requestUrl&#34;: &#34;localhost&#34;},
</code><code _ngcontent-c58="">    &#34;trace&#34;: &#34;01234&#34;
</code><code _ngcontent-c58="">})</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c57=""><p _ngcontent-c57="" innerhtml="&lt;h3&gt;JSON Support in Standard Library Integration&lt;/h3&gt;"><h3>JSON Support in Standard Library Integration</h3></p></paragraph-block></div><div><article-code-block _nghost-c58=""><pre _ngcontent-c58="">  <code _ngcontent-c58="">import logging
</code><code _ngcontent-c58="">import json
</code><code _ngcontent-c58="">
</code><code _ngcontent-c58="">data_dict = {&#34;hello&#34;: &#34;world&#34;}
</code><code _ngcontent-c58="">logging.info(json.dumps(data_dict))</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c57=""><p>2. Pass a `json_fields` dictionary using Python logging&#39;s `extra` argument:</p></paragraph-block></div><div><article-code-block _nghost-c58=""><pre _ngcontent-c58="">  <code _ngcontent-c58="">import logging
</code><code _ngcontent-c58="">
</code><code _ngcontent-c58="">data_dict = {&#34;hello&#34;: &#34;world&#34;}
</code><code _ngcontent-c58="">logging.info(&#34;message field&#34;, extra={&#34;json_fields&#34;: data_dict})</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c57=""><div _ngcontent-c57="" innerhtml="&lt;h3&gt;Next Steps&lt;/h3&gt;&lt;p&gt;With version v3.0.0, the Google Cloud Logging Python library now supports more compute environments, detects more helpful metadata, and provides more thorough support for JSON logs. Along with these major features, there are also user-experience improvements like a new &lt;a href=&#34;https://googleapis.dev/python/logging/latest/UPGRADING.html#new-logger-log-method-316&#34; target=&#34;_blank&#34;&gt;log method&lt;/a&gt; and more &lt;a href=&#34;https://googleapis.dev/python/logging/latest/UPGRADING.html#more-permissive-arguments-422&#34; target=&#34;_blank&#34;&gt;permissive argument parsing&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;If you want to learn more about the latest release, these changes and others are described in more detail in the &lt;a href=&#34;https://googleapis.dev/python/logging/latest/UPGRADING.html&#34; target=&#34;_blank&#34;&gt;v3.0.0 Migration Guide&lt;/a&gt;. If you&amp;#8217;re new to the library, check out the &lt;a href=&#34;https://googleapis.dev/python/logging/latest/index.html&#34; target=&#34;_blank&#34;&gt;google-cloud-logging user guide&lt;/a&gt;. If you want to learn more about observability on GCP in general, you can spin up test environments using &lt;a href=&#34;https://cloud.google.com/blog/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox&#34;&gt;Cloud Ops Sandbox&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Finally, if you have any feedback about the latest release, have new feature requests, or would like to make any contributions, feel free to open issues on &lt;a href=&#34;https://github.com/googleapis/python-logging&#34; target=&#34;_blank&#34;&gt;our GitHub repo&lt;/a&gt;. The Google Cloud Logging libraries are open source software, and we welcome new contributors!&lt;/p&gt;"><h3>Next Steps</h3><p>With version v3.0.0, the Google Cloud Logging Python library now supports more compute environments, detects more helpful metadata, and provides more thorough support for JSON logs. Along with these major features, there are also user-experience improvements like a new <a href="https://googleapis.dev/python/logging/latest/UPGRADING.html#new-logger-log-method-316" target="_blank" track-type="inline link" track-name="22" track-metadata-eventdetail="https://googleapis.dev" track-metadata-module="post">log method</a> and more <a href="https://googleapis.dev/python/logging/latest/UPGRADING.html#more-permissive-arguments-422" target="_blank" track-type="inline link" track-name="23" track-metadata-eventdetail="https://googleapis.dev" track-metadata-module="post">permissive argument parsing</a>. </p><p>If you want to learn more about the latest release, these changes and others are described in more detail in the <a href="https://googleapis.dev/python/logging/latest/UPGRADING.html" target="_blank" track-type="inline link" track-name="24" track-metadata-eventdetail="https://googleapis.dev" track-metadata-module="post">v3.0.0 Migration Guide</a>. If you’re new to the library, check out the <a href="https://googleapis.dev/python/logging/latest/index.html" target="_blank" track-type="inline link" track-name="25" track-metadata-eventdetail="https://googleapis.dev" track-metadata-module="post">google-cloud-logging user guide</a>. If you want to learn more about observability on GCP in general, you can spin up test environments using <a href="https://cloud.google.com/blog/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/blog/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox" track-metadata-module="post">Cloud Ops Sandbox</a>.</p><p>Finally, if you have any feedback about the latest release, have new feature requests, or would like to make any contributions, feel free to open issues on <a href="https://github.com/googleapis/python-logging" target="_blank" track-type="inline link" track-name="27" track-metadata-eventdetail="https://github.com" track-metadata-module="post">our GitHub repo</a>. The Google Cloud Logging libraries are open source software, and we welcome new contributors!</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c56=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Daniel Sanche&lt;/name&gt;&lt;title&gt;Developer Programs Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/logging.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 07 Feb 2022 18:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Certificate Manager to simplify SaaS scale TLS and certificate management</title>
      <link>https://cloud.google.com/blog/products/identity-security/simplify-saas-scale-tls-certificate-management/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We’re excited to announce the public preview of Certificate Manager and its integration with External HTTPS Load Balancing. Certificate Manager enables you to use External HTTPS Load Balancing with as many certificates or domains as you need. You can bring your own TLS certificates and keys if you have an existing certificate lifecycle management solution you&#39;d like to use with Google Cloud, or enjoy the convenience of our fully Managed TLS offerings.  &lt;/p&gt;&lt;h3&gt;Extend the security and performance of the Google network to your customers&lt;/h3&gt;&lt;p&gt;Certificate Manager brings support for multiple certificates per customer. When  coupled with our &lt;a href=&#34;https://cloud.google.com/load-balancing/docs/load-balancing-overview&#34;&gt;global anycast load balancing solution&lt;/a&gt; with automated autoscaling and failover, you now have a powerful platform for building robust SaaS and PaaS offerings. This enables custom domain support for your customers with the lowest latency and the highest level of availability. &lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://il.linkedin.com/in/alonkochba&#34; target=&#34;_blank&#34;&gt;Alon Kochba&lt;/a&gt;, the head of web performance at website-building service Wix, explained how the new features lighten their workload. &lt;/p&gt;&lt;p&gt;“As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. GCP&#39;s Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to rely on our own custom solution for terminating SSL,” Kochba said.&lt;/p&gt;&lt;p&gt;Customers who switch to External HTTPS Load Balancing can also now protect their SaaS users from &lt;a href=&#34;https://cloud.google.com/armor/docs/adaptive-protection-overview&#34;&gt;denial of service attacks&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/architecture/owasp-top-ten-mitigation&#34;&gt;OWASP Top 10 risks&lt;/a&gt;, and other common Web attacks by adopting &lt;a href=&#34;https://cloud.google.com/armor&#34;&gt;Cloud Armor&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;DNS authorization&lt;/h3&gt;&lt;p&gt;This release also now enables you to provision your Google-managed certificates with DNS-based authorizations and have them ready to use before your load-balancing production environment is fully set up. This will help streamline the migration process to Google Cloud, for example. To create a DNS authorization, use the following command:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;This command returns the CNAME record for &lt;code&gt;_acme-challenge.&lt;/code&gt;&lt;a href=&#34;http://www.example.com&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;example.com&lt;/code&gt;&lt;/a&gt; that you must &lt;a href=&#34;https://cloud.devsite.corp.google.com/certificate-manager/docs/certificates#cert-dns-auth&#34; target=&#34;_blank&#34;&gt;add to your DNS configuration&lt;/a&gt; in the DNS zone of the target domain. This CNAME record points to a special Google Cloud domain, e.g.: &#34;&lt;code&gt;534959-1a8a-40cf-90b6-b1f5f8d22517.2.authorize.certificatemanager.goog&lt;/code&gt;” that is used  to verify domain ownership.&lt;/p&gt;&lt;p&gt;When you request a certificate based on the above authorization, Cloud Certificate Manager will work with the Certificate Authority automatically to get and later renew your certificate for that domain.&lt;/p&gt;&lt;h3&gt;Wildcard support&lt;/h3&gt;&lt;p&gt;This &lt;b&gt;DNS-based domain control authorization&lt;/b&gt; also allows us to bring you support for &lt;b&gt;wildcard certificates&lt;/b&gt;. To configure the use of wildcard certificates you first must configure the DNS authorization as we’ve indicated above. Once that has been completed, you can configure the use of a wildcard certificate using the following command. Our example below is for a top-level registered domain and its wildcard subdomains.&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Monitoring for Certificate Expiration&lt;/h3&gt;&lt;p&gt;Another new feature that will be enabled with this product  is the ability to monitor certificate expiration with &lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;Google Cloud Logging&lt;/a&gt;.  Cloud Logging creates a record of certificate expiration, uses the `&lt;code&gt;certificatemanager.googleapis.com/Project&lt;/code&gt;` monitored resource, and is represented by the following message:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The log message is delivered every hour and contains a sample of the certificates being close to expiry or already expired.&lt;/p&gt;&lt;h3&gt;Pricing&lt;/h3&gt;&lt;p&gt;The best part is that there’s no additional charge to use the Certificate Manager for the first 100 certificates. To use more than 100 certificates with the management tools, we will charge on a per-certificate, per-month pricing structure. This empowers you to scale up to as many certificates as you need, and as cost-effectively as possible. The pricing will be enabled when the solution goes to General Availability.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;Certificate Manager pricing.png&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Certificate_Manager_prici.0999064919990490.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;It is our hope that these &lt;a href=&#34;https://cloud.google.com/certificate-manager/docs/how-it-works&#34;&gt;new features&lt;/a&gt;, combined with the programmability offered by &lt;a href=&#34;https://cloud.google.com/certificate-manager/docs/overview&#34;&gt;Certificate Manager&lt;/a&gt;, will enable you to simplify the way you deploy HTTPS and offer a more scalable and secure service to your customers.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/identity-security/10-questions-to-help-boards-safely-maximize-cloud-opportunities/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/cybersecurity_action_team_jl2RU0c.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;10 questions to help boards safely maximize cloud opportunities&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;These 10 questions from a new Google Cloud whitepaper will help boards of directors safely guide their organizations through cloud migrat...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;p&gt;We&amp;#8217;re excited to announce the public preview of Certificate Manager and its integration with External HTTPS Load Balancing. Certificate Manager enables you to use External HTTPS Load Balancing with as many certificates or domains as you need. You can bring your own TLS certificates and keys if you have an existing certificate lifecycle management solution you&#39;d like to use with Google Cloud, or enjoy the convenience of our fully Managed TLS offerings.&amp;#160;&amp;#160;&lt;/p&gt;&lt;h3&gt;Extend the security and performance of the Google network to your customers&lt;/h3&gt;&lt;p&gt;Certificate Manager brings support for multiple certificates per customer. When&amp;#160; coupled with our &lt;a href=&#34;https://cloud.google.com/load-balancing/docs/load-balancing-overview&#34;&gt;global anycast load balancing solution&lt;/a&gt; with automated autoscaling and failover, you now have a powerful platform for building robust SaaS and PaaS offerings. This enables custom domain support for your customers with the lowest latency and the highest level of availability.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://il.linkedin.com/in/alonkochba&#34; target=&#34;_blank&#34;&gt;Alon Kochba&lt;/a&gt;, the head of web performance at website-building service Wix, explained how the new features lighten their workload.&amp;#160;&lt;/p&gt;&lt;p&gt;&amp;#8220;As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. GCP&#39;s Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to rely on our own custom solution for terminating SSL,&amp;#8221; Kochba said.&lt;/p&gt;&lt;p&gt;Customers who switch to External HTTPS Load Balancing can also now protect their SaaS users from &lt;a href=&#34;https://cloud.google.com/armor/docs/adaptive-protection-overview&#34;&gt;denial of service attacks&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/architecture/owasp-top-ten-mitigation&#34;&gt;OWASP Top 10 risks&lt;/a&gt;, and other common Web attacks by adopting &lt;a href=&#34;https://cloud.google.com/armor&#34;&gt;Cloud Armor&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;DNS authorization&lt;/h3&gt;&lt;p&gt;This release also now enables you to provision your Google-managed certificates with DNS-based authorizations and have them ready to use before your load-balancing production environment is fully set up. This will help streamline the migration process to Google Cloud, for example. To create a DNS authorization, use the following command:&lt;/p&gt;"><p>We’re excited to announce the public preview of Certificate Manager and its integration with External HTTPS Load Balancing. Certificate Manager enables you to use External HTTPS Load Balancing with as many certificates or domains as you need. You can bring your own TLS certificates and keys if you have an existing certificate lifecycle management solution you&#39;d like to use with Google Cloud, or enjoy the convenience of our fully Managed TLS offerings.  </p><h3>Extend the security and performance of the Google network to your customers</h3><p>Certificate Manager brings support for multiple certificates per customer. When  coupled with our <a href="https://cloud.google.com/load-balancing/docs/load-balancing-overview" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/load-balancing/docs/load-balancing-overview" track-metadata-module="post">global anycast load balancing solution</a> with automated autoscaling and failover, you now have a powerful platform for building robust SaaS and PaaS offerings. This enables custom domain support for your customers with the lowest latency and the highest level of availability. </p><p><a href="https://il.linkedin.com/in/alonkochba" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://il.linkedin.com" track-metadata-module="post">Alon Kochba</a>, the head of web performance at website-building service Wix, explained how the new features lighten their workload. </p><p>“As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. GCP&#39;s Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to rely on our own custom solution for terminating SSL,” Kochba said.</p><p>Customers who switch to External HTTPS Load Balancing can also now protect their SaaS users from <a href="https://cloud.google.com/armor/docs/adaptive-protection-overview" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/armor/docs/adaptive-protection-overview" track-metadata-module="post">denial of service attacks</a>, <a href="https://cloud.google.com/architecture/owasp-top-ten-mitigation" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/architecture/owasp-top-ten-mitigation" track-metadata-module="post">OWASP Top 10 risks</a>, and other common Web attacks by adopting <a href="https://cloud.google.com/armor" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/armor" track-metadata-module="post">Cloud Armor</a>.</p><h3>DNS authorization</h3><p>This release also now enables you to provision your Google-managed certificates with DNS-based authorizations and have them ready to use before your load-balancing production environment is fully set up. This will help streamline the migration process to Google Cloud, for example. To create a DNS authorization, use the following command:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Babi Seal&lt;/name&gt;&lt;title&gt;Product Manager, Load Balancing&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/google_cloud_security.0999040819990817.max-2000x2000.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 31 Jan 2022 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Google Cloud Deploy, now GA, makes it easier to do continuous delivery to GKE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-now-ga/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Effective software delivery — usually achieved via continuous integration (CI) and continuous delivery (CD) — is a top priority for many product development teams. It’s easy to understand why: the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;2021 State of DevOps report&lt;/a&gt; found that elite performers of software delivery deployed code much more frequently than low performers, with three times fewer change-related failures. Teams who excel at modern software delivery operational practices were also 1.8 times more likely to report better business outcomes.&lt;/p&gt;&lt;p&gt;You need great tools to do software delivery effectively. Without capable tooling, teams have to design, maintain, and scale their software delivery solutions on their own, which can be difficult given the breadth of continuous delivery’s flow control, security and audit, and integration requirements. Deploying container image artifacts adds further complexity, particularly in Kubernetes environments.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_cloud_deploy.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;1 cloud deploy.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_cloud_deploy.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Building on your feedback and Google’s own best practices, we’ve been working on software delivery tooling that helps you meet your continuous delivery goals — especially with respect to &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine (GKE)&lt;/a&gt; environments. Today, we are pleased to announce the general availability of &lt;a href=&#34;https://cloud.google.com/deploy/&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable. &lt;/p&gt;&lt;h3&gt;Solving for continuous delivery challenges&lt;/h3&gt;&lt;p&gt;While designing Google Cloud Deploy, we talked to a number of customers to better understand the challenges they face doing continuous delivery to GKE. While a handful of themes emerged, three stood out: cost of ownership, security and audit, and measurement.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;As shared in our &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke&#34;&gt;Preview launch post&lt;/a&gt; this past September, the operational cost of Kubernetes continuous delivery can be very high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, collecting key metrics, and staying current — to say nothing of maintenance — is resource-intensive and takes time away from the core business. &lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. &lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;retained with each release&lt;/a&gt;. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;4 Delivery pipeline metrics.gif&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/4_Delivery_pipeline_metrics.gif&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Promoting a release&lt;/i&gt;&lt;br/&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Whether or not you already have a continuous delivery capability, you likely already have continuous integration, approval and/or operation workflows, and other systems that intersect with your software delivery practices. &lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating-ci&#34;&gt;connectivity to CI systems&lt;/a&gt;, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;notifications&lt;/a&gt; to enable &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;related software delivery tooling&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;i&gt;“While looking for a Continuous Delivery solution we considered ArgoCD and Spinnaker, however we chose Google Cloud Deploy because it is a managed service, provided proper CD primitives and integrated seamlessly with our GKE clusters. It has empowered every team member to safely and reliably promote their code from commit all the way through to production.”—Jonathan Sokolowski, DevOps Engineer, Search.io&lt;/i&gt;&lt;/p&gt;&lt;p&gt;A variety of GKE roles and personas interact with continuous delivery processes. DevOps engineers are focused on release promotion and rollback decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;3 Deployment approvals.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Deployment_approvals.max-1000x1000.jpg&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Deployment approvals&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Security and control&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy’s security foundations strengthen secure software supply chain practices through delivery flow control and auditability.&lt;/p&gt;&lt;p&gt;Lots of different users interact with a software delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create release candidates, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;restriction through IAM&lt;/a&gt;, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/securing/iam&#34;&gt;discrete access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. Google Cloud Deploy also supports &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment#deploying_to_a_private_cluster_on_a_network&#34;&gt;deploying to private GKE clusters&lt;/a&gt; and  &lt;a href=&#34;https://cloud.google.com/vpc-service-controls/docs/supported-products#table_deploy&#34;&gt;Virtual Private Cloud (VPC) Service Controls&lt;/a&gt; (currently in Beta) to respect security perimeters. For safeguards against unwanted approvals, you can take advantage of flow management features such as release &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application#promoting_a_release&#34;&gt;promotion&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#rolling_back_a_deployment&#34;&gt;rollback&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline. You can also create Google Cloud Deploy pipelines in &lt;a href=&#34;https://cloud.google.com/deploy/docs/regions&#34;&gt;supported locations&lt;/a&gt; to better conform with your business needs.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Measurement&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Great tooling is only part of an effective software delivery strategy — you also need to know what metrics you need to measure, how, and why. By making it easier to measure software delivery performance, Google Cloud Deploy helps teams focus on software delivery optimization and achieve their desired business outcomes.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy collects and makes available &lt;a href=&#34;https://cloud.google.com/deploy/docs/metrics&#34;&gt;built in metrics&lt;/a&gt; about delivery pipelines. These include deployment history and success, and also the DORA metric ‘deployment frequency.’&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;2 Promoting a release.gif&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/2_Promoting_a_release.gif&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;Delivery pipeline metrics&lt;/i&gt;&lt;br/&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Monitoring your deployed resources is another way to measure the effectiveness of your software delivery processes. To aid monitoring, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/labels-annotations#automatic_labels_from&#34;&gt;automatically labels deployed Kubernetes resources&lt;/a&gt;, making it easier to associate  your delivery pipelines with application performance. You can integrate application monitoring further using the &lt;a href=&#34;https://cloud.google.com/deploy/docs/api/reference/rest&#34;&gt;Google Cloud Deploy API&lt;/a&gt;, so you can automatically promote code if it is stable and roll it back if an anomaly is detected. &lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you implement complete CI/CD pipelines. And we’re just getting started! Stay tuned as we introduce exciting new capabilities and features to Google Cloud Deploy in the months to come. &lt;/p&gt;&lt;p&gt;In the meantime, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Introducing Google Cloud Deploy: Managed continuous delivery to GKE&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The new Google Cloud Deploy managed services makes it easier to do continuous delivery to Google Kubernetes Engine, and soon, Anthos.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><promo-banner-block _nghost-c45=""></promo-banner-block><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>Google Cloud Deploy</span></p></figure></article-aspect-image-block><div><div><article-author-block><div><div><p> Victor Szalvay </p><p> Product Manager </p></div><p><span> January 25, 2022 </span></p></div></article-author-block></div><article-cta _nghost-c47=""><div _ngcontent-c47=""><h4 _ngcontent-c47=""><span _ngcontent-c47="">Try Google Cloud</span></h4><p _ngcontent-c47=""><span _ngcontent-c47="">Start building on Google Cloud with $300 in free credits and 20+ always free products.</span></p><p><a _ngcontent-c47="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="free trial" track-metadata-eventdetail="https://cloud.google.com/free/" href="https://cloud.google.com/free/"><span _ngcontent-c47="">Free Trial</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;p&gt;Effective software delivery &amp;#8212; usually achieved via continuous integration (CI) and continuous delivery (CD) &amp;#8212; is a top priority for many product development teams. It&amp;#8217;s easy to understand why: the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;2021 State of DevOps report&lt;/a&gt; found that elite performers of software delivery deployed code much more frequently than low performers, with three times fewer change-related failures. Teams who excel at modern software delivery operational practices were also 1.8 times more likely to report better business outcomes.&lt;/p&gt;&lt;p&gt;You need great tools to do software delivery effectively. Without capable tooling, teams have to design, maintain, and scale their software delivery solutions on their own, which can be difficult given the breadth of continuous delivery&amp;#8217;s flow control, security and audit, and integration requirements. Deploying container image artifacts adds further complexity, particularly in Kubernetes environments.&lt;/p&gt;"><p>Effective software delivery — usually achieved via continuous integration (CI) and continuous delivery (CD) — is a top priority for many product development teams. It’s easy to understand why: the <a href="https://cloud.google.com/devops/state-of-devops" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/devops/state-of-devops" track-metadata-module="post">2021 State of DevOps report</a> found that elite performers of software delivery deployed code much more frequently than low performers, with three times fewer change-related failures. Teams who excel at modern software delivery operational practices were also 1.8 times more likely to report better business outcomes.</p><p>You need great tools to do software delivery effectively. Without capable tooling, teams have to design, maintain, and scale their software delivery solutions on their own, which can be difficult given the breadth of continuous delivery’s flow control, security and audit, and integration requirements. Deploying container image artifacts adds further complexity, particularly in Kubernetes environments.</p></div></paragraph-block></div><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;p&gt;Building on your feedback and Google&amp;#8217;s own best practices, we&amp;#8217;ve been working on software delivery tooling that helps you meet your continuous delivery goals &amp;#8212; especially with respect to &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine (GKE)&lt;/a&gt; environments. Today, we are pleased to announce the general availability of &lt;a href=&#34;https://cloud.google.com/deploy/&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&amp;#160;&lt;/p&gt;&lt;h3&gt;Solving for continuous delivery challenges&lt;/h3&gt;&lt;p&gt;While designing Google Cloud Deploy, we talked to a number of customers to better understand the challenges they face doing continuous delivery to GKE. While a handful of themes emerged, three stood out: cost of ownership, security and audit, and measurement.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;As shared in our &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke&#34;&gt;Preview launch post&lt;/a&gt; this past September, the operational cost of Kubernetes continuous delivery can be very high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, collecting key metrics, and staying current &amp;#8212; to say nothing of maintenance &amp;#8212; is resource-intensive and takes time away from the core business.&amp;#160;&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.&amp;#160;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;retained with each release&lt;/a&gt;. That means if your delivery pipeline changes, the release&amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;/p&gt;"><p>Building on your feedback and Google’s own best practices, we’ve been working on software delivery tooling that helps you meet your continuous delivery goals — especially with respect to <a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">Google Kubernetes Engine (GKE)</a> environments. Today, we are pleased to announce the general availability of <a href="https://cloud.google.com/deploy/" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/deploy/" track-metadata-module="post">Google Cloud Deploy</a>, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable. </p><h3>Solving for continuous delivery challenges</h3><p>While designing Google Cloud Deploy, we talked to a number of customers to better understand the challenges they face doing continuous delivery to GKE. While a handful of themes emerged, three stood out: cost of ownership, security and audit, and measurement.</p><p><b>Cost of ownership</b></p><p>As shared in our <a href="https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke" track-metadata-module="post">Preview launch post</a> this past September, the operational cost of Kubernetes continuous delivery can be very high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, collecting key metrics, and staying current — to say nothing of maintenance — is resource-intensive and takes time away from the core business. </p><p>As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. </p><p>Google Cloud Deploy also provides structure. <a href="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-metadata-module="post">Delivery pipelines</a> and <a href="https://cloud.google.com/deploy/docs/terminology" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology" track-metadata-module="post">targets</a> are defined <a href="https://cloud.google.com/deploy/docs/config-files" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/config-files" track-metadata-module="post">declaratively</a> and <a href="https://cloud.google.com/deploy/docs/pipeline-instances" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/pipeline-instances" track-metadata-module="post">retained with each release</a>. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.</p></div></paragraph-block></div><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;p&gt;Whether or not you already have a continuous delivery capability, you likely already have continuous integration, approval and/or operation workflows, and other systems that intersect with your software delivery practices.&amp;#160;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating-ci&#34;&gt;connectivity to CI systems&lt;/a&gt;, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;notifications&lt;/a&gt; to enable &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;related software delivery tooling&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#8220;While looking for a Continuous Delivery solution we considered ArgoCD and Spinnaker, however we chose Google Cloud Deploy because it is a managed service, provided proper CD primitives and integrated seamlessly with our GKE clusters. It has empowered every team member to safely and reliably promote their code from commit all the way through to production.&amp;#8221;&amp;#8212;Jonathan Sokolowski, DevOps Engineer, Search.io&lt;/i&gt;&lt;/p&gt;&lt;p&gt;A variety of GKE roles and personas interact with continuous delivery processes. DevOps engineers are focused on release promotion and rollback decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy&amp;#8217;s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.&lt;/p&gt;"><p>Whether or not you already have a continuous delivery capability, you likely already have continuous integration, approval and/or operation workflows, and other systems that intersect with your software delivery practices. </p><p>Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/integrating" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating" track-metadata-module="post">embraces the GKE delivery tooling ecosystems</a> in three ways: <a href="https://cloud.google.com/deploy/docs/integrating-ci" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating-ci" track-metadata-module="post">connectivity to CI systems</a>, support for leading configuration (<a href="https://cloud.google.com/deploy/docs/terminology#render" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#render" track-metadata-module="post">rendering</a>) tooling, and <a href="https://cloud.google.com/pubsub" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/pubsub" track-metadata-module="post">Pub/Sub</a> <a href="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-metadata-module="post">notifications</a> to enable <a href="https://cloud.google.com/deploy/docs/integrating" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating" track-metadata-module="post">related software delivery tooling</a>.</p><p><i>“While looking for a Continuous Delivery solution we considered ArgoCD and Spinnaker, however we chose Google Cloud Deploy because it is a managed service, provided proper CD primitives and integrated seamlessly with our GKE clusters. It has empowered every team member to safely and reliably promote their code from commit all the way through to production.”—Jonathan Sokolowski, DevOps Engineer, Search.io</i></p><p>A variety of GKE roles and personas interact with continuous delivery processes. DevOps engineers are focused on release promotion and rollback decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.</p></div></paragraph-block></div><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;p&gt;&lt;b&gt;Security and control&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy&amp;#8217;s security foundations strengthen secure software supply chain practices through delivery flow control and auditability.&lt;/p&gt;&lt;p&gt;Lots of different users interact with a software delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn&amp;#8217;t always mean you can create release candidates, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;restriction through IAM&lt;/a&gt;, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/securing/iam&#34;&gt;discrete access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. Google Cloud Deploy also supports &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment#deploying_to_a_private_cluster_on_a_network&#34;&gt;deploying to private GKE clusters&lt;/a&gt; and&amp;#160; &lt;a href=&#34;https://cloud.google.com/vpc-service-controls/docs/supported-products#table_deploy&#34;&gt;Virtual Private Cloud (VPC) Service Controls&lt;/a&gt; (currently in Beta) to respect security perimeters. For safeguards against unwanted approvals, you can take advantage of flow management features such as release &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application#promoting_a_release&#34;&gt;promotion&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#rolling_back_a_deployment&#34;&gt;rollback&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline. You can also create Google Cloud Deploy pipelines in &lt;a href=&#34;https://cloud.google.com/deploy/docs/regions&#34;&gt;supported locations&lt;/a&gt; to better conform with your business needs.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Measurement&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Great tooling is only part of an effective software delivery strategy &amp;#8212; you also need to know what metrics you need to measure, how, and why. By making it easier to measure software delivery performance, Google Cloud Deploy helps teams focus on software delivery optimization and achieve their desired business outcomes.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy collects and makes available &lt;a href=&#34;https://cloud.google.com/deploy/docs/metrics&#34;&gt;built in metrics&lt;/a&gt; about delivery pipelines. These include deployment history and success, and also the DORA metric &amp;#8216;deployment frequency.&amp;#8217;&lt;/p&gt;"><p><b>Security and control</b></p><p>Google Cloud Deploy’s security foundations strengthen secure software supply chain practices through delivery flow control and auditability.</p><p>Lots of different users interact with a software delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create release candidates, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.</p><p>Throughout, Google Cloud Deploy enables fine-grained <a href="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-metadata-module="post">restriction through IAM</a>, with <a href="https://cloud.google.com/deploy/docs/securing/iam" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/securing/iam" track-metadata-module="post">discrete access control</a> and <a href="https://cloud.google.com/deploy/docs/execution-environment" track-type="inline link" track-name="17" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/execution-environment" track-metadata-module="post">execution-level security</a>. Google Cloud Deploy also supports <a href="https://cloud.google.com/deploy/docs/execution-environment#deploying_to_a_private_cluster_on_a_network" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/execution-environment#deploying_to_a_private_cluster_on_a_network" track-metadata-module="post">deploying to private GKE clusters</a> and  <a href="https://cloud.google.com/vpc-service-controls/docs/supported-products#table_deploy" track-type="inline link" track-name="19" track-metadata-eventdetail="https://cloud.google.com/vpc-service-controls/docs/supported-products#table_deploy" track-metadata-module="post">Virtual Private Cloud (VPC) Service Controls</a> (currently in Beta) to respect security perimeters. For safeguards against unwanted approvals, you can take advantage of flow management features such as release <a href="https://cloud.google.com/deploy/docs/deploying-application#promoting_a_release" track-type="inline link" track-name="20" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/deploying-application#promoting_a_release" track-metadata-module="post">promotion</a>, <a href="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#rolling_back_a_deployment" track-type="inline link" track-name="21" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#rolling_back_a_deployment" track-metadata-module="post">rollback</a>, and <a href="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-metadata-module="post">approvals</a>. </p><p>Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. <a href="https://cloud.google.com/audit-logs" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/audit-logs" track-metadata-module="post">Cloud Audit Logs</a> <a href="https://cloud.google.com/deploy/docs/audit-logs" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/audit-logs" track-metadata-module="post">audits</a> user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline. You can also create Google Cloud Deploy pipelines in <a href="https://cloud.google.com/deploy/docs/regions" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/regions" track-metadata-module="post">supported locations</a> to better conform with your business needs.</p><p><b>Measurement</b></p><p>Great tooling is only part of an effective software delivery strategy — you also need to know what metrics you need to measure, how, and why. By making it easier to measure software delivery performance, Google Cloud Deploy helps teams focus on software delivery optimization and achieve their desired business outcomes.</p><p>Google Cloud Deploy collects and makes available <a href="https://cloud.google.com/deploy/docs/metrics" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/metrics" track-metadata-module="post">built in metrics</a> about delivery pipelines. These include deployment history and success, and also the DORA metric ‘deployment frequency.’</p></div></paragraph-block></div><div><paragraph-block _nghost-c49=""><div _ngcontent-c49="" innerhtml="&lt;p&gt;Monitoring your deployed resources is another way to measure the effectiveness of your software delivery processes. To aid monitoring, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/labels-annotations#automatic_labels_from&#34;&gt;automatically labels deployed Kubernetes resources&lt;/a&gt;, making it easier to associate&amp;#160; your delivery pipelines with application performance. You can integrate application monitoring further using the &lt;a href=&#34;https://cloud.google.com/deploy/docs/api/reference/rest&#34;&gt;Google Cloud Deploy API&lt;/a&gt;, so you can automatically promote code if it is stable and roll it back if an anomaly is detected.&amp;#160;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it&amp;#8217;s our hope that Google Cloud Deploy will help you implement complete CI/CD pipelines. And we&amp;#8217;re just getting started! Stay tuned as we introduce exciting new capabilities and features to Google Cloud Deploy in the months to come.&amp;#160;&lt;/p&gt;&lt;p&gt;In the meantime, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;/p&gt;"><p>Monitoring your deployed resources is another way to measure the effectiveness of your software delivery processes. To aid monitoring, Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/labels-annotations#automatic_labels_from" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/labels-annotations#automatic_labels_from" track-metadata-module="post">automatically labels deployed Kubernetes resources</a>, making it easier to associate  your delivery pipelines with application performance. You can integrate application monitoring further using the <a href="https://cloud.google.com/deploy/docs/api/reference/rest" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/api/reference/rest" track-metadata-module="post">Google Cloud Deploy API</a>, so you can automatically promote code if it is stable and roll it back if an anomaly is detected. </p><h3>The future</h3><p>Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you implement complete CI/CD pipelines. And we’re just getting started! Stay tuned as we introduce exciting new capabilities and features to Google Cloud Deploy in the months to come. </p><p>In the meantime, check out the <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">product page</a>, <a href="https://cloud.google.com/deploy/docs" track-type="inline link" track-name="30" track-metadata-eventdetail="https://cloud.google.com/deploy/docs" track-metadata-module="post">documentation</a>, <a href="https://cloud.google.com/deploy/docs/quickstart-basic" track-type="inline link" track-name="31" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/quickstart-basic" track-metadata-module="post">quickstart</a>, and <a href="https://cloud.google.com/deploy/docs/tutorials" track-type="inline link" track-name="32" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/tutorials" track-metadata-module="post">tutorials</a>. Finally, If you have feedback on Google Cloud Deploy, you can <a href="https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy" target="_blank" track-type="inline link" track-name="33" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">join the conversation</a>. We look forward to hearing from you!</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c48=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Victor Szalvay&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 25 Jan 2022 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Understanding Firestore performance with Key Visualizer</title>
      <link>https://cloud.google.com/blog/products/databases/understanding-firestore-performance-with-key-visualizer/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://cloud/blog/topics/developers-practitioners/all-you-need-know-about-firestore-cheatsheet&#34; target=&#34;_blank&#34;&gt;Firestore&lt;/a&gt; is a serverless, scalable, NoSQL document database. It is ideal for rapid and flexible web and mobile application development, and uniquely supports real-time client device syncing to the database.&lt;/p&gt;&lt;p&gt;To get the best performance out of &lt;a href=&#34;https://cloud.google.com/firestore&#34;&gt;Firestore&lt;/a&gt;, while also making the most out of Firestore&#39;s automatic scaling and load balancing features, you need to make sure the data layout of your application allows requests to be processed optimally, particularly as your user traffic increases. There are some subtleties to be aware of when it comes to what could happen when traffic ramps up, and to help make this easier to identify, we’re announcing the General Availability of &lt;a href=&#34;https://cloud.google.com/firestore/docs/key-visualizer&#34;&gt;&lt;b&gt;Key Visualizer&lt;/b&gt;&lt;/a&gt;, an interactive, performance monitoring tool for Firestore.&lt;/p&gt;&lt;p&gt;Key Visualizer generates visual reports based on Firestore documents accessed over time, that will help you understand and optimize the access patterns of your database, as well as troubleshoot performance issues. With Key Visualizer, you can iteratively design a data model or improve your existing application’s data usage pattern.&lt;/p&gt;&lt;p&gt;Tip: While Key Visualizer can be used with production databases, it’s best to identify performance issues prior to rolling out changes in production. Consider running application load tests with Firestore in a pre-production environment, and using Key Visualizer to identify potential issues.&lt;/p&gt;&lt;h3&gt;Viewing a visualization&lt;/h3&gt;&lt;p&gt;The Key Visualizer tool is available to all Firestore customers. Visualizations are generated at every hour boundary, covering data for the preceding two hours. Visualizations are generated as long as overall database traffic during a selected period meets the &lt;a href=&#34;https://cloud.google.com/firestore/docs/key-visualizer#scan_eligibility&#34;&gt;scan eligibility&lt;/a&gt; criteria.&lt;/p&gt;&lt;p&gt;To get an overview of activity using Key Visualizer, first select a two-hour time period and review the heatmap for the &#34;Total ops/s&#34; metric. This view estimates the number of operations per second and how they are distributed across your database. Total ops/s is an estimated sum of write, lookup, and query operations averaged by seconds.&lt;/p&gt;&lt;p&gt;Firestore automatically scales using a technique called range sharding. When using Firestore, you model data in the form of documents stored in hierarchies of collections. The collection hierarchy and document ID is translated to a single key for each document. Documents are logically stored and ordered lexicographically by this key. We use the term &#34;key range&#34; to refer to a range of keys. The full key range is then automatically split up as-needed, driven by storage and traffic load, and served by many replicated servers inside of Firestore.&lt;/p&gt;&lt;p&gt;The following example of Key Visualizer visualization shows a heatmap where there are some major differences in the usage pattern across the database. The X-axis is time, and the Y-axis is the key range for your database, broken down into buckets by traffic.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Ranges shown in dark colors have little or no activity.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ranges in bright colors have significantly more activity. In the example below, you can see the &#34;Bar&#34; and &#34;Qux&#34; collections going beyond 50 operations per second for some period of time.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;1 Firestore.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Firestore.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Additional methods of interpreting Key Visualizer visualizations are detailed in our &lt;a href=&#34;https://cloud.google.com/firestore/docs/keyvis-patterns&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Besides the total number of operations, Key Visualizer also provides views with metrics for ops per second, average latency, and tail latency, where traffic is broken down for writes and deletes, lookups, and queries. This capability allows you to identify issues with your data layout or poorly balanced traffic that may be contributing to increased latencies.&lt;/p&gt;&lt;h3&gt;Hotspots and heatmap patterns&lt;/h3&gt;&lt;p&gt;Key Visualizer gives you insight into how your traffic is distributed, and lets you understand if latency increases correlate with a hotspot, thus providing you with information to determine what parts of your application need to change. We refer to a &#34;hotspot&#34; as a case where traffic is poorly balanced across the database&#39;s keyspace. For the best performance, requests should be distributed evenly across a keyspace. The effect of a hotspot can vary, but typically hotspotting causes higher latency and in some cases, even failed operations.&lt;/p&gt;&lt;p&gt;Firestore automatically splits a key range into smaller pieces and distributes the work of serving traffic to more servers when needed. However, this has some limitations. Splitting storage and load takes time, and ramping up traffic too fast may cause hotspots while the service adjusts. The best practice is to distribute operations across the key range, while &lt;a href=&#34;https://cloud.google.com/firestore/docs/best-practices#ramping_up_traffic&#34;&gt;ramping up traffic&lt;/a&gt; on a cold database with 500 operations per second, and then increasing traffic by up to 50% every 5 minutes. This is called the &#34;500/50/5&#34; rule, and allows you to rapidly warm up a cold database safely. For example, ramping to 1,000,000 ops/s can be achieved in under two hours.&lt;/p&gt;&lt;p&gt;Firestore can automatically split a key range until it is serving a single document using a dedicated set of replicated servers. Once this threshold is hit, Firestore is unable to create further splits beyond a single document. As a result, high and sustained volumes of concurrent operations on a single document may result in elevated latencies. You can observe these high latencies using Key Visualizer’s average and tail latency metrics. If you encounter sustained high latencies on a single document, you should consider modifying your data model to split or replicate the data across multiple documents.&lt;/p&gt;&lt;p&gt;Key Visualizer will also help you identify additional traffic patterns:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph_with_image&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;figure class=&#34;article-image--wrap-small &#34;&gt;&lt;img alt=&#34;2 Firestore.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Firestore.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Evenly distributed usage&lt;/b&gt;: If a heatmap shows a fine-grained mix of dark and bright colors, then reads and writes are evenly distributed throughout the database. This heatmap represents an effective usage pattern for Firestore, and no additional action is required.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph_with_image&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;figure class=&#34;article-image--wrap-small &#34;&gt;&lt;img alt=&#34;3 Firestore.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Firestore.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Sequential Keys&lt;/b&gt;: A heatmap with a single bright diagonal line can indicate a special hotspotting case where the database is using strictly increasing or decreasing keys (document IDs). Sequential keys are an anti-pattern in Firestore, which will result in elevated latency especially at higher operations per second. In this case, the document IDs that are generated and utilized should be randomized. To learn more, see the &lt;a href=&#34;https://cloud.devsite.corp.google.com/firestore/docs/best-practices#high_read_write_and_delete_rates_to_a_narrow_document_range&#34;&gt;best practices page&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph_with_image&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;figure class=&#34;article-image--wrap-small &#34;&gt;&lt;img alt=&#34;4 Firestore.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/4_Firestore.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Sudden traffic increase&lt;/b&gt;: A heatmap with a key range that suddenly changes from dark to bright indicates a sudden spike in load. If the load increase isn’t well distributed across the key range, and exceeds the 500/50/5 rule best practice, the database can experience elevated latency in the operations. In this case, the data layout should be modified to reflect a better distribution of usage and traffic across the keyspace.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Next steps&lt;/h3&gt;&lt;p&gt;Firestore Key Visualizer is a performance monitoring tool available to administrators and developers to better understand how their applications interact with Firestore. With this launch, Firestore joins our family of Cloud-native databases, including &lt;a href=&#34;https://cloud.google.com/spanner&#34;&gt;Cloud Spanner&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/bigtable&#34;&gt;Cloud Bigtable&lt;/a&gt;, in offering Key Visualizer to its customers. You can get started with Firestore Key Visualizer for free, from the &lt;a href=&#34;https://console.cloud.google.com/firestore/key-visualizer&#34;&gt;Cloud Console&lt;/a&gt;.&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;&lt;i&gt;Acknowledgement&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Special thanks to Minh Nguyen, Lead Product Manager for Firestore, for contributing to this post.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c16=""><div _ngcontent-c16="" innerhtml="&lt;p&gt;&lt;a href=&#34;http://cloud/blog/topics/developers-practitioners/all-you-need-know-about-firestore-cheatsheet&#34; target=&#34;_blank&#34;&gt;Firestore&lt;/a&gt; is a serverless, scalable, NoSQL document database. It is ideal for rapid and flexible web and mobile application development, and uniquely supports real-time client device syncing to the database.&lt;/p&gt;&lt;p&gt;To get the best performance out of &lt;a href=&#34;https://cloud.google.com/firestore&#34;&gt;Firestore&lt;/a&gt;, while also making the most out of Firestore&#39;s automatic scaling and load balancing features, you need to make sure the data layout of your application allows requests to be processed optimally, particularly as your user traffic increases. There are some subtleties to be aware of when it comes to what could happen when traffic ramps up, and to help make this easier to identify, we&amp;#8217;re announcing the General Availability of &lt;a href=&#34;https://cloud.google.com/firestore/docs/key-visualizer&#34;&gt;&lt;b&gt;Key Visualizer&lt;/b&gt;&lt;/a&gt;, an interactive, performance monitoring tool for Firestore.&lt;/p&gt;&lt;p&gt;Key Visualizer generates visual reports based on Firestore documents accessed over time, that will help you understand and optimize the access patterns of your database, as well as troubleshoot performance issues. With Key Visualizer, you can iteratively design a data model or improve your existing application&amp;#8217;s data usage pattern.&lt;/p&gt;&lt;p&gt;Tip: While Key Visualizer can be used with production databases, it&amp;#8217;s best to identify performance issues prior to rolling out changes in production. Consider running application load tests with Firestore in a pre-production environment, and using Key Visualizer to identify potential issues.&lt;/p&gt;&lt;h3&gt;Viewing a visualization&lt;/h3&gt;&lt;p&gt;The Key Visualizer tool is available to all Firestore customers. Visualizations are generated at every hour boundary, covering data for the preceding two hours. Visualizations are generated as long as overall database traffic during a selected period meets the &lt;a href=&#34;https://cloud.google.com/firestore/docs/key-visualizer#scan_eligibility&#34;&gt;scan eligibility&lt;/a&gt; criteria.&lt;/p&gt;&lt;p&gt;To get an overview of activity using Key Visualizer, first select a two-hour time period and review the heatmap for the &amp;#34;Total ops/s&amp;#34; metric. This view estimates the number of operations per second and how they are distributed across your database. Total ops/s is an estimated sum of write, lookup, and query operations averaged by seconds.&lt;/p&gt;&lt;p&gt;Firestore automatically scales using a technique called range sharding. When using Firestore, you model data in the form of documents stored in hierarchies of collections. The collection hierarchy and document ID is translated to a single key for each document. Documents are logically stored and ordered lexicographically by this key. We use the term &amp;#34;key range&amp;#34; to refer to a range of keys. The full key range is then automatically split up as-needed, driven by storage and traffic load, and served by many replicated servers inside of Firestore.&lt;/p&gt;&lt;p&gt;The following example of Key Visualizer visualization shows a heatmap where there are some major differences in the usage pattern across the database. The X-axis is time, and the Y-axis is the key range for your database, broken down into buckets by traffic.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Ranges shown in dark colors have little or no activity.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ranges in bright colors have significantly more activity. In the example below, you can see the &amp;#34;Bar&amp;#34; and &amp;#34;Qux&amp;#34; collections going beyond 50 operations per second for some period of time.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;"><p><a href="http://cloud/blog/topics/developers-practitioners/all-you-need-know-about-firestore-cheatsheet" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="http://cloud" track-metadata-module="post">Firestore</a> is a serverless, scalable, NoSQL document database. It is ideal for rapid and flexible web and mobile application development, and uniquely supports real-time client device syncing to the database.</p><p>To get the best performance out of <a href="https://cloud.google.com/firestore" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/firestore" track-metadata-module="post">Firestore</a>, while also making the most out of Firestore&#39;s automatic scaling and load balancing features, you need to make sure the data layout of your application allows requests to be processed optimally, particularly as your user traffic increases. There are some subtleties to be aware of when it comes to what could happen when traffic ramps up, and to help make this easier to identify, we’re announcing the General Availability of <a href="https://cloud.google.com/firestore/docs/key-visualizer" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/firestore/docs/key-visualizer" track-metadata-module="post"><b>Key Visualizer</b></a>, an interactive, performance monitoring tool for Firestore.</p><p>Key Visualizer generates visual reports based on Firestore documents accessed over time, that will help you understand and optimize the access patterns of your database, as well as troubleshoot performance issues. With Key Visualizer, you can iteratively design a data model or improve your existing application’s data usage pattern.</p><p>Tip: While Key Visualizer can be used with production databases, it’s best to identify performance issues prior to rolling out changes in production. Consider running application load tests with Firestore in a pre-production environment, and using Key Visualizer to identify potential issues.</p><h3>Viewing a visualization</h3><p>The Key Visualizer tool is available to all Firestore customers. Visualizations are generated at every hour boundary, covering data for the preceding two hours. Visualizations are generated as long as overall database traffic during a selected period meets the <a href="https://cloud.google.com/firestore/docs/key-visualizer#scan_eligibility" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/firestore/docs/key-visualizer#scan_eligibility" track-metadata-module="post">scan eligibility</a> criteria.</p><p>To get an overview of activity using Key Visualizer, first select a two-hour time period and review the heatmap for the &#34;Total ops/s&#34; metric. This view estimates the number of operations per second and how they are distributed across your database. Total ops/s is an estimated sum of write, lookup, and query operations averaged by seconds.</p><p>Firestore automatically scales using a technique called range sharding. When using Firestore, you model data in the form of documents stored in hierarchies of collections. The collection hierarchy and document ID is translated to a single key for each document. Documents are logically stored and ordered lexicographically by this key. We use the term &#34;key range&#34; to refer to a range of keys. The full key range is then automatically split up as-needed, driven by storage and traffic load, and served by many replicated servers inside of Firestore.</p><p>The following example of Key Visualizer visualization shows a heatmap where there are some major differences in the usage pattern across the database. The X-axis is time, and the Y-axis is the key range for your database, broken down into buckets by traffic.</p><ul><li><p>Ranges shown in dark colors have little or no activity.</p></li><li><p>Ranges in bright colors have significantly more activity. In the example below, you can see the &#34;Bar&#34; and &#34;Qux&#34; collections going beyond 50 operations per second for some period of time.</p></li></ul></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Amarnath Mullick&lt;/name&gt;&lt;title&gt;Tech Lead and Engineering Manager for Firestore Performance&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/cloud_firestore.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 18 Jan 2022 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>DevOps for tech companies and startups: Learn from over 32,000 professionals on how to drive success with Google Cloud’s DORA research</title>
      <link>https://cloud.google.com/blog/products/devops-sre/implementing-google-cloud-devops-for-your-cloud-native-organization/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Many technology-driven organizations and startups use &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;DevOps&lt;/a&gt; as a business enabler, allowing them to bring ideas to market quickly, increase developer productivity, and increase their customer base. Adopting DevOps workflows can drive success but many companies continue to struggle with how to get started or optimize the DevOps tools they currently have incorporated.&lt;/p&gt;&lt;p&gt;Google Cloud’s DevOps Research and Assessment (&lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;DORA&lt;/a&gt;) team helps organizations to deploy faster, scale on demand, and balance costs by providing essential tools and resources for you to succeed. Over the past seven years, our DORA team has surveyed more than 32,000 professionals worldwide via our yearly &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;Accelerate State of DevOps reports&lt;/a&gt; (SODR). As the largest and longest-running research of its kind, the SODR provides data-driven industry insights that examine the capabilities and practices that drive software delivery and operational and organizational performance - no matter the industry. &lt;/p&gt;&lt;p&gt;Read below to learn more from our DORA team about how and why your organization should focus on DevOps this year: &lt;/p&gt;&lt;h3&gt;Benchmark your team, identify improvement opportunities&lt;/h3&gt;&lt;p&gt;Our &lt;a href=&#34;https://inthecloud.withgoogle.com/devops-quick-check/dl-cd.html?utm_source=google_owned_website&amp;amp;utm_medium=et&amp;amp;utm_campaign=FY20-Q3-global-demandgen-website-other-gcp_gtm_cost_amp_devops_quick_check_mc&amp;amp;utm_content=app_mod_lp_cta&amp;amp;utm_term=-&amp;amp;_ga=2.131280267.562104682.1627912649-1057981864.1627522111&#34; target=&#34;_blank&#34;&gt;DevOps Quick Check&lt;/a&gt; is based on DORA research and allows companies to gauge their DevOps implementation with just five multiple choice questions. The DevOps Quick Check uses the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance&#34;&gt;four key DevOps metrics&lt;/a&gt; to help your team assess your current performance, compare your performance to others in your industry, and identify which capabilities are most likely to impact your performance. Teams in every industry recognize the value of quickly taking code from development to production and are looking for ways to improve their organizations&#39; agility. Compromising stability is not an option. The data from the research program shows that speed and stability go hand-in-hand, in fact elite performing teams are nearly twice as likely to have increased software delivery performance and achieve 6570x faster lead-time-to-deploy changes.&lt;/p&gt;&lt;h3&gt;Increasing developer productivity&lt;/h3&gt;&lt;p&gt;Along with driving software delivery performance, DevOps helps to increase developer productivity by reducing burnout - something that is a top priority for many organizations. According to the SODR, 89% of respondents worked from home during the pandemic but teams with a&lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-culture-westrum-organizational-culture&#34;&gt;generative team culture,&lt;/a&gt; with people who had feelings of inclusion and belonging within their team, were half as likely to experience burnout. A generative culture along with easy to use &lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-tech-teams-empowered-to-choose-tools&#34;&gt;developer tools&lt;/a&gt;help to streamline developers’ workflows and the process of working with cloud infrastructure.&lt;/p&gt;&lt;p&gt;Developer tools that keep developers focused on what they do best, writing code,  is vital to increase developer productivity. Companies should utilize tools that ensure developers spend as little time as possible containerizing applications while increasing automation are paramount. &lt;/p&gt;&lt;p&gt;We commonly see developers leverage &lt;a href=&#34;https://docs.docker.com/&#34; target=&#34;_blank&#34;&gt;Docker&lt;/a&gt; to bring modern applications to life, and there are many benefits of doing so, especially in terms of portability. However, using Docker increases the operational burden on developers. Whereas those that leverage &lt;a href=&#34;https://www.youtube.com/watch?v=suhCr5W_bFc&#34; target=&#34;_blank&#34;&gt;Buildpacks&lt;/a&gt; are able to reduce operational burden while supporting enterprise operators who manage apps at scale. They are able to do this because Buildpacks allows code to go straight from source to production, in addition to making it easier to meet security and compliance requirements without developer intervention. &lt;/p&gt;&lt;p&gt;Plus when developers want to speed up the deployment process with flexibility they have the option to leverage &lt;a href=&#34;https://cloud.google.com/run&#34;&gt;Cloud Run&lt;/a&gt;, our fully managed serverless platform offering, which fully supports the use of Buildpacks. Below are some benefits developers and operators can see from Cloud Run:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Increased Productivity&lt;/b&gt;: make it easier for developers to onboard more quickly and deploy faster &lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;Hiring&lt;/i&gt;: choose a platform that makes all developers productive. Serverless platforms, like Cloud Run, help developers who know fewer programming languages become  productive without the need to also be an IT expert &lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Decreased Time to Market&lt;/b&gt;: accelerate software releases and value creation to customers that ultimately increase revenue and customer loyalty&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Decreased Cost&lt;/b&gt;: Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantaneously - so you only pay for what you use&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Whether you choose Cloud Run or another offering - it’s not just which cloud infrastructure you choose, but how you implement cloud services that really matters. This is especially important when it comes to being able to scale quickly and efficiently. In the SODR, we found that Elite performers were 3.5 times more likely to have met all essential &lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure&#34;&gt;NIST cloud characteristics&lt;/a&gt;. In terms of scaling, the characteristic of rapid elasticity is key to rapidly scaling outward or inward with demand. In other words, it is very important that your capabilities can be elastically provisioned and released - so no matter how much you grow, your customers always have access to your services. &lt;/p&gt;&lt;h3&gt;The Business Case for DevOps&lt;/h3&gt;&lt;p&gt;Maybe you&#39;re convinced that achieving better speed and stability will help your team but how do you convince your boss?&lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper&#34;&gt;ROI of DevOps Transformation&lt;/a&gt; provides IT and business decision makers an industry backed, data-driven foundational basis for measuring their investment in DevOps. We found that money saved from DevOps transformation varies from $10M to $259M a year with a return on investment of approximately 30 days. You can use the metrics you provided in the &lt;a href=&#34;https://www.devops-research.com/quickcheck.html&#34; target=&#34;_blank&#34;&gt;DevOps Quick Check&lt;/a&gt; along with some additional information to get an estimate on your team&#39;s potential return.&lt;/p&gt;&lt;h3&gt;Google Cloud DevOps Awards&lt;/h3&gt;&lt;p&gt;Using a collection of these resources along with an objective assessment of how your team is doing, your organization will be able to get quick insights into improvement areas. Change your work by improving these capabilities to deliver more innovation to your customers and improve the speed and stability of your software delivery.&lt;/p&gt;&lt;p&gt;Now that we have shared some of our DevOps best practices with you, we would love to hear about how you are transforming your organization with DevOps. During our awards celebration in March we will be celebrating how our most advanced teams are using DevOps, so tell us about the positive impact that DevOps has had on your teams, customers, and organization. &lt;a href=&#34;https://cloud.google.com/awards/devops&#34;&gt;Enter your submission for the Google Cloud DevOps Awards&lt;/a&gt; today. But don&#39;t delay! The deadline for submissions is January 31, 2022.&lt;/p&gt;&lt;p&gt;What questions do you have about incorporating DevOps practices into your daily work? &lt;a href=&#34;https://inthecloud.withgoogle.com/born-digital/dl-cd.html&#34; target=&#34;_blank&#34;&gt;Reach out to our experts at Google Cloud.&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c16=""><div _ngcontent-c16="" innerhtml="&lt;p&gt;Many technology-driven organizations and startups use &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;DevOps&lt;/a&gt; as a business enabler, allowing them to bring ideas to market quickly, increase developer productivity, and increase their customer base. Adopting DevOps workflows can drive success but many companies continue to struggle with how to get started or optimize the DevOps tools they currently have incorporated.&lt;/p&gt;&lt;p&gt;Google Cloud&amp;#8217;s DevOps Research and Assessment (&lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;DORA&lt;/a&gt;) team helps organizations to deploy faster, scale on demand, and balance costs by providing essential tools and resources for you to succeed. Over the past seven years, our DORA team has surveyed more than 32,000 professionals worldwide via our yearly &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;Accelerate State of DevOps reports&lt;/a&gt; (SODR). As the largest and longest-running research of its kind, the SODR provides data-driven industry insights that examine the capabilities and practices that drive software delivery and operational and organizational performance - no matter the industry.&amp;#160;&lt;/p&gt;&lt;p&gt;Read below to learn more from our DORA team about how and why your organization should focus on DevOps this year:&amp;#160;&lt;/p&gt;&lt;h3&gt;Benchmark your team, identify improvement opportunities&lt;/h3&gt;&lt;p&gt;Our &lt;a href=&#34;https://inthecloud.withgoogle.com/devops-quick-check/dl-cd.html?utm_source=google_owned_website&amp;amp;utm_medium=et&amp;amp;utm_campaign=FY20-Q3-global-demandgen-website-other-gcp_gtm_cost_amp_devops_quick_check_mc&amp;amp;utm_content=app_mod_lp_cta&amp;amp;utm_term=-&amp;amp;_ga=2.131280267.562104682.1627912649-1057981864.1627522111&#34; target=&#34;_blank&#34;&gt;DevOps Quick Check&lt;/a&gt; is based on DORA research and allows companies to gauge their DevOps implementation with just five multiple choice questions. The DevOps Quick Check uses the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance&#34;&gt;four key DevOps metrics&lt;/a&gt; to help your team assess your current performance, compare your performance to others in your industry, and identify which capabilities are most likely to impact your performance. Teams in every industry recognize the value of quickly taking code from development to production and are looking for ways to improve their organizations&#39; agility. Compromising stability is not an option. The data from the research program shows that speed and stability go hand-in-hand, in fact elite performing teams are nearly twice as likely to have increased software delivery performance and achieve 6570x faster lead-time-to-deploy changes.&lt;/p&gt;&lt;h3&gt;Increasing developer productivity&lt;/h3&gt;&lt;p&gt;Along with driving software delivery performance, DevOps helps to increase developer productivity by reducing burnout - something that is a top priority for many organizations. According to the SODR, 89% of respondents worked from home during the pandemic but teams with a&lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-culture-westrum-organizational-culture&#34;&gt; generative team culture,&lt;/a&gt; with people who had feelings of inclusion and belonging within their team, were half as likely to experience burnout. A generative culture along with easy to use &lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-tech-teams-empowered-to-choose-tools&#34;&gt;developer tools &lt;/a&gt;help to streamline developers&amp;#8217; workflows and the process of working with cloud infrastructure.&lt;/p&gt;&lt;p&gt;Developer tools that keep developers focused on what they do best, writing code,&amp;#160; is vital to increase developer productivity. Companies should utilize tools that ensure developers spend as little time as possible containerizing applications while increasing automation are paramount.&amp;#160;&lt;/p&gt;&lt;p&gt;We commonly see developers leverage &lt;a href=&#34;https://docs.docker.com/&#34; target=&#34;_blank&#34;&gt;Docker&lt;/a&gt; to bring modern applications to life, and there are many benefits of doing so, especially in terms of portability. However, using Docker increases the operational burden on developers. Whereas those that leverage &lt;a href=&#34;https://www.youtube.com/watch?v=suhCr5W_bFc&#34; target=&#34;_blank&#34;&gt;Buildpacks&lt;/a&gt; are able to reduce operational burden while supporting enterprise operators who manage apps at scale. They are able to do this because Buildpacks allows code to go straight from source to production, in addition to making it easier to meet security and compliance requirements without developer intervention.&amp;#160;&lt;/p&gt;&lt;p&gt;Plus when developers want to speed up the deployment process with flexibility they have the option to leverage &lt;a href=&#34;https://cloud.google.com/run&#34;&gt;Cloud Run&lt;/a&gt;, our fully managed serverless platform offering, which fully supports the use of Buildpacks. Below are some benefits developers and operators can see from Cloud Run:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Increased Productivity&lt;/b&gt;: make it easier for developers to onboard more quickly and deploy faster&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;Hiring&lt;/i&gt;: choose a platform that makes all developers productive. Serverless platforms, like Cloud Run, help developers who know fewer programming languages become&amp;#160; productive without the need to also be an IT expert&amp;#160;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Decreased Time to Market&lt;/b&gt;: accelerate software releases and value creation to customers that ultimately increase revenue and customer loyalty&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Decreased Cost&lt;/b&gt;: Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantaneously - so you only pay for what you use&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Whether you choose Cloud Run or another offering - it&amp;#8217;s not just which cloud infrastructure you choose, but how you implement cloud services that really matters. This is especially important when it comes to being able to scale quickly and efficiently. In the SODR, we found that Elite performers were 3.5 times more likely to have met all essential &lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure&#34;&gt;NIST cloud characteristics&lt;/a&gt;. In terms of scaling, the characteristic of rapid elasticity is key to rapidly scaling outward or inward with demand. In other words, it is very important that your capabilities can be elastically provisioned and released - so no matter how much you grow, your customers always have access to your services.&amp;#160;&lt;/p&gt;&lt;h3&gt;The Business Case for DevOps&lt;/h3&gt;&lt;p&gt;Maybe you&#39;re convinced that achieving better speed and stability will help your team but how do you convince your boss?&lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper&#34;&gt;ROI of DevOps Transformation&lt;/a&gt; provides IT and business decision makers an industry backed, data-driven foundational basis for measuring their investment in DevOps. We found that money saved from DevOps transformation varies from $10M to $259M a year with a return on investment of approximately 30 days. You can use the metrics you provided in the &lt;a href=&#34;https://www.devops-research.com/quickcheck.html&#34; target=&#34;_blank&#34;&gt;DevOps Quick Check&lt;/a&gt; along with some additional information to get an estimate on your team&#39;s potential return.&lt;/p&gt;&lt;h3&gt;Google Cloud DevOps Awards&lt;/h3&gt;&lt;p&gt;Using a collection of these resources along with an objective assessment of how your team is doing, your organization will be able to get quick insights into improvement areas. Change your work by improving these capabilities to deliver more innovation to your customers and improve the speed and stability of your software delivery.&lt;/p&gt;&lt;p&gt;Now that we have shared some of our DevOps best practices with you, we would love to hear about how you are transforming your organization with DevOps. During our awards celebration in March we will be celebrating how our most advanced teams are using DevOps, so tell us about the positive impact that DevOps has had on your teams, customers, and organization. &lt;a href=&#34;https://cloud.google.com/awards/devops&#34;&gt;Enter your submission for the Google Cloud DevOps Awards&lt;/a&gt; today. But don&#39;t delay! The deadline for submissions is January 31, 2022.&lt;/p&gt;&lt;p&gt;What questions do you have about incorporating DevOps practices into your daily work? &lt;a href=&#34;https://inthecloud.withgoogle.com/born-digital/dl-cd.html&#34; target=&#34;_blank&#34;&gt;Reach out to our experts at Google Cloud.&lt;/a&gt;&lt;/p&gt;"><p>Many technology-driven organizations and startups use <a href="https://cloud.google.com/devops" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/devops" track-metadata-module="post">DevOps</a> as a business enabler, allowing them to bring ideas to market quickly, increase developer productivity, and increase their customer base. Adopting DevOps workflows can drive success but many companies continue to struggle with how to get started or optimize the DevOps tools they currently have incorporated.</p><p>Google Cloud’s DevOps Research and Assessment (<a href="https://cloud.google.com/devops" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/devops" track-metadata-module="post">DORA</a>) team helps organizations to deploy faster, scale on demand, and balance costs by providing essential tools and resources for you to succeed. Over the past seven years, our DORA team has surveyed more than 32,000 professionals worldwide via our yearly <a href="https://cloud.google.com/devops/state-of-devops" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/devops/state-of-devops" track-metadata-module="post">Accelerate State of DevOps reports</a> (SODR). As the largest and longest-running research of its kind, the SODR provides data-driven industry insights that examine the capabilities and practices that drive software delivery and operational and organizational performance - no matter the industry. </p><p>Read below to learn more from our DORA team about how and why your organization should focus on DevOps this year: </p><h3>Benchmark your team, identify improvement opportunities</h3><p>Our <a href="https://inthecloud.withgoogle.com/devops-quick-check/dl-cd.html?utm_source=google_owned_website&amp;utm_medium=et&amp;utm_campaign=FY20-Q3-global-demandgen-website-other-gcp_gtm_cost_amp_devops_quick_check_mc&amp;utm_content=app_mod_lp_cta&amp;utm_term=-&amp;_ga=2.131280267.562104682.1627912649-1057981864.1627522111" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://inthecloud.withgoogle.com" track-metadata-module="post">DevOps Quick Check</a> is based on DORA research and allows companies to gauge their DevOps implementation with just five multiple choice questions. The DevOps Quick Check uses the <a href="https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance" track-metadata-module="post">four key DevOps metrics</a> to help your team assess your current performance, compare your performance to others in your industry, and identify which capabilities are most likely to impact your performance. Teams in every industry recognize the value of quickly taking code from development to production and are looking for ways to improve their organizations&#39; agility. Compromising stability is not an option. The data from the research program shows that speed and stability go hand-in-hand, in fact elite performing teams are nearly twice as likely to have increased software delivery performance and achieve 6570x faster lead-time-to-deploy changes.</p><h3>Increasing developer productivity</h3><p>Along with driving software delivery performance, DevOps helps to increase developer productivity by reducing burnout - something that is a top priority for many organizations. According to the SODR, 89% of respondents worked from home during the pandemic but teams with a<a href="https://cloud.google.com/architecture/devops/devops-culture-westrum-organizational-culture" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/architecture/devops/devops-culture-westrum-organizational-culture" track-metadata-module="post"> generative team culture,</a> with people who had feelings of inclusion and belonging within their team, were half as likely to experience burnout. A generative culture along with easy to use <a href="https://cloud.google.com/architecture/devops/devops-tech-teams-empowered-to-choose-tools" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/architecture/devops/devops-tech-teams-empowered-to-choose-tools" track-metadata-module="post">developer tools </a>help to streamline developers’ workflows and the process of working with cloud infrastructure.</p><p>Developer tools that keep developers focused on what they do best, writing code,  is vital to increase developer productivity. Companies should utilize tools that ensure developers spend as little time as possible containerizing applications while increasing automation are paramount. </p><p>We commonly see developers leverage <a href="https://docs.docker.com/" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://docs.docker.com" track-metadata-module="post">Docker</a> to bring modern applications to life, and there are many benefits of doing so, especially in terms of portability. However, using Docker increases the operational burden on developers. Whereas those that leverage <a href="https://www.youtube.com/watch?v=suhCr5W_bFc" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://www.youtube.com" track-metadata-module="post">Buildpacks</a> are able to reduce operational burden while supporting enterprise operators who manage apps at scale. They are able to do this because Buildpacks allows code to go straight from source to production, in addition to making it easier to meet security and compliance requirements without developer intervention. </p><p>Plus when developers want to speed up the deployment process with flexibility they have the option to leverage <a href="https://cloud.google.com/run" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/run" track-metadata-module="post">Cloud Run</a>, our fully managed serverless platform offering, which fully supports the use of Buildpacks. Below are some benefits developers and operators can see from Cloud Run:</p><ol><li><p><b>Increased Productivity</b>: make it easier for developers to onboard more quickly and deploy faster </p></li><ul><li><i>Hiring</i>: choose a platform that makes all developers productive. Serverless platforms, like Cloud Run, help developers who know fewer programming languages become  productive without the need to also be an IT expert </li></ul><li><p><b>Decreased Time to Market</b>: accelerate software releases and value creation to customers that ultimately increase revenue and customer loyalty</p></li><li><p><b>Decreased Cost</b>: Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantaneously - so you only pay for what you use</p></li></ol><p>Whether you choose Cloud Run or another offering - it’s not just which cloud infrastructure you choose, but how you implement cloud services that really matters. This is especially important when it comes to being able to scale quickly and efficiently. In the SODR, we found that Elite performers were 3.5 times more likely to have met all essential <a href="https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure" track-metadata-module="post">NIST cloud characteristics</a>. In terms of scaling, the characteristic of rapid elasticity is key to rapidly scaling outward or inward with demand. In other words, it is very important that your capabilities can be elastically provisioned and released - so no matter how much you grow, your customers always have access to your services. </p><h3>The Business Case for DevOps</h3><p>Maybe you&#39;re convinced that achieving better speed and stability will help your team but how do you convince your boss?</p><p>The <a href="https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper" track-metadata-module="post">ROI of DevOps Transformation</a> provides IT and business decision makers an industry backed, data-driven foundational basis for measuring their investment in DevOps. We found that money saved from DevOps transformation varies from $10M to $259M a year with a return on investment of approximately 30 days. You can use the metrics you provided in the <a href="https://www.devops-research.com/quickcheck.html" target="_blank" track-type="inline link" track-name="13" track-metadata-eventdetail="https://www.devops-research.com" track-metadata-module="post">DevOps Quick Check</a> along with some additional information to get an estimate on your team&#39;s potential return.</p><h3>Google Cloud DevOps Awards</h3><p>Using a collection of these resources along with an objective assessment of how your team is doing, your organization will be able to get quick insights into improvement areas. Change your work by improving these capabilities to deliver more innovation to your customers and improve the speed and stability of your software delivery.</p><p>Now that we have shared some of our DevOps best practices with you, we would love to hear about how you are transforming your organization with DevOps. During our awards celebration in March we will be celebrating how our most advanced teams are using DevOps, so tell us about the positive impact that DevOps has had on your teams, customers, and organization. <a href="https://cloud.google.com/awards/devops" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/awards/devops" track-metadata-module="post">Enter your submission for the Google Cloud DevOps Awards</a> today. But don&#39;t delay! The deadline for submissions is January 31, 2022.</p><p>What questions do you have about incorporating DevOps practices into your daily work? <a href="https://inthecloud.withgoogle.com/born-digital/dl-cd.html" target="_blank" track-type="inline link" track-name="15" track-metadata-eventdetail="https://inthecloud.withgoogle.com" track-metadata-module="post">Reach out to our experts at Google Cloud.</a></p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Fer De Oliveira&lt;/name&gt;&lt;title&gt;Head, Serverless Scale Specialist, NorthAM&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_C_Rnd3_n7MW7mI.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 18 Jan 2022 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>The Google Cloud DevOps Awards: Final call for submissions!</title>
      <link>https://cloud.google.com/blog/products/devops-sre/apply-now-for-the-google-cloud-devops-awards/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;DevOps continues to be a major business accelerator for our customers and we continually see success from customers applying DevOps Research and Assessment (&lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DORA&lt;/a&gt;) principles and findings to their organization. This is why the first annual &lt;a href=&#34;https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true&#34;&gt;DevOps Awards&lt;/a&gt;is targeted to recognize customers shaping the future of DevOps with DORA. Share your inspirational story, supported by examples of business transformation and operational excellence, today. &lt;/p&gt;&lt;p&gt;With inputs from over 32,000 professionals worldwide and seven years of research, the &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;Accelerate State of DevOps Report&lt;/a&gt; is the largest and longest running DevOps research of its kind. The different categories of &lt;a href=&#34;https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true&#34;&gt;DevOps Awards&lt;/a&gt; map closely to the practices and capabilities that drive high performance, as identified by the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops/&#34;&gt;report&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Organizations, irrespective of their  size, industry, and region are able to apply to one or all ten categories. Please find the categories and their descriptions below:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Optimizing for Speed without sacrificing stability&lt;/b&gt;: This award recognizes one  Google Cloud customer that has driven improvements in speed without sacrificing quality. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Embracing easy-to-use tools to improve remote productivity&lt;/b&gt;: The research showcases how high performing engineers are 1.5 times more likely to have easy to-use tools. To be eligible for this award, share your stories on how easy to use DevOps tools have helped you improve engineer productivity.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Mastering effective disaster recovery&lt;/b&gt;: This award winner will be awarded to demonstrate how a robust, well-tested&lt;a href=&#34;https://cloud.google.com/architecture/dr-scenarios-planning-guide&#34;&gt;disaster recovery (DR)&lt;/a&gt; plan can  protect business operations.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Leveraging loosely coupled architecture&lt;/b&gt;: This award recognizes one customer that successfully transitioned from a tightly coupled &lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-tech-architecture&#34;&gt;architecture&lt;/a&gt; to service-oriented and microservice architectures.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Unleashing the full power of the Cloud&lt;/b&gt;: This award recognizes a Google Cloud customer leveraging all five capabilities of cloud computing to improve software delivery and organizational performance. Specifically, these five capabilities include: &lt;br/&gt;- On demand self-service&lt;br/&gt;- Broad network access&lt;br/&gt;- Measured service&lt;br/&gt;- Rapid elasticity&lt;br/&gt;- Resource pooling.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Read more about the &lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure&#34;&gt;five essential characteristics of cloud computing&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Most improved documentation quality&lt;/b&gt;: This award recognizes one customer that has successfully integrated documentation into their DevOps workflow using Google Cloud Platform tools.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Reducing burnout during COVID-19&lt;/b&gt;: We will recognize one customer that implemented effective processes to improve work/life balance, foster a healthy DevOps culture, and ultimately prevent burnout.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Utilizing IT operations to drive informed business decisions&lt;/b&gt;: This award will go to one customer that employed DevOps best practices to break down silos between development and operations teams.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Driving inclusion and diversity in DevOps&lt;/b&gt;: To highlight the importance of a diverse organization, this award honors one Google Cloud customer that: &lt;br/&gt;&lt;br/&gt;Prioritizes diversity and inclusion initiatives for their organization to transform and strengthen their business. &lt;br/&gt;&lt;br/&gt;-or&lt;br/&gt;&lt;br/&gt;Creates unique solutions to help build a more diverse, inclusive, and accessible workplace for your customer, leading to higher levels of engagement, productivity, and innovation.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Accelerating DevOps with DORA&lt;/b&gt;: This award recognizes one customer that has successfully integrated the most DORA practices and capabilities into their workflow using Google Cloud Platform tools.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This is your chance to show your innovation globally and become a role model for the industry to improve. Winners will receive invitations to roundtables and discussions, press materials, website and social badges, special announcements and even a trophy award.&lt;/p&gt;&lt;p&gt;We are excited to see all your great submissions. Applications are open until January 31st, so apply for what best suits your company and stay tuned for our awards show in February 2022!&lt;/p&gt;&lt;p&gt;For more information on the awards visit our &lt;a href=&#34;https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true&#34;&gt;webpage&lt;/a&gt;and check out &lt;a href=&#34;https://services.google.com/fh/files/misc/2021_devops_awards_guidebook.pdf&#34; target=&#34;_blank&#34;&gt;The Google Cloud DevOps Awards Guidebook&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div _ngcontent-c60="" innerhtml="&lt;p&gt;DevOps continues to be a major business accelerator for our customers and we continually see success from customers applying DevOps Research and Assessment (&lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DORA&lt;/a&gt;) principles and findings to their organization. This is why the first annual &lt;a href=&#34;https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true&#34;&gt;DevOps Awards &lt;/a&gt;is targeted to recognize customers shaping the future of DevOps with DORA. Share your inspirational story, supported by examples of business transformation and operational excellence, today.&amp;#160;&lt;/p&gt;&lt;p&gt;With inputs from over 32,000 professionals worldwide and seven years of research, the &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;Accelerate State of DevOps Report&lt;/a&gt; is the largest and longest running DevOps research of its kind. The different categories of &lt;a href=&#34;https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true&#34;&gt;DevOps Awards&lt;/a&gt; map closely to the practices and capabilities that drive high performance, as identified by the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops/&#34;&gt;report&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;Organizations, irrespective of their&amp;#160; size, industry, and region are able to apply to one or all ten categories. Please find the categories and their descriptions below:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Optimizing for Speed without sacrificing stability&lt;/b&gt;: This award recognizes one&amp;#160; Google Cloud customer that has driven improvements in speed without sacrificing quality.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Embracing easy-to-use tools to improve remote productivity&lt;/b&gt;: The research showcases how high performing engineers are 1.5 times more likely to have easy to-use tools. To be eligible for this award, share your stories on how easy to use DevOps tools have helped you improve engineer productivity.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Mastering effective disaster recovery&lt;/b&gt;: This award winner will be awarded to demonstrate how a robust, well-tested&lt;a href=&#34;https://cloud.google.com/architecture/dr-scenarios-planning-guide&#34;&gt; disaster recovery (DR)&lt;/a&gt; plan can&amp;#160; protect business operations.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Leveraging loosely coupled architecture&lt;/b&gt;: This award recognizes one customer that successfully transitioned from a tightly coupled &lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-tech-architecture&#34;&gt;architecture&lt;/a&gt; to service-oriented and microservice architectures.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Unleashing the full power of the Cloud&lt;/b&gt;: This award recognizes a Google Cloud customer leveraging all five capabilities of cloud computing to improve software delivery and organizational performance. Specifically, these five capabilities include:&amp;#160;&lt;br&gt;- On demand self-service&lt;br&gt;- Broad network access&lt;br&gt;- Measured service&lt;br&gt;- Rapid elasticity&lt;br&gt;- Resource pooling.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Read more about the &lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure&#34;&gt;five essential characteristics of cloud computing&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Most improved documentation quality&lt;/b&gt;: This award recognizes one customer that has successfully integrated documentation into their DevOps workflow using Google Cloud Platform tools.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Reducing burnout during COVID-19&lt;/b&gt;: We will recognize one customer that implemented effective processes to improve work/life balance, foster a healthy DevOps culture, and ultimately prevent burnout.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Utilizing IT operations to drive informed business decisions&lt;/b&gt;: This award will go to one customer that employed DevOps best practices to break down silos between development and operations teams.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Driving inclusion and diversity in DevOps&lt;/b&gt;: To highlight the importance of a diverse organization, this award honors one Google Cloud customer that:&amp;#160;&lt;br&gt;&lt;br&gt;Prioritizes diversity and inclusion initiatives for their organization to transform and strengthen their business.&amp;#160;&lt;br&gt;&lt;br&gt;-or&lt;br&gt;&lt;br&gt;Creates unique solutions to help build a more diverse, inclusive, and accessible workplace for your customer, leading to higher levels of engagement, productivity, and innovation.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Accelerating DevOps with DORA&lt;/b&gt;: This award recognizes one customer that has successfully integrated the most DORA practices and capabilities into their workflow using Google Cloud Platform tools.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This is your chance to show your innovation globally and become a role model for the industry to improve. Winners will receive invitations to roundtables and discussions, press materials, website and social badges, special announcements and even a trophy award.&lt;/p&gt;&lt;p&gt;We are excited to see all your great submissions. Applications are open until January 31st, so apply for what best suits your company and stay tuned for our awards show in February 2022!&lt;/p&gt;&lt;p&gt;For more information on the awards visit our &lt;a href=&#34;https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true&#34;&gt;webpage &lt;/a&gt;and check out &lt;a href=&#34;https://services.google.com/fh/files/misc/2021_devops_awards_guidebook.pdf&#34; target=&#34;_blank&#34;&gt;The Google Cloud DevOps Awards Guidebook&lt;/a&gt;.&lt;/p&gt;" _nghost-c60=""><p>DevOps continues to be a major business accelerator for our customers and we continually see success from customers applying DevOps Research and Assessment (<a href="https://www.devops-research.com/research.html" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://www.devops-research.com" track-metadata-module="post">DORA</a>) principles and findings to their organization. This is why the first annual <a href="https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true" track-metadata-module="post">DevOps Awards </a>is targeted to recognize customers shaping the future of DevOps with DORA. Share your inspirational story, supported by examples of business transformation and operational excellence, today. </p><p>With inputs from over 32,000 professionals worldwide and seven years of research, the <a href="https://cloud.google.com/devops" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/devops" track-metadata-module="post">Accelerate State of DevOps Report</a> is the largest and longest running DevOps research of its kind. The different categories of <a href="https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true" track-metadata-module="post">DevOps Awards</a> map closely to the practices and capabilities that drive high performance, as identified by the <a href="https://cloud.google.com/devops/state-of-devops/" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/devops/state-of-devops/" track-metadata-module="post">report</a>. </p><p>Organizations, irrespective of their  size, industry, and region are able to apply to one or all ten categories. Please find the categories and their descriptions below:</p><ul><li><p><b>Optimizing for Speed without sacrificing stability</b>: This award recognizes one  Google Cloud customer that has driven improvements in speed without sacrificing quality. </p></li></ul><ul><li><p><b>Embracing easy-to-use tools to improve remote productivity</b>: The research showcases how high performing engineers are 1.5 times more likely to have easy to-use tools. To be eligible for this award, share your stories on how easy to use DevOps tools have helped you improve engineer productivity.</p></li></ul><ul><li><p><b>Mastering effective disaster recovery</b>: This award winner will be awarded to demonstrate how a robust, well-tested<a href="https://cloud.google.com/architecture/dr-scenarios-planning-guide" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/architecture/dr-scenarios-planning-guide" track-metadata-module="post"> disaster recovery (DR)</a> plan can  protect business operations.</p></li></ul><ul><li><p><b>Leveraging loosely coupled architecture</b>: This award recognizes one customer that successfully transitioned from a tightly coupled <a href="https://cloud.google.com/architecture/devops/devops-tech-architecture" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/architecture/devops/devops-tech-architecture" track-metadata-module="post">architecture</a> to service-oriented and microservice architectures.</p></li><li><p><b>Unleashing the full power of the Cloud</b>: This award recognizes a Google Cloud customer leveraging all five capabilities of cloud computing to improve software delivery and organizational performance. Specifically, these five capabilities include: <br/>- On demand self-service<br/>- Broad network access<br/>- Measured service<br/>- Rapid elasticity<br/>- Resource pooling.</p></li></ul><p>Read more about the <a href="https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure" track-metadata-module="post">five essential characteristics of cloud computing</a></p><ul><li><p><b>Most improved documentation quality</b>: This award recognizes one customer that has successfully integrated documentation into their DevOps workflow using Google Cloud Platform tools.</p></li></ul><ul><li><p><b>Reducing burnout during COVID-19</b>: We will recognize one customer that implemented effective processes to improve work/life balance, foster a healthy DevOps culture, and ultimately prevent burnout.</p></li></ul><ul><li><p><b>Utilizing IT operations to drive informed business decisions</b>: This award will go to one customer that employed DevOps best practices to break down silos between development and operations teams.</p></li></ul><ul><li><div><p><b>Driving inclusion and diversity in DevOps</b>: To highlight the importance of a diverse organization, this award honors one Google Cloud customer that: </p><p>Prioritizes diversity and inclusion initiatives for their organization to transform and strengthen their business. </p><p>-or</p><p>Creates unique solutions to help build a more diverse, inclusive, and accessible workplace for your customer, leading to higher levels of engagement, productivity, and innovation.</p></div></li></ul><ul><li><p><b>Accelerating DevOps with DORA</b>: This award recognizes one customer that has successfully integrated the most DORA practices and capabilities into their workflow using Google Cloud Platform tools.</p></li></ul><p>This is your chance to show your innovation globally and become a role model for the industry to improve. Winners will receive invitations to roundtables and discussions, press materials, website and social badges, special announcements and even a trophy award.</p><p>We are excited to see all your great submissions. Applications are open until January 31st, so apply for what best suits your company and stay tuned for our awards show in February 2022!</p><p>For more information on the awards visit our <a href="https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true" track-metadata-module="post">webpage </a>and check out <a href="https://services.google.com/fh/files/misc/2021_devops_awards_guidebook.pdf" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://services.google.com" track-metadata-module="post">The Google Cloud DevOps Awards Guidebook</a>.</p></div></div>]]></content:encoded>
      <author>&lt;name&gt;Brenna Washington&lt;/name&gt;&lt;title&gt;Product Marketing Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/devops_awards.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 11 Jan 2022 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>A cloud built for developers — 2021 year in review</title>
      <link>https://cloud.google.com/blog/products/application-development/a-cloud-built-for-developers-2021-year-in-review/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;2021 was a seminal year for software developers. Every company accelerated their digital and online efforts, while simultaneously moving to remote development. Innovation by driving developer productivity was top of mind for nearly every IT executive we spoke to. Many asked us about Alphabet&#39;s long track record of innovation. From Google search to Waymo’s driverless cars,  is there a secret to developing the next big thing? &lt;/p&gt;&lt;p&gt;The answer is simple: 10X thinking. Look for solutions that help customers drive 10X improvements, through a series of smaller increments that compound to a large impact over time. At Google Cloud, we follow a similar philosophy to help our customers become innovative technology companies. In recent times, we’ve worked closely with partners, customers, and developers on services that help unlock 10X improvements in developer productivity. &lt;/p&gt;&lt;p&gt;Six years ago, we introduced a managed Kubernetes service, Google Kubernetes Engine (GKE). This year, we added &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot&#34;&gt;GKE Autopilot&lt;/a&gt;, which revolutionized Kubernetes management by eliminating all node management operations. Likewise, our &lt;a href=&#34;https://cloud.google.com/run&#34;&gt;Cloud Run&lt;/a&gt; serverless platform was the first service of its kind, allowing developers to go beyond running small bits of code and run full applications in a serverless environment. From September 2020 to September 2021, Cloud Run deployments more than quadrupled. More recently, we co-founded the &lt;a href=&#34;https://openssf.org/&#34; target=&#34;_blank&#34;&gt;Open Source Security Foundation&lt;/a&gt; and began working on secure continuous Integration and delivery (CI/CD) services a year or so ahead of the cybersecurity threats that made it to headlines. &lt;/p&gt;&lt;p&gt;Here are the top developer challenges that customers asked us to solve in 2021: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Driving distributed developer productivity&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Securing the software supply chain&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Simplifying running of cloud-native applications &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Read on for more insights. &lt;/p&gt;&lt;h3&gt;Driving distributed developer productivity&lt;/h3&gt;&lt;p&gt;A critical prerequisite for innovation is time. Investments in developer productivity free developers to work on the important things. Traditionally, developers have spent hours downloading and installing tools to their local environments, updating them with the latest versions, or dependencies. &lt;a href=&#34;https://cloud.google.com/shell/docs/editor-overview&#34;&gt;Cloud Shell Editor&lt;/a&gt; is a full remote development environment with a growing set of built in security capabilities. It comes with developer tools pre-installed, including MySql, Kubernetes, Docker, minikube, Skaffold, etc. Developers just needed a web browser and internet connection to be productive. Developers now have access to &lt;a href=&#34;https://cloud.google.com/shell/docs/cloud-shell-tutorials/tutorials&#34;&gt;tutorials&lt;/a&gt; right from Cloud Shell Editor, and can try code samples directly in &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/run-code-samples-directly-google-cloud-documentation&#34;&gt;our documentation&lt;/a&gt;. Additionally, with support for &lt;a href=&#34;https://cloud.google.com/run/docs/building/containers&#34;&gt;buildpacks&lt;/a&gt;, developers can create container images &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/build-and-deploy-an-app-to-cloud-run-with-a-single-command&#34;&gt;directly from source code&lt;/a&gt;, without knowing anything about docker or containers. &lt;/p&gt;&lt;h3&gt;Securing the software supply chain&lt;/h3&gt;&lt;p&gt;Software supply chain vulnerabilities had far reaching consequences in 2021, with events such as &lt;a href=&#34;https://blogs.microsoft.com/on-the-issues/2020/12/17/cyberattacks-cybersecurity-solarwinds-fireeye/&#34; target=&#34;_blank&#34;&gt;SolarWinds&lt;/a&gt;, &lt;a href=&#34;https://www.mimecast.com/blog/important-update-from-mimecast/&#34; target=&#34;_blank&#34;&gt;Mimecast/Microsoft Exchange&lt;/a&gt;, and &lt;a href=&#34;https://logging.apache.org/log4j/2.x/&#34; target=&#34;_blank&#34;&gt;Log4j&lt;/a&gt;affecting businesses, daily life, and entire &lt;a href=&#34;https://www.cyberscoop.com/dhs-cyber-alert-subpoena-us/&#34; target=&#34;_blank&#34;&gt;governments&lt;/a&gt;. President Biden even issued an &lt;a href=&#34;https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/&#34; target=&#34;_blank&#34;&gt;executive order&lt;/a&gt; to strengthen software supply-chain security standards.&lt;/p&gt;&lt;p&gt;Solving the software supply chain problem requires players across industries to work together. This is why we co-founded the&lt;a href=&#34;https://openssf.org/&#34; target=&#34;_blank&#34;&gt;Open Source Security Foundation&lt;/a&gt; (Open SSF). We also proposed &lt;a href=&#34;https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html&#34; target=&#34;_blank&#34;&gt;SLSA&lt;/a&gt;, an industry-wide framework for maintaining the integrity of software artifacts throughout the software supply chain. &lt;/p&gt;&lt;p&gt;Open source, with its complex dependency trees, continues to remain a prime target for exploitation. In fact, an estimated &lt;a href=&#34;https://venturebeat.com/2021/04/13/synopsys-84-of-codebases-contain-an-open-source-vulnerability/&#34; target=&#34;_blank&#34;&gt;84% of commercial code bases&lt;/a&gt; have at least one open source vulnerability. Today, developers can use our tools such as &lt;a href=&#34;https://github.com/ossf/allstar&#34; target=&#34;_blank&#34;&gt;Allstar GitHub App&lt;/a&gt;, &lt;a href=&#34;https://opensource.googleblog.com/2020/11/security-scorecards-for-open-source.html&#34; target=&#34;_blank&#34;&gt;open source security score cards&lt;/a&gt; and &lt;a href=&#34;https://deps.dev/&#34; target=&#34;_blank&#34;&gt;Open Source Insights&lt;/a&gt; to implement security best practices, determine a risk score for open source projects, and visualize a project&#39;s deep dependencies. And several of these same  kinds of open-source innovations are available out of the box to Google Cloud customers. Here are a few examples: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Detailed &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/recommendations-for-apache-log4j2-vulnerability&#34;&gt;recommendations&lt;/a&gt; to help mitigate the Apache Log4j vulnerability. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/container-analysis/docs/java-overview&#34;&gt;Java scanning feature&lt;/a&gt; of Google Cloud &lt;a href=&#34;https://cloud.google.com/container-analysis/docs/container-scanning-overview&#34;&gt;On-Demand Scanning&lt;/a&gt;, which can be quite handy for developers to identify Linux-based container images that use an impacted version of Log4j. On-Demand Scanning can be used with no charge until December 31, 2021. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Build, our serverless CI/CD service, offers &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/google-introduces-slsa-framework&#34;&gt;SLSA Level 1 compliance&lt;/a&gt; by default. This verifiable build provenance lets you trace a binary to the source code to prevent tampering and prove that the code you’re running is the code you think you’re running. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Build’s new &lt;a href=&#34;https://cloud.google.com/build/docs/securing-builds/use-provenance-and-binary-authorization&#34;&gt;build integrity feature&lt;/a&gt; improves on this by automatically generating digital signatures, which can be validated before deployment by &lt;a href=&#34;https://cloud.google.com/binary-authorization&#34;&gt;Binary Authorization&lt;/a&gt;. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Simplifying running cloud-native applications&lt;/h3&gt;&lt;p&gt;Innovation is rarely a straight road, there are many wrong turns along the way. Developers need a cost effective runtime, a way to run experiments and fail forward fast. That&#39;s why &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot&#34;&gt;GKE Autopilot&lt;/a&gt; takes GKE, the &lt;a href=&#34;https://cloud.google.com/resources/gartner-solution-scorecard-for-kubernetes-analyst-report&#34;&gt;most&lt;/a&gt; mature Kubernetes service on the market and further simplifies Kubernetes operations by providing a managed control and data plane, an optimized configuration out-of-the-box, automated scalability, health checks and repairs, and pay-for-use pricing. &lt;/p&gt;&lt;p&gt;&lt;i&gt;“With GKE Autopilot, we can do more with our business. We can continue developing and upgrading our products, rather than focusing on fine-tuning infrastructure.”—&lt;b&gt;Jun Sakata, Software Engineer, Site Reliability, &lt;a href=&#34;https://cloud.google.com/customers/ubie&#34;&gt;Ubie&lt;/a&gt; &lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Simpler still is no cluster all. Cloud Run provides developers the freedom to run services from code or container images with no cluster or VM to manage. At the same time, it provides a hypervisor grade secure sandbox environment and several built in DevOps capabilities such as, &lt;a href=&#34;https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration&#34;&gt;multi-versioned deployments, gradual rollouts and rollbacks&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/run/docs/building/containers&#34;&gt;Cloud Build&lt;/a&gt; integrations. This is ideal for web and mobile application development. In 2021, with &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/the-next-big-evolution-in-cloud-computing&#34;&gt;additions&lt;/a&gt; like &lt;a href=&#34;https://cloud.google.com/run/docs/about-concurrency&#34;&gt;higher per-instance concurrency&lt;/a&gt;, new &lt;a href=&#34;https://cloud.google.com/run/docs/configuring/cpu-allocation&#34;&gt;CPU allocation controls&lt;/a&gt;, and support for &lt;a href=&#34;https://cloud.google.com/run/docs/deploying&#34;&gt;standard Docker images&lt;/a&gt;, the benefits of serverless can now be expanded to a wider range of workloads, including legacy ones. Additionally, with newer cost controls along with billing flexibility like &lt;a href=&#34;https://cloud.google.com/run/cud&#34;&gt;committed use contracts&lt;/a&gt; and features like &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation&#34;&gt;always-on CPU&lt;/a&gt;, it’s possible to run more steady-state pattern workloads cost effectively in a serverless environment.  Best of all, thanks to improvements like these, organizations using Cloud Run have reported reduction in &lt;a href=&#34;https://cloud.google.com/resources/forrester-cloudrun-benefits-report&#34;&gt;developer recruiting costs by 40%&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Cloud Run is also the first platform to provide developers the option to optimize their carbon footprint.  With the news self-service &lt;a href=&#34;https://cloud.google.com/blog/topics/sustainability/google-cloud-region-picker-helps-you-make-the-green-choice&#34;&gt;Region Picker&lt;/a&gt; you can choose the data center region with the lowest gross carbon cost on which to run your Cloud Run workloads. Further, with just one click, &lt;a href=&#34;https://cloud.google.com/carbon-footprint&#34;&gt;Google Cloud Carbon Footprint&lt;/a&gt;gives you access to the energy-related emissions data for external carbon disclosures. &lt;/p&gt;&lt;p&gt;&lt;i&gt;“With Cloud Run, we only need half the people to manage our systems as compared to before” &lt;b&gt;Google Cloud Platform Architect, &lt;a href=&#34;https://cloud.google.com/resources/forrester-cloudrun-benefits-report&#34;&gt;Cosmetics&lt;/a&gt; &lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;“Cloud Run is one of the easiest services on Google Cloud Platform you can deploy to. It’s just super simple.” &lt;b&gt;CTO,&lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/resources/forrester-cloudrun-benefits-report&#34;&gt;&lt;b&gt;Healthcare SaaS&lt;/b&gt;&lt;/a&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;If you want to give Cloud Run and associated Cloud Functions a try, check out the &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/serverless-hackathon&#34;&gt;Easy as Pie Serverless Hackathon&lt;/a&gt;, which offers  over $20,000 USD in cash prizes.&lt;/p&gt;&lt;h3&gt;2022: More to come  &lt;/h3&gt;&lt;p&gt;2021 brought simplification and greater attention to developer productivity. It is essential that developers continue to operate at even higher levels of the stack, without worrying about infrastructure, security, compliance and integrations. This is the Northstar for 2022. In 2022, look for Google Cloud to co-innovate with our ISV partners, developers, and SecOps team to bring you the 10X innovation you need from the cloud that is built for developers.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div _ngcontent-c77="" innerhtml="&lt;p&gt;2021 was a seminal year for software developers. Every company accelerated their digital and online efforts, while simultaneously moving to remote development. Innovation by driving developer productivity was top of mind for nearly every IT executive we spoke to. Many asked us about Alphabet&#39;s long track record of innovation. From Google search to Waymo&amp;#8217;s driverless cars,&amp;#160; is there a secret to developing the next big thing?&amp;#160;&lt;/p&gt;&lt;p&gt;The answer is simple: 10X thinking. Look for solutions that help customers drive 10X improvements, through a series of smaller increments that compound to a large impact over time. At Google Cloud, we follow a similar philosophy to help our customers become innovative technology companies. In recent times, we&amp;#8217;ve worked closely with partners, customers, and developers on services that help unlock 10X improvements in developer productivity.&amp;#160;&lt;/p&gt;&lt;p&gt;Six years ago, we introduced a managed Kubernetes service, Google Kubernetes Engine (GKE). This year, we added &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot&#34;&gt;GKE Autopilot&lt;/a&gt;, which revolutionized Kubernetes management by eliminating all node management operations. Likewise, our &lt;a href=&#34;https://cloud.google.com/run&#34;&gt;Cloud Run&lt;/a&gt; serverless platform was the first service of its kind, allowing developers to go beyond running small bits of code and run full applications in a serverless environment. From September 2020 to September 2021, Cloud Run deployments more than quadrupled. More recently, we co-founded the &lt;a href=&#34;https://openssf.org/&#34; target=&#34;_blank&#34;&gt;Open Source Security Foundation&lt;/a&gt; and began working on secure continuous Integration and delivery (CI/CD) services a year or so ahead of the cybersecurity threats that made it to headlines.&amp;#160;&lt;/p&gt;&lt;p&gt;Here are the top developer challenges that customers asked us to solve in 2021:&amp;#160;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Driving distributed developer productivity&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Securing the software supply chain&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Simplifying running of cloud-native applications&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Read on for more insights.&amp;#160;&lt;/p&gt;&lt;h3&gt;Driving distributed developer productivity&lt;/h3&gt;&lt;p&gt;A critical prerequisite for innovation is time. Investments in developer productivity free developers to work on the important things. Traditionally, developers have spent hours downloading and installing tools to their local environments, updating them with the latest versions, or dependencies. &lt;a href=&#34;https://cloud.google.com/shell/docs/editor-overview&#34;&gt;Cloud Shell Editor&lt;/a&gt; is a full remote development environment with a growing set of built in security capabilities. It comes with developer tools pre-installed, including MySql, Kubernetes, Docker, minikube, Skaffold, etc. Developers just needed a web browser and internet connection to be productive. Developers now have access to &lt;a href=&#34;https://cloud.google.com/shell/docs/cloud-shell-tutorials/tutorials&#34;&gt;tutorials&lt;/a&gt; right from Cloud Shell Editor, and can try code samples directly in &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/run-code-samples-directly-google-cloud-documentation&#34;&gt;our documentation&lt;/a&gt;. Additionally, with support for &lt;a href=&#34;https://cloud.google.com/run/docs/building/containers&#34;&gt;buildpacks&lt;/a&gt;, developers can create container images &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/build-and-deploy-an-app-to-cloud-run-with-a-single-command&#34;&gt;directly from source code&lt;/a&gt;, without knowing anything about docker or containers.&amp;#160;&lt;/p&gt;&lt;h3&gt;Securing the software supply chain&lt;/h3&gt;&lt;p&gt;Software supply chain vulnerabilities had far reaching consequences in 2021, with events such as &lt;a href=&#34;https://blogs.microsoft.com/on-the-issues/2020/12/17/cyberattacks-cybersecurity-solarwinds-fireeye/&#34; target=&#34;_blank&#34;&gt;SolarWinds&lt;/a&gt;, &lt;a href=&#34;https://www.mimecast.com/blog/important-update-from-mimecast/&#34; target=&#34;_blank&#34;&gt;Mimecast/Microsoft Exchange&lt;/a&gt;, and &lt;a href=&#34;https://logging.apache.org/log4j/2.x/&#34; target=&#34;_blank&#34;&gt;Log4j &lt;/a&gt;affecting businesses, daily life, and entire &lt;a href=&#34;https://www.cyberscoop.com/dhs-cyber-alert-subpoena-us/&#34; target=&#34;_blank&#34;&gt;governments&lt;/a&gt;. President Biden even issued an &lt;a href=&#34;https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/&#34; target=&#34;_blank&#34;&gt;executive order&lt;/a&gt; to strengthen software supply-chain security standards.&lt;/p&gt;&lt;p&gt;Solving the software supply chain problem requires players across industries to work together. This is why we co-founded the&lt;a href=&#34;https://openssf.org/&#34; target=&#34;_blank&#34;&gt; Open Source Security Foundation&lt;/a&gt; (Open SSF). We also proposed &lt;a href=&#34;https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html&#34; target=&#34;_blank&#34;&gt;SLSA&lt;/a&gt;, an industry-wide framework for maintaining the integrity of software artifacts throughout the software supply chain.&amp;#160;&lt;/p&gt;&lt;p&gt;Open source, with its complex dependency trees, continues to remain a prime target for exploitation. In fact, an estimated &lt;a href=&#34;https://venturebeat.com/2021/04/13/synopsys-84-of-codebases-contain-an-open-source-vulnerability/&#34; target=&#34;_blank&#34;&gt;84% of commercial code bases&lt;/a&gt; have at least one open source vulnerability. Today, developers can use our tools such as &lt;a href=&#34;https://github.com/ossf/allstar&#34; target=&#34;_blank&#34;&gt;Allstar GitHub App&lt;/a&gt;, &lt;a href=&#34;https://opensource.googleblog.com/2020/11/security-scorecards-for-open-source.html&#34; target=&#34;_blank&#34;&gt;open source security score cards&lt;/a&gt; and &lt;a href=&#34;https://deps.dev/&#34; target=&#34;_blank&#34;&gt;Open Source Insights&lt;/a&gt; to implement security best practices, determine a risk score for open source projects, and visualize a project&#39;s deep dependencies. And several of these same&amp;#160; kinds of open-source innovations are available out of the box to Google Cloud customers. Here are a few examples:&amp;#160;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Detailed &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/recommendations-for-apache-log4j2-vulnerability&#34;&gt;recommendations&lt;/a&gt; to help mitigate the Apache Log4j vulnerability.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/container-analysis/docs/java-overview&#34;&gt;Java scanning feature&lt;/a&gt; of Google Cloud &lt;a href=&#34;https://cloud.google.com/container-analysis/docs/container-scanning-overview&#34;&gt;On-Demand Scanning&lt;/a&gt;, which can be quite handy for developers to identify Linux-based container images that use an impacted version of Log4j. On-Demand Scanning can be used with no charge until December 31, 2021.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Build, our serverless CI/CD service, offers &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/google-introduces-slsa-framework&#34;&gt;SLSA Level 1 compliance&lt;/a&gt; by default. This verifiable build provenance lets you trace a binary to the source code to prevent tampering and prove that the code you&amp;#8217;re running is the code you think you&amp;#8217;re running.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Build&amp;#8217;s new &lt;a href=&#34;https://cloud.google.com/build/docs/securing-builds/use-provenance-and-binary-authorization&#34;&gt;build integrity feature&lt;/a&gt; improves on this by automatically generating digital signatures, which can be validated before deployment by &lt;a href=&#34;https://cloud.google.com/binary-authorization&#34;&gt;Binary Authorization&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Simplifying running cloud-native applications&lt;/h3&gt;&lt;p&gt;Innovation is rarely a straight road, there are many wrong turns along the way. Developers need a cost effective runtime, a way to run experiments and fail forward fast. That&#39;s why &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot&#34;&gt;GKE Autopilot&lt;/a&gt; takes GKE, the &lt;a href=&#34;https://cloud.google.com/resources/gartner-solution-scorecard-for-kubernetes-analyst-report&#34;&gt;most&lt;/a&gt; mature Kubernetes service on the market and further simplifies Kubernetes operations by providing a managed control and data plane, an optimized configuration out-of-the-box, automated scalability, health checks and repairs, and pay-for-use pricing.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#8220;With GKE Autopilot, we can do more with our business. We can continue developing and upgrading our products, rather than focusing on fine-tuning infrastructure.&amp;#8221;&amp;#8212;&lt;b&gt;Jun Sakata, Software Engineer, Site Reliability, &lt;a href=&#34;https://cloud.google.com/customers/ubie&#34;&gt;Ubie&lt;/a&gt;&amp;#160;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Simpler still is no cluster all. Cloud Run provides developers the freedom to run services from code or container images with no cluster or VM to manage. At the same time, it provides a hypervisor grade secure sandbox environment and several built in DevOps capabilities such as, &lt;a href=&#34;https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration&#34;&gt;multi-versioned deployments, gradual rollouts and rollbacks&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build&#34;&gt;GitHub&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/run/docs/building/containers&#34;&gt;Cloud Build&lt;/a&gt; integrations. This is ideal for web and mobile application development. In 2021, with &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/the-next-big-evolution-in-cloud-computing&#34;&gt;additions&lt;/a&gt; like &lt;a href=&#34;https://cloud.google.com/run/docs/about-concurrency&#34;&gt;higher per-instance concurrency&lt;/a&gt;, new &lt;a href=&#34;https://cloud.google.com/run/docs/configuring/cpu-allocation&#34;&gt;CPU allocation controls&lt;/a&gt;, and support for &lt;a href=&#34;https://cloud.google.com/run/docs/deploying&#34;&gt;standard Docker images&lt;/a&gt;, the benefits of serverless can now be expanded to a wider range of workloads, including legacy ones. Additionally, with newer cost controls along with billing flexibility like &lt;a href=&#34;https://cloud.google.com/run/cud&#34;&gt;committed use contracts&lt;/a&gt; and features like &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation&#34;&gt;always-on CPU&lt;/a&gt;, it&amp;#8217;s possible to run more steady-state pattern workloads cost effectively in a serverless environment.&amp;#160; Best of all, thanks to improvements like these, organizations using Cloud Run have reported reduction in &lt;a href=&#34;https://cloud.google.com/resources/forrester-cloudrun-benefits-report&#34;&gt;developer recruiting costs by 40%&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;Cloud Run is also the first platform to provide developers the option to optimize their carbon footprint.&amp;#160; With the news self-service &lt;a href=&#34;https://cloud.google.com/blog/topics/sustainability/google-cloud-region-picker-helps-you-make-the-green-choice&#34;&gt;Region Picker&lt;/a&gt; you can choose the data center region with the lowest gross carbon cost on which to run your Cloud Run workloads. Further, with just one click, &lt;a href=&#34;https://cloud.google.com/carbon-footprint&#34;&gt;Google Cloud Carbon Footprint &lt;/a&gt;gives you access to the energy-related emissions data for external carbon disclosures.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#8220;With Cloud Run, we only need half the people to manage our systems as compared to before&amp;#8221; &lt;b&gt;Google Cloud Platform Architect, &lt;a href=&#34;https://cloud.google.com/resources/forrester-cloudrun-benefits-report&#34;&gt;Cosmetics&lt;/a&gt;&amp;#160;&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#8220;Cloud Run is one of the easiest services on Google Cloud Platform you can deploy to. It&amp;#8217;s just super simple.&amp;#8221; &lt;b&gt;CTO, &lt;/b&gt;&lt;a href=&#34;https://cloud.google.com/resources/forrester-cloudrun-benefits-report&#34;&gt;&lt;b&gt;Healthcare SaaS&lt;/b&gt;&lt;/a&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;If you want to give Cloud Run and associated Cloud Functions a try, check out the &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/serverless-hackathon&#34;&gt;Easy as Pie Serverless Hackathon&lt;/a&gt;, which offers &amp;#160;over $20,000 USD in cash prizes.&lt;/p&gt;&lt;h3&gt;2022: More to come&amp;#160;&amp;#160;&lt;/h3&gt;&lt;p&gt;2021 brought simplification and greater attention to developer productivity. It is essential that developers continue to operate at even higher levels of the stack, without worrying about infrastructure, security, compliance and integrations. This is the Northstar for 2022. In 2022, look for Google Cloud to co-innovate with our ISV partners, developers, and SecOps team to bring you the 10X innovation you need from the cloud that is built for developers.&lt;/p&gt;" _nghost-c77=""><p>2021 was a seminal year for software developers. Every company accelerated their digital and online efforts, while simultaneously moving to remote development. Innovation by driving developer productivity was top of mind for nearly every IT executive we spoke to. Many asked us about Alphabet&#39;s long track record of innovation. From Google search to Waymo’s driverless cars,  is there a secret to developing the next big thing? </p><p>The answer is simple: 10X thinking. Look for solutions that help customers drive 10X improvements, through a series of smaller increments that compound to a large impact over time. At Google Cloud, we follow a similar philosophy to help our customers become innovative technology companies. In recent times, we’ve worked closely with partners, customers, and developers on services that help unlock 10X improvements in developer productivity. </p><p>Six years ago, we introduced a managed Kubernetes service, Google Kubernetes Engine (GKE). This year, we added <a href="https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot" track-metadata-module="post">GKE Autopilot</a>, which revolutionized Kubernetes management by eliminating all node management operations. Likewise, our <a href="https://cloud.google.com/run" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/run" track-metadata-module="post">Cloud Run</a> serverless platform was the first service of its kind, allowing developers to go beyond running small bits of code and run full applications in a serverless environment. From September 2020 to September 2021, Cloud Run deployments more than quadrupled. More recently, we co-founded the <a href="https://openssf.org/" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://openssf.org" track-metadata-module="post">Open Source Security Foundation</a> and began working on secure continuous Integration and delivery (CI/CD) services a year or so ahead of the cybersecurity threats that made it to headlines. </p><p>Here are the top developer challenges that customers asked us to solve in 2021: </p><ul><li><p>Driving distributed developer productivity</p></li><li><p>Securing the software supply chain</p></li><li><p>Simplifying running of cloud-native applications </p></li></ul><p>Read on for more insights. </p><h3>Driving distributed developer productivity</h3><p>A critical prerequisite for innovation is time. Investments in developer productivity free developers to work on the important things. Traditionally, developers have spent hours downloading and installing tools to their local environments, updating them with the latest versions, or dependencies. <a href="https://cloud.google.com/shell/docs/editor-overview" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/shell/docs/editor-overview" track-metadata-module="post">Cloud Shell Editor</a> is a full remote development environment with a growing set of built in security capabilities. It comes with developer tools pre-installed, including MySql, Kubernetes, Docker, minikube, Skaffold, etc. Developers just needed a web browser and internet connection to be productive. Developers now have access to <a href="https://cloud.google.com/shell/docs/cloud-shell-tutorials/tutorials" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/shell/docs/cloud-shell-tutorials/tutorials" track-metadata-module="post">tutorials</a> right from Cloud Shell Editor, and can try code samples directly in <a href="https://cloud.google.com/blog/topics/developers-practitioners/run-code-samples-directly-google-cloud-documentation" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/run-code-samples-directly-google-cloud-documentation" track-metadata-module="post">our documentation</a>. Additionally, with support for <a href="https://cloud.google.com/run/docs/building/containers" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/run/docs/building/containers" track-metadata-module="post">buildpacks</a>, developers can create container images <a href="https://cloud.google.com/blog/products/serverless/build-and-deploy-an-app-to-cloud-run-with-a-single-command" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/build-and-deploy-an-app-to-cloud-run-with-a-single-command" track-metadata-module="post">directly from source code</a>, without knowing anything about docker or containers. </p><h3>Securing the software supply chain</h3><p>Software supply chain vulnerabilities had far reaching consequences in 2021, with events such as <a href="https://blogs.microsoft.com/on-the-issues/2020/12/17/cyberattacks-cybersecurity-solarwinds-fireeye/" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://blogs.microsoft.com" track-metadata-module="post">SolarWinds</a>, <a href="https://www.mimecast.com/blog/important-update-from-mimecast/" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://www.mimecast.com" track-metadata-module="post">Mimecast/Microsoft Exchange</a>, and <a href="https://logging.apache.org/log4j/2.x/" target="_blank" track-type="inline link" track-name="11" track-metadata-eventdetail="https://logging.apache.org" track-metadata-module="post">Log4j </a>affecting businesses, daily life, and entire <a href="https://www.cyberscoop.com/dhs-cyber-alert-subpoena-us/" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="https://www.cyberscoop.com" track-metadata-module="post">governments</a>. President Biden even issued an <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/" target="_blank" track-type="inline link" track-name="13" track-metadata-eventdetail="https://www.whitehouse.gov" track-metadata-module="post">executive order</a> to strengthen software supply-chain security standards.</p><p>Solving the software supply chain problem requires players across industries to work together. This is why we co-founded the<a href="https://openssf.org/" target="_blank" track-type="inline link" track-name="14" track-metadata-eventdetail="https://openssf.org" track-metadata-module="post"> Open Source Security Foundation</a> (Open SSF). We also proposed <a href="https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html" target="_blank" track-type="inline link" track-name="15" track-metadata-eventdetail="https://security.googleblog.com" track-metadata-module="post">SLSA</a>, an industry-wide framework for maintaining the integrity of software artifacts throughout the software supply chain. </p><p>Open source, with its complex dependency trees, continues to remain a prime target for exploitation. In fact, an estimated <a href="https://venturebeat.com/2021/04/13/synopsys-84-of-codebases-contain-an-open-source-vulnerability/" target="_blank" track-type="inline link" track-name="16" track-metadata-eventdetail="https://venturebeat.com" track-metadata-module="post">84% of commercial code bases</a> have at least one open source vulnerability. Today, developers can use our tools such as <a href="https://github.com/ossf/allstar" target="_blank" track-type="inline link" track-name="17" track-metadata-eventdetail="https://github.com" track-metadata-module="post">Allstar GitHub App</a>, <a href="https://opensource.googleblog.com/2020/11/security-scorecards-for-open-source.html" target="_blank" track-type="inline link" track-name="18" track-metadata-eventdetail="https://opensource.googleblog.com" track-metadata-module="post">open source security score cards</a> and <a href="https://deps.dev/" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://deps.dev" track-metadata-module="post">Open Source Insights</a> to implement security best practices, determine a risk score for open source projects, and visualize a project&#39;s deep dependencies. And several of these same  kinds of open-source innovations are available out of the box to Google Cloud customers. Here are a few examples: </p><ul><li><p>Detailed <a href="https://cloud.google.com/blog/products/identity-security/recommendations-for-apache-log4j2-vulnerability" track-type="inline link" track-name="20" track-metadata-eventdetail="https://cloud.google.com/blog/products/identity-security/recommendations-for-apache-log4j2-vulnerability" track-metadata-module="post">recommendations</a> to help mitigate the Apache Log4j vulnerability. </p></li><li><p>The <a href="https://cloud.google.com/container-analysis/docs/java-overview" track-type="inline link" track-name="21" track-metadata-eventdetail="https://cloud.google.com/container-analysis/docs/java-overview" track-metadata-module="post">Java scanning feature</a> of Google Cloud <a href="https://cloud.google.com/container-analysis/docs/container-scanning-overview" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/container-analysis/docs/container-scanning-overview" track-metadata-module="post">On-Demand Scanning</a>, which can be quite handy for developers to identify Linux-based container images that use an impacted version of Log4j. On-Demand Scanning can be used with no charge until December 31, 2021. </p></li><li><p>Cloud Build, our serverless CI/CD service, offers <a href="https://cloud.google.com/blog/products/application-development/google-introduces-slsa-framework" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/blog/products/application-development/google-introduces-slsa-framework" track-metadata-module="post">SLSA Level 1 compliance</a> by default. This verifiable build provenance lets you trace a binary to the source code to prevent tampering and prove that the code you’re running is the code you think you’re running. </p></li><li><p>Cloud Build’s new <a href="https://cloud.google.com/build/docs/securing-builds/use-provenance-and-binary-authorization" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/build/docs/securing-builds/use-provenance-and-binary-authorization" track-metadata-module="post">build integrity feature</a> improves on this by automatically generating digital signatures, which can be validated before deployment by <a href="https://cloud.google.com/binary-authorization" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/binary-authorization" track-metadata-module="post">Binary Authorization</a>. </p></li></ul><h3>Simplifying running cloud-native applications</h3><p>Innovation is rarely a straight road, there are many wrong turns along the way. Developers need a cost effective runtime, a way to run experiments and fail forward fast. That&#39;s why <a href="https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot" track-metadata-module="post">GKE Autopilot</a> takes GKE, the <a href="https://cloud.google.com/resources/gartner-solution-scorecard-for-kubernetes-analyst-report" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/resources/gartner-solution-scorecard-for-kubernetes-analyst-report" track-metadata-module="post">most</a> mature Kubernetes service on the market and further simplifies Kubernetes operations by providing a managed control and data plane, an optimized configuration out-of-the-box, automated scalability, health checks and repairs, and pay-for-use pricing. </p><p><i>“With GKE Autopilot, we can do more with our business. We can continue developing and upgrading our products, rather than focusing on fine-tuning infrastructure.”—<b>Jun Sakata, Software Engineer, Site Reliability, <a href="https://cloud.google.com/customers/ubie" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/customers/ubie" track-metadata-module="post">Ubie</a> </b></i></p><p>Simpler still is no cluster all. Cloud Run provides developers the freedom to run services from code or container images with no cluster or VM to manage. At the same time, it provides a hypervisor grade secure sandbox environment and several built in DevOps capabilities such as, <a href="https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration" track-metadata-module="post">multi-versioned deployments, gradual rollouts and rollbacks</a>, <a href="https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build" track-type="inline link" track-name="30" track-metadata-eventdetail="https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build" track-metadata-module="post">GitHub</a> and <a href="https://cloud.google.com/run/docs/building/containers" track-type="inline link" track-name="31" track-metadata-eventdetail="https://cloud.google.com/run/docs/building/containers" track-metadata-module="post">Cloud Build</a> integrations. This is ideal for web and mobile application development. In 2021, with <a href="https://cloud.google.com/blog/products/serverless/the-next-big-evolution-in-cloud-computing" track-type="inline link" track-name="32" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/the-next-big-evolution-in-cloud-computing" track-metadata-module="post">additions</a> like <a href="https://cloud.google.com/run/docs/about-concurrency" track-type="inline link" track-name="33" track-metadata-eventdetail="https://cloud.google.com/run/docs/about-concurrency" track-metadata-module="post">higher per-instance concurrency</a>, new <a href="https://cloud.google.com/run/docs/configuring/cpu-allocation" track-type="inline link" track-name="34" track-metadata-eventdetail="https://cloud.google.com/run/docs/configuring/cpu-allocation" track-metadata-module="post">CPU allocation controls</a>, and support for <a href="https://cloud.google.com/run/docs/deploying" track-type="inline link" track-name="35" track-metadata-eventdetail="https://cloud.google.com/run/docs/deploying" track-metadata-module="post">standard Docker images</a>, the benefits of serverless can now be expanded to a wider range of workloads, including legacy ones. Additionally, with newer cost controls along with billing flexibility like <a href="https://cloud.google.com/run/cud" track-type="inline link" track-name="36" track-metadata-eventdetail="https://cloud.google.com/run/cud" track-metadata-module="post">committed use contracts</a> and features like <a href="https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation" track-type="inline link" track-name="37" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation" track-metadata-module="post">always-on CPU</a>, it’s possible to run more steady-state pattern workloads cost effectively in a serverless environment.  Best of all, thanks to improvements like these, organizations using Cloud Run have reported reduction in <a href="https://cloud.google.com/resources/forrester-cloudrun-benefits-report" track-type="inline link" track-name="38" track-metadata-eventdetail="https://cloud.google.com/resources/forrester-cloudrun-benefits-report" track-metadata-module="post">developer recruiting costs by 40%</a>. </p><p>Cloud Run is also the first platform to provide developers the option to optimize their carbon footprint.  With the news self-service <a href="https://cloud.google.com/blog/topics/sustainability/google-cloud-region-picker-helps-you-make-the-green-choice" track-type="inline link" track-name="39" track-metadata-eventdetail="https://cloud.google.com/blog/topics/sustainability/google-cloud-region-picker-helps-you-make-the-green-choice" track-metadata-module="post">Region Picker</a> you can choose the data center region with the lowest gross carbon cost on which to run your Cloud Run workloads. Further, with just one click, <a href="https://cloud.google.com/carbon-footprint" track-type="inline link" track-name="40" track-metadata-eventdetail="https://cloud.google.com/carbon-footprint" track-metadata-module="post">Google Cloud Carbon Footprint </a>gives you access to the energy-related emissions data for external carbon disclosures. </p><p><i>“With Cloud Run, we only need half the people to manage our systems as compared to before” <b>Google Cloud Platform Architect, <a href="https://cloud.google.com/resources/forrester-cloudrun-benefits-report" track-type="inline link" track-name="41" track-metadata-eventdetail="https://cloud.google.com/resources/forrester-cloudrun-benefits-report" track-metadata-module="post">Cosmetics</a> </b></i></p><p><i>“Cloud Run is one of the easiest services on Google Cloud Platform you can deploy to. It’s just super simple.” <b>CTO, </b><a href="https://cloud.google.com/resources/forrester-cloudrun-benefits-report" track-type="inline link" track-name="42" track-metadata-eventdetail="https://cloud.google.com/resources/forrester-cloudrun-benefits-report" track-metadata-module="post"><b>Healthcare SaaS</b></a></i></p><p>If you want to give Cloud Run and associated Cloud Functions a try, check out the <a href="https://cloud.google.com/blog/products/serverless/serverless-hackathon" track-type="inline link" track-name="43" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/serverless-hackathon" track-metadata-module="post">Easy as Pie Serverless Hackathon</a>, which offers  over $20,000 USD in cash prizes.</p><h3>2022: More to come  </h3><p>2021 brought simplification and greater attention to developer productivity. It is essential that developers continue to operate at even higher levels of the stack, without worrying about infrastructure, security, compliance and integrations. This is the Northstar for 2022. In 2022, look for Google Cloud to co-innovate with our ISV partners, developers, and SecOps team to bring you the 10X innovation you need from the cloud that is built for developers.</p></div></div>]]></content:encoded>
      <author>&lt;name&gt;Urs Hölzle&lt;/name&gt;&lt;title&gt;Senior Vice President, Technical Infrastructure&lt;/title&gt;&lt;department&gt;Google Cloud&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_GCP_HLl2OQm.max-2200x2200.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 23 Dec 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Postmortems at Loon: a guiding force for rapid development</title>
      <link>https://cloud.google.com/blog/products/devops-sre/loon-sre-use-postmortems-to-launch-and-iterate/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Founded by Google SRE alumni, it is no surprise that Loon&#39;s Production Engineering/SRE team instituted a culture of blameless postmortems that became a key feature of Loon&#39;s approach to incident response. Blameless postmortems originated as an aerospace practice in the mid-20th century, so it was particularly fitting that they came full circle to be used at a company that melded cutting edge aerospace work with the development of a communications platform and the world&#39;s first stratospheric temporospatial software defined network. The use of postmortems became a standardizing factor across Loon&#39;s teams— from avionics and manufacturing, to flight operations, to software platforms and network service. This blog post discusses how Loon moved from a heterogeneous approach to postmortems to eventually standardize and share this practice across the organization— a shift that helped the company move from R&amp;amp;D to commercial service in 2020.&lt;/p&gt;&lt;h2&gt;Background&lt;/h2&gt;&lt;h3&gt;Postmortems&lt;/h3&gt;&lt;p&gt;Many industries have adopted the use of postmortems— they are fairly common in high-risk fields where mistakes can be fatal or extremely expensive. Postmortems are also widespread in industries and projects where bad processes or assumptions can incur expensive project development costs and avoiding repeat mistakes is a priority. Individual industries and organizations often develop their own postmortem standards or templates so that postmortems are easier to create and digest across teams.&lt;/p&gt;&lt;p&gt;Blameless postmortems likely originated in the healthcare and aerospace industries in the mid-20th century. Because of the high cost of failure, these industries needed to create a culture of transparency and continuous improvement that could only come from openly discussing failure. As the original SRE book states, blameless postmortems are key to &#34;an environment where every &#39;mistake&#39; is seen as an opportunity to strengthen the system.&#34; &lt;/p&gt;&lt;p&gt;The goal of a postmortem is to document an incident or event in order to foster learning from it, both among the affected teams and beyond. The postmortem usually includes a timeline of what happened, the solutions implemented, the incident&#39;s impact, the investigation into root causes, and changes or follow-ups to stop it from happening again. To facilitate learning, SRE&#39;s postmortem format includes both what went well— acknowledging the successes that should be maintained and expanded— and what went poorly and needs to be changed. In this way, postmortem action items are key to prioritizing work that ensures the same failures don&#39;t happen again.&lt;/p&gt;&lt;h3&gt;Loon&lt;/h3&gt;&lt;p&gt;Loon aimed to supply internet access to unserved and underserved populations around the world by providing connectivity via stratospheric balloons. These high altitude “flying cell towers” covered a much wider footprint than a terrestrial tower, and could be deployed (and repositioned) into the most remote corners of the earth without expensive overland transportation and installation. As the first company to attempt anything like this, Loon dealt with a number of systems that were complex, challenging, or novel: superpressure balloons designed to stay aloft for hundreds of days, wind-dependant steering, a software defined network consisting of constantly moving nodes, and extremes of temperature and weather at 20km above Earth&#39;s surface.&lt;/p&gt;&lt;h3&gt;Prod Team&lt;/h3&gt;&lt;p&gt;The initial high-risk operations of Loon&#39;s mission were avionic: could we launch and steer balloons carrying a networking payload long enough to reach and serve the targeted region? As such, the earliest failure reports within Loon (which weren&#39;t officially called &#34;postmortems&#34; at the time) mostly involved balloon construction or flight, and drew on the experience of team members who had worked in the Avionics, Reliability Engineering, and/or Flight Safety fields. As Loon&#39;s systems evolved and matured, they started to require operational reliability, as well. Just before graduating from a purely R&amp;amp;D project in Google&#39;s &#34;moonshot factory&#34; incubator X to a company with commercial goals, Loon started building a Site Reliability Engineering (SRE) team known internally as Prod Team. &lt;/p&gt;&lt;p&gt;In order to effectively offer internet connectivity to users, Loon had to solve network serving failures with the same rigor as hardware failures. Prod Team took the lead on a number of practices to improve network reliability. The Prod Team had three primary goals: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Ensure that the fleet&#39;s automation, management, and safety-critical systems were built and operated to meet the high safety bar of the aviation industry.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Lead the integration of the communications services (e.g., LTE) end to end.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Own the mission of fielding and providing a reliable commercial service (Loon Library) in the real world.  &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Postmortems at Loon&lt;/h2&gt;&lt;h3&gt;The Early Days&lt;/h3&gt;&lt;p&gt;Postmortems were one tool for reaching Prod Team&#39;s (SRE&#39;s) goals. Prod Team often interacted with SREs in other infrastructure support teams that the Loon service connected to, such as the team developing the Evolved Packet Core (EPC), our telco partner counterparts, and teams that handle edge network connectivity. Postmortems provided a common tool for sharing incident information across all these teams, and could even span multiple companies when upstream problems impacted customers.&lt;/p&gt;&lt;p&gt;At Loon, postmortems served the following goals:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Document and transcribe the events, actions, and remedies related to an incident.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Provide a feedback loop to rectify problems.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Indicate where to build better safeguards and alerts.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Break down silos between teams in order to facilitate cross-functional knowledge sharing and accelerate development.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Identify macro themes and blind spots over the longer term. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The combination of aerospace and high tech brought two strong practices of writing postmortems, but also the challenge of how to own, investigate, or follow up on problems that crossed those boundaries, or when it wasn&#39;t clear where the system fault lay.&lt;/p&gt;&lt;p&gt;Loon&#39;s teams across hardware, software, and operations orgs used postmortems, as was standard practice in their fields for incident response. The Flight Operations Team, which handled the day-to-day operations of steering launched balloons, captured in-flight issues in a tracking system. The tracking system was part of the anomaly resolution system devised to identify and resolve root cause problems. Seeking to complement the anomaly resolution system, the Flight Operations Team incorporated the SRE software team&#39;s postmortem format for incidents that needed further investigation— for example, failure to avoid a storm system, deviations from the simulated (expected) flight path that led to an incident, and flight operator actions that directly or indirectly caused an incident. Given that most incidents spanned multiple teams (e.g., when automation failed to catch an incorrect command sent by a flight operator, which resulted in a hardware failure), utilizing a consistent postmortem format across teams simplified collaboration.&lt;/p&gt;&lt;p&gt;The Aviation and Systems Safety Team, which focused on safety related to the flight system and flight process, also brought their own tradition and best practices of postmortems. Their motto, &#34;Own our Safety&#34;, brought a commitment to continually improving safety performance and building a positive safety culture across the company. This was one of the strengths of Loon&#39;s culture: all the organizations were aligned not just on our audacious vision to &#34;connect people everywhere&#34;, but also on doing so safely and effectively. However, because industry standards for postmortems and how to handle different types of problems varied across teams, there was some divergence in process. We proactively encouraged teams to share postmortems between teams, between orgs, and across the company so that anyone could provide feedback and insight into an incident. In that way, anyone at Loon could contribute to a postmortem, see how an incident was handled, and learn about the breadth of challenges that Loon was solving. &lt;/p&gt;&lt;h3&gt;Challenges&lt;/h3&gt;&lt;p&gt;While everyone agreed that postmortems were an important practice, in a fast moving start-up culture, it was a struggle to comprehensively follow through on action items. This probably comes as no surprise to developers in similar environments— when the platform or services that require investment are rapidly changing or being replaced, it&#39;s hard to spend resources on not repeating the same mistakes. Ideally, we would have prioritized postmortems that focused on best practices and learnings that were applicable to multiple generations of the platform, but those weren&#39;t easy to identify at the time of each incident.&lt;/p&gt;&lt;p&gt;Even though the company was not especially large, the novelty of Loon&#39;s platform and interconnectedness of its operations made determining which team was responsible for writing a postmortem and investigating root causes difficult. For example, a 20 minute service disruption on the ground might be caused by a loss of connectivity from the balloon to the backhaul network, a pointing error with the antennae on the payload, insufficient battery levels, or wind that temporarily blew the balloon out of range. Actual causes could be quite nuanced, and often were attributable to interactions between multiple sub-systems. Thus, we had a chicken-and-egg problem: which team should start the postmortem and investigation, and when should they hand off the postmortem to the teams that likely owned the faulty system or process? Not all teams had a culture of postmortems, so the process could stall depending on the system where the root cause originated. For that reason, Loon&#39;s Prod Team/SREs advocated for a company-wide blameless postmortem culture. &lt;/p&gt;&lt;p&gt;Much of how Loon used postmortems, especially in software development and Prod Team, was in line with SRE industry standards. In the early days of Loon, however, there were no service level objectives or agreements (SLO/As). As Loon was an R&amp;amp;D project, we wrote postmortems when a test network failed to boot after launch, or when performance didn&#39;t meet the team&#39;s predictions, rather than for &#34;service outages&#34;. Later on, when Loon supplied commercial service in disaster relief areas in Peru and Kenya, the Prod Team could more clearly identify the types of user-facing incidents that required postmortems due to failure to meet SLAs.&lt;/p&gt;&lt;h3&gt;Improving and Standardizing Loon&#39;s Postmortem Processes&lt;/h3&gt;&lt;p&gt;Moving Loon from an R&amp;amp;D model to the model of reliability and safety necessary for a commercial offering required more than simply performing postmortems. Sharing the postmortems openly and widely across Loon was critical to building a culture of continuous improvement and addressing root causes. &lt;/p&gt;&lt;p&gt;To increase cross-team awareness of incidents, in 2019 we instituted a Postmortem Working Group. In addition to reading and discussing recent postmortems from across the company, the goals of the working group were to make it easier to write postmortems, promote the practice of writing postmortems, increase sharing across teams, and discuss the findings of these incidents in order to learn the patterns of failure. Its founding goal was to &#34;&lt;i&gt;Cultivate a postmortem culture in Loon to encourage thoughtful risk taking, to take advantage of mistakes, and to provide structure to support improvement over time.&lt;/i&gt;&#34; While the volume of postmortems could ebb and flow across weeks and months, over multiple years of commercial service we expected to be able to identify macro-trends that needed to be addressed with the cooperation of multiple teams.&lt;/p&gt;&lt;p&gt;In addition to the Postmortem Working Group, we also created a postmortem mailing list and a repository of all postmortems, and presented a &#34;Lunch &amp;amp; Learn&#34; on blameless postmortems (see example slide below). Prod Team and several other teams&#39; meetings had a standing agenda item to review postmortems of interest from across the company, and we sent a semi-annual email celebrating Loon&#39;s &#34;best-of&#34; recent incidents: the most interesting or educational outages.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;loon.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/loon.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Once we had a standardized postmortem template in place, we could adopt and reuse it to document commercial service field tests. By recording a timeline and incidents, defining a process and space to determine root causes of problems, recording measurements and metrics, and providing the structure for action item tracking, we brought the benefits of postmortem retrospectives to prospective tasks. &lt;/p&gt;&lt;p&gt;When Loon began commercial trials in countries like Peru and Kenya, we conducted numerous field tests. These tests required engineers from Loon and/or the telco partner to travel to remote locations to measure the strength of the LTE signal on the ground. Prod Team proactively used the postmortem template to document the field tests. It provided a useful format to record the log of test events, results that did and did not match expectations, and links to further investigations into those failures. As a cutting edge project in a highly variable operating environment, using the postmortem template as our default testing template was an acknowledgement that we were in a state of constant and rapid iteration and improvement. These trials took place in early to mid 2020, under the sudden specter of Covid and the subsequent shift towards working from home. The structured communications at the core of Loon&#39;s postmortem structure were particularly helpful as we moved from in-person coordination rooms to WFH.&lt;/p&gt;&lt;h3&gt;What Loon Learned from Standardizing Postmortems&lt;/h3&gt;&lt;p&gt;Postmortems are widely used in various industries because they are effective. At Loon, we saw that even fast moving startups and R&amp;amp;D projects should invest early in a transparent and blameless postmortem culture. That culture should include a clear process for writing postmortems, clear guidelines for when to conduct a postmortem, and a staffed commitment to follow up on action items. &lt;/p&gt;&lt;p&gt;Meta-reviews across postmortems and outages revealed several trends. &lt;/p&gt;&lt;p&gt;The many points of failure we observed across the range of postmortems were indicative of both the complexity of Loon&#39;s systems and the complexity of some of its supporting infrastructure. Postmortems are equally adept at finding flaky tests and fragile processes vs. hardware failures or satellite network outages. These are complexities familiar to many startups, where postmortems can help manage the tradeoff between making changes safely vs. moving quickly and trying many new things.&lt;/p&gt;&lt;p&gt;Loon was still operating a superhero culture: across a wide range of issues, a small set of experts were repeatedly called upon to fix the system. This dynamic is common in startups, and not meant as a pejorative, but was markedly different from the system maturity that many of Prod Team/SRE were used to. Once we identified this pattern, our plan for commercial service was to staff a 24x7 oncall rotation, complemented by Program Managers driving intention processes to de-risk production&lt;/p&gt;&lt;p&gt;Postmortems provided a space to ask questions like, &#34;What other issues could pop up in this realm?&#34;, which prompted us to solve for the broader case of problems rather than specific problems we&#39;d already seen. This practice also stopped people from brushing off problems in the name of development speed, or from dismissing issues because they &#34;just concerned a prototype&#34;.&lt;/p&gt;&lt;h2&gt;Tips and Takeaways&lt;/h2&gt;&lt;p&gt;While the specifics of Loon&#39;s journey to standardize postmortems tell the story of one company, we have some tips and takeaways that should be applicable at most organizations.&lt;/p&gt;&lt;h3&gt;Tip 1: Adopting a blameless postmortem culture requires everyone to participate&lt;/h3&gt;&lt;p&gt;Although the initiative of writing postmortems often originates with a software team, if you want every team to adopt the practice, we suggest trying the following:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Give a talk about postmortems and how and why they could benefit all.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Form a postmortem working group.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Invite people representing different teams to be part of the postmortem working group. They will give insights into what could work better for their respective teams.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Don&#39;t make the postmortem working group responsible for writing the postmortems— this approach doesn&#39;t scale. Reviewing and consulting on postmortems may be in scope of their duties, especially while new teams are adopting this practice.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Tip 2: Define a lightweight postmortem process&lt;/h3&gt;&lt;p&gt;Especially during adoption, you want teams to see the benefits of postmortems, not the burden of writing them. Creating a postmortem template with minimum requirements can be helpful.&lt;/p&gt;&lt;h3&gt;Tip 3: Define a clear owner for postmortems&lt;/h3&gt;&lt;p&gt;Who should write a postmortem and when? For software teams with an oncall rotation, the answer is clear: the person who was oncall during the incident is the owner, and we write postmortems when a service interruption breached SLOs. But when the service has no SLOs, or when a team doesn&#39;t have an oncall rotation, you need defined criteria. Bonus points if the outage involves multiple systems and teams. The following exercises can help in this area:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Reflect on these topics from the point of view of each team, and from the point of view of the interaction between teams.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;For each team, define what type of incident(s) should trigger a postmortem.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Within the team, define who should own writing each postmortem. Avoid putting the entire burden on the same person frequently; consider forming a rotation.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Tip 4: Encourage blameless postmortems and make people proud of them&lt;/h3&gt;&lt;p&gt;Consider some activities that can help foster the blameless postmortem culture:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Write a report of the best postmortems over a given period and circulate them broadly.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Conduct training on how to write postmortems.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Train managers and encourage them to prioritize postmortems on their teams.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;When Loon shut down, addressing all these points was still a work in progress. We don&#39;t have a teachable moment of “this postmortem process will solve your failures”, because postmortems don&#39;t do that. However, we could see where postmortems stopped us from needing to deal with the same failures repeatedly… and where sometimes we did experience repeat incidents because the AIs from the first postmortem weren&#39;t prioritized enough. And so this piece of writing— effectively, a postmortem on Loon&#39;s postmortems—serves up a familiar lesson: postmortems work, but only as well as they are widely accepted and adhered to.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c67=""><div _ngcontent-c67="" innerhtml="&lt;p&gt;Founded by Google SRE alumni, it is no surprise that Loon&#39;s Production Engineering/SRE team instituted a culture of blameless postmortems that became a key feature of Loon&#39;s approach to incident response. Blameless postmortems originated as an aerospace practice in the mid-20th century, so it was particularly fitting that they came full circle to be used at a company that melded cutting edge aerospace work with the development of a communications platform and the world&#39;s first stratospheric temporospatial software defined network. The use of postmortems became a standardizing factor across Loon&#39;s teams&amp;#8212; from avionics and manufacturing, to flight operations, to software platforms and network service. This blog post discusses how Loon moved from a heterogeneous approach to postmortems to eventually standardize and share this practice across the organization&amp;#8212; a shift that helped the company move from R&amp;amp;D to commercial service in 2020.&lt;/p&gt;&lt;h2&gt;Background&lt;/h2&gt;&lt;h3&gt;Postmortems&lt;/h3&gt;&lt;p&gt;Many industries have adopted the use of postmortems&amp;#8212; they are fairly common in high-risk fields where mistakes can be fatal or extremely expensive. Postmortems are also widespread in industries and projects where bad processes or assumptions can incur expensive project development costs and avoiding repeat mistakes is a priority. Individual industries and organizations often develop their own postmortem standards or templates so that postmortems are easier to create and digest across teams.&lt;/p&gt;&lt;p&gt;Blameless postmortems likely originated in the healthcare and aerospace industries in the mid-20th century. Because of the high cost of failure, these industries needed to create a culture of transparency and continuous improvement that could only come from openly discussing failure. As the original SRE book states, blameless postmortems are key to &amp;#34;an environment where every &#39;mistake&#39; is seen as an opportunity to strengthen the system.&amp;#34;&amp;#160;&lt;/p&gt;&lt;p&gt;The goal of a postmortem is to document an incident or event in order to foster learning from it, both among the affected teams and beyond. The postmortem usually includes a timeline of what happened, the solutions implemented, the incident&#39;s impact, the investigation into root causes, and changes or follow-ups to stop it from happening again. To facilitate learning, SRE&#39;s postmortem format includes both what went well&amp;#8212; acknowledging the successes that should be maintained and expanded&amp;#8212; and what went poorly and needs to be changed. In this way, postmortem action items are key to prioritizing work that ensures the same failures don&#39;t happen again.&lt;/p&gt;&lt;h3&gt;Loon&lt;/h3&gt;&lt;p&gt;Loon aimed to supply internet access to unserved and underserved populations around the world by providing connectivity via stratospheric balloons. These high altitude &amp;#8220;flying cell towers&amp;#8221; covered a much wider footprint than a terrestrial tower, and could be deployed (and repositioned) into the most remote corners of the earth without expensive overland transportation and installation. As the first company to attempt anything like this, Loon dealt with a number of systems that were complex, challenging, or novel: superpressure balloons designed to stay aloft for hundreds of days, wind-dependant steering, a software defined network consisting of constantly moving nodes, and extremes of temperature and weather at 20km above Earth&#39;s surface.&lt;/p&gt;&lt;h3&gt;Prod Team&lt;/h3&gt;&lt;p&gt;The initial high-risk operations of Loon&#39;s mission were avionic: could we launch and steer balloons carrying a networking payload long enough to reach and serve the targeted region? As such, the earliest failure reports within Loon (which weren&#39;t officially called &amp;#34;postmortems&amp;#34; at the time) mostly involved balloon construction or flight, and drew on the experience of team members who had worked in the Avionics, Reliability Engineering, and/or Flight Safety fields. As Loon&#39;s systems evolved and matured, they started to require operational reliability, as well. Just before graduating from a purely R&amp;amp;D project in Google&#39;s &amp;#34;moonshot factory&amp;#34; incubator X to a company with commercial goals, Loon started building a Site Reliability Engineering (SRE) team known internally as Prod Team.&amp;#160;&lt;/p&gt;&lt;p&gt;In order to effectively offer internet connectivity to users, Loon had to solve network serving failures with the same rigor as hardware failures. Prod Team took the lead on a number of practices to improve network reliability. The Prod Team had three primary goals:&amp;#160;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Ensure that the fleet&#39;s automation, management, and safety-critical systems were built and operated to meet the high safety bar of the aviation industry.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Lead the integration of the communications services (e.g., LTE) end to end.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Own the mission of fielding and providing a reliable commercial service (Loon Library) in the real world.&amp;#160;&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;Postmortems at Loon&lt;/h2&gt;&lt;h3&gt;The Early Days&lt;/h3&gt;&lt;p&gt;Postmortems were one tool for reaching Prod Team&#39;s (SRE&#39;s) goals. Prod Team often interacted with SREs in other infrastructure support teams that the Loon service connected to, such as the team developing the Evolved Packet Core (EPC), our telco partner counterparts, and teams that handle edge network connectivity. Postmortems provided a common tool for sharing incident information across all these teams, and could even span multiple companies when upstream problems impacted customers.&lt;/p&gt;&lt;p&gt;At Loon, postmortems served the following goals:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Document and transcribe the events, actions, and remedies related to an incident.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Provide a feedback loop to rectify problems.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Indicate where to build better safeguards and alerts.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Break down silos between teams in order to facilitate cross-functional knowledge sharing and accelerate development.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Identify macro themes and blind spots over the longer term.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The combination of aerospace and high tech brought two strong practices of writing postmortems, but also the challenge of how to own, investigate, or follow up on problems that crossed those boundaries, or when it wasn&#39;t clear where the system fault lay.&lt;/p&gt;&lt;p&gt;Loon&#39;s teams across hardware, software, and operations orgs used postmortems, as was standard practice in their fields for incident response. The Flight Operations Team, which handled the day-to-day operations of steering launched balloons, captured in-flight issues in a tracking system. The tracking system was part of the anomaly resolution system devised to identify and resolve root cause problems. Seeking to complement the anomaly resolution system, the Flight Operations Team incorporated the SRE software team&#39;s postmortem format for incidents that needed further investigation&amp;#8212; for example, failure to avoid a storm system, deviations from the simulated (expected) flight path that led to an incident, and flight operator actions that directly or indirectly caused an incident. Given that most incidents spanned multiple teams (e.g., when automation failed to catch an incorrect command sent by a flight operator, which resulted in a hardware failure), utilizing a consistent postmortem format across teams simplified collaboration.&lt;/p&gt;&lt;p&gt;The Aviation and Systems Safety Team, which focused on safety related to the flight system and flight process, also brought their own tradition and best practices of postmortems. Their motto, &amp;#34;Own our Safety&amp;#34;, brought a commitment to continually improving safety performance and building a positive safety culture across the company. This was one of the strengths of Loon&#39;s culture: all the organizations were aligned not just on our audacious vision to &amp;#34;connect people everywhere&amp;#34;, but also on doing so safely and effectively. However, because industry standards for postmortems and how to handle different types of problems varied across teams, there was some divergence in process. We proactively encouraged teams to share postmortems between teams, between orgs, and across the company so that anyone could provide feedback and insight into an incident. In that way, anyone at Loon could contribute to a postmortem, see how an incident was handled, and learn about the breadth of challenges that Loon was solving.&amp;#160;&lt;/p&gt;&lt;h3&gt;Challenges&lt;/h3&gt;&lt;p&gt;While everyone agreed that postmortems were an important practice, in a fast moving start-up culture, it was a struggle to comprehensively follow through on action items. This probably comes as no surprise to developers in similar environments&amp;#8212; when the platform or services that require investment are rapidly changing or being replaced, it&#39;s hard to spend resources on not repeating the same mistakes. Ideally, we would have prioritized postmortems that focused on best practices and learnings that were applicable to multiple generations of the platform, but those weren&#39;t easy to identify at the time of each incident.&lt;/p&gt;&lt;p&gt;Even though the company was not especially large, the novelty of Loon&#39;s platform and interconnectedness of its operations made determining which team was responsible for writing a postmortem and investigating root causes difficult. For example, a 20 minute service disruption on the ground might be caused by a loss of connectivity from the balloon to the backhaul network, a pointing error with the antennae on the payload, insufficient battery levels, or wind that temporarily blew the balloon out of range. Actual causes could be quite nuanced, and often were attributable to interactions between multiple sub-systems. Thus, we had a chicken-and-egg problem: which team should start the postmortem and investigation, and when should they hand off the postmortem to the teams that likely owned the faulty system or process? Not all teams had a culture of postmortems, so the process could stall depending on the system where the root cause originated. For that reason, Loon&#39;s Prod Team/SREs advocated for a company-wide blameless postmortem culture.&amp;#160;&lt;/p&gt;&lt;p&gt;Much of how Loon used postmortems, especially in software development and Prod Team, was in line with SRE industry standards. In the early days of Loon, however, there were no service level objectives or agreements (SLO/As). As Loon was an R&amp;amp;D project, we wrote postmortems when a test network failed to boot after launch, or when performance didn&#39;t meet the team&#39;s predictions, rather than for &amp;#34;service outages&amp;#34;. Later on, when Loon supplied commercial service in disaster relief areas in Peru and Kenya, the Prod Team could more clearly identify the types of user-facing incidents that required postmortems due to failure to meet SLAs.&lt;/p&gt;&lt;h3&gt;Improving and Standardizing Loon&#39;s Postmortem Processes&lt;/h3&gt;&lt;p&gt;Moving Loon from an R&amp;amp;D model to the model of reliability and safety necessary for a commercial offering required more than simply performing postmortems. Sharing the postmortems openly and widely across Loon was critical to building a culture of continuous improvement and addressing root causes.&amp;#160;&lt;/p&gt;&lt;p&gt;To increase cross-team awareness of incidents, in 2019 we instituted a Postmortem Working Group. In addition to reading and discussing recent postmortems from across the company, the goals of the working group were to make it easier to write postmortems, promote the practice of writing postmortems, increase sharing across teams, and discuss the findings of these incidents in order to learn the patterns of failure. Its founding goal was to &amp;#34;&lt;i&gt;Cultivate a postmortem culture in Loon to encourage thoughtful risk taking, to take advantage of mistakes, and to provide structure to support improvement over time.&lt;/i&gt;&amp;#34; While the volume of postmortems could ebb and flow across weeks and months, over multiple years of commercial service we expected to be able to identify macro-trends that needed to be addressed with the cooperation of multiple teams.&lt;/p&gt;&lt;p&gt;In addition to the Postmortem Working Group, we also created a postmortem mailing list and a repository of all postmortems, and presented a &amp;#34;Lunch &amp;amp; Learn&amp;#34; on blameless postmortems (see example slide below). Prod Team and several other teams&#39; meetings had a standing agenda item to review postmortems of interest from across the company, and we sent a semi-annual email celebrating Loon&#39;s &amp;#34;best-of&amp;#34; recent incidents: the most interesting or educational outages.&lt;/p&gt;"><p>Founded by Google SRE alumni, it is no surprise that Loon&#39;s Production Engineering/SRE team instituted a culture of blameless postmortems that became a key feature of Loon&#39;s approach to incident response. Blameless postmortems originated as an aerospace practice in the mid-20th century, so it was particularly fitting that they came full circle to be used at a company that melded cutting edge aerospace work with the development of a communications platform and the world&#39;s first stratospheric temporospatial software defined network. The use of postmortems became a standardizing factor across Loon&#39;s teams— from avionics and manufacturing, to flight operations, to software platforms and network service. This blog post discusses how Loon moved from a heterogeneous approach to postmortems to eventually standardize and share this practice across the organization— a shift that helped the company move from R&amp;D to commercial service in 2020.</p><h2>Background</h2><h3>Postmortems</h3><p>Many industries have adopted the use of postmortems— they are fairly common in high-risk fields where mistakes can be fatal or extremely expensive. Postmortems are also widespread in industries and projects where bad processes or assumptions can incur expensive project development costs and avoiding repeat mistakes is a priority. Individual industries and organizations often develop their own postmortem standards or templates so that postmortems are easier to create and digest across teams.</p><p>Blameless postmortems likely originated in the healthcare and aerospace industries in the mid-20th century. Because of the high cost of failure, these industries needed to create a culture of transparency and continuous improvement that could only come from openly discussing failure. As the original SRE book states, blameless postmortems are key to &#34;an environment where every &#39;mistake&#39; is seen as an opportunity to strengthen the system.&#34; </p><p>The goal of a postmortem is to document an incident or event in order to foster learning from it, both among the affected teams and beyond. The postmortem usually includes a timeline of what happened, the solutions implemented, the incident&#39;s impact, the investigation into root causes, and changes or follow-ups to stop it from happening again. To facilitate learning, SRE&#39;s postmortem format includes both what went well— acknowledging the successes that should be maintained and expanded— and what went poorly and needs to be changed. In this way, postmortem action items are key to prioritizing work that ensures the same failures don&#39;t happen again.</p><h3>Loon</h3><p>Loon aimed to supply internet access to unserved and underserved populations around the world by providing connectivity via stratospheric balloons. These high altitude “flying cell towers” covered a much wider footprint than a terrestrial tower, and could be deployed (and repositioned) into the most remote corners of the earth without expensive overland transportation and installation. As the first company to attempt anything like this, Loon dealt with a number of systems that were complex, challenging, or novel: superpressure balloons designed to stay aloft for hundreds of days, wind-dependant steering, a software defined network consisting of constantly moving nodes, and extremes of temperature and weather at 20km above Earth&#39;s surface.</p><h3>Prod Team</h3><p>The initial high-risk operations of Loon&#39;s mission were avionic: could we launch and steer balloons carrying a networking payload long enough to reach and serve the targeted region? As such, the earliest failure reports within Loon (which weren&#39;t officially called &#34;postmortems&#34; at the time) mostly involved balloon construction or flight, and drew on the experience of team members who had worked in the Avionics, Reliability Engineering, and/or Flight Safety fields. As Loon&#39;s systems evolved and matured, they started to require operational reliability, as well. Just before graduating from a purely R&amp;D project in Google&#39;s &#34;moonshot factory&#34; incubator X to a company with commercial goals, Loon started building a Site Reliability Engineering (SRE) team known internally as Prod Team. </p><p>In order to effectively offer internet connectivity to users, Loon had to solve network serving failures with the same rigor as hardware failures. Prod Team took the lead on a number of practices to improve network reliability. The Prod Team had three primary goals: </p><ul><li><p>Ensure that the fleet&#39;s automation, management, and safety-critical systems were built and operated to meet the high safety bar of the aviation industry.</p></li><li><p>Lead the integration of the communications services (e.g., LTE) end to end.</p></li><li><p>Own the mission of fielding and providing a reliable commercial service (Loon Library) in the real world.  </p></li></ul><h2>Postmortems at Loon</h2><h3>The Early Days</h3><p>Postmortems were one tool for reaching Prod Team&#39;s (SRE&#39;s) goals. Prod Team often interacted with SREs in other infrastructure support teams that the Loon service connected to, such as the team developing the Evolved Packet Core (EPC), our telco partner counterparts, and teams that handle edge network connectivity. Postmortems provided a common tool for sharing incident information across all these teams, and could even span multiple companies when upstream problems impacted customers.</p><p>At Loon, postmortems served the following goals:</p><ul><li><p>Document and transcribe the events, actions, and remedies related to an incident.</p></li><li><p>Provide a feedback loop to rectify problems.</p></li><li><p>Indicate where to build better safeguards and alerts.</p></li><li><p>Break down silos between teams in order to facilitate cross-functional knowledge sharing and accelerate development.</p></li><li><p>Identify macro themes and blind spots over the longer term. </p></li></ul><p>The combination of aerospace and high tech brought two strong practices of writing postmortems, but also the challenge of how to own, investigate, or follow up on problems that crossed those boundaries, or when it wasn&#39;t clear where the system fault lay.</p><p>Loon&#39;s teams across hardware, software, and operations orgs used postmortems, as was standard practice in their fields for incident response. The Flight Operations Team, which handled the day-to-day operations of steering launched balloons, captured in-flight issues in a tracking system. The tracking system was part of the anomaly resolution system devised to identify and resolve root cause problems. Seeking to complement the anomaly resolution system, the Flight Operations Team incorporated the SRE software team&#39;s postmortem format for incidents that needed further investigation— for example, failure to avoid a storm system, deviations from the simulated (expected) flight path that led to an incident, and flight operator actions that directly or indirectly caused an incident. Given that most incidents spanned multiple teams (e.g., when automation failed to catch an incorrect command sent by a flight operator, which resulted in a hardware failure), utilizing a consistent postmortem format across teams simplified collaboration.</p><p>The Aviation and Systems Safety Team, which focused on safety related to the flight system and flight process, also brought their own tradition and best practices of postmortems. Their motto, &#34;Own our Safety&#34;, brought a commitment to continually improving safety performance and building a positive safety culture across the company. This was one of the strengths of Loon&#39;s culture: all the organizations were aligned not just on our audacious vision to &#34;connect people everywhere&#34;, but also on doing so safely and effectively. However, because industry standards for postmortems and how to handle different types of problems varied across teams, there was some divergence in process. We proactively encouraged teams to share postmortems between teams, between orgs, and across the company so that anyone could provide feedback and insight into an incident. In that way, anyone at Loon could contribute to a postmortem, see how an incident was handled, and learn about the breadth of challenges that Loon was solving. </p><h3>Challenges</h3><p>While everyone agreed that postmortems were an important practice, in a fast moving start-up culture, it was a struggle to comprehensively follow through on action items. This probably comes as no surprise to developers in similar environments— when the platform or services that require investment are rapidly changing or being replaced, it&#39;s hard to spend resources on not repeating the same mistakes. Ideally, we would have prioritized postmortems that focused on best practices and learnings that were applicable to multiple generations of the platform, but those weren&#39;t easy to identify at the time of each incident.</p><p>Even though the company was not especially large, the novelty of Loon&#39;s platform and interconnectedness of its operations made determining which team was responsible for writing a postmortem and investigating root causes difficult. For example, a 20 minute service disruption on the ground might be caused by a loss of connectivity from the balloon to the backhaul network, a pointing error with the antennae on the payload, insufficient battery levels, or wind that temporarily blew the balloon out of range. Actual causes could be quite nuanced, and often were attributable to interactions between multiple sub-systems. Thus, we had a chicken-and-egg problem: which team should start the postmortem and investigation, and when should they hand off the postmortem to the teams that likely owned the faulty system or process? Not all teams had a culture of postmortems, so the process could stall depending on the system where the root cause originated. For that reason, Loon&#39;s Prod Team/SREs advocated for a company-wide blameless postmortem culture. </p><p>Much of how Loon used postmortems, especially in software development and Prod Team, was in line with SRE industry standards. In the early days of Loon, however, there were no service level objectives or agreements (SLO/As). As Loon was an R&amp;D project, we wrote postmortems when a test network failed to boot after launch, or when performance didn&#39;t meet the team&#39;s predictions, rather than for &#34;service outages&#34;. Later on, when Loon supplied commercial service in disaster relief areas in Peru and Kenya, the Prod Team could more clearly identify the types of user-facing incidents that required postmortems due to failure to meet SLAs.</p><h3>Improving and Standardizing Loon&#39;s Postmortem Processes</h3><p>Moving Loon from an R&amp;D model to the model of reliability and safety necessary for a commercial offering required more than simply performing postmortems. Sharing the postmortems openly and widely across Loon was critical to building a culture of continuous improvement and addressing root causes. </p><p>To increase cross-team awareness of incidents, in 2019 we instituted a Postmortem Working Group. In addition to reading and discussing recent postmortems from across the company, the goals of the working group were to make it easier to write postmortems, promote the practice of writing postmortems, increase sharing across teams, and discuss the findings of these incidents in order to learn the patterns of failure. Its founding goal was to &#34;<i>Cultivate a postmortem culture in Loon to encourage thoughtful risk taking, to take advantage of mistakes, and to provide structure to support improvement over time.</i>&#34; While the volume of postmortems could ebb and flow across weeks and months, over multiple years of commercial service we expected to be able to identify macro-trends that needed to be addressed with the cooperation of multiple teams.</p><p>In addition to the Postmortem Working Group, we also created a postmortem mailing list and a repository of all postmortems, and presented a &#34;Lunch &amp; Learn&#34; on blameless postmortems (see example slide below). Prod Team and several other teams&#39; meetings had a standing agenda item to review postmortems of interest from across the company, and we sent a semi-annual email celebrating Loon&#39;s &#34;best-of&#34; recent incidents: the most interesting or educational outages.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Giselle Font&lt;/name&gt;&lt;title&gt;Site Reliability Engineer, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Genric_GCP_upA1oyz.max-2200x2200.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 07 Dec 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Ensuring scale and compliance of your Terraform Deployment with Cloud Build</title>
      <link>https://cloud.google.com/blog/products/devops-sre/terraform-gitops-with-google-cloud-build-and-storage/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.terraform.io/&#34; target=&#34;_blank&#34;&gt;Terraform&lt;/a&gt; is an open source Infrastructure as Code tool that is popular with platform developers building reusable cloud automation. The &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs&#34; target=&#34;_blank&#34;&gt;Terraform Provider for Google Cloud Platform&lt;/a&gt; continues to add support for the latest Google Cloud features, such as &lt;a href=&#34;https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster&#34;&gt;Anthos on GKE&lt;/a&gt;, and our teams continue to expand Terraform integrations including &lt;a href=&#34;https://cloud.google.com/foundation-toolkit&#34;&gt;Cloud Foundation Toolkit&lt;/a&gt; and &lt;a href=&#34;https://github.com/GoogleCloudPlatform/terraform-validator#terraform-validator&#34; target=&#34;_blank&#34;&gt;Terraform Validator&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;How do teams use Terraform on Google Cloud? While the simplest approach is to run &lt;code&gt;terraform init&lt;/code&gt;, &lt;code&gt;plan&lt;/code&gt; and &lt;code&gt;apply&lt;/code&gt; directly from your terminal,  it cannot be recommended for automating your production deployments. First, there is a decision on how to store your Terraform &lt;a href=&#34;https://www.terraform.io/docs/language/state/index.html&#34; target=&#34;_blank&#34;&gt;state&lt;/a&gt; in a way that is secure, compliant and enables team collaboration. Secondly there’s a question of scale and reliability. Over the course of even the simplest cloud deployment, Terraform can end up making thousands of Create/Read/Update/Delete API calls to the endpoints used by the Terraform providers, some of which will inevitably hit quota issues or need to be retried for other reasons. For platform administrators, who are looking to ensure the best deployment practices for their curated Terraform solutions,  while benefiting from the simplicity of Google Cloud Console, there’s &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/private-catalog-uses-terraform-to-update-available-solutions&#34;&gt;Terraform Private Catalog integration&lt;/a&gt; that we enabled earlier this year.&lt;/p&gt;&lt;p&gt;Outside of Private Catalog, &lt;a href=&#34;https://cloud.google.com/build&#34;&gt;Cloud Build&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/storage&#34;&gt;Cloud Storage&lt;/a&gt; have been the recommended approach to use Terraform on Google Cloud. Using a remote&lt;a href=&#34;https://www.terraform.io/docs/language/settings/backends/index.html&#34; target=&#34;_blank&#34;&gt;backend&lt;/a&gt; prevents race conditions and simplifies sharing reusable modules between different configurations. With Cloud Build you can configure a GitOps CI/CD pipeline to automatically &lt;code&gt;plan&lt;/code&gt; and &lt;code&gt;apply&lt;/code&gt; your Terraform configuration when changes are pushed into the repo. &lt;/p&gt;&lt;p&gt;These are widely popularized benefits explored in &lt;a href=&#34;https://cloud.google.com/architecture/managing-infrastructure-as-code&#34;&gt;Managing infrastructure as code with Terraform, Cloud Build, and GitOps&lt;/a&gt;. In addition, there are lesser known advantages of Cloud Build, particularly for enterprise customers: Cloud Build’s concurrency capabilities and VPC-SC support, Cloud Storage versioning, security and compliance. Let’s explore these benefits in more detail.&lt;/p&gt;&lt;p&gt;Cloud Build’s ability to scale makes it capable to process multiple Terraform deployments across the regions globally and simultaneously. By default, Cloud Build supports 30 concurrent builds, with additional builds queued and processed after the running builds complete. In some cases it may not be enough. Customers who initiate parallel deployments to multiple zones, or, those who provision infrastructure on behalf of multiple tenants, often require running more concurrent deployments to complete all of them within the allotted deployment window. Cloud Build private pool feature allows up to &lt;a href=&#34;https://cloud.google.com/build/quotas&#34;&gt;100 concurrent builds&lt;/a&gt; which may be further adjusted upon request. This is an example of creating a private pool and then using it when submitting a build:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;A full step by step example of creating a private pool and submitting 80+ Terraform deployments with Cloud Build simultaneously is available &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/terraform/examples/infra_at_scale&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Using Cloud Build removes the need to build a custom high-scale Terraform provisioning service and provides &lt;a href=&#34;https://cloud.google.com/build/docs/view-build-results&#34;&gt;observability and diagnostics&lt;/a&gt; for each of the build instances launched and their results. &lt;/p&gt;&lt;p&gt;Using Cloud Build with private pools enables recommended security features, such as &lt;a href=&#34;https://cloud.google.com/build/docs/private-pools/using-vpc-service-controls&#34;&gt;VPC Service Controls&lt;/a&gt;that allows setting secure perimeter to protect against data exfiltration, with additional restrictions to further restrict it to using the specified private pools. This makes it unnecessary to configure a dedicated &lt;a href=&#34;https://en.wikipedia.org/wiki/Bastion_host&#34; target=&#34;_blank&#34;&gt;bastion host&lt;/a&gt; inside the perimeter, which improves the overall security posture.&lt;/p&gt;Beyond just using Cloud Storage for remote storage, additional reasons to use Cloud Storage include versioning, security and compliance. Enabling versioning protects against state file corruption and allows you to view earlier versions. Versioning can be enabled with &lt;code&gt;gsutil&lt;/code&gt; command:&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In addition to versioning, you can use &lt;a href=&#34;https://cloud.google.com/security/encryption/customer-supplied-encryption-keys&#34;&gt;Customer-Supplied Encryption Keys&lt;/a&gt; to encrypt the Terraform state file. After you generated the key you can specify it as encryption_key parameter of your backend object:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Once encrypted you can still view the contents of your state by &lt;a href=&#34;https://cloud.google.com/storage/docs/encryption/customer-supplied-keys#gsutil&#34;&gt;adding encryption_key option to boto configuration file&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Finally, Cloud Storage is one of the Google Cloud services covered by &lt;a href=&#34;https://cloud.google.com/security/compliance/fedramp&#34;&gt;FedRAMP High&lt;/a&gt;, which is important for enterprises  that are seeking their own FedRAMP on top of Google Cloud (for more details see &lt;a href=&#34;https://cloud.google.com/security/compliance&#34;&gt;Compliance resource center&lt;/a&gt;).&lt;/p&gt;&lt;p&gt;To summarize, using Cloud Build and Cloud Storage for your Terraform deployments enable high scalability, security and compliance with simpler configuration and via familiar &lt;code&gt;gcloud&lt;/code&gt; and Google Cloud console interface. Please check out this &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/terraform/examples/infra_at_scale&#34; target=&#34;_blank&#34;&gt;sample&lt;/a&gt; for step by step guidance.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/cloud-build-private-pools-offers-cicd-for-private-networks/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/devops.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Introducing Cloud Build private pools: Secure CI/CD for private networks&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;With new private pools, you can use Google Cloud’s hosted Cloud Build CI/CD service on resources in your private network or in other clouds.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;&lt;a href=&#34;https://www.terraform.io/&#34; target=&#34;_blank&#34;&gt;Terraform&lt;/a&gt; is an open source Infrastructure as Code tool that is popular with platform developers building reusable cloud automation. The &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs&#34; target=&#34;_blank&#34;&gt;Terraform Provider for Google Cloud Platform&lt;/a&gt; continues to add support for the latest Google Cloud features, such as &lt;a href=&#34;https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster&#34;&gt;Anthos on GKE&lt;/a&gt;, and our teams continue to expand Terraform integrations including &lt;a href=&#34;https://cloud.google.com/foundation-toolkit&#34;&gt;Cloud Foundation Toolkit&lt;/a&gt; and &lt;a href=&#34;https://github.com/GoogleCloudPlatform/terraform-validator#terraform-validator&#34; target=&#34;_blank&#34;&gt;Terraform Validator&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;How do teams use Terraform on Google Cloud? While the simplest approach is to run &lt;code&gt;terraform init&lt;/code&gt;, &lt;code&gt;plan&lt;/code&gt; and &lt;code&gt;apply&lt;/code&gt; directly from your terminal,&amp;#160; it cannot be recommended for automating your production deployments. First, there is a decision on how to store your Terraform &lt;a href=&#34;https://www.terraform.io/docs/language/state/index.html&#34; target=&#34;_blank&#34;&gt;state&lt;/a&gt; in a way that is secure, compliant and enables team collaboration. Secondly there&amp;#8217;s a question of scale and reliability. Over the course of even the simplest cloud deployment, Terraform can end up making thousands of Create/Read/Update/Delete API calls to the endpoints used by the Terraform providers, some of which will inevitably hit quota issues or need to be retried for other reasons. For platform administrators, who are looking to ensure the best deployment practices for their curated Terraform solutions,&amp;#160; while benefiting from the simplicity of Google Cloud Console, there&amp;#8217;s &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/private-catalog-uses-terraform-to-update-available-solutions&#34;&gt;Terraform Private Catalog integration&lt;/a&gt; that we enabled earlier this year.&lt;/p&gt;&lt;p&gt;Outside of Private Catalog, &lt;a href=&#34;https://cloud.google.com/build&#34;&gt;Cloud Build&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/storage&#34;&gt;Cloud Storage&lt;/a&gt; have been the recommended approach to use Terraform on Google Cloud. Using a remote&lt;a href=&#34;https://www.terraform.io/docs/language/settings/backends/index.html&#34; target=&#34;_blank&#34;&gt; backend&lt;/a&gt; prevents race conditions and simplifies sharing reusable modules between different configurations. With Cloud Build you can configure a GitOps CI/CD pipeline to automatically &lt;code&gt;plan&lt;/code&gt; and &lt;code&gt;apply&lt;/code&gt; your Terraform configuration when changes are pushed into the repo.&amp;#160;&lt;/p&gt;&lt;p&gt;These are widely popularized benefits explored in &lt;a href=&#34;https://cloud.google.com/architecture/managing-infrastructure-as-code&#34;&gt;Managing infrastructure as code with Terraform, Cloud Build, and GitOps&lt;/a&gt;. In addition, there are lesser known advantages of Cloud Build, particularly for enterprise customers: Cloud Build&amp;#8217;s concurrency capabilities and VPC-SC support, Cloud Storage versioning, security and compliance. Let&amp;#8217;s explore these benefits in more detail.&lt;/p&gt;&lt;p&gt;Cloud Build&amp;#8217;s ability to scale makes it capable to process multiple Terraform deployments across the regions globally and simultaneously. By default, Cloud Build supports 30 concurrent builds, with additional builds queued and processed after the running builds complete. In some cases it may not be enough. Customers who initiate parallel deployments to multiple zones, or, those who provision infrastructure on behalf of multiple tenants, often require running more concurrent deployments to complete all of them within the allotted deployment window. Cloud Build private pool feature allows up to &lt;a href=&#34;https://cloud.google.com/build/quotas&#34;&gt;100 concurrent builds&lt;/a&gt; which may be further adjusted upon request. This is an example of creating a private pool and then using it when submitting a build:&lt;/p&gt;"><p><a href="https://www.terraform.io/" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://www.terraform.io" track-metadata-module="post">Terraform</a> is an open source Infrastructure as Code tool that is popular with platform developers building reusable cloud automation. The <a href="https://registry.terraform.io/providers/hashicorp/google/latest/docs" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://registry.terraform.io" track-metadata-module="post">Terraform Provider for Google Cloud Platform</a> continues to add support for the latest Google Cloud features, such as <a href="https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster" track-metadata-module="post">Anthos on GKE</a>, and our teams continue to expand Terraform integrations including <a href="https://cloud.google.com/foundation-toolkit" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/foundation-toolkit" track-metadata-module="post">Cloud Foundation Toolkit</a> and <a href="https://github.com/GoogleCloudPlatform/terraform-validator#terraform-validator" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://github.com" track-metadata-module="post">Terraform Validator</a>.</p><p>How do teams use Terraform on Google Cloud? While the simplest approach is to run <code>terraform init</code>, <code>plan</code> and <code>apply</code> directly from your terminal,  it cannot be recommended for automating your production deployments. First, there is a decision on how to store your Terraform <a href="https://www.terraform.io/docs/language/state/index.html" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://www.terraform.io" track-metadata-module="post">state</a> in a way that is secure, compliant and enables team collaboration. Secondly there’s a question of scale and reliability. Over the course of even the simplest cloud deployment, Terraform can end up making thousands of Create/Read/Update/Delete API calls to the endpoints used by the Terraform providers, some of which will inevitably hit quota issues or need to be retried for other reasons. For platform administrators, who are looking to ensure the best deployment practices for their curated Terraform solutions,  while benefiting from the simplicity of Google Cloud Console, there’s <a href="https://cloud.google.com/blog/products/management-tools/private-catalog-uses-terraform-to-update-available-solutions" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/private-catalog-uses-terraform-to-update-available-solutions" track-metadata-module="post">Terraform Private Catalog integration</a> that we enabled earlier this year.</p><p>Outside of Private Catalog, <a href="https://cloud.google.com/build" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/build" track-metadata-module="post">Cloud Build</a> and <a href="https://cloud.google.com/storage" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/storage" track-metadata-module="post">Cloud Storage</a> have been the recommended approach to use Terraform on Google Cloud. Using a remote<a href="https://www.terraform.io/docs/language/settings/backends/index.html" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://www.terraform.io" track-metadata-module="post"> backend</a> prevents race conditions and simplifies sharing reusable modules between different configurations. With Cloud Build you can configure a GitOps CI/CD pipeline to automatically <code>plan</code> and <code>apply</code> your Terraform configuration when changes are pushed into the repo. </p><p>These are widely popularized benefits explored in <a href="https://cloud.google.com/architecture/managing-infrastructure-as-code" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/architecture/managing-infrastructure-as-code" track-metadata-module="post">Managing infrastructure as code with Terraform, Cloud Build, and GitOps</a>. In addition, there are lesser known advantages of Cloud Build, particularly for enterprise customers: Cloud Build’s concurrency capabilities and VPC-SC support, Cloud Storage versioning, security and compliance. Let’s explore these benefits in more detail.</p><p>Cloud Build’s ability to scale makes it capable to process multiple Terraform deployments across the regions globally and simultaneously. By default, Cloud Build supports 30 concurrent builds, with additional builds queued and processed after the running builds complete. In some cases it may not be enough. Customers who initiate parallel deployments to multiple zones, or, those who provision infrastructure on behalf of multiple tenants, often require running more concurrent deployments to complete all of them within the allotted deployment window. Cloud Build private pool feature allows up to <a href="https://cloud.google.com/build/quotas" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/build/quotas" track-metadata-module="post">100 concurrent builds</a> which may be further adjusted upon request. This is an example of creating a private pool and then using it when submitting a build:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Alex Bulankou&lt;/name&gt;&lt;title&gt;Engineering Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 06 Dec 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Want to supercharge your DevOps practice? Research says try SRE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/supercharge-your-devops-practice-with-sre-principles/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Reliability matters. When users can’t access your application, if it’s slow to respond, or it behaves unexpectedly, they don’t get the value that you intend to provide. That’s why at Google we like to say that &lt;a href=&#34;https://sre.google/workbook/reaching-beyond/&#34; target=&#34;_blank&#34;&gt;&lt;i&gt;reliability is the most important feature of any system&lt;/i&gt;&lt;/a&gt;. Its impact can be seen all the way to the bottom line, as downtime comes with steep costs—to revenue, to reputation, and to user loyalty. &lt;/p&gt;&lt;p&gt;From the beginning of the &lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DevOps Research and Assessment&lt;/a&gt; (DORA) project, we’ve recognized the importance of delivering a consistent experience to users. We measure this with the &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance&#34;&gt;Four Key&lt;/a&gt; metrics—two metrics that track the velocity of deploying new releases, balanced against two that capture the initial stability of those releases. A team that rates well on all four metrics is not only good at shipping code, they’re shipping code that’s good. &lt;/p&gt;&lt;p&gt;However, these four signals, which focus on the path to a deployment and its immediate effects, are less diagnostic of subsequent success throughout the lifespan of a release. In 2018, DORA began to study the ongoing stability of software delivered as a service (as typified by web applications), which we captured in an additional metric for availability, to explore the impact of technical operations on organizational performance. This year, we expanded our inquiry into this area, starting by renaming availability to reliability. Reliability (sometimes abbreviated as r9y) is a more general term that encompasses dimensions including response latency and content validity, as well as availability.&lt;/p&gt;&lt;p&gt;In the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops/&#34;&gt;2021 State of DevOps Report’s&lt;/a&gt; cluster analysis, teams were segmented into four groups based on the Four Key metrics of software delivery. At first glance, we found that the application of reliability practices is not directly correlated to software delivery performance —  teams that score well on delivery metrics may not be the same as those who consistently practice modern operations. However, in combination, software delivery performance and reliability engineering exert a powerful influence on organizational outcomes: elite software delivery teams that also meet their reliability goals are 1.8 times more likely to report better business outcomes.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;SRE.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/SRE.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;How Google achieves reliability: SRE&lt;/h3&gt;&lt;p&gt;In Google’s early days, we took a traditional approach to technical operations; the bulk of the work involved manual interventions in reaction to discrete problems. However, as our products began to rapidly acquire users across the globe, we realized that this approach wasn’t sustainable. It couldn’t scale to match the increasing size and complexity of our systems, and even attempting to keep up would require an untenable investment in our operations workforce. So, for the past 15+ years, we’ve been practicing and iterating on an approach called &lt;a href=&#34;http://sre.google&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering&lt;/a&gt; (SRE). &lt;/p&gt;&lt;p&gt;SRE provides a framework for measurement, prioritization, and information sharing to help teams balance between the velocity of feature releases and the predictable behavior of deployed services. It emphasizes the use of automation to reduce risk and to free up engineering capacity for strategic work. This may sound a lot like a description of DevOps; indeed, these disciplines have many shared values. That similarity meant that when, in 2016, Google published the &lt;a href=&#34;http://sre.google/books&#34; target=&#34;_blank&#34;&gt;first book on Site Reliability Engineering&lt;/a&gt;, it made waves in the DevOps community as practitioners recognized a like-minded movement. It also caused some confusion: some have framed DevOps and SRE as being in conflict or competition with each other.&lt;/p&gt;&lt;p&gt;Our view is that, having arisen from similar challenges and espousing similar objectives, DevOps and SRE can be mutually compatible. We posited that, metaphorically, “&lt;a href=&#34;https://youtu.be/uTEL8Ff1Zvk&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;class SRE implements DevOps&lt;/code&gt;&lt;/a&gt;&#39;&#39;—SRE provides a way to realize DevOps objectives. Inspired by these communities’ continued growth and ongoing exchange of ideas, we sought to investigate their relationship further. This year, we expanded the scope of data collection to assess the extent of SRE adoption across the industry, and to learn how such modern operational practices interact with DORA’s model of software delivery performance.&lt;/p&gt;&lt;p&gt;Starting from the &lt;a href=&#34;http://sre.google/books&#34; target=&#34;_blank&#34;&gt;published literature on SRE&lt;/a&gt;, we added the key elements of the framework as items in our survey of practitioners. We took care to avoid as much as possible any jargon, instead preferring plain language to describe how modern operations teams go about their work. Respondents reported on such practices as: defining reliability in terms of user-visible behavior; the use of automation to allow engineers to focus on strategic work; and having well-defined, well-practiced protocols for incident response. &lt;/p&gt;&lt;p&gt;Along the way, we found that using SRE to implement DevOps is much more widely practiced than we thought. SRE, and related disciplines like Facebook’s Production Engineering, have a reputation for being niche disciplines, practiced only by a handful of tech giants. To the contrary, we found that SRE is used in some capacity by a majority of the teams in the DORA survey, with 52% of respondents reporting the use of one or more SRE practices.&lt;/p&gt;&lt;h3&gt;SRE is a force multiplier for software delivery excellence&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph_with_image&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;figure class=&#34;article-image--wrap-medium &#34;&gt;&lt;img alt=&#34;SRE 1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/SRE_1.1203064715921295.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Analyzing the results, we found compelling evidence that SRE is an effective approach to modern operations across the spectrum of organizations. In addition to driving better business outcomes, SRE helps focus efforts—teams that achieve their reliability goals report that they are able to spend more time coding, as they’re less consumed by reacting to incidents. These findings are consistent with the observation that &lt;a href=&#34;https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper&#34;&gt;having reliable services can directly impact revenue&lt;/a&gt;, as well as offering engineers greater flexibility to use their time to improve their systems, rather than simply repairing them.&lt;/p&gt;&lt;p&gt;But while SRE is widely used and has demonstrable benefits, few respondents indicated that their teams have fully implemented every SRE technique we examined. Increased application of SRE has benefits at all levels: within every cluster of software delivery performance, teams that also meet their reliability goals outperform other members of their cluster in regard to business outcomes. &lt;/p&gt;&lt;h3&gt;On the SRE road to DevOps excellence&lt;/h3&gt;&lt;p&gt;SRE is more than a toolset; it’s also a cultural mindset about the role of operations staff. SRE is a learning discipline, aimed at understanding information and continuously iterating in response. Accordingly, adopting SRE takes time, and success requires starting small, and applying an iterative approach to SRE itself.&lt;/p&gt;&lt;p&gt;Here are some ways to get started:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Find free books and articles at &lt;a href=&#34;http://sre.google&#34;&gt;sre.google&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Join a conversation with fellow practitioners, at all different stages of SRE implementation, at &lt;a href=&#34;http://bit.ly/reliability-discuss&#34;&gt;bit.ly/reliability-discuss&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Speak to your GCP account manager about our professional service offerings &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;Apply to the &lt;a href=&#34;https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true&#34;&gt;DevOps awards&lt;/a&gt; to show how your organization is implementing award winning SRE practices along with the DORA principles!&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Dave Stanke&lt;/name&gt;&lt;title&gt;Developer Relations Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/DevOps_BlogHeader_C_Rnd3_n7MW7mI.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 29 Nov 2021 18:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Empowering DevOps to foster customer loyalty in modern retail with MongoDB Atlas on Google Cloud</title>
      <link>https://cloud.google.com/blog/topics/retail/unlocking-the-power-of-modern-devops/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Consumer demands are becoming more complex, driven by high expectations for personalized experiences that strike the right chord at the perfect time. One study from McKinsey found that&lt;a href=&#34;https://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/the-value-of-getting-personalization-right-or-wrong-is-multiplying&#34; target=&#34;_blank&#34;&gt;nearly three-quarters of consumers demand personalization&lt;/a&gt; when interacting with retailers.&lt;/p&gt;&lt;p&gt;Retailers old and new of any size must embrace the challenges head on and learn to capture customer loyalty. While each business has a unique journey toward modernizing its business, all of them share something in common: Effective approaches to DevOps and data analytics underpin their success.&lt;/p&gt;&lt;p&gt;Retailers sometimes struggle to change previous retail models into the much more intimate, personalized, and real-time retail experiences that consumers now want, whether shopping in-store or online. At the same time, retailers and many newcomers are jumping all in, and devising exceptional experiences that transform shopper experiences and elevate expectations even further.&lt;/p&gt;&lt;p&gt;MongoDB and Google Cloud have been helping retailers of all sizes better address quickly changing market opportunities. As retailers continue to need more powerful systems of engagement and data analytics, the combination of MongoDB Atlas and Google Cloud solutions offer retailers such as 1-800-FLOWERS.COM, Inc. a solid mix of proven IT infrastructure and expertise.&lt;/p&gt;&lt;h3&gt;Maximizing data value for developers&lt;/h3&gt;&lt;p&gt;DevOps is increasingly tasked with creating experiences that will bring customers to a retail website, and guide them through the purchasing process. Along the way, they need to build in steps that keep customers fully engaged in the buying process and discourage things like cart abandonment. A successful build depends a lot on how much quality data is available about customer shopping experiences and how easy it is for DevOps teams to derive insights from that information.&lt;/p&gt;&lt;p&gt;Google Cloud is very much a developer’s cloud, and we at MongoDB are very much a developer’s database. We like the breadth of Google Cloud services, which pair well with our products. Our collaboration with Google Cloud feels very natural both in terms of the technology we develop and how we are approaching serving our clients’ needs. Together, we give DevOps teams at retailers a modern toolkit to maximize the value of their work.&lt;/p&gt;&lt;p&gt;The cloud-based environment supported by Google Cloud and MongoDB Atlas increases the speed and success of experimentation and ultimately delivers solutions with the greatest impact. With agile environments like ours, teams experiment much faster, leading to more innovative shopping experiences that differentiate a retailer from their competitors.&lt;/p&gt;&lt;p&gt;Any cloud solution has to be usable for retailers of all sizes so that they can develop services according to their unique needs, expertise, and visions. The goal should be to empower a retailer’s DevOps team to be as self-sufficient as possible, and not have to rely on a third-party every time changes need to be made.&lt;/p&gt;&lt;p&gt;In an industry facing extremely tight margins—and where a two percent efficiency gain or 2x acceleration of time to market can make or break the success of a project—gaining any edge is essential for retailers. Google Cloud and MongoDB provide that edge to retailers, as well as to other companies across industries.&lt;/p&gt;&lt;h3&gt;Cultivating a vision at 1-800-FLOWERS.COM, Inc.&lt;/h3&gt;&lt;p&gt;1-800-FLOWERS.COM, Inc. is an exceptional example of what can be achieved when going all in with modern data and DevOps solutions. Chief Technology Officer Abi Sachdeva, has pursued emerging technologies to support its business teams with the latest technologies to drive value for its customers.&lt;/p&gt;&lt;p&gt;Abi has been laser-focused on delivering new personalized experiences by continually innovating customer-facing services. Driven by a commitment to foster engagement across its industry-leading brands through a centralized customer experience, 1-800-FLOWERS.COM, Inc. built an e-commerce platform that is inclusive of both products and resources aimed at improving how people express themselves.&lt;/p&gt;&lt;p&gt;To best manage all the eCommerce environments associated and ensure outstanding customer service, 1-800-FLOWERS.COM, Inc. with MongoDB and Google Cloud to revolutionize its DevOps.&lt;/p&gt;&lt;p&gt;“With the help of MongoDB and Google Cloud, we transformed people, processes, and our technology. It has been a very stable experience requiring little administrative work,” says Abi. “Traditional technologies weighed us down in the past. MongoDB and Google Cloud deliver data models and DevOps solutions that accelerate our development and deployment.”&lt;/p&gt;&lt;p&gt;MongoDB Atlas , Inc. with aggregation pipelines and a distributed system design that help it to scale quickly, while Google Cloud made its new approach to agile and DevOps a reality.&lt;/p&gt;&lt;p&gt;he speed and agility that come with cloud services, companies like 1-800-FLOWERS.COM, Inc.  keep up with the constantly changing customer preferences. With proven cloud solutions that at once increase overall IT effectiveness and decrease the burdens on IT teams, 1-800-FLOWERS.COM, Inc.  is better positioned to constantly experiment, innovate, and deliver experiences that delight customers.&lt;/p&gt;&lt;p&gt;“The fully managed MongoDB Atlas database on Google Cloud has unlocked tremendous potential in our IT architecture,” says Abi. “From agility in scaling and improved resource management to seamless global clusters and premium monitoring, MongoDB and Google Cloud reduce complexity and allow our teams to stay lean and focused on innovation rather than infrastructure.”&lt;/p&gt;&lt;h3&gt;Looking toward the holidays and beyond&lt;/h3&gt;&lt;p&gt;The very same systems that encourage experimentation and innovation can position retailers and other companies to excel in during and long after the holiday season. Companies need elasticity, scalability, and agility to facilitate experimentation and to navigate the turbulent external factors across their marketplace.&lt;/p&gt;&lt;p&gt;Every holiday season is challenging for retailers, but current supply chain concerns combined with massive changes to how people shop as a result of the COVID-19 pandemic will make 2021 a particularly important year for the industry. I believe that companies that have increased their backend elasticity and improved their DevOps culture will fare especially well amid the market upheaval.&lt;/p&gt;&lt;p&gt;As organizations modernize IT, it will be increasingly important to pair the possibilities of software and infrastructure to enable smaller DevOps teams to act independently and quickly. This improves culture across the business as people are more empowered and supported.&lt;/p&gt;&lt;p&gt;In addition, I encourage DevOps professionals to place more importance on understanding customers, business values, and to be people-first in their approaches to work. By combining this level of business experience with great coding skills, smaller teams can bolster a retailer’s performance this holiday season and beyond.&lt;/p&gt;&lt;p&gt;We are proud to work with Google Cloud to develop and deliver new ways for DevOps in retail and other industries to experiment, innovate, and deploy groundbreaking experiences that transform how people achieve their goals.&lt;/p&gt;&lt;p&gt;To learn more about the future of retail innovation,&lt;a href=&#34;https://www.youtube.com/watch?v=waNVXeHtzOs&#34; target=&#34;_blank&#34;&gt;watch this video&lt;/a&gt; featuring members of the 1-800-FLOWERS.COM, Inc., MongoDB, and Google Cloud teams.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/topics/retail/how-one-retailer-migrated-its-ecommerce-platform-to-google-cloud/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/google_flowers.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Delivering smiles and sparking innovation at 1-800-FLOWERS.COM, Inc.&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;See how gift retailer 1-800-FLOWERS.COM, Inc., migrated its customer touchpoints to cloud, including GKE and BigQuery, to build a microse...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c60=""><div _ngcontent-c60="" innerhtml="&lt;p&gt;Consumer demands are becoming more complex, driven by high expectations for personalized experiences that strike the right chord at the perfect time. One study from McKinsey found that&lt;a href=&#34;https://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/the-value-of-getting-personalization-right-or-wrong-is-multiplying&#34; target=&#34;_blank&#34;&gt; nearly three-quarters of consumers demand personalization&lt;/a&gt; when interacting with retailers.&lt;/p&gt;&lt;p&gt;Retailers old and new of any size must embrace the challenges head on and learn to capture customer loyalty. While each business has a unique journey toward modernizing its business, all of them share something in common: Effective approaches to DevOps and data analytics underpin their success.&lt;/p&gt;&lt;p&gt;Retailers sometimes struggle to change previous retail models into the much more intimate, personalized, and real-time retail experiences that consumers now want, whether shopping in-store or online. At the same time, retailers and many newcomers are jumping all in, and devising exceptional experiences that transform shopper experiences and elevate expectations even further.&lt;/p&gt;&lt;p&gt;MongoDB and Google Cloud have been helping retailers of all sizes better address quickly changing market opportunities. As retailers continue to need more powerful systems of engagement and data analytics, the combination of MongoDB Atlas and Google Cloud solutions offer retailers such as 1-800-FLOWERS.COM, Inc. a solid mix of proven IT infrastructure and expertise.&lt;/p&gt;&lt;h3&gt;Maximizing data value for developers&lt;/h3&gt;&lt;p&gt;DevOps is increasingly tasked with creating experiences that will bring customers to a retail website, and guide them through the purchasing process. Along the way, they need to build in steps that keep customers fully engaged in the buying process and discourage things like cart abandonment. A successful build depends a lot on how much quality data is available about customer shopping experiences and how easy it is for DevOps teams to derive insights from that information.&lt;/p&gt;&lt;p&gt;Google Cloud is very much a developer&amp;#8217;s cloud, and we at MongoDB are very much a developer&amp;#8217;s database. We like the breadth of Google Cloud services, which pair well with our products. Our collaboration with Google Cloud feels very natural both in terms of the technology we develop and how we are approaching serving our clients&amp;#8217; needs. Together, we give DevOps teams at retailers a modern toolkit to maximize the value of their work.&lt;/p&gt;&lt;p&gt;The cloud-based environment supported by Google Cloud and MongoDB Atlas increases the speed and success of experimentation and ultimately delivers solutions with the greatest impact. With agile environments like ours, teams experiment much faster, leading to more innovative shopping experiences that differentiate a retailer from their competitors.&lt;/p&gt;&lt;p&gt;Any cloud solution has to be usable for retailers of all sizes so that they can develop services according to their unique needs, expertise, and visions. The goal should be to empower a retailer&amp;#8217;s DevOps team to be as self-sufficient as possible, and not have to rely on a third-party every time changes need to be made.&lt;/p&gt;&lt;p&gt;In an industry facing extremely tight margins&amp;#8212;and where a two percent efficiency gain or 2x acceleration of time to market can make or break the success of a project&amp;#8212;gaining any edge is essential for retailers. Google Cloud and MongoDB provide that edge to retailers, as well as to other companies across industries.&lt;/p&gt;&lt;h3&gt;Cultivating a vision at 1-800-FLOWERS.COM, Inc.&lt;/h3&gt;&lt;p&gt;1-800-FLOWERS.COM, Inc. is an exceptional example of what can be achieved when going all in with modern data and DevOps solutions. Chief Technology Officer Abi Sachdeva, has pursued emerging technologies to support its business teams with the latest technologies to drive value for its customers.&lt;/p&gt;&lt;p&gt;Abi has been laser-focused on delivering new personalized experiences by continually innovating customer-facing services. Driven by a commitment to foster engagement across its industry-leading brands through a centralized customer experience, 1-800-FLOWERS.COM, Inc. built an e-commerce platform that is inclusive of both products and resources aimed at improving how people express themselves.&lt;/p&gt;&lt;p&gt;To best manage all the eCommerce environments associated and ensure outstanding customer service, 1-800-FLOWERS.COM, Inc. with MongoDB and Google Cloud to revolutionize its DevOps.&lt;/p&gt;&lt;p&gt;&amp;#8220;With the help of MongoDB and Google Cloud, we transformed people, processes, and our technology. It has been a very stable experience requiring little administrative work,&amp;#8221; says Abi. &amp;#8220;Traditional technologies weighed us down in the past. MongoDB and Google Cloud deliver data models and DevOps solutions that accelerate our development and deployment.&amp;#8221;&lt;/p&gt;&lt;p&gt;MongoDB Atlas , Inc. with aggregation pipelines and a distributed system design that help it to scale quickly, while Google Cloud made its new approach to agile and DevOps a reality.&lt;/p&gt;&lt;p&gt;he speed and agility that come with cloud services, companies like 1-800-FLOWERS.COM, Inc.&amp;#160; keep up with the constantly changing customer preferences. With proven cloud solutions that at once increase overall IT effectiveness and decrease the burdens on IT teams, 1-800-FLOWERS.COM, Inc.&amp;#160; is better positioned to constantly experiment, innovate, and deliver experiences that delight customers.&lt;/p&gt;&lt;p&gt;&amp;#8220;The fully managed MongoDB Atlas database on Google Cloud has unlocked tremendous potential in our IT architecture,&amp;#8221; says Abi. &amp;#8220;From agility in scaling and improved resource management to seamless global clusters and premium monitoring, MongoDB and Google Cloud reduce complexity and allow our teams to stay lean and focused on innovation rather than infrastructure.&amp;#8221;&lt;/p&gt;&lt;h3&gt;Looking toward the holidays and beyond&lt;/h3&gt;&lt;p&gt;The very same systems that encourage experimentation and innovation can position retailers and other companies to excel in during and long after the holiday season. Companies need elasticity, scalability, and agility to facilitate experimentation and to navigate the turbulent external factors across their marketplace.&lt;/p&gt;&lt;p&gt;Every holiday season is challenging for retailers, but current supply chain concerns combined with massive changes to how people shop as a result of the COVID-19 pandemic will make 2021 a particularly important year for the industry. I believe that companies that have increased their backend elasticity and improved their DevOps culture will fare especially well amid the market upheaval.&lt;/p&gt;&lt;p&gt;As organizations modernize IT, it will be increasingly important to pair the possibilities of software and infrastructure to enable smaller DevOps teams to act independently and quickly. This improves culture across the business as people are more empowered and supported.&lt;/p&gt;&lt;p&gt;In addition, I encourage DevOps professionals to place more importance on understanding customers, business values, and to be people-first in their approaches to work. By combining this level of business experience with great coding skills, smaller teams can bolster a retailer&amp;#8217;s performance this holiday season and beyond.&lt;/p&gt;&lt;p&gt;We are proud to work with Google Cloud to develop and deliver new ways for DevOps in retail and other industries to experiment, innovate, and deploy groundbreaking experiences that transform how people achieve their goals.&lt;/p&gt;&lt;p&gt;To learn more about the future of retail innovation,&lt;a href=&#34;https://www.youtube.com/watch?v=waNVXeHtzOs&#34; target=&#34;_blank&#34;&gt; watch this video&lt;/a&gt; featuring members of the 1-800-FLOWERS.COM, Inc., MongoDB, and Google Cloud teams.&lt;/p&gt;"><p>Consumer demands are becoming more complex, driven by high expectations for personalized experiences that strike the right chord at the perfect time. One study from McKinsey found that<a href="https://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/the-value-of-getting-personalization-right-or-wrong-is-multiplying" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://www.mckinsey.com" track-metadata-module="post"> nearly three-quarters of consumers demand personalization</a> when interacting with retailers.</p><p>Retailers old and new of any size must embrace the challenges head on and learn to capture customer loyalty. While each business has a unique journey toward modernizing its business, all of them share something in common: Effective approaches to DevOps and data analytics underpin their success.</p><p>Retailers sometimes struggle to change previous retail models into the much more intimate, personalized, and real-time retail experiences that consumers now want, whether shopping in-store or online. At the same time, retailers and many newcomers are jumping all in, and devising exceptional experiences that transform shopper experiences and elevate expectations even further.</p><p>MongoDB and Google Cloud have been helping retailers of all sizes better address quickly changing market opportunities. As retailers continue to need more powerful systems of engagement and data analytics, the combination of MongoDB Atlas and Google Cloud solutions offer retailers such as 1-800-FLOWERS.COM, Inc. a solid mix of proven IT infrastructure and expertise.</p><h3>Maximizing data value for developers</h3><p>DevOps is increasingly tasked with creating experiences that will bring customers to a retail website, and guide them through the purchasing process. Along the way, they need to build in steps that keep customers fully engaged in the buying process and discourage things like cart abandonment. A successful build depends a lot on how much quality data is available about customer shopping experiences and how easy it is for DevOps teams to derive insights from that information.</p><p>Google Cloud is very much a developer’s cloud, and we at MongoDB are very much a developer’s database. We like the breadth of Google Cloud services, which pair well with our products. Our collaboration with Google Cloud feels very natural both in terms of the technology we develop and how we are approaching serving our clients’ needs. Together, we give DevOps teams at retailers a modern toolkit to maximize the value of their work.</p><p>The cloud-based environment supported by Google Cloud and MongoDB Atlas increases the speed and success of experimentation and ultimately delivers solutions with the greatest impact. With agile environments like ours, teams experiment much faster, leading to more innovative shopping experiences that differentiate a retailer from their competitors.</p><p>Any cloud solution has to be usable for retailers of all sizes so that they can develop services according to their unique needs, expertise, and visions. The goal should be to empower a retailer’s DevOps team to be as self-sufficient as possible, and not have to rely on a third-party every time changes need to be made.</p><p>In an industry facing extremely tight margins—and where a two percent efficiency gain or 2x acceleration of time to market can make or break the success of a project—gaining any edge is essential for retailers. Google Cloud and MongoDB provide that edge to retailers, as well as to other companies across industries.</p><h3>Cultivating a vision at 1-800-FLOWERS.COM, Inc.</h3><p>1-800-FLOWERS.COM, Inc. is an exceptional example of what can be achieved when going all in with modern data and DevOps solutions. Chief Technology Officer Abi Sachdeva, has pursued emerging technologies to support its business teams with the latest technologies to drive value for its customers.</p><p>Abi has been laser-focused on delivering new personalized experiences by continually innovating customer-facing services. Driven by a commitment to foster engagement across its industry-leading brands through a centralized customer experience, 1-800-FLOWERS.COM, Inc. built an e-commerce platform that is inclusive of both products and resources aimed at improving how people express themselves.</p><p>To best manage all the eCommerce environments associated and ensure outstanding customer service, 1-800-FLOWERS.COM, Inc. with MongoDB and Google Cloud to revolutionize its DevOps.</p><p>“With the help of MongoDB and Google Cloud, we transformed people, processes, and our technology. It has been a very stable experience requiring little administrative work,” says Abi. “Traditional technologies weighed us down in the past. MongoDB and Google Cloud deliver data models and DevOps solutions that accelerate our development and deployment.”</p><p>MongoDB Atlas , Inc. with aggregation pipelines and a distributed system design that help it to scale quickly, while Google Cloud made its new approach to agile and DevOps a reality.</p><p>he speed and agility that come with cloud services, companies like 1-800-FLOWERS.COM, Inc.  keep up with the constantly changing customer preferences. With proven cloud solutions that at once increase overall IT effectiveness and decrease the burdens on IT teams, 1-800-FLOWERS.COM, Inc.  is better positioned to constantly experiment, innovate, and deliver experiences that delight customers.</p><p>“The fully managed MongoDB Atlas database on Google Cloud has unlocked tremendous potential in our IT architecture,” says Abi. “From agility in scaling and improved resource management to seamless global clusters and premium monitoring, MongoDB and Google Cloud reduce complexity and allow our teams to stay lean and focused on innovation rather than infrastructure.”</p><h3>Looking toward the holidays and beyond</h3><p>The very same systems that encourage experimentation and innovation can position retailers and other companies to excel in during and long after the holiday season. Companies need elasticity, scalability, and agility to facilitate experimentation and to navigate the turbulent external factors across their marketplace.</p><p>Every holiday season is challenging for retailers, but current supply chain concerns combined with massive changes to how people shop as a result of the COVID-19 pandemic will make 2021 a particularly important year for the industry. I believe that companies that have increased their backend elasticity and improved their DevOps culture will fare especially well amid the market upheaval.</p><p>As organizations modernize IT, it will be increasingly important to pair the possibilities of software and infrastructure to enable smaller DevOps teams to act independently and quickly. This improves culture across the business as people are more empowered and supported.</p><p>In addition, I encourage DevOps professionals to place more importance on understanding customers, business values, and to be people-first in their approaches to work. By combining this level of business experience with great coding skills, smaller teams can bolster a retailer’s performance this holiday season and beyond.</p><p>We are proud to work with Google Cloud to develop and deliver new ways for DevOps in retail and other industries to experiment, innovate, and deploy groundbreaking experiences that transform how people achieve their goals.</p><p>To learn more about the future of retail innovation,<a href="https://www.youtube.com/watch?v=waNVXeHtzOs" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://www.youtube.com" track-metadata-module="post"> watch this video</a> featuring members of the 1-800-FLOWERS.COM, Inc., MongoDB, and Google Cloud teams.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Mark Porter&lt;/name&gt;&lt;title&gt;CTO, MongoDB&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/retail_GV8DIe0.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 23 Nov 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How Sabre is using SRE to lead a successful digital transformation</title>
      <link>https://cloud.google.com/blog/products/devops-sre/sabre-leverages-google-cloud-and-site-reliability-engineering/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;Editor’s note&lt;/b&gt;: Today we hear from Kenny Kon, an SRE Director at Sabre. Kenny shares about how they have been able to successfully adopt Google’s SRE framework by leveraging their partnership with Google Cloud. &lt;/i&gt;&lt;/p&gt;&lt;p&gt;As a leader in the travel industry, Sabre Corporation is driving innovation in the global travel industry and developing solutions that help airlines, hotels, and travel agencies transform the traveler experience and satisfy the ever-evolving needs of its customers. &lt;/p&gt;&lt;p&gt;In order to build these solutions, we joined forces with Google Cloud as our preferred cloud provider to accelerate our digital transformation. We chose Google because they understand the industry we are in as they also manage travel products such as Google Travel. Google also created &lt;a href=&#34;http://cloud.google.com/sre&#34;&gt;SRE (Site Reliability Engineering)&lt;/a&gt;, and operates with SRE principles at the Google scale, which is what intrigued us the most.&lt;/p&gt;&lt;p&gt;Initially we started with a multi-cloud model, but that didn’t help us move faster so we consolidated to just Google Cloud. To speed our transformation along, we have adopted Google SRE (Site Reliability Engineering) practices which enables us to balance reliability and speed. We have been able to make this transformation with the direct help of Google Cloud’s &lt;a href=&#34;https://cloud.google.com/consulting&#34;&gt;Professional Services Organization (PSO)&lt;/a&gt; along with Google Cloud’s tooling, like &lt;a href=&#34;https://cloud.google.com/monitoring&#34;&gt;Cloud Monitoring&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt;, and operating on &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine (GKE)&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/spanner&#34;&gt;Cloud Spanner&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;In adopting SRE at Sabre, we’d like to highlight three key takeaways from the journey: &lt;/p&gt;&lt;h3&gt;1. Find colleagues who are also passionate about shifting culture and adopting SRE&lt;/h3&gt;&lt;p&gt;Create a community within your organization who is dedicated to the SRE journey and motivated to make things happen. As we adopted SRE at Sabre I saw more and more people rallying and coming together to support the culture change. With some momentum built it was great to bring shared experiences to the team as we all spoke in the same language talking about SLOs, SLIs, and about how we measure things. &lt;/p&gt;&lt;p&gt;Some of the ways in which we built our community was by hosting monthly brown bag sessions. This is an informal gathering where teams come in and share their experiences and challenges, or teach on specific SRE topics such as SLOs or toil. We also created a &lt;a href=&#34;https://gdg.community.dev/gdg-cloud-southlake/&#34; target=&#34;_blank&#34;&gt;public Google Developer Group (GDG)&lt;/a&gt; and have hosted several Google SRE subject matter experts to speak on SRE principles and best practices. &lt;/p&gt;&lt;h3&gt;2. Get your mid level leadership stakeholders on board&lt;/h3&gt;&lt;p&gt;We know how important &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board&#34;&gt;getting leadership buy in&lt;/a&gt; is to creating a successful SRE movement within an organization. That top-level buy-in is highly important to get resources and drive transformation across the organization, but what is sometimes missed is making it a priority to get mid-level leadership on board as well. It’s difficult to enact change from the ground up starting with practitioners at the bottom, and it’s also difficult to just have leadership buy in, as once it gets down to the middle, things may fall apart. It is imperative to have mid-level leaders on board as well, as they directly affect the culture and decisions of their teams. To avoid resistance, it is also important that the mid-level leadership (product, operations and engineering managers), i.e. people managers, will understand the motivations behind change so they will be onboard. Without that understanding, it will hinder mid-level leadership’s ability to communicate changes to the practitioners level and can impact the teams&#39; goal and allocated bandwidth.&lt;/p&gt;&lt;h3&gt;3. Don’t be afraid to get help from professionals&lt;/h3&gt;&lt;p&gt;Adopting SRE at a large organization is no simple feat. Partnering with &lt;a href=&#34;https://services.google.com/fh/files/misc/pso_sre_google_cloud.pdf&#34; target=&#34;_blank&#34;&gt;Google’s SRE consulting experts&lt;/a&gt; has brought about a huge shift at Sabre. The value PSO brings is not just training, it&#39;s also listening. We’ve had experienced Googlers who understand our problems and have been at our stage in the SRE journey listen, analyze and tailor the approach specific to our team&#39;s goals. PSO helped us by shifting our engineering teams to be more customer centric, and aligning our product, operations, and development teams. But most importantly, they’ve helped to make our current teams happier, because they&#39;re not spinning their wheels, waiting around on blocked requests.&lt;/p&gt;&lt;p&gt;When we partnered with PSO we were aware of who the key stakeholders in our organization are: the mid-level leadership and people managers. We made sure to bring them into our PSO discussions and decision making sessions and as a result, helped us to get more traction and solve the gap we had, enabling the middle-level and bringing them on board.&lt;/p&gt;&lt;p&gt;Some of the actions we have taken with help from our PSO SRE partners include adding a &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started&#34;&gt;tiers of service&lt;/a&gt; approach, improving incident management through &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents&#34;&gt;wheels of misfortune (WoM)&lt;/a&gt;, defining &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos&#34;&gt;critical user journeys (CUJs)&lt;/a&gt;, and implementing &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows&#34;&gt;error budgets&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Since putting these SRE practices into place, our business is more aligned to customer experience. We now invest org resources according to the needs of our customers and with that have reduced silos across our teams. Our Ops team is much happier since they can move faster and not have to block requests. SRE has taught us a common language, a common framework. Moreover, it gives this whole discipline a culture and meaning.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Four steps to jumpstarting your SRE practice&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Once you have leadership buy-in, there are some things you can do to get the SRE ball rolling, fast.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c69=""><div _ngcontent-c69="" innerhtml="&lt;p&gt;&lt;i&gt;&lt;b&gt;Editor&amp;#8217;s note&lt;/b&gt;: Today we hear from Kenny Kon, an SRE Director at Sabre. Kenny shares about how they have been able to successfully adopt Google&amp;#8217;s SRE framework by leveraging their partnership with Google Cloud.&amp;#160;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;As a leader in the travel industry, Sabre Corporation is driving innovation in the global travel industry and developing solutions that help airlines, hotels, and travel agencies transform the traveler experience and satisfy the ever-evolving needs of its customers.&amp;#160;&lt;/p&gt;&lt;p&gt;In order to build these solutions, we joined forces with Google Cloud as our preferred cloud provider to accelerate our digital transformation. We chose Google because they understand the industry we are in as they also manage travel products such as Google Travel. Google also created &lt;a href=&#34;http://cloud.google.com/sre&#34;&gt;SRE (Site Reliability Engineering)&lt;/a&gt;, and operates with SRE principles at the Google scale, which is what intrigued us the most.&lt;/p&gt;&lt;p&gt;Initially we started with a multi-cloud model, but that didn&amp;#8217;t help us move faster so we consolidated to just Google Cloud. To speed our transformation along, we have adopted Google SRE (Site Reliability Engineering) practices which enables us to balance reliability and speed. We have been able to make this transformation with the direct help of Google Cloud&amp;#8217;s &lt;a href=&#34;https://cloud.google.com/consulting&#34;&gt;Professional Services Organization (PSO)&lt;/a&gt; along with Google Cloud&amp;#8217;s tooling, like &lt;a href=&#34;https://cloud.google.com/monitoring&#34;&gt;Cloud Monitoring&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt;, and operating on &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine (GKE)&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/spanner&#34;&gt;Cloud Spanner&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;In adopting SRE at Sabre, we&amp;#8217;d like to highlight three key takeaways from the journey:&amp;#160;&lt;/p&gt;&lt;h3&gt;1. Find colleagues who are also passionate about shifting culture and adopting SRE&lt;/h3&gt;&lt;p&gt;Create a community within your organization who is dedicated to the SRE journey and motivated to make things happen. As we adopted SRE at Sabre I saw more and more people rallying and coming together to support the culture change. With some momentum built it was great to bring shared experiences to the team as we all spoke in the same language talking about SLOs, SLIs, and about how we measure things.&amp;#160;&lt;/p&gt;&lt;p&gt;Some of the ways in which we built our community was by hosting monthly brown bag sessions. This is an informal gathering where teams come in and share their experiences and challenges, or teach on specific SRE topics such as SLOs or toil. We also created a &lt;a href=&#34;https://gdg.community.dev/gdg-cloud-southlake/&#34; target=&#34;_blank&#34;&gt;public Google Developer Group (GDG)&lt;/a&gt; and have hosted several Google SRE subject matter experts to speak on SRE principles and best practices.&amp;#160;&lt;/p&gt;&lt;h3&gt;2. Get your mid level leadership stakeholders on board&lt;/h3&gt;&lt;p&gt;We know how important &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board&#34;&gt;getting leadership buy in&lt;/a&gt; is to creating a successful SRE movement within an organization. That top-level buy-in is highly important to get resources and drive transformation across the organization, but what is sometimes missed is making it a priority to get mid-level leadership on board as well. It&amp;#8217;s difficult to enact change from the ground up starting with practitioners at the bottom, and it&amp;#8217;s also difficult to just have leadership buy in, as once it gets down to the middle, things may fall apart. It is imperative to have mid-level leaders on board as well, as they directly affect the culture and decisions of their teams. To avoid resistance, it is also important that the mid-level leadership (product, operations and engineering managers), i.e. people managers, will understand the motivations behind change so they will be onboard. Without that understanding, it will hinder mid-level leadership&amp;#8217;s ability to communicate changes to the practitioners level and can impact the teams&#39; goal and allocated bandwidth.&lt;/p&gt;&lt;h3&gt;3. Don&amp;#8217;t be afraid to get help from professionals&lt;/h3&gt;&lt;p&gt;Adopting SRE at a large organization is no simple feat. Partnering with &lt;a href=&#34;https://services.google.com/fh/files/misc/pso_sre_google_cloud.pdf&#34; target=&#34;_blank&#34;&gt;Google&amp;#8217;s SRE consulting experts&lt;/a&gt; has brought about a huge shift at Sabre. The value PSO brings is not just training, it&#39;s also listening. We&amp;#8217;ve had experienced Googlers who understand our problems and have been at our stage in the SRE journey listen, analyze and tailor the approach specific to our team&#39;s goals. PSO helped us by shifting our engineering teams to be more customer centric, and aligning our product, operations, and development teams. But most importantly, they&amp;#8217;ve helped to make our current teams happier, because they&#39;re not spinning their wheels, waiting around on blocked requests.&lt;/p&gt;&lt;p&gt;When we partnered with PSO we were aware of who the key stakeholders in our organization are: the mid-level leadership and people managers. We made sure to bring them into our PSO discussions and decision making sessions and as a result, helped us to get more traction and solve the gap we had, enabling the middle-level and bringing them on board.&lt;/p&gt;&lt;p&gt;Some of the actions we have taken with help from our PSO SRE partners include adding a &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started&#34;&gt;tiers of service&lt;/a&gt; approach, improving incident management through &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents&#34;&gt;wheels of misfortune (WoM)&lt;/a&gt;, defining &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos&#34;&gt;critical user journeys (CUJs)&lt;/a&gt;, and implementing &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows&#34;&gt;error budgets&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;Since putting these SRE practices into place, our business is more aligned to customer experience. We now invest org resources according to the needs of our customers and with that have reduced silos across our teams. Our Ops team is much happier since they can move faster and not have to block requests. SRE has taught us a common language, a common framework. Moreover, it gives this whole discipline a culture and meaning.&lt;/p&gt;"><p><i><b>Editor’s note</b>: Today we hear from Kenny Kon, an SRE Director at Sabre. Kenny shares about how they have been able to successfully adopt Google’s SRE framework by leveraging their partnership with Google Cloud. </i></p><p>As a leader in the travel industry, Sabre Corporation is driving innovation in the global travel industry and developing solutions that help airlines, hotels, and travel agencies transform the traveler experience and satisfy the ever-evolving needs of its customers. </p><p>In order to build these solutions, we joined forces with Google Cloud as our preferred cloud provider to accelerate our digital transformation. We chose Google because they understand the industry we are in as they also manage travel products such as Google Travel. Google also created <a href="http://cloud.google.com/sre" track-type="inline link" track-name="1" track-metadata-eventdetail="http://cloud.google.com/sre" track-metadata-module="post">SRE (Site Reliability Engineering)</a>, and operates with SRE principles at the Google scale, which is what intrigued us the most.</p><p>Initially we started with a multi-cloud model, but that didn’t help us move faster so we consolidated to just Google Cloud. To speed our transformation along, we have adopted Google SRE (Site Reliability Engineering) practices which enables us to balance reliability and speed. We have been able to make this transformation with the direct help of Google Cloud’s <a href="https://cloud.google.com/consulting" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/consulting" track-metadata-module="post">Professional Services Organization (PSO)</a> along with Google Cloud’s tooling, like <a href="https://cloud.google.com/monitoring" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/monitoring" track-metadata-module="post">Cloud Monitoring</a> and <a href="https://cloud.google.com/logging" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/logging" track-metadata-module="post">Cloud Logging</a>, and operating on <a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">Google Kubernetes Engine (GKE)</a>, and <a href="https://cloud.google.com/spanner" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/spanner" track-metadata-module="post">Cloud Spanner</a>. </p><p>In adopting SRE at Sabre, we’d like to highlight three key takeaways from the journey: </p><h3>1. Find colleagues who are also passionate about shifting culture and adopting SRE</h3><p>Create a community within your organization who is dedicated to the SRE journey and motivated to make things happen. As we adopted SRE at Sabre I saw more and more people rallying and coming together to support the culture change. With some momentum built it was great to bring shared experiences to the team as we all spoke in the same language talking about SLOs, SLIs, and about how we measure things. </p><p>Some of the ways in which we built our community was by hosting monthly brown bag sessions. This is an informal gathering where teams come in and share their experiences and challenges, or teach on specific SRE topics such as SLOs or toil. We also created a <a href="https://gdg.community.dev/gdg-cloud-southlake/" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://gdg.community.dev" track-metadata-module="post">public Google Developer Group (GDG)</a> and have hosted several Google SRE subject matter experts to speak on SRE principles and best practices. </p><h3>2. Get your mid level leadership stakeholders on board</h3><p>We know how important <a href="https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board" track-metadata-module="post">getting leadership buy in</a> is to creating a successful SRE movement within an organization. That top-level buy-in is highly important to get resources and drive transformation across the organization, but what is sometimes missed is making it a priority to get mid-level leadership on board as well. It’s difficult to enact change from the ground up starting with practitioners at the bottom, and it’s also difficult to just have leadership buy in, as once it gets down to the middle, things may fall apart. It is imperative to have mid-level leaders on board as well, as they directly affect the culture and decisions of their teams. To avoid resistance, it is also important that the mid-level leadership (product, operations and engineering managers), i.e. people managers, will understand the motivations behind change so they will be onboard. Without that understanding, it will hinder mid-level leadership’s ability to communicate changes to the practitioners level and can impact the teams&#39; goal and allocated bandwidth.</p><h3>3. Don’t be afraid to get help from professionals</h3><p>Adopting SRE at a large organization is no simple feat. Partnering with <a href="https://services.google.com/fh/files/misc/pso_sre_google_cloud.pdf" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://services.google.com" track-metadata-module="post">Google’s SRE consulting experts</a> has brought about a huge shift at Sabre. The value PSO brings is not just training, it&#39;s also listening. We’ve had experienced Googlers who understand our problems and have been at our stage in the SRE journey listen, analyze and tailor the approach specific to our team&#39;s goals. PSO helped us by shifting our engineering teams to be more customer centric, and aligning our product, operations, and development teams. But most importantly, they’ve helped to make our current teams happier, because they&#39;re not spinning their wheels, waiting around on blocked requests.</p><p>When we partnered with PSO we were aware of who the key stakeholders in our organization are: the mid-level leadership and people managers. We made sure to bring them into our PSO discussions and decision making sessions and as a result, helped us to get more traction and solve the gap we had, enabling the middle-level and bringing them on board.</p><p>Some of the actions we have taken with help from our PSO SRE partners include adding a <a href="https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started" track-metadata-module="post">tiers of service</a> approach, improving incident management through <a href="https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents" track-metadata-module="post">wheels of misfortune (WoM)</a>, defining <a href="https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos" track-metadata-module="post">critical user journeys (CUJs)</a>, and implementing <a href="https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows" track-metadata-module="post">error budgets</a>. </p><p>Since putting these SRE practices into place, our business is more aligned to customer experience. We now invest org resources according to the needs of our customers and with that have reduced silos across our teams. Our Ops team is much happier since they can move faster and not have to block requests. SRE has taught us a common language, a common framework. Moreover, it gives this whole discipline a culture and meaning.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Kenny Kon&lt;/name&gt;&lt;title&gt;SRE Director at Sabre&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/devops.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 22 Nov 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Foundations of a scalable website on GCP</title>
      <link>https://cloud.google.com/blog/products/networking/hosting-a-website-on-google-cloud-from-start-to-finish/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Starting a website can be hard, we get it. There are many vendors you have to work with and steps to tie together. What DNS records do I need to add? How do I enable DNSSEC? Is my website secure and safe from cyber attacks? These types of questions plague millions of website operators globally. We are excited to share that it is possible to manage all of these steps in one location using Google Cloud.&lt;/p&gt;&lt;p&gt;Google Cloud offers you the ability to manage the entire lifecycle of a website from start to finish. You no longer have to worry about managing different subscriptions and understanding the integration between vendors. Leveraging the Google Cloud offering will allow for you to have a scalable, reliable, and safe deployment. Additionally, there are extra benefits that you can take advantage of, like getting Google Managed SSL certificates for free and taking advantage of best in class DDoS protection with our Cloud Armor solution.&lt;/p&gt;&lt;h3&gt;Architecture diagram&lt;/h3&gt;&lt;p&gt;The following architecture diagram illustrates all of the components of the solution.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;architecture diagram.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/architecture_diagram.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Key components of the solution:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Cloud Domains&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud DNS&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Compute and Storage&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Global HTTPs Load Balancer&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Armor&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud CDN&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Buying a Domain on Google Cloud&lt;/h3&gt;&lt;p&gt;Purchasing and verifying a domain can be a tricky process with many steps. Cloud Domains makes this easy and straightforward to manage. Cloud Domains integrates seamlessly with Cloud DNS making the management even easier. There is full API support which allows for programmatic management if you are managing a larger portfolio. &lt;/p&gt;&lt;h3&gt;Managing DNS with Google Cloud&lt;/h3&gt;&lt;p&gt;Our Cloud DNS solution is a managed DNS infrastructure which is scalable and highly available. Easy management of private and public DNS zones makes this a one stop shop for DNS management. Public DNS records are anycasted globally using Google’s distributed network. It is easy and straightforward to enable DNSSEC which will help protect your end users from malicious actors.  &lt;/p&gt;&lt;h3&gt;Initializing Compute and setting up static object storage&lt;/h3&gt;&lt;p&gt;Running your backends on Google Cloud compute has numerous advantages. You can use a managed instance group to run your websites. Managed instance groups allow for a highly scalable and efficient deployment. When demand goes up the number of instances will scale seamlessly, and likewise if demand falls the active compute can scale down. This allows for you to only be running what you need at a given moment. You can easily create multi-zone deployments which increases reliability and performance. With full API support, automation and management is easy and fast. Using a managed instance group allows for you to automatically and safely deploy updates with a variety of customizations available.&lt;/p&gt;&lt;p&gt;For static objects you can store them in our Cloud Storage solution. This is perfect for content like images and videos which are not constantly changing. You can store large quantities of data which is available worldwide. It is easy to transfer content into Cloud Storage with multiple tools available.&lt;/p&gt;&lt;h3&gt;Setting up an external https load balancer&lt;/h3&gt;&lt;p&gt;The external https load balancer is a global proxy-based layer 7 solution that serves as the entry point for all of your traffic onto Google’s network. Our advanced load balancing solution allows for integrated traffic management and is highly customizable to fit your needs. You can leverage a Google managed SSL certificate for easy deployment and ongoing management.&lt;/p&gt;&lt;h3&gt;Securing your traffic with Cloud Armor&lt;/h3&gt;&lt;p&gt;Cloud Armor is Google’s best in class DDoS defense solution and Web Application Firewall (WAF). You can rest easier knowing that Google’s network has your back. We have a long history of mitigating some of the most complicated and largest DDoS attacks on record ( blog link). With Cloud Armor you can additionally take advantage of preconfigured WAF rules (Mod Security Rule Set 3.02), adaptive protection, and recently rate limiting. All of this ensures that your website stays online and is protected from attacks.&lt;/p&gt;&lt;h3&gt;Caching static content with Cloud CDN&lt;/h3&gt;&lt;p&gt;For content that is cacheable like images or short videos, you can use Cloud CDN to enable fast and cost efficient delivery. Google has Cloud CDN pops all over the world which will help ensure that users from the regions that matter to you have a seamless and fast experience. Cloud CDN is easy to enable and get started with. &lt;/p&gt;&lt;h3&gt;Youtube video&lt;/h3&gt;&lt;p&gt;If you would like to see a further overview of the architecture and components of this solution as well as a detailed configuration walkthrough please check out this &lt;a href=&#34;https://www.youtube.com/watch?v=I5qLiG0vIGE&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For more information on any of these solutions please check out their respective documentation hubs:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/domains/docs/overview&#34;&gt;Cloud Domains&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/dns/docs/overview/&#34;&gt;Cloud DNS&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/compute/docs/instance-groups&#34;&gt;Managed Instance Group&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/storage/docs&#34;&gt;Cloud Storage&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/load-balancing/docs/https&#34;&gt;External HTTPs Load Balancer&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/armor/docs&#34;&gt;Cloud Armor&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/cdn/docs/overview&#34;&gt;Cloud CDN&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/networking/cloud-domains-is-generally-available/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/google_domain.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Cloud Domains, now GA, makes it easy to register and manage custom domains&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Cloud Domains, now generally available, makes performing domain-related tasks in Google Cloud simple.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Arman Rye&lt;/name&gt;&lt;title&gt;Customer Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/GCP_Networking_7oH4Ie3.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 18 Nov 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Anthos for VMs and tools to simplify the developer experience</title>
      <link>https://cloud.google.com/blog/topics/hybrid-cloud/introducing-anthos-for-vms-and-other-app-modernization-tools/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;When it comes to software development using Google Cloud, we have three guiding principles. First, developing on Google Cloud needs to be open—we rely heavily on open-source technologies so that it&#39;s easier to move apps between environments, recruit skilled developers, and access the latest innovations sooner. Second, developing for Google Cloud should also be easy—we strive to offer intuitive, integrated tools that run well wherever you build your code, while minimizing your operational overhead. Finally, running on Google Cloud should be transformative—we offer services that help unleash your imagination, along with best practices and professional services to help you bring your ideas to life. &lt;/p&gt;&lt;p&gt;Today, at &lt;a href=&#34;https://cloud.withgoogle.com/next&#34; target=&#34;_blank&#34;&gt;Google Cloud Next ‘21&lt;/a&gt;, we announced a variety of new tools and capabilities to deliver on those principles. &lt;/p&gt;&lt;h3&gt;Opening Anthos to virtual machines &lt;/h3&gt;&lt;p&gt;Since announcing Anthos, our open-source-based platform for hybrid and mutlicloud deployments in 2018, we have continued to receive strong reception from customers and partners. In fact, in Q2 2021, Anthos compute under management grew more than 500% year-over-year. Anthos unifies the management of infrastructure and applications across on-premises, edge, and multiple public clouds, as well as ensuring consistent operation at scale. Based on Google Kubernetes Engine (GKE), Anthos was originally designed to run applications in containers. To help you make that transition, we automated the process to migrate and modernize existing apps using &lt;a href=&#34;https://cloud.google.com/migrate/anthos&#34;&gt;Migrate for Anthos and GKE&lt;/a&gt; from various virtual machine environments to containers. &lt;/p&gt;&lt;p&gt;While we have seen many customers make the leap to containerization, some are not quite ready to move completely off of virtual machines (VMs). They want a unified development platform where developers can build, modify, and deploy applications residing in both containers and VMs in a common, shared environment. Today, we are announcing &lt;a href=&#34;http://cloud.google.com/anthos&#34;&gt;Anthos for Virtual Machines&lt;/a&gt; in preview, allowing you to standardize on Kubernetes while continuing to run some workloads that cannot be easily containerized in virtual machines. Anthos for VMs will help platform developers standardize on an operation model, process and tooling; enable incremental modernization efforts; and support traditional workloads like Virtual Network Functions (VNFs) or stateful monolithic workloads. &lt;/p&gt;&lt;p&gt;You can take advantage of Anthos for VMs in two ways – either by attaching your vSphere VMs, or shifting your VMs as-is. For customers with active VMware environments, the Anthos control plane can now connect to your vSphere environment and attach your vSphere VMs, allowing you to apply consistent security and policies across clusters, gain visibility into the health and performance of your services, and manage traffic for both VMs and containers. Alternately, Anthos for VMs allows you to shift VMs as-is onto Anthos with &lt;a href=&#34;https://kubevirt.io/&#34; target=&#34;_blank&#34;&gt;KubeVirt&lt;/a&gt;, an open-source virtualization API for Kubernetes. Now you can build, modify, and deploy applications residing in both application containers as well as VMs on a common, shared Anthos environment. This is a great option for organizations that prefer to use open-source virtualization, as those same organizations often prefer to run &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/1.6/concepts/about-bare-metal&#34;&gt;Anthos on bare metal&lt;/a&gt;. To help get started, we provide you with a &lt;a href=&#34;https://cloud.google.com/anthos&#34;&gt;fit assessment tool&lt;/a&gt; to identify which approach to take. &lt;/p&gt;&lt;h3&gt;Taking your Anthos experience further&lt;/h3&gt;&lt;p&gt;We’re also making it easier for you to manage containerized workloads already running in other clouds through Anthos. While you can already run containers in AWS and Azure from Anthos, we’re taking this a step further with the new &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/multi-cloud&#34;&gt;Anthos Multi-Cloud API&lt;/a&gt;. Generally available in Q4 ‘21, this new API lets you provision and manage GKE clusters running on AWS or Azure infrastructure directly from the command line interface or the Google Cloud Console, all while being managed by a central control plane. This gives you a single API to manage all your container deployments regardless of which major public cloud you&#39;re using, thus minimizing the time you spend jumping between user interfaces to accomplish day-to-day management tasks like creating, managing, and updating clusters. &lt;/p&gt;&lt;p&gt;Over the past year, we’ve brought some of the innovations originally developed for hybrid and multicloud use cases in Anthos back to GKE running in Google Cloud. Specifically, &lt;a href=&#34;https://cloud.google.com/anthos/config-management&#34;&gt;Anthos Config Management&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/anthos/service-mesh&#34;&gt;Anthos Service Mesh&lt;/a&gt; are now generally available for GKE as standalone services with pay-as-you-go pricing. GKE customers can now use Anthos Config Management to take advantage of config and policy automation at a low incremental per-cluster cost, and use Anthos Service Mesh to enable next-level security and networking on container-based microservices.&lt;/p&gt;&lt;p&gt;Last but not least, we are excited to announce that starting today, Anthos Service Mesh is generally available to support a hybrid mesh. This gives you the flexibility to have a common mesh that spans both your Google Cloud and on-prem deployments. &lt;/p&gt;&lt;p&gt;Customers like &lt;a href=&#34;http://www.westerndigital.com&#34; target=&#34;_blank&#34;&gt;Western Digital&lt;/a&gt; have already experienced many benefits from adopting Anthos as their application modernization platform:&lt;/p&gt;&lt;p&gt;&lt;i&gt;&#34;As a global storage leader with sophisticated manufacturing facilities around the world, Western Digital sees cloud technology as an enabler of our key business priorities: reducing time to deliver products and services, rationalizing our entire application footprint, and meeting customer demand for IoT and edge applications,”&lt;/i&gt; said Jahidul Khandaker, senior vice president and CIO, Western Digital. &lt;i&gt;“Anthos is our unified management platform of choice—it gives us insights across our Google Cloud and on-premises environments, while keeping the doors open for a multi-cloud future. Anthos has delivered several advantages for our developers: a richer user experience, greater security, and enhanced flexibility to manage factory applications—no matter where they reside—on-prem, in the cloud or a mix of both.&#34;&lt;/i&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Easy does it&lt;/h3&gt;&lt;p&gt;In addition to being an open platform, we strive to make Google Cloud easy to use for operators as well as developers. For example, earlier this year &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot&#34;&gt;we introduced GKE Autopilot&lt;/a&gt;, a mode of operations in GKE that empowers you to simplify operations by offloading the management of infrastructure, control plane, and nodes. With GKE Autopilot, customers like &lt;a href=&#34;https://cloud.google.com/customers/ubie&#34;&gt;Ubie&lt;/a&gt;, a Japanese-based healthcare technology company, have eliminated the need to configure and maintain infrastructure, which helped their development teams focus on making healthcare more accessible.&lt;/p&gt;&lt;p&gt;With Cloud Run, our serverless compute platform, you can abstract away infrastructure management entirely. This year, our focus has been on bringing the simplicity of Cloud Run to more workloads, like traditional applications written in Java Spring Boot, ASP.NET, and Django, among others. Along with a new second generation execution environment for enhanced network and CPU performance, we’ve added &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/introducing-committed-use-discounts-for-cloud-run&#34;&gt;committed-use discounts&lt;/a&gt; and introduced &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation&#34;&gt;new CPU allocation controls and price&lt;/a&gt;, allowing you to save up to 17% and 25%, respectively, on your compute bill. New &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/introducing-new-connectors-workflows&#34;&gt;connectors&lt;/a&gt; for Workflows, &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-functions-integrates-with-google-secret-manager&#34;&gt;integration&lt;/a&gt; between Cloud Functions and Secret Manager, and support for &lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-functions-supports-min-instances&#34;&gt;min instances&lt;/a&gt; are just a few of the other ways we’ve made it easier to build modern, serverless apps. &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Easy for developers &lt;/h3&gt;Developers spend a lot of time inside their integrated development environments (IDEs), writing code. Last year we announced &lt;a href=&#34;https://cloud.google.com/shell&#34;&gt;Cloud Shell Editor&lt;/a&gt;, which makes the process of writing code as seamless as possible. It comes with your favorite developer tools (e.g., docker, minikube, skaffold, and many more) preinstalled, and this year, we added ~100 live tutorials to it—no more switching between the documentation, the terminal, and your code! &lt;p&gt;&lt;/p&gt;&lt;p&gt;Once that code is ready, you want building it and deploying it to be as seamless as possible. Today we are announcing &lt;a href=&#34;https://cloud.google.com/build/docs/hybrid/overview&#34;&gt;Cloud Build Hybrid&lt;/a&gt;, which lets you build, test, and deploy across clouds and on-prem systems, so developers get consistent CI/CD tooling across their environments, and platform engineers don&#39;t have to worry about maintaining and scaling their systems. Cloud Build is also integrated with &lt;a href=&#34;https://docs.google.com/document/d/1Z4vYdjF66UTOJgIpaDlDnXOuDCV6YulMhvNDjgoFiYo/edit?resourcekey=0-Vxxrly14dbfaBHg21HnOfg#&#34; target=&#34;_blank&#34;&gt;Artifact Registry&lt;/a&gt;, which now allows you to store not only in containers, but also language-specific artifacts in one place. Finally, with the recently launched &lt;a href=&#34;https://cloud.google.com/deploy/docs/overview&#34;&gt;Google Cloud Deploy&lt;/a&gt;, which is a managed, continuous delivery service initially for GKE, we’re making it easy to scale your pipelines across your organization.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Easy for operators&lt;/h3&gt;When your applications are up and running, you need to observe and analyze them for better operations and business insights. While we already offer a fully managed metrics and alerting service with &lt;a href=&#34;https://cloud.google.com/monitoring&#34;&gt;Cloud Monitoring&lt;/a&gt;, some Kubernetes users want to continue using open-source Prometheus without the scaling and management headaches. This is precisely why today we are announcing the preview of &lt;a href=&#34;http://cloud.google.com/monitoring&#34;&gt;Managed Service for Prometheus&lt;/a&gt;, helping you avoid vendor lock-in and delivering compatibility with your existing Prometheus alerts, workflows, and Grafana dashboards. Now you have all of the benefits of Prometheus, minus the management hassle. &lt;p&gt;&lt;/p&gt;&lt;p&gt;To give you easy diagnostics and deeper insights from across your business and systems, today we also combined &lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt; with the performance and power of BigQuery to introduce &lt;a href=&#34;https://cloud.google.com/logging/docs/log-analytics&#34;&gt;Log Analytics&lt;/a&gt;. Currently in preview, Log Analytics allows you to rapidly store, manage, and analyze log data. This enables you to effectively move your operations from a reactive to proactive model. &lt;/p&gt;&lt;h3&gt;Zero-trust simplified for application developers&lt;/h3&gt;&lt;p&gt;We also make it easy for developers to build secure applications from the get-go, whether they’re writing code, running it through the CI/CD pipeline, or in production. This zero-trust software supply chain is made possible by fully managed services that provide you with a consistent way to define and enforce policy, establish provenance, and prevent modification or tampering. &lt;/p&gt;&lt;p&gt;And we’re continuing to enhance our &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/applying-zero-trust-to-user-access-and-production-services&#34;&gt;zero-trust software supply chain capabilities&lt;/a&gt; with new features. For example, developers can now scan &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/scan-for-vulnerabilities-early-to-shift-security-left-in-cicd&#34;&gt;containers for vulnerabilities&lt;/a&gt; using the simple “gcloud artifacts docker images scan” command. Now generally available, we’re also announcing that you can pair Cloud Run with Binary Authorization and, in a few clicks, ensure that only trusted container images make it to production. In addition, Binary Authorization now integrates with Cloud Build to automatically generate digital signatures and make it easy to set up deploy-time constraints that ensure only images signed by Cloud Build are sanctioned. Learn more about how we are making security easier &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/next21-how-google-cloud-secures-the-world&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Transform your cloud with Google&lt;/h3&gt;No matter where you are along the journey to transform your applications, we are here to partner with you. Whether its with the new product functionality we described today at Next, research and best practices such as the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops/&#34;&gt;2021 Accelerate State of DevOps report&lt;/a&gt; from Cloud’s DevOps Research and Assessment (DORA) team, or professional services such as the &lt;a href=&#34;https://cloud.google.com/camp&#34;&gt;Google Cloud Application Modernization Program (CAMP)&lt;/a&gt;, we’re here to help.&lt;br/&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div _ngcontent-c20="" innerhtml="&lt;p&gt;When it comes to software development using Google Cloud, we have three guiding principles. First, developing on Google Cloud needs to be open&amp;#8212;we rely heavily on open-source technologies so that it&#39;s easier to move apps between environments, recruit skilled developers, and access the latest innovations sooner. Second, developing for Google Cloud should also be easy&amp;#8212;we strive to offer intuitive, integrated tools that run well wherever you build your code, while minimizing your operational overhead. Finally, running on Google Cloud should be transformative&amp;#8212;we offer services that help unleash your imagination, along with best practices and professional services to help you bring your ideas to life.&amp;#160;&lt;/p&gt;&lt;p&gt;Today, at &lt;a href=&#34;https://cloud.withgoogle.com/next&#34; target=&#34;_blank&#34;&gt;Google Cloud Next &amp;#8216;21&lt;/a&gt;, we announced a variety of new tools and capabilities to deliver on those principles.&amp;#160;&lt;/p&gt;&lt;h3&gt;Opening Anthos to virtual machines&amp;#160;&lt;/h3&gt;&lt;p&gt;Since announcing Anthos, our open-source-based platform for hybrid and mutlicloud deployments in 2018, we have continued to receive strong reception from customers and partners. In fact, in Q2 2021, Anthos compute under management grew more than 500% year-over-year. Anthos unifies the management of infrastructure and applications across on-premises, edge, and multiple public clouds, as well as ensuring consistent operation at scale. Based on Google Kubernetes Engine (GKE), Anthos was originally designed to run applications in containers. To help you make that transition, we automated the process to migrate and modernize existing apps using &lt;a href=&#34;https://cloud.google.com/migrate/anthos&#34;&gt;Migrate for Anthos and GKE&lt;/a&gt; from various virtual machine environments to containers.&amp;#160;&lt;/p&gt;&lt;p&gt;While we have seen many customers make the leap to containerization, some are not quite ready to move completely off of virtual machines (VMs). They want a unified development platform where developers can build, modify, and deploy applications residing in both containers and VMs in a common, shared environment. Today, we are announcing &lt;a href=&#34;http://cloud.google.com/anthos&#34;&gt;Anthos for Virtual Machines&lt;/a&gt; in preview, allowing you to standardize on Kubernetes while continuing to run some workloads that cannot be easily containerized in virtual machines. Anthos for VMs will help platform developers standardize on an operation model, process and tooling; enable incremental modernization efforts; and support traditional workloads like Virtual Network Functions (VNFs) or stateful monolithic workloads.&amp;#160;&lt;/p&gt;&lt;p&gt;You can take advantage of Anthos for VMs in two ways &amp;#8211; either by attaching your vSphere VMs, or shifting your VMs as-is. For customers with active VMware environments, the Anthos control plane can now connect to your vSphere environment and attach your vSphere VMs, allowing you to apply consistent security and policies across clusters, gain visibility into the health and performance of your services, and manage traffic for both VMs and containers. Alternately, Anthos for VMs allows you to shift VMs as-is onto Anthos with &lt;a href=&#34;https://kubevirt.io/&#34; target=&#34;_blank&#34;&gt;KubeVirt&lt;/a&gt;, an open-source virtualization API for Kubernetes. Now you can build, modify, and deploy applications residing in both application containers as well as VMs on a common, shared Anthos environment. This is a great option for organizations that prefer to use open-source virtualization, as those same organizations often prefer to run &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/1.6/concepts/about-bare-metal&#34;&gt;Anthos on bare metal&lt;/a&gt;. To help get started, we provide you with a &lt;a href=&#34;https://cloud.google.com/anthos&#34;&gt;fit assessment tool&lt;/a&gt; to identify which approach to take.&amp;#160;&lt;/p&gt;&lt;h3&gt;Taking your Anthos experience further&lt;/h3&gt;&lt;p&gt;We&amp;#8217;re also making it easier for you to manage containerized workloads already running in other clouds through Anthos. While you can already run containers in AWS and Azure from Anthos, we&amp;#8217;re taking this a step further with the new &lt;a href=&#34;https://cloud.google.com/anthos/clusters/docs/multi-cloud&#34;&gt;Anthos Multi-Cloud API&lt;/a&gt;. Generally available in Q4 &amp;#8216;21, this new API lets you provision and manage GKE clusters running on AWS or Azure infrastructure directly from the command line interface or the Google Cloud Console, all while being managed by a central control plane. This gives you a single API to manage all your container deployments regardless of which major public cloud you&#39;re using, thus minimizing the time you spend jumping between user interfaces to accomplish day-to-day management tasks like creating, managing, and updating clusters.&amp;#160;&lt;/p&gt;&lt;p&gt;Over the past year, we&amp;#8217;ve brought some of the innovations originally developed for hybrid and multicloud use cases in Anthos back to GKE running in Google Cloud. Specifically, &lt;a href=&#34;https://cloud.google.com/anthos/config-management&#34;&gt;Anthos Config Management&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/anthos/service-mesh&#34;&gt;Anthos Service Mesh&lt;/a&gt; are now generally available for GKE as standalone services with pay-as-you-go pricing. GKE customers can now use Anthos Config Management to take advantage of config and policy automation at a low incremental per-cluster cost, and use Anthos Service Mesh to enable next-level security and networking on container-based microservices.&lt;/p&gt;&lt;p&gt;Last but not least, we are excited to announce that starting today, Anthos Service Mesh is generally available to support a hybrid mesh. This gives you the flexibility to have a common mesh that spans both your Google Cloud and on-prem deployments.&amp;#160;&lt;/p&gt;&lt;p&gt;Customers like &lt;a href=&#34;http://www.westerndigital.com&#34; target=&#34;_blank&#34;&gt;Western Digital&lt;/a&gt; have already experienced many benefits from adopting Anthos as their application modernization platform:&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#34;As a global storage leader with sophisticated manufacturing facilities around the world, Western Digital sees cloud technology as an enabler of our key business priorities: reducing time to deliver products and services, rationalizing our entire application footprint, and meeting customer demand for IoT and edge applications,&amp;#8221;&lt;/i&gt; said Jahidul Khandaker, senior vice president and CIO, Western Digital. &lt;i&gt;&amp;#8220;Anthos is our unified management platform of choice&amp;#8212;it gives us insights across our Google Cloud and on-premises environments, while keeping the doors open for a multi-cloud future. Anthos has delivered several advantages for our developers: a richer user experience, greater security, and enhanced flexibility to manage factory applications&amp;#8212;no matter where they reside&amp;#8212;on-prem, in the cloud or a mix of both.&amp;#34;&lt;/i&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Easy does it&lt;/h3&gt;&lt;p&gt;In addition to being an open platform, we strive to make Google Cloud easy to use for operators as well as developers. For example, earlier this year&amp;#160;&lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot&#34;&gt;we introduced GKE Autopilot&lt;/a&gt;, a mode of operations in GKE that empowers you to simplify operations by offloading the management of infrastructure, control plane, and nodes. With GKE Autopilot, customers like&amp;#160;&lt;a href=&#34;https://cloud.google.com/customers/ubie&#34;&gt;Ubie&lt;/a&gt;, a Japanese-based healthcare technology company, have eliminated the need to configure and maintain infrastructure, which helped their development teams focus on making healthcare more accessible.&lt;/p&gt;&lt;p&gt;With Cloud Run, our serverless compute platform, you can abstract away infrastructure management entirely. This year, our focus has been on bringing the simplicity of Cloud Run to more workloads, like traditional applications written in Java Spring Boot, ASP.NET, and Django, among others. Along with a new second generation execution environment for enhanced network and CPU performance, we&amp;#8217;ve added&amp;#160;&lt;a href=&#34;https://cloud.google.com/blog/products/serverless/introducing-committed-use-discounts-for-cloud-run&#34;&gt;committed-use discounts&lt;/a&gt;&amp;#160;and introduced&amp;#160;&lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation&#34;&gt;new CPU allocation controls and price&lt;/a&gt;, allowing you to save up to 17% and 25%, respectively, on your compute bill. New&amp;#160;&lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/introducing-new-connectors-workflows&#34;&gt;connectors&lt;/a&gt;&amp;#160;for Workflows,&amp;#160;&lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-functions-integrates-with-google-secret-manager&#34;&gt;integration&lt;/a&gt;&amp;#160;between Cloud Functions and Secret Manager, and support for&amp;#160;&lt;a href=&#34;https://cloud.google.com/blog/products/serverless/cloud-functions-supports-min-instances&#34;&gt;min instances&lt;/a&gt;&amp;#160;are just a few of the other ways we&amp;#8217;ve made it easier to build modern, serverless apps.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Easy for developers&amp;#160;&lt;/h3&gt;Developers spend a lot of time inside their integrated development environments (IDEs), writing code. Last year we announced&amp;#160;&lt;a href=&#34;https://cloud.google.com/shell&#34;&gt;Cloud Shell Editor&lt;/a&gt;, which makes the process of writing code as seamless as possible. It comes with your favorite developer tools (e.g., docker, minikube, skaffold, and many more) preinstalled, and this year, we added ~100 live tutorials to it&amp;#8212;no more switching between the documentation, the terminal, and your code!&amp;#160;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Once that code is ready, you want building it and deploying it to be as seamless as possible. Today we are announcing&amp;#160;&lt;a href=&#34;https://cloud.google.com/build/docs/hybrid/overview&#34;&gt;Cloud Build Hybrid&lt;/a&gt;, which lets you build, test, and deploy across clouds and on-prem systems, so developers get consistent CI/CD tooling across their environments, and platform engineers don&#39;t have to worry about maintaining and scaling their systems. Cloud Build is also integrated with&amp;#160;&lt;a href=&#34;https://docs.google.com/document/d/1Z4vYdjF66UTOJgIpaDlDnXOuDCV6YulMhvNDjgoFiYo/edit?resourcekey=0-Vxxrly14dbfaBHg21HnOfg#&#34; target=&#34;_blank&#34;&gt;Artifact Registry&lt;/a&gt;, which now allows you to store not only in containers, but also language-specific artifacts in one place. Finally, with the recently launched&amp;#160;&lt;a href=&#34;https://cloud.google.com/deploy/docs/overview&#34;&gt;Google Cloud Deploy&lt;/a&gt;, which is a managed, continuous delivery service initially for GKE, we&amp;#8217;re making it easy to scale your pipelines across your organization.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Easy for operators&lt;/h3&gt;When your applications are up and running, you need to observe and analyze them for better operations and business insights. While we already offer a fully managed metrics and alerting service with&amp;#160;&lt;a href=&#34;https://cloud.google.com/monitoring&#34;&gt;Cloud Monitoring&lt;/a&gt;, some Kubernetes users want to continue using open-source Prometheus without the scaling and management headaches. This is precisely why today we are announcing the preview of&amp;#160;&lt;a href=&#34;http://cloud.google.com/monitoring&#34;&gt;Managed Service for Prometheus&lt;/a&gt;, helping you avoid vendor lock-in and delivering compatibility with your existing Prometheus alerts, workflows, and Grafana dashboards. Now you have all of the benefits of Prometheus, minus the management hassle.&amp;#160;&lt;p&gt;&lt;/p&gt;&lt;p&gt;To give you easy diagnostics and deeper insights from across your business and systems, today we also combined&amp;#160;&lt;a href=&#34;https://cloud.google.com/logging&#34;&gt;Cloud Logging&lt;/a&gt;&amp;#160;with the performance and power of BigQuery to introduce&amp;#160;&lt;a href=&#34;https://cloud.google.com/logging/docs/log-analytics&#34;&gt;Log Analytics&lt;/a&gt;. Currently in preview, Log Analytics allows you to rapidly store, manage, and analyze log data. This enables you to effectively move your operations from a reactive to proactive model.&amp;#160;&lt;/p&gt;&lt;h3&gt;Zero-trust simplified for application developers&lt;/h3&gt;&lt;p&gt;We also make it easy for developers to build secure applications from the get-go, whether they&amp;#8217;re writing code, running it through the CI/CD pipeline, or in production. This zero-trust software supply chain is made possible by fully managed services that provide you with a consistent way to define and enforce policy, establish provenance, and prevent modification or tampering.&amp;#160;&lt;/p&gt;&lt;p&gt;And we&amp;#8217;re continuing to enhance our&amp;#160;&lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/applying-zero-trust-to-user-access-and-production-services&#34;&gt;zero-trust software supply chain capabilities&lt;/a&gt;&amp;#160;with new features. For example, developers can now scan&amp;#160;&lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/scan-for-vulnerabilities-early-to-shift-security-left-in-cicd&#34;&gt;containers for vulnerabilities&lt;/a&gt;&amp;#160;using the simple &amp;#8220;gcloud artifacts docker images scan&amp;#8221; command. Now generally available, we&amp;#8217;re also announcing that you can pair Cloud Run with Binary Authorization and, in a few clicks, ensure that only trusted container images make it to production. In addition, Binary Authorization now integrates with Cloud Build to automatically generate digital signatures and make it easy to set up deploy-time constraints that ensure only images signed by Cloud Build are sanctioned. Learn more about how we are making security easier&amp;#160;&lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/next21-how-google-cloud-secures-the-world&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;Transform your cloud with Google&lt;/h3&gt;No matter where you are along the journey to transform your applications, we are here to partner with you. Whether its with the new product functionality we described today at Next, research and best practices such as the&amp;#160;&lt;a href=&#34;https://cloud.google.com/devops/state-of-devops/&#34;&gt;2021 Accelerate State of DevOps report&lt;/a&gt;&amp;#160;from Cloud&amp;#8217;s DevOps Research and Assessment (DORA) team, or professional services such as the&amp;#160;&lt;a href=&#34;https://cloud.google.com/camp&#34;&gt;Google Cloud Application Modernization Program (CAMP)&lt;/a&gt;, we&amp;#8217;re here to help.&lt;br&gt;&lt;p&gt;&lt;/p&gt;" _nghost-c20=""><p>When it comes to software development using Google Cloud, we have three guiding principles. First, developing on Google Cloud needs to be open—we rely heavily on open-source technologies so that it&#39;s easier to move apps between environments, recruit skilled developers, and access the latest innovations sooner. Second, developing for Google Cloud should also be easy—we strive to offer intuitive, integrated tools that run well wherever you build your code, while minimizing your operational overhead. Finally, running on Google Cloud should be transformative—we offer services that help unleash your imagination, along with best practices and professional services to help you bring your ideas to life. </p><p>Today, at <a href="https://cloud.withgoogle.com/next" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.withgoogle.com" track-metadata-module="post">Google Cloud Next ‘21</a>, we announced a variety of new tools and capabilities to deliver on those principles. </p><h3>Opening Anthos to virtual machines </h3><p>Since announcing Anthos, our open-source-based platform for hybrid and mutlicloud deployments in 2018, we have continued to receive strong reception from customers and partners. In fact, in Q2 2021, Anthos compute under management grew more than 500% year-over-year. Anthos unifies the management of infrastructure and applications across on-premises, edge, and multiple public clouds, as well as ensuring consistent operation at scale. Based on Google Kubernetes Engine (GKE), Anthos was originally designed to run applications in containers. To help you make that transition, we automated the process to migrate and modernize existing apps using <a href="https://cloud.google.com/migrate/anthos" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/migrate/anthos" track-metadata-module="post">Migrate for Anthos and GKE</a> from various virtual machine environments to containers. </p><p>While we have seen many customers make the leap to containerization, some are not quite ready to move completely off of virtual machines (VMs). They want a unified development platform where developers can build, modify, and deploy applications residing in both containers and VMs in a common, shared environment. Today, we are announcing <a href="http://cloud.google.com/anthos" track-type="inline link" track-name="3" track-metadata-eventdetail="http://cloud.google.com/anthos" track-metadata-module="post">Anthos for Virtual Machines</a> in preview, allowing you to standardize on Kubernetes while continuing to run some workloads that cannot be easily containerized in virtual machines. Anthos for VMs will help platform developers standardize on an operation model, process and tooling; enable incremental modernization efforts; and support traditional workloads like Virtual Network Functions (VNFs) or stateful monolithic workloads. </p><p>You can take advantage of Anthos for VMs in two ways – either by attaching your vSphere VMs, or shifting your VMs as-is. For customers with active VMware environments, the Anthos control plane can now connect to your vSphere environment and attach your vSphere VMs, allowing you to apply consistent security and policies across clusters, gain visibility into the health and performance of your services, and manage traffic for both VMs and containers. Alternately, Anthos for VMs allows you to shift VMs as-is onto Anthos with <a href="https://kubevirt.io/" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://kubevirt.io" track-metadata-module="post">KubeVirt</a>, an open-source virtualization API for Kubernetes. Now you can build, modify, and deploy applications residing in both application containers as well as VMs on a common, shared Anthos environment. This is a great option for organizations that prefer to use open-source virtualization, as those same organizations often prefer to run <a href="https://cloud.google.com/anthos/clusters/docs/bare-metal/1.6/concepts/about-bare-metal" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/anthos/clusters/docs/bare-metal/1.6/concepts/about-bare-metal" track-metadata-module="post">Anthos on bare metal</a>. To help get started, we provide you with a <a href="https://cloud.google.com/anthos" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/anthos" track-metadata-module="post">fit assessment tool</a> to identify which approach to take. </p><h3>Taking your Anthos experience further</h3><p>We’re also making it easier for you to manage containerized workloads already running in other clouds through Anthos. While you can already run containers in AWS and Azure from Anthos, we’re taking this a step further with the new <a href="https://cloud.google.com/anthos/clusters/docs/multi-cloud" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/anthos/clusters/docs/multi-cloud" track-metadata-module="post">Anthos Multi-Cloud API</a>. Generally available in Q4 ‘21, this new API lets you provision and manage GKE clusters running on AWS or Azure infrastructure directly from the command line interface or the Google Cloud Console, all while being managed by a central control plane. This gives you a single API to manage all your container deployments regardless of which major public cloud you&#39;re using, thus minimizing the time you spend jumping between user interfaces to accomplish day-to-day management tasks like creating, managing, and updating clusters. </p><p>Over the past year, we’ve brought some of the innovations originally developed for hybrid and multicloud use cases in Anthos back to GKE running in Google Cloud. Specifically, <a href="https://cloud.google.com/anthos/config-management" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/anthos/config-management" track-metadata-module="post">Anthos Config Management</a> and <a href="https://cloud.google.com/anthos/service-mesh" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/anthos/service-mesh" track-metadata-module="post">Anthos Service Mesh</a> are now generally available for GKE as standalone services with pay-as-you-go pricing. GKE customers can now use Anthos Config Management to take advantage of config and policy automation at a low incremental per-cluster cost, and use Anthos Service Mesh to enable next-level security and networking on container-based microservices.</p><p>Last but not least, we are excited to announce that starting today, Anthos Service Mesh is generally available to support a hybrid mesh. This gives you the flexibility to have a common mesh that spans both your Google Cloud and on-prem deployments. </p><p>Customers like <a href="http://www.westerndigital.com" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="http://www.westerndigital.com" track-metadata-module="post">Western Digital</a> have already experienced many benefits from adopting Anthos as their application modernization platform:</p><p><i>&#34;As a global storage leader with sophisticated manufacturing facilities around the world, Western Digital sees cloud technology as an enabler of our key business priorities: reducing time to deliver products and services, rationalizing our entire application footprint, and meeting customer demand for IoT and edge applications,”</i> said Jahidul Khandaker, senior vice president and CIO, Western Digital. <i>“Anthos is our unified management platform of choice—it gives us insights across our Google Cloud and on-premises environments, while keeping the doors open for a multi-cloud future. Anthos has delivered several advantages for our developers: a richer user experience, greater security, and enhanced flexibility to manage factory applications—no matter where they reside—on-prem, in the cloud or a mix of both.&#34;</i><br/></p><h3>Easy does it</h3><p>In addition to being an open platform, we strive to make Google Cloud easy to use for operators as well as developers. For example, earlier this year <a href="https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot" track-metadata-module="post">we introduced GKE Autopilot</a>, a mode of operations in GKE that empowers you to simplify operations by offloading the management of infrastructure, control plane, and nodes. With GKE Autopilot, customers like <a href="https://cloud.google.com/customers/ubie" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/customers/ubie" track-metadata-module="post">Ubie</a>, a Japanese-based healthcare technology company, have eliminated the need to configure and maintain infrastructure, which helped their development teams focus on making healthcare more accessible.</p><p>With Cloud Run, our serverless compute platform, you can abstract away infrastructure management entirely. This year, our focus has been on bringing the simplicity of Cloud Run to more workloads, like traditional applications written in Java Spring Boot, ASP.NET, and Django, among others. Along with a new second generation execution environment for enhanced network and CPU performance, we’ve added <a href="https://cloud.google.com/blog/products/serverless/introducing-committed-use-discounts-for-cloud-run" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/introducing-committed-use-discounts-for-cloud-run" track-metadata-module="post">committed-use discounts</a> and introduced <a href="https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation" track-metadata-module="post">new CPU allocation controls and price</a>, allowing you to save up to 17% and 25%, respectively, on your compute bill. New <a href="https://cloud.google.com/blog/topics/developers-practitioners/introducing-new-connectors-workflows" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/blog/topics/developers-practitioners/introducing-new-connectors-workflows" track-metadata-module="post">connectors</a> for Workflows, <a href="https://cloud.google.com/blog/products/serverless/cloud-functions-integrates-with-google-secret-manager" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/cloud-functions-integrates-with-google-secret-manager" track-metadata-module="post">integration</a> between Cloud Functions and Secret Manager, and support for <a href="https://cloud.google.com/blog/products/serverless/cloud-functions-supports-min-instances" track-type="inline link" track-name="17" track-metadata-eventdetail="https://cloud.google.com/blog/products/serverless/cloud-functions-supports-min-instances" track-metadata-module="post">min instances</a> are just a few of the other ways we’ve made it easier to build modern, serverless apps. </p><h3>Easy for developers </h3><p>Developers spend a lot of time inside their integrated development environments (IDEs), writing code. Last year we announced <a href="https://cloud.google.com/shell" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/shell" track-metadata-module="post">Cloud Shell Editor</a>, which makes the process of writing code as seamless as possible. It comes with your favorite developer tools (e.g., docker, minikube, skaffold, and many more) preinstalled, and this year, we added ~100 live tutorials to it—no more switching between the documentation, the terminal, and your code! </p><p>Once that code is ready, you want building it and deploying it to be as seamless as possible. Today we are announcing <a href="https://cloud.google.com/build/docs/hybrid/overview" track-type="inline link" track-name="19" track-metadata-eventdetail="https://cloud.google.com/build/docs/hybrid/overview" track-metadata-module="post">Cloud Build Hybrid</a>, which lets you build, test, and deploy across clouds and on-prem systems, so developers get consistent CI/CD tooling across their environments, and platform engineers don&#39;t have to worry about maintaining and scaling their systems. Cloud Build is also integrated with <a href="https://docs.google.com/document/d/1Z4vYdjF66UTOJgIpaDlDnXOuDCV6YulMhvNDjgoFiYo/edit?resourcekey=0-Vxxrly14dbfaBHg21HnOfg#" target="_blank" track-type="inline link" track-name="20" track-metadata-eventdetail="https://docs.google.com" track-metadata-module="post">Artifact Registry</a>, which now allows you to store not only in containers, but also language-specific artifacts in one place. Finally, with the recently launched <a href="https://cloud.google.com/deploy/docs/overview" track-type="inline link" track-name="21" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/overview" track-metadata-module="post">Google Cloud Deploy</a>, which is a managed, continuous delivery service initially for GKE, we’re making it easy to scale your pipelines across your organization.</p><h3>Easy for operators</h3><p>When your applications are up and running, you need to observe and analyze them for better operations and business insights. While we already offer a fully managed metrics and alerting service with <a href="https://cloud.google.com/monitoring" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/monitoring" track-metadata-module="post">Cloud Monitoring</a>, some Kubernetes users want to continue using open-source Prometheus without the scaling and management headaches. This is precisely why today we are announcing the preview of <a href="http://cloud.google.com/monitoring" track-type="inline link" track-name="23" track-metadata-eventdetail="http://cloud.google.com/monitoring" track-metadata-module="post">Managed Service for Prometheus</a>, helping you avoid vendor lock-in and delivering compatibility with your existing Prometheus alerts, workflows, and Grafana dashboards. Now you have all of the benefits of Prometheus, minus the management hassle. </p><p>To give you easy diagnostics and deeper insights from across your business and systems, today we also combined <a href="https://cloud.google.com/logging" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/logging" track-metadata-module="post">Cloud Logging</a> with the performance and power of BigQuery to introduce <a href="https://cloud.google.com/logging/docs/log-analytics" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/logging/docs/log-analytics" track-metadata-module="post">Log Analytics</a>. Currently in preview, Log Analytics allows you to rapidly store, manage, and analyze log data. This enables you to effectively move your operations from a reactive to proactive model. </p><h3>Zero-trust simplified for application developers</h3><p>We also make it easy for developers to build secure applications from the get-go, whether they’re writing code, running it through the CI/CD pipeline, or in production. This zero-trust software supply chain is made possible by fully managed services that provide you with a consistent way to define and enforce policy, establish provenance, and prevent modification or tampering. </p><p>And we’re continuing to enhance our <a href="https://cloud.google.com/blog/products/identity-security/applying-zero-trust-to-user-access-and-production-services" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/blog/products/identity-security/applying-zero-trust-to-user-access-and-production-services" track-metadata-module="post">zero-trust software supply chain capabilities</a> with new features. For example, developers can now scan <a href="https://cloud.google.com/blog/products/identity-security/scan-for-vulnerabilities-early-to-shift-security-left-in-cicd" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/blog/products/identity-security/scan-for-vulnerabilities-early-to-shift-security-left-in-cicd" track-metadata-module="post">containers for vulnerabilities</a> using the simple “gcloud artifacts docker images scan” command. Now generally available, we’re also announcing that you can pair Cloud Run with Binary Authorization and, in a few clicks, ensure that only trusted container images make it to production. In addition, Binary Authorization now integrates with Cloud Build to automatically generate digital signatures and make it easy to set up deploy-time constraints that ensure only images signed by Cloud Build are sanctioned. Learn more about how we are making security easier <a href="https://cloud.google.com/blog/products/identity-security/next21-how-google-cloud-secures-the-world" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/blog/products/identity-security/next21-how-google-cloud-secures-the-world" track-metadata-module="post">here</a>.</p><h3>Transform your cloud with Google</h3><p>No matter where you are along the journey to transform your applications, we are here to partner with you. Whether its with the new product functionality we described today at Next, research and best practices such as the <a href="https://cloud.google.com/devops/state-of-devops/" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/devops/state-of-devops/" track-metadata-module="post">2021 Accelerate State of DevOps report</a> from Cloud’s DevOps Research and Assessment (DORA) team, or professional services such as the <a href="https://cloud.google.com/camp" track-type="inline link" track-name="30" track-metadata-eventdetail="https://cloud.google.com/camp" track-metadata-module="post">Google Cloud Application Modernization Program (CAMP)</a>, we’re here to help.</p></div></div>]]></content:encoded>
      <author>&lt;name&gt;Chen Goldberg&lt;/name&gt;&lt;title&gt;VP of Engineering, Application Modernization Platform&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/CloudNext21_11.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 13 Oct 2021 12:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Artifact Registry for language packages now generally available</title>
      <link>https://cloud.google.com/blog/products/application-development/node-python-and-javarepos-are-generally-available/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Using a centralized, private repository to host your internal code as a package not only enables code reuse, but also simplifies and secures your existing software delivery pipeline. By using the same formats and tools as you would in the open-source ecosystem, you can leverage the same advantages, simplify your build, and keep your business logic and applications secure.&lt;/p&gt;&lt;h3&gt;Language repository formats, now generally available&lt;/h3&gt;&lt;p&gt;As of today, support for language repositories in &lt;a href=&#34;https://cloud.google.com/artifact-registry/&#34;&gt;Artifact Registry is now generally available&lt;/a&gt;, allowing you to store all your language-specific artifacts in one place. Supported package types include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/java&#34;&gt;Java&lt;/a&gt; packages  (using the Maven repository format)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/nodejs&#34;&gt;Node.js&lt;/a&gt; packages (using the npm repository format)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/python&#34;&gt;Python&lt;/a&gt; packages (using the PyPI repository format)&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;OS repository formats in preview&lt;/h3&gt;&lt;p&gt;Additionally, support for new repository formats for Linux distributions is in public preview, allowing developers to create private internal-only packages and securely use them across multiple applications deployed to Linux environments. New supported artifact formats include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/os-packages/debian&#34;&gt;Debian&lt;/a&gt; packages (using the Apt repository format)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/os-packages/rpm&#34;&gt;RPM&lt;/a&gt; packages (using the Yum repository format)&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This is in addition to existing container images and Helm charts (using the Docker repository format). &lt;/p&gt;&lt;p&gt;Your own secure supply chain&lt;/p&gt;&lt;p&gt;Storing your packages in Artifact Registry not only enables code reuse, but also simplifies and secures your existing build pipeline. In addition to bringing your internal packages to a managed repository, using Artifact Registry also allows you to take additional steps to improve the security of your software delivery pipeline:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Use &lt;a href=&#34;https://cloud.google.com/container-analysis/docs/container-scanning-overview&#34;&gt;Container Analysis&lt;/a&gt; to scan containers that use your private packages for vulnerabilities&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Include your repositories in a &lt;a href=&#34;https://cloud.google.com/vpc&#34;&gt;Virtual Private Cloud&lt;/a&gt; to control access&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Monitor repository usage with &lt;a href=&#34;https://cloud.google.com/logging/docs/audit&#34;&gt;Cloud Audit Logs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/binauthz-attestation&#34; target=&#34;_blank&#34;&gt;binauthz-attestation&lt;/a&gt; builder with &lt;a href=&#34;https://cloud.google.com/build&#34;&gt;Cloud Build&lt;/a&gt; to create attestations that &lt;a href=&#34;https://cloud.google.com/binary-authorization&#34;&gt;Binary Authorization&lt;/a&gt; verifies before allowing container deployment&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use  Cloud Identity and Access Management (IAM) for &lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/access-control&#34;&gt;repository access control&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Seamless authentication&lt;/h3&gt;&lt;p&gt;With credential helpers to authenticate access for installers based on &lt;a href=&#34;https://cloud.google.com/iam&#34;&gt;Cloud Identity and Access Management (IAM)&lt;/a&gt; permissions, using Artifact Registry to host your packages makes authentication to private repositories easy. By managing IAM groups, administrators can control access to repositories via the same tools used across Google Cloud.&lt;/p&gt;&lt;h3&gt;Regional repositories lower cost and enable data compliance&lt;/h3&gt;&lt;p&gt;Artifact Registry provides &lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/repo-locations&#34;&gt;regional support&lt;/a&gt;, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.&lt;/p&gt;&lt;h3&gt;Get started today&lt;/h3&gt;&lt;p&gt;These repository formats are now generally available to all Artifact Registry customers. Pricing for language repositories is the same as container pricing; see the &lt;a href=&#34;https://cloud.google.com/artifact-registry/pricing&#34;&gt;pricing documentation&lt;/a&gt; for details. To get started using language and OS repositories, try the quickstarts in the Artifact Registry documentation.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/nodejs&#34;&gt;Node.js&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/python&#34;&gt;Python&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/java&#34;&gt;Java&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/os-packages/debian/apt-quickstart&#34;&gt;Apt&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/os-packages/rpm/yum-quickstart&#34;&gt;RPM&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/application-development/artifact-registry-adds-node-python-and-java-repositories/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_7fdTm09.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Node, Python and Java repositories now available in Artifact Registry&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Expanded language support lets you store Java, Node and Python artifacts in Artifact Registry, for a more secure software supply chain.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c48=""><div _ngcontent-c48="" innerhtml="&lt;p&gt;Using a centralized, private repository to host your internal code as a package not only enables code reuse, but also simplifies and secures your existing software delivery pipeline. By using the same formats and tools as you would in the open-source ecosystem, you can leverage the same advantages, simplify your build, and keep your business logic and applications secure.&lt;/p&gt;&lt;h3&gt;Language repository formats, now generally available&lt;/h3&gt;&lt;p&gt;As of today, support for language repositories in &lt;a href=&#34;https://cloud.google.com/artifact-registry/&#34;&gt;Artifact Registry is now generally available&lt;/a&gt;, allowing you to store all your language-specific artifacts in one place. Supported package types include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/java&#34;&gt;Java&lt;/a&gt; packages&amp;#160; (using the Maven repository format)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/nodejs&#34;&gt;Node.js&lt;/a&gt; packages (using the npm repository format)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/python&#34;&gt;Python&lt;/a&gt; packages (using the PyPI repository format)&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;OS repository formats in preview&lt;/h3&gt;&lt;p&gt;Additionally, support for new repository formats for Linux distributions is in public preview, allowing developers to create private internal-only packages and securely use them across multiple applications deployed to Linux environments. New supported artifact formats include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/os-packages/debian&#34;&gt;Debian&lt;/a&gt; packages (using the Apt repository format)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/os-packages/rpm&#34;&gt;RPM&lt;/a&gt; packages (using the Yum repository format)&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This is in addition to existing container images and Helm charts (using the Docker repository format).&amp;#160;&lt;/p&gt;&lt;p&gt;Your own secure supply chain&lt;/p&gt;&lt;p&gt;Storing your packages in Artifact Registry not only enables code reuse, but also simplifies and secures your existing build pipeline. In addition to bringing your internal packages to a managed repository, using Artifact Registry also allows you to take additional steps to improve the security of your software delivery pipeline:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Use &lt;a href=&#34;https://cloud.google.com/container-analysis/docs/container-scanning-overview&#34;&gt;Container Analysis&lt;/a&gt; to scan containers that use your private packages for vulnerabilities&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Include your repositories in a &lt;a href=&#34;https://cloud.google.com/vpc&#34;&gt;Virtual Private Cloud&lt;/a&gt; to control access&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Monitor repository usage with &lt;a href=&#34;https://cloud.google.com/logging/docs/audit&#34;&gt;Cloud Audit Logs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/binauthz-attestation&#34; target=&#34;_blank&#34;&gt;binauthz-attestation&lt;/a&gt; builder with &lt;a href=&#34;https://cloud.google.com/build&#34;&gt;Cloud Build&lt;/a&gt; to create attestations that &lt;a href=&#34;https://cloud.google.com/binary-authorization&#34;&gt;Binary Authorization&lt;/a&gt; verifies before allowing container deployment&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use&amp;#160; Cloud Identity and Access Management (IAM) for &lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/access-control&#34;&gt;repository access control&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Seamless authentication&lt;/h3&gt;&lt;p&gt;With credential helpers to authenticate access for installers based on &lt;a href=&#34;https://cloud.google.com/iam&#34;&gt;Cloud Identity and Access Management (IAM)&lt;/a&gt; permissions, using Artifact Registry to host your packages makes authentication to private repositories easy. By managing IAM groups, administrators can control access to repositories via the same tools used across Google Cloud.&lt;/p&gt;&lt;h3&gt;Regional repositories lower cost and enable data compliance&lt;/h3&gt;&lt;p&gt;Artifact Registry provides &lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/repo-locations&#34;&gt;regional support&lt;/a&gt;, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.&lt;/p&gt;&lt;h3&gt;Get started today&lt;/h3&gt;&lt;p&gt;These repository formats are now generally available to all Artifact Registry customers. Pricing for language repositories is the same as container pricing; see the &lt;a href=&#34;https://cloud.google.com/artifact-registry/pricing&#34;&gt;pricing documentation&lt;/a&gt; for details. To get started using language and OS repositories, try the quickstarts in the Artifact Registry documentation.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/nodejs&#34;&gt;Node.js&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/python&#34;&gt;Python&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/java&#34;&gt;Java&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/os-packages/debian/apt-quickstart&#34;&gt;Apt&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/os-packages/rpm/yum-quickstart&#34;&gt;RPM&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;"><p>Using a centralized, private repository to host your internal code as a package not only enables code reuse, but also simplifies and secures your existing software delivery pipeline. By using the same formats and tools as you would in the open-source ecosystem, you can leverage the same advantages, simplify your build, and keep your business logic and applications secure.</p><h3>Language repository formats, now generally available</h3><p>As of today, support for language repositories in <a href="https://cloud.google.com/artifact-registry/" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/" track-metadata-module="post">Artifact Registry is now generally available</a>, allowing you to store all your language-specific artifacts in one place. Supported package types include:</p><ul><li><p><a href="https://cloud.google.com/artifact-registry/docs/java" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/java" track-metadata-module="post">Java</a> packages  (using the Maven repository format)</p></li><li><p><a href="https://cloud.google.com/artifact-registry/docs/nodejs" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/nodejs" track-metadata-module="post">Node.js</a> packages (using the npm repository format)</p></li><li><p><a href="https://cloud.google.com/artifact-registry/docs/python" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/python" track-metadata-module="post">Python</a> packages (using the PyPI repository format)</p></li></ul><h3>OS repository formats in preview</h3><p>Additionally, support for new repository formats for Linux distributions is in public preview, allowing developers to create private internal-only packages and securely use them across multiple applications deployed to Linux environments. New supported artifact formats include:</p><ul><li><p><a href="https://cloud.google.com/artifact-registry/docs/os-packages/debian" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/os-packages/debian" track-metadata-module="post">Debian</a> packages (using the Apt repository format)</p></li><li><p><a href="https://cloud.google.com/artifact-registry/docs/os-packages/rpm" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/os-packages/rpm" track-metadata-module="post">RPM</a> packages (using the Yum repository format)</p></li></ul><p>This is in addition to existing container images and Helm charts (using the Docker repository format). </p><p>Your own secure supply chain</p><p>Storing your packages in Artifact Registry not only enables code reuse, but also simplifies and secures your existing build pipeline. In addition to bringing your internal packages to a managed repository, using Artifact Registry also allows you to take additional steps to improve the security of your software delivery pipeline:</p><ul><li><p>Use <a href="https://cloud.google.com/container-analysis/docs/container-scanning-overview" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/container-analysis/docs/container-scanning-overview" track-metadata-module="post">Container Analysis</a> to scan containers that use your private packages for vulnerabilities</p></li><li><p>Include your repositories in a <a href="https://cloud.google.com/vpc" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/vpc" track-metadata-module="post">Virtual Private Cloud</a> to control access</p></li><li><p>Monitor repository usage with <a href="https://cloud.google.com/logging/docs/audit" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/logging/docs/audit" track-metadata-module="post">Cloud Audit Logs</a></p></li><li><p>Use the <a href="https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/binauthz-attestation" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://github.com" track-metadata-module="post">binauthz-attestation</a> builder with <a href="https://cloud.google.com/build" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/build" track-metadata-module="post">Cloud Build</a> to create attestations that <a href="https://cloud.google.com/binary-authorization" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/binary-authorization" track-metadata-module="post">Binary Authorization</a> verifies before allowing container deployment</p></li><li><p>Use  Cloud Identity and Access Management (IAM) for <a href="https://cloud.google.com/artifact-registry/docs/access-control" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/access-control" track-metadata-module="post">repository access control</a></p></li></ul><h3>Seamless authentication</h3><p>With credential helpers to authenticate access for installers based on <a href="https://cloud.google.com/iam" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/iam" track-metadata-module="post">Cloud Identity and Access Management (IAM)</a> permissions, using Artifact Registry to host your packages makes authentication to private repositories easy. By managing IAM groups, administrators can control access to repositories via the same tools used across Google Cloud.</p><h3>Regional repositories lower cost and enable data compliance</h3><p>Artifact Registry provides <a href="https://cloud.google.com/artifact-registry/docs/repo-locations" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/repo-locations" track-metadata-module="post">regional support</a>, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.</p><h3>Get started today</h3><p>These repository formats are now generally available to all Artifact Registry customers. Pricing for language repositories is the same as container pricing; see the <a href="https://cloud.google.com/artifact-registry/pricing" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/pricing" track-metadata-module="post">pricing documentation</a> for details. To get started using language and OS repositories, try the quickstarts in the Artifact Registry documentation.</p><ul><li><p><a href="https://cloud.google.com/artifact-registry/docs/nodejs" track-type="inline link" track-name="17" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/nodejs" track-metadata-module="post">Node.js</a> Quickstart Guide</p></li><li><p><a href="https://cloud.google.com/artifact-registry/docs/python" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/python" track-metadata-module="post">Python</a> Quickstart Guide</p></li><li><p><a href="https://cloud.google.com/artifact-registry/docs/java" track-type="inline link" track-name="19" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/java" track-metadata-module="post">Java</a> Quickstart Guide</p></li><li><p><a href="https://cloud.google.com/artifact-registry/docs/os-packages/debian/apt-quickstart" track-type="inline link" track-name="20" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/os-packages/debian/apt-quickstart" track-metadata-module="post">Apt</a> Quickstart Guide</p></li><li><p><a href="https://cloud.google.com/artifact-registry/docs/os-packages/rpm/yum-quickstart" track-type="inline link" track-name="21" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/os-packages/rpm/yum-quickstart" track-metadata-module="post">RPM</a> Quickstart Guide</p></li></ul></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Dustin Ingram&lt;/name&gt;&lt;title&gt;Senior Developer Advocate&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/appdev.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 07 Oct 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Deploy Anthos on GKE with Terraform Part 3: Enabling Cloud Resources Provisioning</title>
      <link>https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-connector-on-a-gke-cluster/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In the previous two parts of the series (&lt;a href=&#34;https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-policy-controller-on-a-gke-cluster&#34;&gt;2&lt;/a&gt;) we discussed how new features in &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs&#34; target=&#34;_blank&#34;&gt;Terraform Provider for GCP&lt;/a&gt; make it easier for platform administrators to extend their Terraform automation to add &lt;a href=&#34;https://cloud.google.com/anthos/config-management&#34;&gt;Anthos Config Management (ACM)&lt;/a&gt; features to their GKE clusters. Using familiar Terraform resource syntax, you can add &lt;code&gt;google_gke_hub_feature&lt;/code&gt; and &lt;code&gt;google_gke_hub_feature_membership&lt;/code&gt; resource with &lt;code&gt;configmanagement&lt;/code&gt; section to enable Config Sync for GitOps integration and &lt;code&gt;policy_controller&lt;/code&gt; section for policy validation.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;So far the cluster in our example was only used to host the configuration consisting of Kubernetes native resources - containerized Wordpress application powered by in-cluster MySQL database. We are getting all the advantages of Kubernetes: continuous &lt;a href=&#34;https://cloud.google.com/config-connector/docs/concepts/reconciliation&#34;&gt;reconciliation&lt;/a&gt; and drift correction, &lt;a href=&#34;https://en.wikipedia.org/wiki/Eventual_consistency&#34; target=&#34;_blank&#34;&gt;eventual consistency,&lt;/a&gt; order independence and &lt;a href=&#34;https://en.wikipedia.org/wiki/Idempotence&#34; target=&#34;_blank&#34;&gt;idempotence&lt;/a&gt;. We are also deriving benefits from GitOps approach using the repo as the source of truth, enabling reviewable and version-controlled workflow.&lt;/p&gt;&lt;p&gt;Now let’s take it even further. &lt;b&gt;We’ll demonstrate how&lt;/b&gt; &lt;a href=&#34;https://cloud.google.com/blog/topics/developers-practitioners/build-platform-krm-part-1-whats-platform&#34;&gt;&lt;b&gt;the same model&lt;/b&gt;&lt;/a&gt; &lt;b&gt;can be expanded to create and manage not just native Kubernetes resources (Kubernetes service accounts, pods, deployments) but also GCP cloud resources - Cloud databases, storage buckets, VM instances and&lt;/b&gt; &lt;a href=&#34;https://cloud.google.com/config-connector/docs/reference/overview&#34;&gt;many other GCP resources&lt;/a&gt;. Since &lt;a href=&#34;https://cloud.google.com/config-connector/docs/overview&#34;&gt;Config Connector&lt;/a&gt; was &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/config-connector-bridges-kubernetes-gcp-resources&#34;&gt;launched&lt;/a&gt; in 2020, many Kuberentes shops have embraced its convenient way of managing GCP resources. Now that we have enabled Terraform support for Anthos features, combined with a Terraform configuration &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster#config_connector_config&#34; target=&#34;_blank&#34;&gt;option&lt;/a&gt; to install Config Connector on the cluster,  the full GitOps workflow and Kubernetes lifecycle spanning native and cloud resources can be enabled during cluster creation.&lt;/p&gt;&lt;p&gt;In our &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/master/examples/acm-terraform-blog-part3/terraform/gke.tf&#34; target=&#34;_blank&#34;&gt;example&lt;/a&gt;, we are enabling Config Connector using the &lt;code&gt;config_connector&lt;/code&gt; setting in the &lt;code&gt;gke&lt;/code&gt; Terraform module. We also use the &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity&#34;&gt;&lt;code&gt;workload-identity&lt;/code&gt;&lt;/a&gt; module to create a GCP service account that will be used to make the changes to K8s resources and bind it to Kubernetes Service Account (&lt;code&gt;cnrm-controller-manager&lt;/code&gt; in &lt;code&gt;cnrm-system namespace&lt;/code&gt;). You can choose the appropriate permissions to GCP service account -  in our examples we are giving it the &lt;code&gt;owner&lt;/code&gt; role for simplicity.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Let’s review what changed in this part in the repo that is synchronized with our cluster via Config Sync. In  &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/tree/master/examples/acm-terraform-blog-part1/config-root&#34; target=&#34;_blank&#34;&gt;the first part&lt;/a&gt;, we added a collection of configs, all native Kubernetes objects. These configs provisioned an in-cluster Wordpress application with an in-cluster MySQL database. In the second part, we added a set of rules used by PolicyController to audit our cluster. In this part, we start by  adding a &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/master/examples/acm-terraform-blog-part3/config-root/configconnector.yaml&#34; target=&#34;_blank&#34;&gt;config&lt;/a&gt; representing an instance of the Config Connector addon. While the addon is enabled on the cluster, this config instance is required to activate it. It specifies the settings, such as &lt;code&gt;mode&lt;/code&gt; (cluster or namespace) and GCP service account, linking it to the cnrmsa account that we created above using the &lt;code&gt;workload-identity&lt;/code&gt; module.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;And now we can create GCP resources in addition to in-cluster native resources directly in our K8s configuration. We are configuring an &lt;a href=&#34;https://cloud.google.com/config-connector/docs/reference/resource-docs/sql/sqldatabase&#34;&gt;SQLDatabase&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/config-connector/docs/reference/resource-docs/sql/sqlinstance&#34;&gt;SQLInstance&lt;/a&gt; resources:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;as well as &lt;a href=&#34;https://cloud.google.com/config-connector/docs/reference/resource-docs/sql/sqluser&#34;&gt;SQLUser&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/config-connector/docs/reference/resource-docs/iam/iampolicy&#34;&gt;IAMPolicy&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/config-connector/docs/reference/resource-docs/iam/iampolicymember&#34;&gt;IAMPolicyMember&lt;/a&gt; (see complete example &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/master/examples/acm-terraform-blog-part3/config-root/wordpress-bundle.yaml&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;). Overall, &lt;a href=&#34;https://cloud.google.com/config-connector/docs/reference/overview&#34;&gt;more than 130 resources are now supported with Config Connector&lt;/a&gt; covering many of the most popular GCP configuration patterns.&lt;/p&gt;&lt;p&gt;You will notice that this configuration is expanded and parameterized for our specific project. How did we specify the parameter values? While many tools can be used, including &lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt; and &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt; and some of them used together, we recommend &lt;a href=&#34;https://cloud.google.com/architecture/managing-cloud-infrastructure-using-kpt&#34;&gt;Kpt&lt;/a&gt; that fully embraces the principles of &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes&#34;&gt;configuration-as-data&lt;/a&gt;. In this example we used set-project-id kpt function to specify project id on the config-root directory before submitting the change to Git.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;This &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/tree/master/examples/acm-terraform-blog-part3&#34; target=&#34;_blank&#34;&gt;repo&lt;/a&gt; provides a complete example of provisioning a cluster that is synchronized with a repo that contains a WordPress configuration powered, this time, by GCP MySQL database.&lt;/p&gt;&lt;p&gt;This was the third and the final article of the three part series that showcased Terraform support for ACM features and how it simplifies cluster provisioning for platform administrators.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Anthos.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Deploy Anthos on GKE with Terraform part 1: GitOps with Config Sync&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;It is now simple to use Terraform to configure Anthos features on your GKE clusters. This is the first part of the 3 part series that des...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Steven Linde&lt;/name&gt;&lt;title&gt;Engineering Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/Google_Cloud_Anthos_A.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 06 Oct 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Google Cloud Deploy: Managed continuous delivery to GKE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn’t have to be this way. &lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 2.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Solving for continuous delivery challenges&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. &lt;/p&gt;&lt;p&gt;&lt;i&gt;“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. &lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy GIF&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application&#34;&gt;promotion and rollback&lt;/a&gt; decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Contextualized deployment approvals&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple &lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 3.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release’s lifecycle&lt;/a&gt;.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-video&#34;&gt;&lt;div class=&#34;article-module article-video &#34;&gt;&lt;figure&gt;&lt;a class=&#34;h-c-video h-c-video--marquee&#34; data-glue-modal-disabled-on-mobile=&#34;true&#34; data-glue-modal-trigger=&#34;uni-modal-Il8FlhR9jKM-&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34;&gt;&lt;img alt=&#34;Introducing Cloud Deploy&#34; src=&#34;//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg&#34;/&gt;&lt;svg class=&#34;h-c-video__play h-c-icon h-c-icon--color-white&#34; role=&#34;img&#34;&gt;&lt;use xlink:href=&#34;#mi-youtube-icon&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;div class=&#34;h-c-modal--video&#34; data-glue-modal=&#34;uni-modal-Il8FlhR9jKM-&#34; data-glue-modal-close-label=&#34;Close Dialog&#34;&gt;&lt;a class=&#34;glue-yt-video&#34; data-glue-yt-video-autoplay=&#34;true&#34; data-glue-yt-video-height=&#34;99%&#34; data-glue-yt-video-vid=&#34;Il8FlhR9jKM&#34; data-glue-yt-video-width=&#34;100%&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34; ng-cloak=&#34;&#34;&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;S. Bogdan&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/gcp_ZPje3k8.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 22 Sep 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>What’s your org’s reliability mindset? Insights from Google SREs</title>
      <link>https://cloud.google.com/blog/products/devops-sre/the-five-phases-of-organizational-reliability/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;Editor’s note: There’s more to ensuring a product’s reliability than following a bunch of prescriptive rules. Today, we hear from some Google SREs—Vartika Agarwal, Senior Technical Program Manager, Development; Tracy Ferrell, Senior SRE Manager; Mahesh Palekar, Director SRE; and Magi Agrama, Senior Technical Program Manager, SRE—about how to evaluate your team’s current reliability mindset, and what you want it to be.&lt;/i&gt;&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;Having a reliable software product can improve users’ trust in your organization, the effectiveness of your development processes, and the quality of your products overall. More than ever, product reliability is front and center, as outages negatively impact customers and their businesses. But in an effort to develop new features, many organizations limit their reliability efforts to what happens after an outage, and tactically solve for the immediate problems that sparked it. They often fail to realize that they can move quickly while still improving their product’s reliability.&lt;/p&gt;&lt;p&gt;At Google, we’ve given a lot of thought to product reliability—and several of its aspects are well understood, for example product or system design. What people think about less is the culture and the mindset of the organization that creates a reliable product in the first place. We believe that the reliability of a product is a property of the architecture of its system, processes, culture, as well as the mindset of the product team or organization that built it. In other words, reliability should be woven into the fabric of an organization, not just the result of a strong design ethos. &lt;/p&gt;&lt;p&gt;In this blog post, we discuss the lessons we’ve learned relevant to organizational or product leads who have the ability to influence the culture of the entire product team, from (but not limited to) engineering, product management, marketing, reliability engineering, and support organizations.&lt;/p&gt;&lt;h3&gt;Goals&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Reliability should be woven into the fabric of how an organization executes. At Google, we’ve developed a terminology to categorize and describe your organization’s reliability mindset, to help you understand how intentional your organization is in this respect. Our ultimate goal is to help you improve and adopt product reliability practices that will permeate the ethos of the organization.&lt;/p&gt;&lt;p&gt;By identifying these reliability phases, we do not mean to offer a prescriptive list of things to do that will improve your product’s reliability. Nor should they be read as a set of mandated principles that everyone should apply, or be used to publicly label a team, spurring competition between teams. Rather, leaders should consider these phases as a way to help them develop their team’s culture, on the road to sustainably building reliable products.  &lt;/p&gt;&lt;h3&gt;The organizational reliability continuum&lt;/h3&gt;&lt;p&gt;Based on our observations here at Google, there are five basic stages of organizational reliability, and they are based on the classic organizational model of absent, reactive, proactive, strategic and visionary. These phases describe the mindset of an organization at a point in time, and each one of them is characterized by a series of attributes, and is appropriate for different classes of workloads.&lt;/p&gt;&lt;p&gt;Absent: Reliability is a secondary consideration for the organization. &lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;A feature launch is the key organizational metric and is the focus for incentives&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The majority of issues are found by users or testers. This organization is not aware of their long-term reliability risks. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Developer velocity is rarely exchanged for reliability.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;This reliability phase maybe appropriate for products and projects that are still under development.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Reactive&lt;/b&gt;&lt;b&gt;:&lt;/b&gt;Responses to reliability issues/risks are tied to recent outages with sporadic follow-through and rarely are there longer-term investments in fixing system issues.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Teams have some reliability metrics defined and react when required.&lt;/li&gt;&lt;li&gt;They write postmortems for outages and create action items for tactical fixes.&lt;/li&gt;&lt;li&gt;Reasonable availability is maintained through heroic efforts by a few individuals or teams &lt;/li&gt;&lt;li&gt;Developer productivity is throttled due to a temporary shift in priority on reliability work due to outages. Feature development may be frozen for a short period of time.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;This level is appropriate for products/projects in pre-launch or in a stable long-term maintenance phase.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Proactive:&lt;/b&gt;&lt;b&gt;&lt;/b&gt;Potential reliability risks are identified and addressed through regular organizational processes.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Risks are regularly reviewed and prioritized.&lt;/li&gt;&lt;li&gt;Teams proactively manage dependencies and review their reliability metrics (SLOs)&lt;/li&gt;&lt;li&gt;New designs are assessed for known risks and failure modes early on. Graceful degradation is a basic requirement.&lt;/li&gt;&lt;li&gt;The business understands the need to continuously invest in reliability and maintain its balance with developer velocity. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Most services/products should be at this level; particularly if they have a large blast radius or are critical to the business.&lt;/i&gt;&lt;/p&gt;&lt;b&gt;Strategic:&lt;/b&gt;Organizations at this level manage classes of risk via systemic changes to  architectures, products and processes.&lt;br/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Reliability is inherent and ingrained in how the organization designs, operates and develops software. Reliability is systemic.&lt;/li&gt;&lt;li&gt;Complexity is addressed holistically through product architecture. Dependencies are constantly reduced or improved.&lt;/li&gt;&lt;li&gt;The cross-functional organization can sustain reliability and developer velocity simultaneously.&lt;/li&gt;&lt;li&gt;Organizations widely celebrate quality and stability milestones.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;This level is appropriate for services and products that need very high availability to meet business-critical needs.&lt;/i&gt;&lt;/p&gt;&lt;b&gt;Visionary:&lt;/b&gt;The organization has reached the highest order of reliability and is able to drive broader reliability efforts within and outside the company (e.g., writing papers, sharing knowledge), based on their best practices and experiences. &lt;br/&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Reliability knowledge exists broadly across all engineers and teams at a fairly advanced level and is carried forward as they move across organizations.&lt;/li&gt;&lt;li&gt;Systems are self-healing.&lt;/li&gt;&lt;li&gt;Architectural improvements for reliability positively impact productivity (release velocity) due to reduction of maintenance work/toil.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Very few services or products are at this level, and when they are, are industry leading.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;&lt;h3&gt;&lt;i&gt;&lt;i&gt;&lt;i&gt;&lt;i&gt;&lt;i&gt;&lt;i&gt;&lt;i&gt;&lt;i&gt;&lt;i&gt;&lt;i&gt;Where should you be on the reliability spectrum?&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/i&gt;&lt;/h3&gt;&lt;p&gt;&lt;i&gt;&lt;i&gt;It is very important to understand your organization does not necessarily need to be at the strategic or visionary phase. There is a significant cost associated with moving from one phase to another and a cost to remain very high on this curve. In our experience, being proactive is a healthy level to target and is ideal for most products. &lt;/i&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;i&gt;To illustrate this point, here is a simple graph of where various Google product teams are on the organizational reliability spectrum; as you can see, it produces a standard bell-curve distribution. While many Google’s product teams have a reactive or proactive reliability culture, most can be described as proactive. You, as an organizational leader, must consciously decide to be at a level based on the product requirements and client expectations.&lt;/i&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Google’s Reliability culture.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Further, it’s common to have attributes across several phases, for example, an organization may be largely reactive with a few proactive attributes. Team culture will wax and wane between phases, as it takes effort to maintain a strategic reliability culture. However, as more of the organization embraces and celebrates reliability as a key feature, the cost of maintenance decreases. &lt;/p&gt;&lt;p&gt;The key to success is making an honest assessment of what phase you’re in, and then doing concerted work to move to the phase that makes sense for your product. If your organization is in the absent or reactive phase, remember that many products in nascent stages of their life cycle may be comfortable there (in both the startup and long term maintenance of a stable product).&lt;/p&gt;&lt;h3&gt;Reliability phases in action&lt;/h3&gt;&lt;p&gt;To illustrate the reliability phases in practice, it is interesting to look at examples of organizations and how they have progressed or regressed through them.  &lt;/p&gt;&lt;p&gt;It should be noted that all companies and teams are different and the progress through these phases can take varying amounts of time. It is not uncommon to take two to three years to move into a truly proactive state. In a proactive state all parts of the organization contribute to reliability without worrying that it will negatively impact feature velocity. Staying in the proactive phase also takes time and effort.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Nobody can be a hero forever&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;One infrastructure services team started small with a few well understood APIs. One key member of the team, a product architect, understood the system well and ensured that things ran smoothly by ensuring design decisions were sound and being at each major incident to rapidly mitigate the issue. This was the one person who understood the entire system and was able to predict what can and cannot impact its stability. But when they left the team, the system complexity grew by leaps and bounds. Suddenly there were many critical user-facing and internal outages. &lt;/p&gt;&lt;p&gt;Organizational leaders initiated both short and long-term reliability programs to restore stability. They focused on reducing the blast radius and the impact of global outages. Leadership recognized that to sustain this trajectory, they recognized that they had to go beyond engineering solutions and implement cultural changes such as recognizing reliability as their number-one feature. This led to broad training around reliability best practices, incorporating reliability in architectural/design reviews and recognizing and rewarding reliability beyond hero moments. &lt;/p&gt;&lt;p&gt;As a result, the organization evolved from a reactive to a strategic reliability mindset, aided by setting reliability as their number-one feature, recognizing and rewarding long-term reliability improvements, and adopting the systemic belief that reliability is everyone’s responsibility—not just that of a few heroes.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture_4.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Google’s Reliability culture 4.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture_4.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;If you think you are done, think again&lt;/h3&gt;&lt;p&gt;End users are highly dependent on the reliability of this product and it ties directly to user trust. For this reason, reliability was top of mind for one Google organization for years, and the product was held as the gold standard of reliability by other Google teams. The org was deemed visionary in its reliability processes and work. &lt;/p&gt;&lt;p&gt;However, over the years, new products were added to the base service. The high level of reliability did not come as freely and easily as it did with the simpler product. Reliability was impacted at the cost of developer velocity and the organization moved to a more reactive reliability mindset.&lt;/p&gt;&lt;p&gt;To turn the ship around, the organization’s leaders had to be intentional about their reliability posture and overall practices, for example, how much they thought about and prioritized reliability. It took several years to move the team back to a strategic mindset. &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture_3.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Google’s Reliability culture 3.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture_3.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Embrace reliability principles from the start&lt;/h3&gt;&lt;p&gt;Another team with a new user-facing product was focused on adding features and growing their user base. Before they knew it, the product took off and saw exponential growth.&lt;/p&gt;&lt;p&gt;Unfortunately, their laser-focus on managing user requirements and growing user adoption led to high technical debt and reliability issues. Since the service didn’t start off with reliability as a primary focus, it was very hard to incorporate it after the fact. &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Much of the code had to be re-written and re-architected to reach a sustainable state. The team’s leaders incentivized attention to reliability throughout the organization, from product management through to development and UX domains, constantly reminding the organization about the importance of reliability to the long-term success of the product. This mindshift took years to set in.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;Google’s Reliability culture 2.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture_2.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;It is important that cross-functional organizations be honest about their reliability journeys and determine what is appropriate for their business and product. It is not uncommon for organizations to move from one level to another and then back again as the product matures, stabilizes and then is sunset for the next generation. Getting to a strategic level can be 4+ years in the making and require very high levels of investment from all aspects of the business.  Leaders should ensure their product requires this level of continued investment.&lt;/p&gt;&lt;p&gt;We encourage you to study your culture of reliability, assess what phase you are in, determine where you should be on the continuum and carefully and thoughtfully move there. Changing culture is hard and can not be done by edicts or penalties. Most of all, remember that this is a journey and the business is ever-evolving; you cannot set reliability on the shelf and expect it to maintain itself in perpetuity.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Are we there yet? Thoughts on assessing an SRE team’s maturity&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Examining the key indicators that signal a mature SRE team.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Google Site Reliability Engineering team &lt;/name&gt;&lt;title&gt;&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 22 Sep 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Google Cloud Deploy: Managed continuous delivery to GKE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn’t have to be this way. &lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 2.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Solving for continuous delivery challenges&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. &lt;/p&gt;&lt;p&gt;&lt;i&gt;“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. &lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy GIF&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application&#34;&gt;promotion and rollback&lt;/a&gt; decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Contextualized deployment approvals&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple &lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 3.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release’s lifecycle&lt;/a&gt;.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-video&#34;&gt;&lt;div class=&#34;article-module article-video &#34;&gt;&lt;figure&gt;&lt;a class=&#34;h-c-video h-c-video--marquee&#34; data-glue-modal-disabled-on-mobile=&#34;true&#34; data-glue-modal-trigger=&#34;uni-modal-Il8FlhR9jKM-&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34;&gt;&lt;img alt=&#34;Introducing Cloud Deploy&#34; src=&#34;//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg&#34;/&gt;&lt;svg class=&#34;h-c-video__play h-c-icon h-c-icon--color-white&#34; role=&#34;img&#34;&gt;&lt;use xlink:href=&#34;#mi-youtube-icon&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;div class=&#34;h-c-modal--video&#34; data-glue-modal=&#34;uni-modal-Il8FlhR9jKM-&#34; data-glue-modal-close-label=&#34;Close Dialog&#34;&gt;&lt;a class=&#34;glue-yt-video&#34; data-glue-yt-video-autoplay=&#34;true&#34; data-glue-yt-video-height=&#34;99%&#34; data-glue-yt-video-vid=&#34;Il8FlhR9jKM&#34; data-glue-yt-video-width=&#34;100%&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34; ng-cloak=&#34;&#34;&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>#gcp</span></p></figure></article-aspect-image-block><div><div><article-author-block><div><div><p> S. Bogdan </p><p> Product Manager </p></div><p><span> September 22, 2021 </span></p></div></article-author-block></div><article-cta _nghost-c17=""><div _ngcontent-c17=""><h4 _ngcontent-c17=""><span _ngcontent-c17="">Next ’21 registration is open</span></h4><p _ngcontent-c17=""><span _ngcontent-c17="">Join us October 12–14, 2021, for our digital flagship event</span></p><p><a _ngcontent-c17="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="Next21 registration" track-metadata-eventdetail="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration" href="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration"><span _ngcontent-c17="">Register now</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery &amp;#8212;deploying container image artifacts into your various environments&amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn&amp;#8217;t have to be this way.&amp;#160;&lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;"><p>Continuous delivery is frequently top-of-mind for organizations adopting <a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">Google Kubernetes Engine</a> (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.</p><p>It doesn’t have to be this way. </p><p>Today, we are pleased to announce <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">Google Cloud Deploy</a>, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;h3&gt;Solving for continuous delivery challenges&lt;br&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current&amp;#8212;to say nothing of maintenance&amp;#8212;is resource-intensive and takes time away from the core business.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#8220;We can&amp;#8217;t afford to be innovating in continuous delivery,&amp;#8221; one customer told us. &amp;#8220;We want an opinionated product that supports best practices out of the box.&amp;#8221;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.&amp;#160;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release&amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Solving for continuous delivery challenges<br/></h3><p>Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.</p><p>Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.</p><p><b>Cost of ownership</b></p><p>Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. </p><p><i>“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”</i></p><p>Google Cloud Deploy addresses cost of ownership head-on.</p><p>As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. </p><p>Google Cloud Deploy also provides structure. <a href="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-metadata-module="post">Delivery pipelines</a> and <a href="https://cloud.google.com/deploy/docs/terminology" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology" track-metadata-module="post">targets</a> are defined <a href="https://cloud.google.com/deploy/docs/config-files" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/config-files" track-metadata-module="post">declaratively</a> and are <a href="https://cloud.google.com/deploy/docs/pipeline-instances" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/pipeline-instances" track-metadata-module="post">stored alongside each release</a>. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.<b><br/></b></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><p>We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release <a href="https://cloud.google.com/deploy/docs/deploying-application" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/deploying-application" track-metadata-module="post">promotion and rollback</a> decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.<br/></p></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn&amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple&amp;#160;&lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br&gt;&lt;/p&gt;"><p><b>Security and audit</b></p><p>Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.</p><p>Throughout, Google Cloud Deploy enables fine-grained restriction, with <a href="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-metadata-module="post">discrete resource access control</a> and <a href="https://cloud.google.com/deploy/docs/execution-environment" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/execution-environment" track-metadata-module="post">execution-level security</a>. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and <a href="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-metadata-module="post">approvals</a>.</p><p>Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. <a href="https://cloud.google.com/audit-logs" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/audit-logs" track-metadata-module="post">Cloud Audit Logs</a> <a href="https://cloud.google.com/deploy/docs/audit-logs" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/audit-logs" track-metadata-module="post">audits</a> user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.</p><p><b>Integration</b></p><p>Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.</p><p>Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/integrating" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating" track-metadata-module="post">embraces the GKE delivery tooling ecosystems</a> in three ways: connectivity to CI systems, support for leading configuration (<a href="https://cloud.google.com/deploy/docs/terminology#render" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#render" track-metadata-module="post">rendering</a>) tooling, and <a href="https://cloud.google.com/pubsub" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/pubsub" track-metadata-module="post">Pub/Sub</a> notifications to enable third-party integrations.</p><p><a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-metadata-module="post">Connecting Google Cloud Deploy</a> to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple <i>`</i><i>gcloud beta deploy releases create`.</i><br/></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy&amp;#160; leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release&amp;#8217;s lifecycle&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it&amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we&amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br&gt;&lt;/p&gt;"><p>Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages <a href="https://skaffold.dev/" target="_blank" track-type="inline link" track-name="17" track-metadata-eventdetail="https://skaffold.dev" track-metadata-module="post">Skaffold</a>, allowing you to <a href="https://cloud.google.com/deploy/docs/skaffold" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/skaffold" track-metadata-module="post">standardize your configuration</a> between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (<a href="https://helm.sh/" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://helm.sh" track-metadata-module="post">Helm</a>, <a href="https://kustomize.io/" target="_blank" track-type="inline link" track-name="20" track-metadata-eventdetail="https://kustomize.io" track-metadata-module="post">Kustomize</a>, <a href="https://kpt.dev/" target="_blank" track-type="inline link" track-name="21" track-metadata-eventdetail="https://kpt.dev" track-metadata-module="post">kpt</a>). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.</p><p>Finally, to facilitate other integrations, such as a <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-metadata-module="post">post-deployment test execution</a> or <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-metadata-module="post">third party approval workflows</a>, Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-metadata-module="post">emits Pub/Sub messages</a> throughout a <a href="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-metadata-module="post">release’s lifecycle</a>.<br/></p><h3>The future</h3><p>Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.</p><p>In the meantime, to get started with the Preview, check out the <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">product page</a>, <a href="https://cloud.google.com/deploy/docs" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/deploy/docs" track-metadata-module="post">documentation</a>, <a href="https://cloud.google.com/deploy/docs/quickstart-basic" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/quickstart-basic" track-metadata-module="post">quickstart</a>, and <a href="https://cloud.google.com/deploy/docs/tutorials" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/tutorials" track-metadata-module="post">tutorials</a>. Finally, If you have feedback on Google Cloud Deploy, you can <a href="https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy" target="_blank" track-type="inline link" track-name="30" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">join the conversation</a>. We look forward to hearing from you!<br/></p></div></paragraph-block></div><div><article-video-block _nghost-c16=""><p _ngcontent-c16=""><iframe _ngcontent-c16="" allow="encrypted-media" allowfullscreen="" frameborder="0" height="100%" position="absolute" width="100%" src="https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1&amp;"></iframe></p></article-video-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c18=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;S. Bogdan&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 22 Sep 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Google Cloud Deploy: Managed continuous delivery to GKE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn’t have to be this way. &lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 2.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Solving for continuous delivery challenges&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. &lt;/p&gt;&lt;p&gt;&lt;i&gt;“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. &lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy GIF&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application&#34;&gt;promotion and rollback&lt;/a&gt; decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Contextualized deployment approvals&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple &lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 3.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release’s lifecycle&lt;/a&gt;.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-video&#34;&gt;&lt;div class=&#34;article-module article-video &#34;&gt;&lt;figure&gt;&lt;a class=&#34;h-c-video h-c-video--marquee&#34; data-glue-modal-disabled-on-mobile=&#34;true&#34; data-glue-modal-trigger=&#34;uni-modal-Il8FlhR9jKM-&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34;&gt;&lt;img alt=&#34;Introducing Cloud Deploy&#34; src=&#34;//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg&#34;/&gt;&lt;svg class=&#34;h-c-video__play h-c-icon h-c-icon--color-white&#34; role=&#34;img&#34;&gt;&lt;use xlink:href=&#34;#mi-youtube-icon&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;div class=&#34;h-c-modal--video&#34; data-glue-modal=&#34;uni-modal-Il8FlhR9jKM-&#34; data-glue-modal-close-label=&#34;Close Dialog&#34;&gt;&lt;a class=&#34;glue-yt-video&#34; data-glue-yt-video-autoplay=&#34;true&#34; data-glue-yt-video-height=&#34;99%&#34; data-glue-yt-video-vid=&#34;Il8FlhR9jKM&#34; data-glue-yt-video-width=&#34;100%&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34; ng-cloak=&#34;&#34;&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>#gcp</span></p></figure></article-aspect-image-block><div><div><article-author-block><div><div><p> S. Bogdan </p><p> Product Manager </p></div><p><span> September 22, 2021 </span></p></div></article-author-block></div><article-cta _nghost-c38=""><div _ngcontent-c38=""><h4 _ngcontent-c38=""><span _ngcontent-c38="">Next ’21 registration is open</span></h4><p _ngcontent-c38=""><span _ngcontent-c38="">Join us October 12–14, 2021, for our digital flagship event</span></p><p><a _ngcontent-c38="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="Next21 registration" track-metadata-eventdetail="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration" href="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration"><span _ngcontent-c38="">Register now</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c40=""><div _ngcontent-c40="" innerhtml="&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery &amp;#8212;deploying container image artifacts into your various environments&amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn&amp;#8217;t have to be this way.&amp;#160;&lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;"><p>Continuous delivery is frequently top-of-mind for organizations adopting <a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">Google Kubernetes Engine</a> (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.</p><p>It doesn’t have to be this way. </p><p>Today, we are pleased to announce <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">Google Cloud Deploy</a>, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.</p></div></paragraph-block></div><div><paragraph-block _nghost-c40=""><div _ngcontent-c40="" innerhtml="&lt;h3&gt;Solving for continuous delivery challenges&lt;br&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current&amp;#8212;to say nothing of maintenance&amp;#8212;is resource-intensive and takes time away from the core business.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#8220;We can&amp;#8217;t afford to be innovating in continuous delivery,&amp;#8221; one customer told us. &amp;#8220;We want an opinionated product that supports best practices out of the box.&amp;#8221;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.&amp;#160;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release&amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Solving for continuous delivery challenges<br/></h3><p>Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.</p><p>Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.</p><p><b>Cost of ownership</b></p><p>Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. </p><p><i>“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”</i></p><p>Google Cloud Deploy addresses cost of ownership head-on.</p><p>As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. </p><p>Google Cloud Deploy also provides structure. <a href="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-metadata-module="post">Delivery pipelines</a> and <a href="https://cloud.google.com/deploy/docs/terminology" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology" track-metadata-module="post">targets</a> are defined <a href="https://cloud.google.com/deploy/docs/config-files" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/config-files" track-metadata-module="post">declaratively</a> and are <a href="https://cloud.google.com/deploy/docs/pipeline-instances" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/pipeline-instances" track-metadata-module="post">stored alongside each release</a>. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.<b><br/></b></p></div></paragraph-block></div><div><paragraph-block _nghost-c40=""><p>We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release <a href="https://cloud.google.com/deploy/docs/deploying-application" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/deploying-application" track-metadata-module="post">promotion and rollback</a> decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.<br/></p></paragraph-block></div><div><paragraph-block _nghost-c40=""><div _ngcontent-c40="" innerhtml="&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn&amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple&amp;#160;&lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br&gt;&lt;/p&gt;"><p><b>Security and audit</b></p><p>Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.</p><p>Throughout, Google Cloud Deploy enables fine-grained restriction, with <a href="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-metadata-module="post">discrete resource access control</a> and <a href="https://cloud.google.com/deploy/docs/execution-environment" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/execution-environment" track-metadata-module="post">execution-level security</a>. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and <a href="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-metadata-module="post">approvals</a>.</p><p>Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. <a href="https://cloud.google.com/audit-logs" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/audit-logs" track-metadata-module="post">Cloud Audit Logs</a> <a href="https://cloud.google.com/deploy/docs/audit-logs" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/audit-logs" track-metadata-module="post">audits</a> user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.</p><p><b>Integration</b></p><p>Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.</p><p>Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/integrating" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating" track-metadata-module="post">embraces the GKE delivery tooling ecosystems</a> in three ways: connectivity to CI systems, support for leading configuration (<a href="https://cloud.google.com/deploy/docs/terminology#render" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#render" track-metadata-module="post">rendering</a>) tooling, and <a href="https://cloud.google.com/pubsub" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/pubsub" track-metadata-module="post">Pub/Sub</a> notifications to enable third-party integrations.</p><p><a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-metadata-module="post">Connecting Google Cloud Deploy</a> to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple <i>`</i><i>gcloud beta deploy releases create`.</i><br/></p></div></paragraph-block></div><div><paragraph-block _nghost-c40=""><div _ngcontent-c40="" innerhtml="&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy&amp;#160; leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release&amp;#8217;s lifecycle&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it&amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we&amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br&gt;&lt;/p&gt;"><p>Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages <a href="https://skaffold.dev/" target="_blank" track-type="inline link" track-name="17" track-metadata-eventdetail="https://skaffold.dev" track-metadata-module="post">Skaffold</a>, allowing you to <a href="https://cloud.google.com/deploy/docs/skaffold" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/skaffold" track-metadata-module="post">standardize your configuration</a> between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (<a href="https://helm.sh/" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://helm.sh" track-metadata-module="post">Helm</a>, <a href="https://kustomize.io/" target="_blank" track-type="inline link" track-name="20" track-metadata-eventdetail="https://kustomize.io" track-metadata-module="post">Kustomize</a>, <a href="https://kpt.dev/" target="_blank" track-type="inline link" track-name="21" track-metadata-eventdetail="https://kpt.dev" track-metadata-module="post">kpt</a>). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.</p><p>Finally, to facilitate other integrations, such as a <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-metadata-module="post">post-deployment test execution</a> or <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-metadata-module="post">third party approval workflows</a>, Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-metadata-module="post">emits Pub/Sub messages</a> throughout a <a href="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-metadata-module="post">release’s lifecycle</a>.<br/></p><h3>The future</h3><p>Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.</p><p>In the meantime, to get started with the Preview, check out the <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">product page</a>, <a href="https://cloud.google.com/deploy/docs" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/deploy/docs" track-metadata-module="post">documentation</a>, <a href="https://cloud.google.com/deploy/docs/quickstart-basic" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/quickstart-basic" track-metadata-module="post">quickstart</a>, and <a href="https://cloud.google.com/deploy/docs/tutorials" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/tutorials" track-metadata-module="post">tutorials</a>. Finally, If you have feedback on Google Cloud Deploy, you can <a href="https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy" target="_blank" track-type="inline link" track-name="30" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">join the conversation</a>. We look forward to hearing from you!<br/></p></div></paragraph-block></div><div><article-video-block _nghost-c37=""><p _ngcontent-c37=""><iframe _ngcontent-c37="" allow="encrypted-media" allowfullscreen="" frameborder="0" height="100%" position="absolute" width="100%" src="https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1&amp;"></iframe></p></article-video-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c39=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;S. Bogdan&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 22 Sep 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Google Cloud Deploy: Managed continuous delivery to GKE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn’t have to be this way. &lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 2.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Solving for continuous delivery challenges&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. &lt;/p&gt;&lt;p&gt;&lt;i&gt;“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. &lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy GIF&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application&#34;&gt;promotion and rollback&lt;/a&gt; decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Contextualized deployment approvals&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple &lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 3.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release’s lifecycle&lt;/a&gt;.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-video&#34;&gt;&lt;div class=&#34;article-module article-video &#34;&gt;&lt;figure&gt;&lt;a class=&#34;h-c-video h-c-video--marquee&#34; data-glue-modal-disabled-on-mobile=&#34;true&#34; data-glue-modal-trigger=&#34;uni-modal-Il8FlhR9jKM-&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34;&gt;&lt;img alt=&#34;Introducing Cloud Deploy&#34; src=&#34;//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg&#34;/&gt;&lt;svg class=&#34;h-c-video__play h-c-icon h-c-icon--color-white&#34; role=&#34;img&#34;&gt;&lt;use xlink:href=&#34;#mi-youtube-icon&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;div class=&#34;h-c-modal--video&#34; data-glue-modal=&#34;uni-modal-Il8FlhR9jKM-&#34; data-glue-modal-close-label=&#34;Close Dialog&#34;&gt;&lt;a class=&#34;glue-yt-video&#34; data-glue-yt-video-autoplay=&#34;true&#34; data-glue-yt-video-height=&#34;99%&#34; data-glue-yt-video-vid=&#34;Il8FlhR9jKM&#34; data-glue-yt-video-width=&#34;100%&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34; ng-cloak=&#34;&#34;&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>#gcp</span></p></figure></article-aspect-image-block><div><div><article-author-block><div><div><p> S. Bogdan </p><p> Product Manager </p></div><p><span> September 22, 2021 </span></p></div></article-author-block></div><article-cta _nghost-c17=""><div _ngcontent-c17=""><h4 _ngcontent-c17=""><span _ngcontent-c17="">Next ’21 registration is open</span></h4><p _ngcontent-c17=""><span _ngcontent-c17="">Join us October 12–14, 2021, for our digital flagship event</span></p><p><a _ngcontent-c17="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="Next21 registration" track-metadata-eventdetail="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration" href="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration"><span _ngcontent-c17="">Register now</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery &amp;#8212;deploying container image artifacts into your various environments&amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn&amp;#8217;t have to be this way.&amp;#160;&lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;"><p>Continuous delivery is frequently top-of-mind for organizations adopting <a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">Google Kubernetes Engine</a> (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.</p><p>It doesn’t have to be this way. </p><p>Today, we are pleased to announce <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">Google Cloud Deploy</a>, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;h3&gt;Solving for continuous delivery challenges&lt;br&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current&amp;#8212;to say nothing of maintenance&amp;#8212;is resource-intensive and takes time away from the core business.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#8220;We can&amp;#8217;t afford to be innovating in continuous delivery,&amp;#8221; one customer told us. &amp;#8220;We want an opinionated product that supports best practices out of the box.&amp;#8221;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.&amp;#160;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release&amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Solving for continuous delivery challenges<br/></h3><p>Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.</p><p>Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.</p><p><b>Cost of ownership</b></p><p>Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. </p><p><i>“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”</i></p><p>Google Cloud Deploy addresses cost of ownership head-on.</p><p>As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. </p><p>Google Cloud Deploy also provides structure. <a href="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-metadata-module="post">Delivery pipelines</a> and <a href="https://cloud.google.com/deploy/docs/terminology" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology" track-metadata-module="post">targets</a> are defined <a href="https://cloud.google.com/deploy/docs/config-files" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/config-files" track-metadata-module="post">declaratively</a> and are <a href="https://cloud.google.com/deploy/docs/pipeline-instances" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/pipeline-instances" track-metadata-module="post">stored alongside each release</a>. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.<b><br/></b></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><p>We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release <a href="https://cloud.google.com/deploy/docs/deploying-application" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/deploying-application" track-metadata-module="post">promotion and rollback</a> decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.<br/></p></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn&amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple&amp;#160;&lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br&gt;&lt;/p&gt;"><p><b>Security and audit</b></p><p>Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.</p><p>Throughout, Google Cloud Deploy enables fine-grained restriction, with <a href="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-metadata-module="post">discrete resource access control</a> and <a href="https://cloud.google.com/deploy/docs/execution-environment" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/execution-environment" track-metadata-module="post">execution-level security</a>. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and <a href="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-metadata-module="post">approvals</a>.</p><p>Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. <a href="https://cloud.google.com/audit-logs" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/audit-logs" track-metadata-module="post">Cloud Audit Logs</a> <a href="https://cloud.google.com/deploy/docs/audit-logs" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/audit-logs" track-metadata-module="post">audits</a> user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.</p><p><b>Integration</b></p><p>Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.</p><p>Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/integrating" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating" track-metadata-module="post">embraces the GKE delivery tooling ecosystems</a> in three ways: connectivity to CI systems, support for leading configuration (<a href="https://cloud.google.com/deploy/docs/terminology#render" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#render" track-metadata-module="post">rendering</a>) tooling, and <a href="https://cloud.google.com/pubsub" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/pubsub" track-metadata-module="post">Pub/Sub</a> notifications to enable third-party integrations.</p><p><a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-metadata-module="post">Connecting Google Cloud Deploy</a> to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple <i>`</i><i>gcloud beta deploy releases create`.</i><br/></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy&amp;#160; leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release&amp;#8217;s lifecycle&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it&amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we&amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br&gt;&lt;/p&gt;"><p>Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages <a href="https://skaffold.dev/" target="_blank" track-type="inline link" track-name="17" track-metadata-eventdetail="https://skaffold.dev" track-metadata-module="post">Skaffold</a>, allowing you to <a href="https://cloud.google.com/deploy/docs/skaffold" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/skaffold" track-metadata-module="post">standardize your configuration</a> between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (<a href="https://helm.sh/" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://helm.sh" track-metadata-module="post">Helm</a>, <a href="https://kustomize.io/" target="_blank" track-type="inline link" track-name="20" track-metadata-eventdetail="https://kustomize.io" track-metadata-module="post">Kustomize</a>, <a href="https://kpt.dev/" target="_blank" track-type="inline link" track-name="21" track-metadata-eventdetail="https://kpt.dev" track-metadata-module="post">kpt</a>). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.</p><p>Finally, to facilitate other integrations, such as a <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-metadata-module="post">post-deployment test execution</a> or <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-metadata-module="post">third party approval workflows</a>, Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-metadata-module="post">emits Pub/Sub messages</a> throughout a <a href="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-metadata-module="post">release’s lifecycle</a>.<br/></p><h3>The future</h3><p>Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.</p><p>In the meantime, to get started with the Preview, check out the <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">product page</a>, <a href="https://cloud.google.com/deploy/docs" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/deploy/docs" track-metadata-module="post">documentation</a>, <a href="https://cloud.google.com/deploy/docs/quickstart-basic" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/quickstart-basic" track-metadata-module="post">quickstart</a>, and <a href="https://cloud.google.com/deploy/docs/tutorials" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/tutorials" track-metadata-module="post">tutorials</a>. Finally, If you have feedback on Google Cloud Deploy, you can <a href="https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy" target="_blank" track-type="inline link" track-name="30" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">join the conversation</a>. We look forward to hearing from you!<br/></p></div></paragraph-block></div><div><article-video-block _nghost-c16=""><p _ngcontent-c16=""><iframe _ngcontent-c16="" allow="encrypted-media" allowfullscreen="" frameborder="0" height="100%" position="absolute" width="100%" src="https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1&amp;"></iframe></p></article-video-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c18=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;S. Bogdan&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 22 Sep 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Google Cloud Deploy: Managed continuous delivery to GKE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn’t have to be this way. &lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 2.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Solving for continuous delivery challenges&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. &lt;/p&gt;&lt;p&gt;&lt;i&gt;“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. &lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy GIF&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application&#34;&gt;promotion and rollback&lt;/a&gt; decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Contextualized deployment approvals&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple &lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 3.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release’s lifecycle&lt;/a&gt;.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-video&#34;&gt;&lt;div class=&#34;article-module article-video &#34;&gt;&lt;figure&gt;&lt;a class=&#34;h-c-video h-c-video--marquee&#34; data-glue-modal-disabled-on-mobile=&#34;true&#34; data-glue-modal-trigger=&#34;uni-modal-Il8FlhR9jKM-&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34;&gt;&lt;img alt=&#34;Introducing Cloud Deploy&#34; src=&#34;//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg&#34;/&gt;&lt;svg class=&#34;h-c-video__play h-c-icon h-c-icon--color-white&#34; role=&#34;img&#34;&gt;&lt;use xlink:href=&#34;#mi-youtube-icon&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;div class=&#34;h-c-modal--video&#34; data-glue-modal=&#34;uni-modal-Il8FlhR9jKM-&#34; data-glue-modal-close-label=&#34;Close Dialog&#34;&gt;&lt;a class=&#34;glue-yt-video&#34; data-glue-yt-video-autoplay=&#34;true&#34; data-glue-yt-video-height=&#34;99%&#34; data-glue-yt-video-vid=&#34;Il8FlhR9jKM&#34; data-glue-yt-video-width=&#34;100%&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34; ng-cloak=&#34;&#34;&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>#gcp</span></p></figure></article-aspect-image-block><div><div><article-author-block><div><div><p> S. Bogdan </p><p> Product Manager </p></div><p><span> September 22, 2021 </span></p></div></article-author-block></div><article-cta _nghost-c17=""><div _ngcontent-c17=""><h4 _ngcontent-c17=""><span _ngcontent-c17="">Next ’21 registration is open</span></h4><p _ngcontent-c17=""><span _ngcontent-c17="">Join us October 12–14, 2021, for our digital flagship event</span></p><p><a _ngcontent-c17="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="Next21 registration" track-metadata-eventdetail="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration" href="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration"><span _ngcontent-c17="">Register now</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery &amp;#8212;deploying container image artifacts into your various environments&amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn&amp;#8217;t have to be this way.&amp;#160;&lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;"><p>Continuous delivery is frequently top-of-mind for organizations adopting <a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">Google Kubernetes Engine</a> (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.</p><p>It doesn’t have to be this way. </p><p>Today, we are pleased to announce <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">Google Cloud Deploy</a>, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;h3&gt;Solving for continuous delivery challenges&lt;br&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current&amp;#8212;to say nothing of maintenance&amp;#8212;is resource-intensive and takes time away from the core business.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#8220;We can&amp;#8217;t afford to be innovating in continuous delivery,&amp;#8221; one customer told us. &amp;#8220;We want an opinionated product that supports best practices out of the box.&amp;#8221;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.&amp;#160;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release&amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Solving for continuous delivery challenges<br/></h3><p>Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.</p><p>Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.</p><p><b>Cost of ownership</b></p><p>Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. </p><p><i>“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”</i></p><p>Google Cloud Deploy addresses cost of ownership head-on.</p><p>As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. </p><p>Google Cloud Deploy also provides structure. <a href="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-metadata-module="post">Delivery pipelines</a> and <a href="https://cloud.google.com/deploy/docs/terminology" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology" track-metadata-module="post">targets</a> are defined <a href="https://cloud.google.com/deploy/docs/config-files" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/config-files" track-metadata-module="post">declaratively</a> and are <a href="https://cloud.google.com/deploy/docs/pipeline-instances" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/pipeline-instances" track-metadata-module="post">stored alongside each release</a>. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.<b><br/></b></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><p>We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release <a href="https://cloud.google.com/deploy/docs/deploying-application" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/deploying-application" track-metadata-module="post">promotion and rollback</a> decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.<br/></p></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn&amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple&amp;#160;&lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br&gt;&lt;/p&gt;"><p><b>Security and audit</b></p><p>Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.</p><p>Throughout, Google Cloud Deploy enables fine-grained restriction, with <a href="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-metadata-module="post">discrete resource access control</a> and <a href="https://cloud.google.com/deploy/docs/execution-environment" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/execution-environment" track-metadata-module="post">execution-level security</a>. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and <a href="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-metadata-module="post">approvals</a>.</p><p>Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. <a href="https://cloud.google.com/audit-logs" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/audit-logs" track-metadata-module="post">Cloud Audit Logs</a> <a href="https://cloud.google.com/deploy/docs/audit-logs" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/audit-logs" track-metadata-module="post">audits</a> user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.</p><p><b>Integration</b></p><p>Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.</p><p>Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/integrating" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating" track-metadata-module="post">embraces the GKE delivery tooling ecosystems</a> in three ways: connectivity to CI systems, support for leading configuration (<a href="https://cloud.google.com/deploy/docs/terminology#render" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#render" track-metadata-module="post">rendering</a>) tooling, and <a href="https://cloud.google.com/pubsub" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/pubsub" track-metadata-module="post">Pub/Sub</a> notifications to enable third-party integrations.</p><p><a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-metadata-module="post">Connecting Google Cloud Deploy</a> to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple <i>`</i><i>gcloud beta deploy releases create`.</i><br/></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy&amp;#160; leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release&amp;#8217;s lifecycle&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it&amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we&amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br&gt;&lt;/p&gt;"><p>Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages <a href="https://skaffold.dev/" target="_blank" track-type="inline link" track-name="17" track-metadata-eventdetail="https://skaffold.dev" track-metadata-module="post">Skaffold</a>, allowing you to <a href="https://cloud.google.com/deploy/docs/skaffold" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/skaffold" track-metadata-module="post">standardize your configuration</a> between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (<a href="https://helm.sh/" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://helm.sh" track-metadata-module="post">Helm</a>, <a href="https://kustomize.io/" target="_blank" track-type="inline link" track-name="20" track-metadata-eventdetail="https://kustomize.io" track-metadata-module="post">Kustomize</a>, <a href="https://kpt.dev/" target="_blank" track-type="inline link" track-name="21" track-metadata-eventdetail="https://kpt.dev" track-metadata-module="post">kpt</a>). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.</p><p>Finally, to facilitate other integrations, such as a <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-metadata-module="post">post-deployment test execution</a> or <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-metadata-module="post">third party approval workflows</a>, Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-metadata-module="post">emits Pub/Sub messages</a> throughout a <a href="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-metadata-module="post">release’s lifecycle</a>.<br/></p><h3>The future</h3><p>Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.</p><p>In the meantime, to get started with the Preview, check out the <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">product page</a>, <a href="https://cloud.google.com/deploy/docs" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/deploy/docs" track-metadata-module="post">documentation</a>, <a href="https://cloud.google.com/deploy/docs/quickstart-basic" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/quickstart-basic" track-metadata-module="post">quickstart</a>, and <a href="https://cloud.google.com/deploy/docs/tutorials" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/tutorials" track-metadata-module="post">tutorials</a>. Finally, If you have feedback on Google Cloud Deploy, you can <a href="https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy" target="_blank" track-type="inline link" track-name="30" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">join the conversation</a>. We look forward to hearing from you!<br/></p></div></paragraph-block></div><div><article-video-block _nghost-c16=""><p _ngcontent-c16=""><iframe _ngcontent-c16="" allow="encrypted-media" allowfullscreen="" frameborder="0" height="100%" position="absolute" width="100%" src="https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1&amp;"></iframe></p></article-video-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c18=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;S. Bogdan&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 22 Sep 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Google Cloud Deploy: Managed continuous delivery to GKE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn’t have to be this way. &lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 2.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Solving for continuous delivery challenges&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. &lt;/p&gt;&lt;p&gt;&lt;i&gt;“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. &lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy GIF&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application&#34;&gt;promotion and rollback&lt;/a&gt; decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Contextualized deployment approvals&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple &lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 3.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release’s lifecycle&lt;/a&gt;.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-video&#34;&gt;&lt;div class=&#34;article-module article-video &#34;&gt;&lt;figure&gt;&lt;a class=&#34;h-c-video h-c-video--marquee&#34; data-glue-modal-disabled-on-mobile=&#34;true&#34; data-glue-modal-trigger=&#34;uni-modal-Il8FlhR9jKM-&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34;&gt;&lt;img alt=&#34;Introducing Cloud Deploy&#34; src=&#34;//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg&#34;/&gt;&lt;svg class=&#34;h-c-video__play h-c-icon h-c-icon--color-white&#34; role=&#34;img&#34;&gt;&lt;use xlink:href=&#34;#mi-youtube-icon&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;div class=&#34;h-c-modal--video&#34; data-glue-modal=&#34;uni-modal-Il8FlhR9jKM-&#34; data-glue-modal-close-label=&#34;Close Dialog&#34;&gt;&lt;a class=&#34;glue-yt-video&#34; data-glue-yt-video-autoplay=&#34;true&#34; data-glue-yt-video-height=&#34;99%&#34; data-glue-yt-video-vid=&#34;Il8FlhR9jKM&#34; data-glue-yt-video-width=&#34;100%&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34; ng-cloak=&#34;&#34;&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>#gcp</span></p></figure></article-aspect-image-block><div><div><article-author-block><div><div><p> S. Bogdan </p><p> Product Manager </p></div><p><span> September 22, 2021 </span></p></div></article-author-block></div><article-cta _nghost-c46=""><div _ngcontent-c46=""><h4 _ngcontent-c46=""><span _ngcontent-c46="">Next ’21 registration is open</span></h4><p _ngcontent-c46=""><span _ngcontent-c46="">Join us October 12–14, 2021, for our digital flagship event</span></p><p><a _ngcontent-c46="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="Next21 registration" track-metadata-eventdetail="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration" href="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration"><span _ngcontent-c46="">Register now</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c48=""><div _ngcontent-c48="" innerhtml="&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery &amp;#8212;deploying container image artifacts into your various environments&amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn&amp;#8217;t have to be this way.&amp;#160;&lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;"><p>Continuous delivery is frequently top-of-mind for organizations adopting <a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">Google Kubernetes Engine</a> (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.</p><p>It doesn’t have to be this way. </p><p>Today, we are pleased to announce <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">Google Cloud Deploy</a>, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.</p></div></paragraph-block></div><div><paragraph-block _nghost-c48=""><div _ngcontent-c48="" innerhtml="&lt;h3&gt;Solving for continuous delivery challenges&lt;br&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current&amp;#8212;to say nothing of maintenance&amp;#8212;is resource-intensive and takes time away from the core business.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#8220;We can&amp;#8217;t afford to be innovating in continuous delivery,&amp;#8221; one customer told us. &amp;#8220;We want an opinionated product that supports best practices out of the box.&amp;#8221;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.&amp;#160;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release&amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Solving for continuous delivery challenges<br/></h3><p>Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.</p><p>Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.</p><p><b>Cost of ownership</b></p><p>Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. </p><p><i>“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”</i></p><p>Google Cloud Deploy addresses cost of ownership head-on.</p><p>As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. </p><p>Google Cloud Deploy also provides structure. <a href="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-metadata-module="post">Delivery pipelines</a> and <a href="https://cloud.google.com/deploy/docs/terminology" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology" track-metadata-module="post">targets</a> are defined <a href="https://cloud.google.com/deploy/docs/config-files" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/config-files" track-metadata-module="post">declaratively</a> and are <a href="https://cloud.google.com/deploy/docs/pipeline-instances" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/pipeline-instances" track-metadata-module="post">stored alongside each release</a>. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.<b><br/></b></p></div></paragraph-block></div><div><paragraph-block _nghost-c48=""><p>We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release <a href="https://cloud.google.com/deploy/docs/deploying-application" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/deploying-application" track-metadata-module="post">promotion and rollback</a> decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.<br/></p></paragraph-block></div><div><paragraph-block _nghost-c48=""><div _ngcontent-c48="" innerhtml="&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn&amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple&amp;#160;&lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br&gt;&lt;/p&gt;"><p><b>Security and audit</b></p><p>Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.</p><p>Throughout, Google Cloud Deploy enables fine-grained restriction, with <a href="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-metadata-module="post">discrete resource access control</a> and <a href="https://cloud.google.com/deploy/docs/execution-environment" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/execution-environment" track-metadata-module="post">execution-level security</a>. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and <a href="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-metadata-module="post">approvals</a>.</p><p>Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. <a href="https://cloud.google.com/audit-logs" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/audit-logs" track-metadata-module="post">Cloud Audit Logs</a> <a href="https://cloud.google.com/deploy/docs/audit-logs" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/audit-logs" track-metadata-module="post">audits</a> user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.</p><p><b>Integration</b></p><p>Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.</p><p>Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/integrating" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating" track-metadata-module="post">embraces the GKE delivery tooling ecosystems</a> in three ways: connectivity to CI systems, support for leading configuration (<a href="https://cloud.google.com/deploy/docs/terminology#render" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#render" track-metadata-module="post">rendering</a>) tooling, and <a href="https://cloud.google.com/pubsub" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/pubsub" track-metadata-module="post">Pub/Sub</a> notifications to enable third-party integrations.</p><p><a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-metadata-module="post">Connecting Google Cloud Deploy</a> to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple <i>`</i><i>gcloud beta deploy releases create`.</i><br/></p></div></paragraph-block></div><div><paragraph-block _nghost-c48=""><div _ngcontent-c48="" innerhtml="&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy&amp;#160; leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release&amp;#8217;s lifecycle&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it&amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we&amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br&gt;&lt;/p&gt;"><p>Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages <a href="https://skaffold.dev/" target="_blank" track-type="inline link" track-name="17" track-metadata-eventdetail="https://skaffold.dev" track-metadata-module="post">Skaffold</a>, allowing you to <a href="https://cloud.google.com/deploy/docs/skaffold" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/skaffold" track-metadata-module="post">standardize your configuration</a> between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (<a href="https://helm.sh/" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://helm.sh" track-metadata-module="post">Helm</a>, <a href="https://kustomize.io/" target="_blank" track-type="inline link" track-name="20" track-metadata-eventdetail="https://kustomize.io" track-metadata-module="post">Kustomize</a>, <a href="https://kpt.dev/" target="_blank" track-type="inline link" track-name="21" track-metadata-eventdetail="https://kpt.dev" track-metadata-module="post">kpt</a>). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.</p><p>Finally, to facilitate other integrations, such as a <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-metadata-module="post">post-deployment test execution</a> or <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-metadata-module="post">third party approval workflows</a>, Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-metadata-module="post">emits Pub/Sub messages</a> throughout a <a href="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-metadata-module="post">release’s lifecycle</a>.<br/></p><h3>The future</h3><p>Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.</p><p>In the meantime, to get started with the Preview, check out the <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">product page</a>, <a href="https://cloud.google.com/deploy/docs" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/deploy/docs" track-metadata-module="post">documentation</a>, <a href="https://cloud.google.com/deploy/docs/quickstart-basic" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/quickstart-basic" track-metadata-module="post">quickstart</a>, and <a href="https://cloud.google.com/deploy/docs/tutorials" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/tutorials" track-metadata-module="post">tutorials</a>. Finally, If you have feedback on Google Cloud Deploy, you can <a href="https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy" target="_blank" track-type="inline link" track-name="30" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">join the conversation</a>. We look forward to hearing from you!<br/></p></div></paragraph-block></div><div><article-video-block _nghost-c45=""><p _ngcontent-c45=""><iframe _ngcontent-c45="" allow="encrypted-media" allowfullscreen="" frameborder="0" height="100%" position="absolute" width="100%" src="https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1&amp;"></iframe></p></article-video-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c47=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;S. Bogdan&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 22 Sep 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Google Cloud Deploy: Managed continuous delivery to GKE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn’t have to be this way. &lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 2.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Solving for continuous delivery challenges&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. &lt;/p&gt;&lt;p&gt;&lt;i&gt;“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. &lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy GIF&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application&#34;&gt;promotion and rollback&lt;/a&gt; decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Contextualized deployment approvals&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple &lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 3.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release’s lifecycle&lt;/a&gt;.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-video&#34;&gt;&lt;div class=&#34;article-module article-video &#34;&gt;&lt;figure&gt;&lt;a class=&#34;h-c-video h-c-video--marquee&#34; data-glue-modal-disabled-on-mobile=&#34;true&#34; data-glue-modal-trigger=&#34;uni-modal-Il8FlhR9jKM-&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34;&gt;&lt;img alt=&#34;Introducing Cloud Deploy&#34; src=&#34;//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg&#34;/&gt;&lt;svg class=&#34;h-c-video__play h-c-icon h-c-icon--color-white&#34; role=&#34;img&#34;&gt;&lt;use xlink:href=&#34;#mi-youtube-icon&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;div class=&#34;h-c-modal--video&#34; data-glue-modal=&#34;uni-modal-Il8FlhR9jKM-&#34; data-glue-modal-close-label=&#34;Close Dialog&#34;&gt;&lt;a class=&#34;glue-yt-video&#34; data-glue-yt-video-autoplay=&#34;true&#34; data-glue-yt-video-height=&#34;99%&#34; data-glue-yt-video-vid=&#34;Il8FlhR9jKM&#34; data-glue-yt-video-width=&#34;100%&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34; ng-cloak=&#34;&#34;&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>#gcp</span></p></figure></article-aspect-image-block><div><div><article-author-block><div><div><p> S. Bogdan </p><p> Product Manager </p></div><p><span> September 22, 2021 </span></p></div></article-author-block></div><article-cta _nghost-c50=""><div _ngcontent-c50=""><h4 _ngcontent-c50=""><span _ngcontent-c50="">Next ’21 registration is open</span></h4><p _ngcontent-c50=""><span _ngcontent-c50="">Join us October 12–14, 2021, for our digital flagship event</span></p><p><a _ngcontent-c50="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="Next21 registration" track-metadata-eventdetail="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration" href="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration"><span _ngcontent-c50="">Register now</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c52=""><div _ngcontent-c52="" innerhtml="&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery &amp;#8212;deploying container image artifacts into your various environments&amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn&amp;#8217;t have to be this way.&amp;#160;&lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;"><p>Continuous delivery is frequently top-of-mind for organizations adopting <a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">Google Kubernetes Engine</a> (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.</p><p>It doesn’t have to be this way. </p><p>Today, we are pleased to announce <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">Google Cloud Deploy</a>, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.</p></div></paragraph-block></div><div><paragraph-block _nghost-c52=""><div _ngcontent-c52="" innerhtml="&lt;h3&gt;Solving for continuous delivery challenges&lt;br&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current&amp;#8212;to say nothing of maintenance&amp;#8212;is resource-intensive and takes time away from the core business.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#8220;We can&amp;#8217;t afford to be innovating in continuous delivery,&amp;#8221; one customer told us. &amp;#8220;We want an opinionated product that supports best practices out of the box.&amp;#8221;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.&amp;#160;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release&amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Solving for continuous delivery challenges<br/></h3><p>Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.</p><p>Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.</p><p><b>Cost of ownership</b></p><p>Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. </p><p><i>“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”</i></p><p>Google Cloud Deploy addresses cost of ownership head-on.</p><p>As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. </p><p>Google Cloud Deploy also provides structure. <a href="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-metadata-module="post">Delivery pipelines</a> and <a href="https://cloud.google.com/deploy/docs/terminology" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology" track-metadata-module="post">targets</a> are defined <a href="https://cloud.google.com/deploy/docs/config-files" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/config-files" track-metadata-module="post">declaratively</a> and are <a href="https://cloud.google.com/deploy/docs/pipeline-instances" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/pipeline-instances" track-metadata-module="post">stored alongside each release</a>. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.<b><br/></b></p></div></paragraph-block></div><div><paragraph-block _nghost-c52=""><p>We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release <a href="https://cloud.google.com/deploy/docs/deploying-application" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/deploying-application" track-metadata-module="post">promotion and rollback</a> decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.<br/></p></paragraph-block></div><div><paragraph-block _nghost-c52=""><div _ngcontent-c52="" innerhtml="&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn&amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple&amp;#160;&lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br&gt;&lt;/p&gt;"><p><b>Security and audit</b></p><p>Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.</p><p>Throughout, Google Cloud Deploy enables fine-grained restriction, with <a href="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-metadata-module="post">discrete resource access control</a> and <a href="https://cloud.google.com/deploy/docs/execution-environment" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/execution-environment" track-metadata-module="post">execution-level security</a>. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and <a href="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-metadata-module="post">approvals</a>.</p><p>Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. <a href="https://cloud.google.com/audit-logs" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/audit-logs" track-metadata-module="post">Cloud Audit Logs</a> <a href="https://cloud.google.com/deploy/docs/audit-logs" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/audit-logs" track-metadata-module="post">audits</a> user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.</p><p><b>Integration</b></p><p>Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.</p><p>Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/integrating" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating" track-metadata-module="post">embraces the GKE delivery tooling ecosystems</a> in three ways: connectivity to CI systems, support for leading configuration (<a href="https://cloud.google.com/deploy/docs/terminology#render" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#render" track-metadata-module="post">rendering</a>) tooling, and <a href="https://cloud.google.com/pubsub" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/pubsub" track-metadata-module="post">Pub/Sub</a> notifications to enable third-party integrations.</p><p><a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-metadata-module="post">Connecting Google Cloud Deploy</a> to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple <i>`</i><i>gcloud beta deploy releases create`.</i><br/></p></div></paragraph-block></div><div><paragraph-block _nghost-c52=""><div _ngcontent-c52="" innerhtml="&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy&amp;#160; leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release&amp;#8217;s lifecycle&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it&amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we&amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br&gt;&lt;/p&gt;"><p>Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages <a href="https://skaffold.dev/" target="_blank" track-type="inline link" track-name="17" track-metadata-eventdetail="https://skaffold.dev" track-metadata-module="post">Skaffold</a>, allowing you to <a href="https://cloud.google.com/deploy/docs/skaffold" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/skaffold" track-metadata-module="post">standardize your configuration</a> between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (<a href="https://helm.sh/" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://helm.sh" track-metadata-module="post">Helm</a>, <a href="https://kustomize.io/" target="_blank" track-type="inline link" track-name="20" track-metadata-eventdetail="https://kustomize.io" track-metadata-module="post">Kustomize</a>, <a href="https://kpt.dev/" target="_blank" track-type="inline link" track-name="21" track-metadata-eventdetail="https://kpt.dev" track-metadata-module="post">kpt</a>). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.</p><p>Finally, to facilitate other integrations, such as a <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-metadata-module="post">post-deployment test execution</a> or <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-metadata-module="post">third party approval workflows</a>, Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-metadata-module="post">emits Pub/Sub messages</a> throughout a <a href="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-metadata-module="post">release’s lifecycle</a>.<br/></p><h3>The future</h3><p>Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.</p><p>In the meantime, to get started with the Preview, check out the <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">product page</a>, <a href="https://cloud.google.com/deploy/docs" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/deploy/docs" track-metadata-module="post">documentation</a>, <a href="https://cloud.google.com/deploy/docs/quickstart-basic" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/quickstart-basic" track-metadata-module="post">quickstart</a>, and <a href="https://cloud.google.com/deploy/docs/tutorials" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/tutorials" track-metadata-module="post">tutorials</a>. Finally, If you have feedback on Google Cloud Deploy, you can <a href="https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy" target="_blank" track-type="inline link" track-name="30" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">join the conversation</a>. We look forward to hearing from you!<br/></p></div></paragraph-block></div><div><article-video-block _nghost-c49=""><p _ngcontent-c49=""><iframe _ngcontent-c49="" allow="encrypted-media" allowfullscreen="" frameborder="0" height="100%" position="absolute" width="100%" src="https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1&amp;"></iframe></p></article-video-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c51=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;S. Bogdan&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 22 Sep 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Google Cloud Deploy: Managed continuous delivery to GKE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn’t have to be this way. &lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 2.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Solving for continuous delivery challenges&lt;br/&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. &lt;/p&gt;&lt;p&gt;&lt;i&gt;“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. &lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br/&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy GIF&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release &lt;a href=&#34;https://cloud.google.com/deploy/docs/deploying-application&#34;&gt;promotion and rollback&lt;/a&gt; decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;Contextualized deployment approvals&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple &lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Deploy 3.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release’s lifecycle&lt;/a&gt;.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-video&#34;&gt;&lt;div class=&#34;article-module article-video &#34;&gt;&lt;figure&gt;&lt;a class=&#34;h-c-video h-c-video--marquee&#34; data-glue-modal-disabled-on-mobile=&#34;true&#34; data-glue-modal-trigger=&#34;uni-modal-Il8FlhR9jKM-&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34;&gt;&lt;img alt=&#34;Introducing Cloud Deploy&#34; src=&#34;//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg&#34;/&gt;&lt;svg class=&#34;h-c-video__play h-c-icon h-c-icon--color-white&#34; role=&#34;img&#34;&gt;&lt;use xlink:href=&#34;#mi-youtube-icon&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;div class=&#34;h-c-modal--video&#34; data-glue-modal=&#34;uni-modal-Il8FlhR9jKM-&#34; data-glue-modal-close-label=&#34;Close Dialog&#34;&gt;&lt;a class=&#34;glue-yt-video&#34; data-glue-yt-video-autoplay=&#34;true&#34; data-glue-yt-video-height=&#34;99%&#34; data-glue-yt-video-vid=&#34;Il8FlhR9jKM&#34; data-glue-yt-video-width=&#34;100%&#34; href=&#34;https://youtube.com/watch?v=Il8FlhR9jKM&#34; ng-cloak=&#34;&#34;&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;2021 Accelerate State of DevOps report addresses burnout, team performance&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>#gcp</span></p></figure></article-aspect-image-block><div><div><article-author-block><div><div><p> S. Bogdan </p><p> Product Manager </p></div><p><span> September 22, 2021 </span></p></div></article-author-block></div><article-cta _nghost-c13=""><div _ngcontent-c13=""><h4 _ngcontent-c13=""><span _ngcontent-c13="">Next ’21 registration is open</span></h4><p _ngcontent-c13=""><span _ngcontent-c13="">Join us October 12–14, 2021, for our digital flagship event</span></p><p><a _ngcontent-c13="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="Next21 registration" track-metadata-eventdetail="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration" href="https://cloud.withgoogle.com/next/register?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021&amp;utm_content=blog-next-21-registration"><span _ngcontent-c13="">Register now</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c15=""><div _ngcontent-c15="" innerhtml="&lt;p&gt;Continuous delivery is frequently top-of-mind for organizations adopting &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE). However, continuous delivery &amp;#8212;deploying container image artifacts into your various environments&amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.&lt;/p&gt;&lt;p&gt;It doesn&amp;#8217;t have to be this way.&amp;#160;&lt;/p&gt;Today, we are pleased to announce &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;Google Cloud Deploy&lt;/a&gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.&lt;p&gt;&lt;/p&gt;"><p>Continuous delivery is frequently top-of-mind for organizations adopting <a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">Google Kubernetes Engine</a> (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.</p><p>It doesn’t have to be this way. </p><p>Today, we are pleased to announce <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">Google Cloud Deploy</a>, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.</p></div></paragraph-block></div><div><paragraph-block _nghost-c15=""><div _ngcontent-c15="" innerhtml="&lt;h3&gt;Solving for continuous delivery challenges&lt;br&gt;&lt;/h3&gt;&lt;p&gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cost of ownership&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current&amp;#8212;to say nothing of maintenance&amp;#8212;is resource-intensive and takes time away from the core business.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&amp;#8220;We can&amp;#8217;t afford to be innovating in continuous delivery,&amp;#8221; one customer told us. &amp;#8220;We want an opinionated product that supports best practices out of the box.&amp;#8221;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy addresses cost of ownership head-on.&lt;/p&gt;&lt;p&gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.&amp;#160;&lt;/p&gt;&lt;p&gt;Google Cloud Deploy also provides structure. &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline&#34;&gt;Delivery pipelines&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology&#34;&gt;targets&lt;/a&gt; are defined &lt;a href=&#34;https://cloud.google.com/deploy/docs/config-files&#34;&gt;declaratively&lt;/a&gt; and are &lt;a href=&#34;https://cloud.google.com/deploy/docs/pipeline-instances&#34;&gt;stored alongside each release&lt;/a&gt;. That means if your delivery pipeline changes, the release&amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;"><h3>Solving for continuous delivery challenges<br/></h3><p>Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.</p><p>Let’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.</p><p><b>Cost of ownership</b></p><p>Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. </p><p><i>“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”</i></p><p>Google Cloud Deploy addresses cost of ownership head-on.</p><p>As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. </p><p>Google Cloud Deploy also provides structure. <a href="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#delivery_pipeline" track-metadata-module="post">Delivery pipelines</a> and <a href="https://cloud.google.com/deploy/docs/terminology" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology" track-metadata-module="post">targets</a> are defined <a href="https://cloud.google.com/deploy/docs/config-files" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/config-files" track-metadata-module="post">declaratively</a> and are <a href="https://cloud.google.com/deploy/docs/pipeline-instances" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/pipeline-instances" track-metadata-module="post">stored alongside each release</a>. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.<b><br/></b></p></div></paragraph-block></div><div><paragraph-block _nghost-c15=""><p>We have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release <a href="https://cloud.google.com/deploy/docs/deploying-application" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/deploying-application" track-metadata-module="post">promotion and rollback</a> decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.<br/></p></paragraph-block></div><div><paragraph-block _nghost-c15=""><div _ngcontent-c15="" innerhtml="&lt;p&gt;&lt;b&gt;Security and audit&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn&amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.&lt;/p&gt;&lt;p&gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with &lt;a href=&#34;https://cloud.google.com/deploy/docs/iam-roles-permissions&#34;&gt;discrete resource access control&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/deploy/docs/execution-environment&#34;&gt;execution-level security&lt;/a&gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval&#34;&gt;approvals&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. &lt;a href=&#34;https://cloud.google.com/audit-logs&#34;&gt;Cloud Audit Logs&lt;/a&gt; &lt;a href=&#34;https://cloud.google.com/deploy/docs/audit-logs&#34;&gt;audits&lt;/a&gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Integration&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.&lt;/p&gt;&lt;p&gt;Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating&#34;&gt;embraces the GKE delivery tooling ecosystems&lt;/a&gt; in three ways: connectivity to CI systems, support for leading configuration (&lt;a href=&#34;https://cloud.google.com/deploy/docs/terminology#render&#34;&gt;rendering&lt;/a&gt;) tooling, and &lt;a href=&#34;https://cloud.google.com/pubsub&#34;&gt;Pub/Sub&lt;/a&gt; notifications to enable third-party integrations.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system&#34;&gt;Connecting Google Cloud Deploy&lt;/a&gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple&amp;#160;&lt;i&gt;`&lt;/i&gt;&lt;i&gt;gcloud beta deploy releases create`.&lt;/i&gt;&lt;br&gt;&lt;/p&gt;"><p><b>Security and audit</b></p><p>Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.</p><p>Throughout, Google Cloud Deploy enables fine-grained restriction, with <a href="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/iam-roles-permissions" track-metadata-module="post">discrete resource access control</a> and <a href="https://cloud.google.com/deploy/docs/execution-environment" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/execution-environment" track-metadata-module="post">execution-level security</a>. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and <a href="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval" track-metadata-module="post">approvals</a>.</p><p>Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. <a href="https://cloud.google.com/audit-logs" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/audit-logs" track-metadata-module="post">Cloud Audit Logs</a> <a href="https://cloud.google.com/deploy/docs/audit-logs" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/audit-logs" track-metadata-module="post">audits</a> user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.</p><p><b>Integration</b></p><p>Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.</p><p>Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/integrating" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating" track-metadata-module="post">embraces the GKE delivery tooling ecosystems</a> in three ways: connectivity to CI systems, support for leading configuration (<a href="https://cloud.google.com/deploy/docs/terminology#render" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/terminology#render" track-metadata-module="post">rendering</a>) tooling, and <a href="https://cloud.google.com/pubsub" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/pubsub" track-metadata-module="post">Pub/Sub</a> notifications to enable third-party integrations.</p><p><a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system" track-metadata-module="post">Connecting Google Cloud Deploy</a> to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple <i>`</i><i>gcloud beta deploy releases create`.</i><br/></p></div></paragraph-block></div><div><paragraph-block _nghost-c15=""><div _ngcontent-c15="" innerhtml="&lt;p&gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy&amp;#160; leverages &lt;a href=&#34;https://skaffold.dev/&#34; target=&#34;_blank&#34;&gt;Skaffold&lt;/a&gt;, allowing you to &lt;a href=&#34;https://cloud.google.com/deploy/docs/skaffold&#34;&gt;standardize your configuration&lt;/a&gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34;&gt;Helm&lt;/a&gt;, &lt;a href=&#34;https://kustomize.io/&#34; target=&#34;_blank&#34;&gt;Kustomize&lt;/a&gt;, &lt;a href=&#34;https://kpt.dev/&#34; target=&#34;_blank&#34;&gt;kpt&lt;/a&gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.&lt;/p&gt;&lt;p&gt;Finally, to facilitate other integrations, such as a &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing&#34;&gt;post-deployment test execution&lt;/a&gt; or &lt;a href=&#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management&#34;&gt;third party approval workflows&lt;/a&gt;, Google Cloud Deploy &lt;a href=&#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications&#34;&gt;emits Pub/Sub messages&lt;/a&gt; throughout a &lt;a href=&#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release&#34;&gt;release&amp;#8217;s lifecycle&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;&lt;h3&gt;The future&lt;/h3&gt;&lt;p&gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it&amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we&amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.&lt;/p&gt;&lt;p&gt;In the meantime, to get started with the Preview, check out the &lt;a href=&#34;https://cloud.google.com/deploy&#34;&gt;product page&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs&#34;&gt;documentation&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/deploy/docs/quickstart-basic&#34;&gt;quickstart&lt;/a&gt;, and &lt;a href=&#34;https://cloud.google.com/deploy/docs/tutorials&#34;&gt;tutorials&lt;/a&gt;. Finally, If you have feedback on Google Cloud Deploy, you can &lt;a href=&#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy&#34; target=&#34;_blank&#34;&gt;join the conversation&lt;/a&gt;. We look forward to hearing from you!&lt;br&gt;&lt;/p&gt;"><p>Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages <a href="https://skaffold.dev/" target="_blank" track-type="inline link" track-name="17" track-metadata-eventdetail="https://skaffold.dev" track-metadata-module="post">Skaffold</a>, allowing you to <a href="https://cloud.google.com/deploy/docs/skaffold" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/skaffold" track-metadata-module="post">standardize your configuration</a> between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (<a href="https://helm.sh/" target="_blank" track-type="inline link" track-name="19" track-metadata-eventdetail="https://helm.sh" track-metadata-module="post">Helm</a>, <a href="https://kustomize.io/" target="_blank" track-type="inline link" track-name="20" track-metadata-eventdetail="https://kustomize.io" track-metadata-module="post">Kustomize</a>, <a href="https://kpt.dev/" target="_blank" track-type="inline link" track-name="21" track-metadata-eventdetail="https://kpt.dev" track-metadata-module="post">kpt</a>). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.</p><p>Finally, to facilitate other integrations, such as a <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing" track-metadata-module="post">post-deployment test execution</a> or <a href="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management" track-metadata-module="post">third party approval workflows</a>, Google Cloud Deploy <a href="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/subscribe-deploy-notifications" track-metadata-module="post">emits Pub/Sub messages</a> throughout a <a href="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release" track-metadata-module="post">release’s lifecycle</a>.<br/></p><h3>The future</h3><p>Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.</p><p>In the meantime, to get started with the Preview, check out the <a href="https://cloud.google.com/deploy" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/deploy" track-metadata-module="post">product page</a>, <a href="https://cloud.google.com/deploy/docs" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/deploy/docs" track-metadata-module="post">documentation</a>, <a href="https://cloud.google.com/deploy/docs/quickstart-basic" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/quickstart-basic" track-metadata-module="post">quickstart</a>, and <a href="https://cloud.google.com/deploy/docs/tutorials" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/deploy/docs/tutorials" track-metadata-module="post">tutorials</a>. Finally, If you have feedback on Google Cloud Deploy, you can <a href="https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy" target="_blank" track-type="inline link" track-name="30" track-metadata-eventdetail="https://www.googlecloudcommunity.com" track-metadata-module="post">join the conversation</a>. We look forward to hearing from you!<br/></p></div></paragraph-block></div><div><article-video-block _nghost-c12=""><p _ngcontent-c12=""><iframe _ngcontent-c12="" allow="encrypted-media" allowfullscreen="" frameborder="0" height="100%" position="absolute" width="100%" src="https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1&amp;"></iframe></p></article-video-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c14=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;S. Bogdan&lt;/name&gt;&lt;title&gt;Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 22 Sep 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>2021 Accelerate State of DevOps report addresses burnout, team performance</title>
      <link>https://cloud.google.com/blog/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Over the past seven years, more than 32,000 professionals worldwide have taken part in the &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;Accelerate State of DevOps reports&lt;/a&gt;, making it the largest and longest-running research of its kind. Year over year, the Accelerate State of DevOps reports provide data-driven industry insights that examine the capabilities and practices that drive software delivery as well as operational and organizational performance. That is why Google Cloud’s DevOps Research and Assessment (DORA) team is very excited to announce our &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops/&#34;&gt;2021 Accelerate State of DevOps Report&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Our research continues to illustrate that excellence in software delivery and operational performance drives organizational performance in technology transformations. This year we also investigated the effects of SRE best practices, a secure software supply chain, quality documentation, and multicloud—all while gaining a deeper understanding of how this past year affected team’s culture and burnout.  &lt;/p&gt;&lt;p&gt;Read below to find some of the new findings from this year’s report:&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Software delivery performance metrics&lt;/h3&gt;&lt;p&gt;Based on key findings from previous Accelerate State of DevOps reports, we again used &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance&#34;&gt;four metrics&lt;/a&gt; to classify teams as elite, high, medium or low performers based on their software delivery: deployment frequency, lead time for changes, mean-time-to-restore, and change fail rate. This year we saw that elite performers continue to accelerate their pace of software delivery, increasing their lead time for changes from less than one day to less than one hour. Not only that, but elite performers deploy 973x more frequently than low performers, have a 6570x faster lead time to deploy, a 3x lower change failure rate, and an impressive 6570x faster time-to-recover from incidents when failure does happen. You read that right: compared to low performers, elite performers are continually able to empirically demonstrate organizational success with DevOps.&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_1.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;SODR_2021_1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_1.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;The fifth metric: from availability to reliability&lt;/h3&gt;&lt;p&gt;Historically we have measured availability rather than reliability, but because availability is a specific focus of reliability engineering, we’ve expanded our measure to reliability so that availability, latency, performance, and scalability are more broadly represented. Specifically, we asked respondents to rate their ability to meet or exceed their reliability targets. We found that teams with varying degrees of delivery performance see better outcomes when they also prioritize operational performance. &lt;/p&gt;&lt;h3&gt;2021 insights: the impact of reliability, COVID and secure software supply chains&lt;/h3&gt;&lt;p&gt;In addition to measuring the impact of DevOps adoption on software delivery performance, this year’s DORA report also revealed many other new trends. Here’s a sampling. &lt;/p&gt;&lt;p&gt;&lt;b&gt;1) A healthy team culture mitigates burnout during challenging times&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Respondents who worked from home because of the pandemic experienced more burnout than those who stayed in the office (a small portion of our sample). Inclusive teams with a generative culture were half as likely to experience burnout during the COVID-19 pandemic. &lt;/p&gt;&lt;p&gt;&lt;b&gt;2) The highest performers continue to raise the bar&lt;/b&gt;&lt;/p&gt;&lt;p&gt;For the first time, high and elite performers make up two-thirds of respondents—compared to the &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;2019 report&lt;/a&gt; where low and medium performers made up 56% of respondents. We can confidently say that as the industry continues to accelerate its adoption of DevOps principles teams see meaningful benefits as a result.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_2.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;SODR_2021_2.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_2.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;b&gt;3) SRE and DevOps are complementary philosophies &lt;/b&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Extending from its core principles, &lt;a href=&#34;https://sre.google/&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering (SRE)&lt;/a&gt; provides practical techniques, including the service level indicator/service level objective (SLI/SLO) metrics framework. The SRE framework offers definitions on practices and tooling that can enhance a team’s ability to consistently keep promises to their users. Teams that prioritize both delivery and operational excellence report the highest organizational performance. &lt;/p&gt;&lt;p&gt;To investigate this, we included &lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;operations&lt;/a&gt; questions in the survey for the first time this year. The evidence from the survey indicated teams who excel at modern operational practices are 1.4 times more likely to report greater software delivery and operational (SDO) performance  performance, and 1.8 times more likely to report better business outcomes.&lt;/p&gt;&lt;p&gt;&lt;b&gt;4) Cloud adoption continues to drive performance&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Teams continue to move workloads to the cloud and those that leverage &lt;a href=&#34;https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure&#34;&gt;all five capabilities&lt;/a&gt; of cloud see increases in SDO performance, as well as in organizational performance. Multicloud adoption is also on the rise so that teams can leverage the unique capabilities of each provider. In fact, respondents who use hybrid or multicloud were 1.6 times more likely to exceed their organizational performance targets. &lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_3.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;SODR_2021_3.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_3.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;b&gt;5) A secure software supply chain is both essential and drives performance &lt;/b&gt;&lt;p&gt;Security can no longer be an afterthought—it must be integrated throughout every stage of the software development lifecycle to build a &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/best-practices-and-tools-for-software-supply-chain-security&#34;&gt;secure software supply chain&lt;/a&gt;. Elite performers who met or exceeded their reliability targets were twice as likely to have shifted their security practices left, i.e., implemented security practices earlier on in the software development lifecycle, and deliver reliable software quickly, and safely. &lt;/p&gt;&lt;p&gt;&lt;b&gt;6) Good documentation is foundational for successfully implementing DevOps capabilities&lt;/b&gt;&lt;/p&gt;&lt;p&gt;For the first time, we measured the quality of internal documentation and its effect on other capabilities and practices. We found documentation is foundational for successfully implementing DevOps capabilities. Teams with high-quality documentation are 3.8x more likely to implement security best practices and 2.5x more likely to fully leverage the cloud to its fullest potential.&lt;/p&gt;&lt;h3&gt;Introducing the DevOps Awards&lt;/h3&gt;&lt;p&gt;Now that we have shared some of our DevOps best practices with you, we would love to hear about how you are transforming your organization with DevOps. In our first annual DevOps Awards, we’ll recognize Google Cloud customers that have improved their deployment frequency, successfully shifted left on security, or improved their change fail rate percentage, etc. Tell us about the positive impact that DevOps has had on your teams, customers, and organization. Enter your submission &lt;a href=&#34;https://cloud.google.com/awards/devops&#34;&gt;here&lt;/a&gt; today!&lt;/p&gt;&lt;p&gt;Thanks to everyone who took our 2021 survey. We hope this Accelerate State of DevOps report helps organizations of all sizes, industries, and regions improve their DevOps capabilities, and we look forward to hearing your thoughts and feedback. To learn more  about the report and implementing DevOps with Google cloud, check out the following resources:&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/devops/state-of-devops/&#34;&gt;Download the report&lt;/a&gt;&lt;br/&gt;&lt;/li&gt;&lt;li&gt;To find out more about how your organization stacks up against others in your industry, take the&lt;a href=&#34;https://www.devops-research.com/quickcheck.html&#34; target=&#34;_blank&#34;&gt;DevOps Quick Check&lt;/a&gt;&lt;br/&gt;&lt;/li&gt;&lt;li&gt;For customized DevOps solutions for your organization, check out our newly launched&lt;a href=&#34;http://cloud.google.com/camp&#34;&gt;CAMP website&lt;/a&gt;&lt;br/&gt;&lt;/li&gt;&lt;li&gt;Learn more about DevOps capabilities for &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;elite performance&lt;/a&gt;&lt;br/&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Dustin Smith&lt;/name&gt;&lt;title&gt;DORA Research Lead&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/SODR2021_1920x1080.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 21 Sep 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How Lowe’s SRE reduced its mean time to recovery (MTTR) by over 80 percent</title>
      <link>https://cloud.google.com/blog/products/devops-sre/how-lowes-improved-incident-response-processes-with-sre/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;Editor’s Note:&lt;/b&gt;In a &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices&#34;&gt;previous blog&lt;/a&gt;, we discussed how home improvement retailer Lowe’s was able to increase the number of releases it supports by &lt;a href=&#34;https://cloud.google.com/sre&#34;&gt;adopting Google’s Site Reliability Engineering (SRE) framework on Google Cloud&lt;/a&gt;. Lowe’s went from one release every two weeks to 20+ releases daily, helping meet its customer needs faster and more effectively. Today, the Lowe’s SRE team shares how they used SRE principles to decrease their mean-time-to-recovery (MTTR) by over 80 percent.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;The stakes of managing Lowes.com have never been higher, and that means spotting, troubleshooting and recovering from incidents as quickly as possible, so that customers can continue to do business on our site. &lt;/p&gt;&lt;p&gt;To do that, it’s crucial to have solid incident engineering practices in place. Resolving an incident means mitigating the impact and/or restoring the service to its previous condition. The average time it takes to do this is called mean time to recovery (MTTR). Tracking this metric helps us stay on top of the overall reliability of our systems at Lowe’s, while simultaneously improving the speed with which we recover. Our goal is to keep the MTTR metric as low as possible, so that failures don’t negatively impact our business. Here are the four areas we addressed to drive holistic improvement in our MTTR.&lt;/p&gt;&lt;h3&gt;Lowe’s incident reporting process&lt;/h3&gt;&lt;p&gt;To reduce MTTR, we created a seamless incident reporting process following SRE principles. Our incident reporting process is a workflow that starts at the time an incident occurs, and ends with an SRE captain who closes the action items after a postmortem report. With this approach, we are able to limit the number of critical incidents. The reporting process involves three core components: monitoring, alerting, and blameless postmortems.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Monitoring and alerting&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Having proper monitoring and alerting in place is crucial when it comes to incident management. Monitoring and alerting tools let you detect issues as soon as they occur, and notify the right person in the shortest possible time to take action. From a measurement standpoint, we track this as our mean time to acknowledge (MTTA). This is the average time it takes from when an alert is triggered, to when work on the issue begins.&lt;br/&gt;&lt;/p&gt;&lt;p&gt;At the time of an incident, our &lt;a href=&#34;https://cloud.google.com/monitoring&#34;&gt;monitoring and alerting tools&lt;/a&gt; notify the on-call SRE first responder via &lt;a href=&#34;https://cloud.google.com/monitoring/support/notification-options&#34;&gt;PagerDuty&lt;/a&gt; in the form of a phone call, text message and email. Our SRE software engineering team has done a lot of automation to enable various &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-sli-vs-slo-vs-sla&#34;&gt;Service Level Indicator (SLI) alerts and Service Level Agreement (SLA)&lt;/a&gt; notifications. The on-call SRE then initiates a triage call with our service/domain stakeholders to resolve the incident. As a result, we reduced our MTTA from 30 minutes in 2019, to one minute – a 97 percent decrease. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Blameless postmortems: learning from incidents&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A postmortem is a written record of an incident, its impact, the actions taken to resolve it, the root cause and the follow-up actions to prevent the incident from recurring (&lt;a href=&#34;https://sre.google/sre-book/example-postmortem/&#34; target=&#34;_blank&#34;&gt;see example here&lt;/a&gt;). A blameless postmortem builds on that and is a core part of an SRE culture, and our culture at Lowe’s. We ensure that individuals are not singled out, and the outcome for all postmortems are directed toward learnings and process improvement.&lt;/p&gt;&lt;p&gt;For us, the postmortem process is the biggest part of our incident workflow. When an SRE creates a new postmortem report, the first step is to conduct a &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/getting-the-most-out-of-shared-postmortems-cre-life-lessons&#34;&gt;postmortem session&lt;/a&gt; with domain stakeholders to review the report. The postmortem then goes into the review stage and gets reviewed by more stakeholders in our weekly postmortem meeting. In the final stage of this process, the SRE captain will close the report once everyone in the weekly meeting agrees that the report is complete.&lt;/p&gt;&lt;p&gt;To conduct a successful postmortem, it is critical to keep the focus on identifying gaps and issues with the system and operations processes, rather than an individual, and generate concrete actions to address the problems we’ve identified. To ensure this, we follow a couple of best practices:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;We start by gathering the facts from the person who identified the problem, and each SLI owner has to identify a gap or the next SLI upstream owner who created the impact for them.&lt;/li&gt;&lt;li&gt;Every SLI owner is provided full opportunity to present their case, and identifying the issue is done as a community exercise. &lt;/li&gt;&lt;li&gt;Once action items and process changes are identified, an owner is nominated to complete the actions, or they will volunteer. &lt;/li&gt;&lt;li&gt;For easy reference, we publish and store postmortems in our incident knowledge base. This process helps SREs continuously improve as future incidents arise. &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Continuous Improvement &lt;/b&gt;&lt;/p&gt;&lt;p&gt;Encouraging a culture of honest, transparent and direct feedback that you need for blameless postmortems is often an iterative process that needs sponsorship from executives, empowering incident captains to lead the entirety of the discussion and outcomes. Running successful postmortems, and completing action items from them, needs to be recognized and accounted for in SRE performance objective assessment. As shared in &lt;a href=&#34;https://sre.google/sre-book/postmortem-culture/&#34; target=&#34;_blank&#34;&gt;Google’s SRE book&lt;/a&gt;, the best practice is to ensure that writing effective postmortems is a rewarded and celebrated practice, with leadership’s acknowledgement and participation. This is possibly the hardest part to accomplish in an effective postmortem during a cultural transformation unless you have full buy-in from leadership.&lt;/p&gt;&lt;p&gt;However, it’s all well worth it. This process is a key part of how we were able to improve our MTTR over time—from two hours in 2019 to just 17 minutes! &lt;/p&gt;&lt;p&gt;Our SRE incident reporting process has also transformed how our company solves issues. By streamlining this workflow from alerting, to solving an issue, to blameless postmortems, we have reduced our MTTR by 82 percent and our MTTA by 97 percent. Most importantly, our team is learning from every incident and becoming better engineers as a result. Visit the &lt;a href=&#34;https://cloud.google.com/sre&#34;&gt;SRE Google Cloud website&lt;/a&gt; to learn more about implementing SRE best practices in the cloud.&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;&lt;i&gt;Acknowledgement&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Special thanks to Rahul Mohan Kola Kandy, Vivek Balivada, and the Digital SRE team at Lowe’s for contributing to this blog post.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/how-lowes-leverages-google-sre-practices/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/devops.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;How Lowe’s meets customer demand with Google SRE practices&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Lowe’s has adopted Google SRE practices to help developer and operations teams keep up with ecommerce demand.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Nishanth Prasad&lt;/name&gt;&lt;title&gt;Lead Software Engineer, Digital SRE, Lowe’s Companies, Inc.&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/gcp_ZPje3k8.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 07 Sep 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Artifact Registry: the next generation of Container Registry</title>
      <link>https://cloud.google.com/blog/products/application-development/understanding-artifact-registry-vs-container-registry/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Enterprise application teams need to manage more than just containers in their software supply chain. That’s why we created &lt;a href=&#34;https://cloud.google.com/artifact-registry&#34;&gt;Artifact Registry&lt;/a&gt;, a fully-managed service with support for both container images and non-container artifacts.&lt;/p&gt;&lt;p&gt;Artifact Registry improves and extends upon the existing capabilities of &lt;a href=&#34;https://cloud.google.com/container-registry&#34;&gt;Container Registry&lt;/a&gt;, such as customer-managed encryption keys, VPC-SC support, Pub/Sub notifications, and more, providing a foundation for major upgrades in security, scalability and control. While Container Registry is still available and will continue to be supported as a &lt;a href=&#34;https://cloud.google.com/blog/topics/inside-google-cloud/new-api-stability-tenets-govern-google-enterprise-apis&#34;&gt;Google Enterprise API&lt;/a&gt;, going forward new features will only be available in Artifact Registry, and Container Registry will only receive critical security fixes.&lt;/p&gt;&lt;p&gt;Below, we’ll highlight the key improvements Artifact Registry provides over Container Registry, as well as the steps to start using it today.&lt;/p&gt;&lt;h3&gt;A unified control plane for container, OS and language repositories&lt;/h3&gt;&lt;p&gt;Artifact Registry includes more than just container images: as a developer, you can store multiple artifact formats, including OS packages for Debian and RPM, as well as language packages for popular languages like Python, Java, and Node. In addition, you can manage them all from a single, unified interface. &lt;/p&gt;&lt;h3&gt;A more granular permission model with Cloud IAM&lt;/h3&gt;&lt;p&gt;Artifact Registry comes with fine-grained access control via &lt;a href=&#34;https://cloud.google.com/iam&#34;&gt;Cloud IAM&lt;/a&gt;. Unlike Container Registry, this allows you to control access on a per-repository basis, rather than all images stored in a project. This enables you to scope permissions as granularly as possible, for example to specific regions or environments as necessary.&lt;/p&gt;&lt;h3&gt;Repositories in the region of your choice&lt;/h3&gt;&lt;p&gt;Artifact Registry supports the creation of regional repositories, which allows you to put your artifacts and data directly in the location that they&#39;ll be used, allowing for higher availability and speed. In Container Registry, you’re limited to “multi-regions”: for example, the closest multi-region for Australia is Asia. However, with Artifact Registry’s regional support, you can create a repository directly in the Sydney data center.&lt;/p&gt;&lt;h3&gt;A pricing model that respects your region&lt;/h3&gt;&lt;p&gt;While Artifact Registry’s pricing is still based on a combination of network egress and storage usage, support for regional repositories means that you can choose in what region to host your container repositories. Although per unit storage costs are higher for Artifact Registry, optimizing the locations of your repositories to be hosted in the same region where they are used can result in cost savings, because any network traffic within the same region is not considered egress and is thus free.&lt;/p&gt;&lt;h3&gt;Part of a secure supply chain&lt;/h3&gt;&lt;p&gt;Artifact Registry was designed from the ground up to integrate into our suite of secure supply chain products. This means that it can optionally use &lt;a href=&#34;https://cloud.google.com/container-analysis/&#34;&gt;Container Analysis&lt;/a&gt; to scan your container images for vulnerabilities as they’re uploaded to Artifact Registry, and works directly with &lt;a href=&#34;https://cloud.google.com/binary-authorization&#34;&gt;Binary Authorization&lt;/a&gt; to secure your deployments.&lt;/p&gt;&lt;h3&gt;We’re here to help you migrate&lt;/h3&gt;&lt;p&gt;If you already use Container Registry, you can take advantage of all the current and upcoming features of container image storage with Artifact Registry by migrating to it. To help, we’ve prepared the following guides:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr&#34;&gt;Transitioning from Container Registry&lt;/a&gt; provides an overview of how to use Artifact Registry instead of Container Registry in a backwards-compatible way&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/docker/copy-from-gcr&#34;&gt;Copying images from Container Registry&lt;/a&gt; guide you to move container images from an existing repository to an Artifact Registry repository&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If you’re currently hosting your container images with a third party, you can begin using Artifact Registry directly, by following the instructions in our guide, &lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/docker/migrate-external-containers&#34;&gt;Migrating containers from a third-party registry&lt;/a&gt;, which shows you how to avoid rate limits on image pulls or third-party outages which can disrupt your builds and deployments.&lt;/p&gt;&lt;p&gt;And if you&#39;re just getting started storing container images, you can begin using Artifact Registry as your image repository right away. To learn how, check out &lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/docker/quickstart&#34;&gt;Artifact Registry quickstart for Docker&lt;/a&gt;, a guide to using Artifact Registry as a single location for managing private packages and Docker container images.&lt;/p&gt;&lt;h3&gt;Join our community &lt;/h3&gt;&lt;p&gt;Our Artifact Registry communities are also great resources to help answer your questions and for guidance on best practices: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Ask questions on Stack Overflow using the &lt;a href=&#34;https://stackoverflow.com/questions/tagged/google-artifact-registry&#34; target=&#34;_blank&#34;&gt;google-artifact-registry&lt;/a&gt; tag&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Visit the &lt;a href=&#34;https://googlecloud-community.slack.com/&#34; target=&#34;_blank&#34;&gt;Google Cloud Slack community&lt;/a&gt; and ask a question in the #artifact-registry channel. If you haven&#39;t already joined the Slack community, use &lt;a href=&#34;https://join.slack.com/t/googlecloud-community/shared_invite/zt-m973j990-IMij2Xh8qKPu7SaHfOcCFg&#34; target=&#34;_blank&#34;&gt;this form&lt;/a&gt; to sign up.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/application-development/artifact-registry-adds-node-python-and-java-repositories/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_Artifact_Registry.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Node, Python and Java repositories now available in Artifact Registry&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Expanded language support lets you store Java, Node and Python artifacts in Artifact Registry, for a more secure software supply chain.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Dustin Ingram&lt;/name&gt;&lt;title&gt;Senior Developer Advocate&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/gcp_Artifact_Registry.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 19 Aug 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Deploy Anthos on GKE with Terraform part 1: GitOps with Config Sync</title>
      <link>https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/anthos/config-management&#34;&gt;Anthos Config Management (ACM)&lt;/a&gt;offers cloud platform administrators a variety of techniques to streamline cluster configuration. One ACM feature, &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/config-sync-overview&#34;&gt;Config Sync&lt;/a&gt;, allows them to use a Git repository to create common configurations that are automatically applied on Kubernetes clusters in their fleet, bringing a familiar code review collaboration process to config management. Another ACM feature, &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller&#34;&gt;Policy Controller&lt;/a&gt;, enforces security guardrails in compliance with their organization’s requirements. This blog series explores these offerings and how to get started using them with Terraform.&lt;/p&gt;&lt;p&gt;Many platform administrators prefer &lt;a href=&#34;https://cloud.google.com/solutions/infrastructure-as-code&#34;&gt;Infrastructure as Code&lt;/a&gt; to achieve repeatable and predictable deployments. This also applies to configuring ACM features on Kubernetes clusters. &lt;/p&gt;&lt;p&gt;In the past, platform administrators who used Terraform lacked a smooth transition from HCL to modeling cluster configuration. They had to resort to manual processes that required additional temporary permissions granted to operators to complete provisioning.&lt;/p&gt;&lt;p&gt;The new &lt;a href=&#34;https://cloud.google.com/anthos/multicluster-management/reference/rest/v1beta/projects.locations.features&#34;&gt;GKEHub API&lt;/a&gt; and new resources enabled in &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs&#34; target=&#34;_blank&#34;&gt;Terraform Provider for Google Cloud Platform&lt;/a&gt; —&lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/gke_hub_feature&#34; target=&#34;_blank&#34;&gt;google_gke_hub_feature&lt;/a&gt;, &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/gke_hub_feature_membership&#34; target=&#34;_blank&#34;&gt;google_hub_feature_membership&lt;/a&gt; and &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/gke_hub_membership&#34; target=&#34;_blank&#34;&gt;google_gke_hub_membership—&lt;/a&gt;make it possible to automate last mile cluster configuration, including pointing it to a Git repository and turning on the Policy Controller.&lt;/p&gt;&lt;p&gt;For platform administrators, this solves previous challenges of modeling cluster configuration such as namespaces, services accounts, RBAC, in a Kubernetes idiomatic way, i.e. without the awkward Terraform HCL counterparts. Better still this natural, IaC approach improves auditability and transparency and reduces risk of misconfigurations or security gaps.&lt;/p&gt;&lt;p&gt;In this 3 part blog series, we’ll show how you can enable Anthos features on GKE. We’ll start with &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/config-sync-overview&#34;&gt;Config Sync&lt;/a&gt; to reconcile the cluster state with the specified Git repository. &lt;/p&gt;&lt;p&gt;Based on a GKE cluster resource in your Terraform configuration:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;You can then enable GKE Hub membership, and the &lt;b&gt;configmanagement&lt;/b&gt; feature:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Additional settings can then be configured for each of the features - &lt;b&gt;sync_repo&lt;/b&gt; to point at the repo storing your cluster configurations, &lt;b&gt;poliy_dir&lt;/b&gt; to point at the root of the repo to reconcile, and the specific &lt;b&gt;sync_branch&lt;/b&gt; in the repo.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Applying this configuration with Terraform will enable Config Sync and will automatically synchronize the state of the cluster with the repo, immediately creating the Kubernetes config objects on the cluster. Your pods, deployments, services and other native K8s objects will automatically be created. See this &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/concepts/configs&#34;&gt;article&lt;/a&gt; for more details on how to organize configs in a repo.&lt;/p&gt;&lt;p&gt;The cluster now is fully provisioned and requires no “last mile” configuration steps.&lt;/p&gt;&lt;p&gt;This &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/tree/master/examples/acm-terraform-blog-part1&#34; target=&#34;_blank&#34;&gt;repo&lt;/a&gt; provides a complete example of provisioning a cluster that is synchronized with a repo that contains a popular WordPress configuration. &lt;/p&gt;&lt;p&gt;In the next part of the series we’ll show you how you can use Terraform to configure another ACM feature - Policy Controller.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/anthos-config-management-config-controller-available-on-gke/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/GCP_Kubernetes_A.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Get in sync: Consistent Kubernetes with new Anthos Config Management features&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Anthos Config Management and Config Controller bring Kubernetes-style declarative policy and config management to GKE environments.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <author>&lt;name&gt;Alex Bulankou&lt;/name&gt;&lt;title&gt;Engineering Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/original_images/Anthos.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 16 Aug 2021 16:30:00 +0000</pubDate>
    </item>
    <item>
      <title>Get in sync: Consistent Kubernetes with new Anthos Config Management features</title>
      <link>https://cloud.google.com/blog/products/containers-kubernetes/anthos-config-management-config-controller-available-on-gke/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;From large digital-native powerhouses to midsized manufacturing firms, every company today is creating and deploying &lt;b&gt;more software to more places more often&lt;/b&gt;. &lt;a href=&#34;https://cloud.google.com/anthos/config-management&#34;&gt;Anthos Config Management&lt;/a&gt; lets you set and enforce consistent configurations and policies for your Kubernetes resources—wherever you build and run them—and manage Google Cloud services the same way. &lt;/p&gt;&lt;p&gt;Today, as a part of Anthos Config Management, we are introducing &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview&#34;&gt;Config Controller&lt;/a&gt;, a hosted service to provision and orchestrate Google Cloud resources. This service offers an API endpoint that can provision, actuate, and orchestrate Google Cloud resources the same way it manages Kubernetes resources. You don’t have to install or manage the components—or be an expert in Kubernetes resource management or GitOps—because Google Cloud will manage them for you. &lt;/p&gt;&lt;p&gt;Today, we’re also announcing that, in addition to using it for hybrid and multicloud use cases, Anthos Config Management is now available for Google Kubernetes Engine (GKE) as a standalone service. GKE customers can now take advantage of config and policy automation in Google Cloud at a low incremental per-cluster cost.&lt;/p&gt;&lt;p&gt;These announcements deliver a whole new approach to config and policy management—one that’s descriptive or &lt;i&gt;declarative&lt;/i&gt;, rather than procedural or &lt;i&gt;imperative&lt;/i&gt;. Let’s take a closer look.  &lt;/p&gt;&lt;h3&gt;Let Kubernetes automate your configs and policies &lt;/h3&gt;&lt;p&gt;Development teams need stable and secure environments to build apps quickly and deploy them easily. Today, platform teams often scramble to provision and configure the necessary infrastructure components, apps, and cloud services the same way—in many different places—and keep them all up-to-date, patched, and secure. &lt;/p&gt;&lt;p&gt;The struggle is real, and it’s not new. Platform administrators have been hand-crafting and partially automating configuration with new infrastructure-as-code languages and tools for years. We can spin up new containerized dev environments in minutes in the cloud and on-prem. We can push code to production hundreds of times a day with automated CI/CD processes. So why do configurations drift and fall out of sync with production? &lt;/p&gt;&lt;p&gt;Because it takes time and toil to develop a &lt;b&gt;consistent and automated way to &lt;i&gt;describe&lt;/i&gt; what we want, &lt;i&gt;create&lt;/i&gt; what we need, and repair what we break.&lt;/b&gt; The declarative &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes&#34;&gt;Kubernetes Resource Model (KRM)&lt;/a&gt; reduces this toil with a consistent way to define and update resources: describe what you want and Kubernetes makes it happen. ACM makes it even easier by adding &lt;b&gt;pre-built, opinionated config and policy automations&lt;/b&gt;, such as creating a &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/tutorials/landing-zone&#34;&gt;secure landing zone&lt;/a&gt; and provisioning a &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/tutorials/gke-cluster-blueprint&#34;&gt;GKE cluster from a blueprint&lt;/a&gt;. Blueprints help platform teams configure both Kubernetes and Google Cloud services the same way every time.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/GKE_cluster_from_a_blueprint.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;GKE cluster from a blueprint.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/GKE_cluster_from_a_blueprint.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Describe your intent with a single resource model&lt;/h3&gt;&lt;p&gt;The Kubernetes API server includes controllers that make sure your container infrastructure state always matches the state you declare in YAML. For example, Kubernetes can ensure that a load balancer and service proxy are always created, connected to the right pods, and configured properly. But KRM can manage more than just container infrastructure. You can use KRM to deploy and manage resources such as cloud databases, storage, and networks. It can also manage your custom-developed apps and services using &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34; target=&#34;_blank&#34;&gt;custom resource definitions&lt;/a&gt;. &lt;/p&gt;&lt;h3&gt;Create what you need from a single source of truth&lt;/h3&gt;&lt;p&gt;With Anthos Config Management, you declare and set configurations once and forget them. You don’t have to be an expert in KRM or GitOps-style configuration because the hosted Config Controller service takes care of it. Config Controller provisions infrastructure, apps, and cloud services; configures them to meet your desired intent; monitors them for configuration drift; and applies changes every time you push a new resource declaration to your Git repository. Config changes are as easy as a git push—and easily integrate with your development workflows. &lt;/p&gt;&lt;p&gt;Anthos Config Management uses &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/config-sync-overview&#34;&gt;Config Sync&lt;/a&gt; to continuously reconcile the state of your registered clusters and resources—that means any GKE, Anthos, or &lt;a href=&#34;https://cloud.google.com/anthos/multicluster-management/connect/registering-a-cluster&#34;&gt;other registered&lt;/a&gt; cluster—and makes sure unvetted changes are never pushed to live clusters. Anthos Config Management reduces the risk of dev or ops teams making any changes outside the Git source of truth by requiring code reviews and rolling back any breaking changes to a good working state. In short, using Anthos Config Management both encourages and enforces best practices.&lt;/p&gt;&lt;h3&gt;Repair what breaks for automated compliance&lt;/h3&gt;&lt;p&gt;Anthos Config Management’s &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller&#34;&gt;Policy Controller&lt;/a&gt; makes it easier to create and enforce fully programmable policies across all connected clusters. Policies act as guardrails to prevent any changes to configuration from violating your custom security, operational, or compliance controls. For example, you can set policies to actively block any non-compliant API requests, require every namespace to have a label, prevent pods from running privileged containers, restrict the types of storage volumes a container can mount, and more.&lt;/p&gt;&lt;p&gt;Policy Controller is based on the open source &lt;a href=&#34;https://open-policy-agent.github.io/gatekeeper/website/docs/&#34; target=&#34;_blank&#34;&gt;Open Policy Agent Gatekeeper&lt;/a&gt; project, augmented by Google Cloud with a ready-to-use &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/reference/constraint-template-library&#34;&gt;library of pre-built policies&lt;/a&gt; for the most common security and compliance controls. Customers can establish a secure baseline easily without deep expertise and ACM applies policies to a single cluster (e.g. GKE) or to a distributed set of Anthos clusters on-prem or in other cloud platforms. You can audit and add your own custom policies by allowing your security-savvy experts to &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/how-to/write-a-constraint-template&#34;&gt;create constraint templates&lt;/a&gt; which anyone &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/how-to/creating-constraints&#34;&gt;can invoke&lt;/a&gt; in different dev or production environments without learning how to write or manage policy code. The &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/how-to/auditing-constraints&#34;&gt;audit functionality&lt;/a&gt; included allows platform admins to audit all violations, simplifying compliance reviews.&lt;/p&gt;&lt;h3&gt;Configure and control every cluster consistently&lt;/h3&gt;&lt;p&gt;The hosted service, &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview&#34;&gt;Config Controller&lt;/a&gt;, which runs Config Connector, Config Sync, and Policy Controller for you, is available in Preview. Config Controller leverages &lt;a href=&#34;https://cloud.google.com/config-connector/docs/overview&#34;&gt;Config Connector,&lt;/a&gt; which lets you manage &lt;a href=&#34;https://cloud.google.com/config-connector/docs/reference/overview&#34;&gt;Google Cloud resources&lt;/a&gt; the same way you manage other Kubernetes resources, with continuous monitoring and self-healing. For example, you can ask Config Connector to create a Cloud SQL instance and a database. Config Connector can manage more than 60 Google Cloud resources, including Bigtable, BigQuery, Pub/Sub, Spanner, Cloud Storage, and Cloud Load Balancer.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/anthos_config_manager.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;anthos config manager.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/anthos_config_manager.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Once you’ve embraced a consistent resource model, using ACM to enforce configuration and policy automatically for individual resources, take the next step with blueprints. A &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/concepts/blueprints&#34;&gt;blueprint&lt;/a&gt; is a package of config and policy that documents an opinionated solution to deploy and manage &lt;b&gt;multiple&lt;/b&gt; resources at once. Blueprints capture best practices and policy guardrails, package them together, and let you deploy them as a complete solution to any Kubernetes clusters using Config Controller. Use Blueprints to manage multiple resources at once, or to create customized &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/tutorials/landing-zone&#34;&gt;landing zones&lt;/a&gt;—compliant, properly configured, and easily duplicated environments that meet your own best practice guidelines and that are properly networked and secured. &lt;/p&gt;&lt;p&gt;The Vienna Insurance Group uses Anthos Config Management in its Viesure Innovation Center, which it credits with improving its compliance posture.&lt;/p&gt;&lt;p&gt;&lt;i&gt;&#34;Google&#39;s Landing Zones and Config Controller equipped us with an extensive set of tools to set up our Google Cloud infrastructure quickly and securely. Their policy controllers are a powerful instrument for ensuring compliance for all our Google Cloud resources.&#34;&lt;/i&gt; —Rene Schakmann, Head of Technology at viesure innovation center GmbH&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;Get started today&lt;/h3&gt;&lt;p&gt;Anthos Config Management on GKE is generally available today. If you’re a GKE customer, you can also now use Anthos Config Management at a low incremental cost. By making it available to GKE customers, and offering it as a hosted, managed service for everyone, we’re making it easier than ever for you to leverage “KRM as a service” to simplify and secure Kubernetes resource management from the data center to the cloud.&lt;/p&gt;&lt;p&gt;To learn more about the technical details behind ACM, check out &lt;a href=&#34;https://kubernetespodcast.com/episode/154-gatekeeper-and-policy-controller/&#34; target=&#34;_blank&#34;&gt;this recent episode&lt;/a&gt; of the &lt;a href=&#34;https://kubernetespodcast.com/&#34; target=&#34;_blank&#34;&gt;Kubernetes Podcast from Google&lt;/a&gt; with the TL for Policy Controller, Max Smythe.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;I do declare! Infrastructure automation with Configuration as Data&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Configuration as Data enables operational consistency, security, and velocity on Google Cloud with products like Config Connector.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;h3&gt;Describe your intent with a single resource model&lt;/h3&gt;&lt;p&gt;The Kubernetes API server includes controllers that make sure your container infrastructure state always matches the state you declare in YAML. For example, Kubernetes can ensure that a load balancer and service proxy are always created, connected to the right pods, and configured properly. But KRM can manage more than just container infrastructure. You can use KRM to deploy and manage resources such as cloud databases, storage, and networks. It can also manage your custom-developed apps and services using &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/&#34; target=&#34;_blank&#34;&gt;custom resource definitions&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;h3&gt;Create what you need from a single source of truth&lt;/h3&gt;&lt;p&gt;With Anthos Config Management, you declare and set configurations once and forget them. You don&amp;#8217;t have to be an expert in KRM or GitOps-style configuration because the hosted Config Controller service takes care of it. Config Controller provisions infrastructure, apps, and cloud services; configures them to meet your desired intent; monitors them for configuration drift; and applies changes every time you push a new resource declaration to your Git repository. Config changes are as easy as a git push&amp;#8212;and easily integrate with your development workflows.&amp;#160;&lt;/p&gt;&lt;p&gt;Anthos Config Management uses &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/config-sync-overview&#34;&gt;Config Sync&lt;/a&gt; to continuously reconcile the state of your registered clusters and resources&amp;#8212;that means any GKE, Anthos, or &lt;a href=&#34;https://cloud.google.com/anthos/multicluster-management/connect/registering-a-cluster&#34;&gt;other registered&lt;/a&gt; cluster&amp;#8212;and makes sure unvetted changes are never pushed to live clusters. Anthos Config Management reduces the risk of dev or ops teams making any changes outside the Git source of truth by requiring code reviews and rolling back any breaking changes to a good working state. In short, using Anthos Config Management both encourages and enforces best practices.&lt;/p&gt;&lt;h3&gt;Repair what breaks for automated compliance&lt;/h3&gt;&lt;p&gt;Anthos Config Management&amp;#8217;s &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller&#34;&gt;Policy Controller&lt;/a&gt; makes it easier to create and enforce fully programmable policies across all connected clusters. Policies act as guardrails to prevent any changes to configuration from violating your custom security, operational, or compliance controls. For example, you can set policies to actively block any non-compliant API requests, require every namespace to have a label, prevent pods from running privileged containers, restrict the types of storage volumes a container can mount, and more.&lt;/p&gt;&lt;p&gt;Policy Controller is based on the open source &lt;a href=&#34;https://open-policy-agent.github.io/gatekeeper/website/docs/&#34; target=&#34;_blank&#34;&gt;Open Policy Agent Gatekeeper&lt;/a&gt; project, augmented by Google Cloud with a ready-to-use &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/reference/constraint-template-library&#34;&gt;library of pre-built policies&lt;/a&gt; for the most common security and compliance controls. Customers can establish a secure baseline easily without deep expertise and ACM applies policies to a single cluster (e.g. GKE) or to a distributed set of Anthos clusters on-prem or in other cloud platforms. You can audit and add your own custom policies by allowing your security-savvy experts to &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/how-to/write-a-constraint-template&#34;&gt;create constraint templates&lt;/a&gt; which anyone &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/how-to/creating-constraints&#34;&gt;can invoke&lt;/a&gt; in different dev or production environments without learning how to write or manage policy code. The &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/how-to/auditing-constraints&#34;&gt;audit functionality&lt;/a&gt; included allows platform admins to audit all violations, simplifying compliance reviews.&lt;/p&gt;&lt;h3&gt;Configure and control every cluster consistently&lt;/h3&gt;&lt;p&gt;The hosted service, &lt;a href=&#34;https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview&#34;&gt;Config Controller&lt;/a&gt;, which runs Config Connector, Config Sync, and Policy Controller for you, is available in Preview. Config Controller leverages &lt;a href=&#34;https://cloud.google.com/config-connector/docs/overview&#34;&gt;Config Connector,&lt;/a&gt; which lets you manage &lt;a href=&#34;https://cloud.google.com/config-connector/docs/reference/overview&#34;&gt;Google Cloud resources&lt;/a&gt; the same way you manage other Kubernetes resources, with continuous monitoring and self-healing. For example, you can ask Config Connector to create a Cloud SQL instance and a database. Config Connector can manage more than 60 Google Cloud resources, including Bigtable, BigQuery, Pub/Sub, Spanner, Cloud Storage, and Cloud Load Balancer.&lt;/p&gt;"><h3>Describe your intent with a single resource model</h3><p>The Kubernetes API server includes controllers that make sure your container infrastructure state always matches the state you declare in YAML. For example, Kubernetes can ensure that a load balancer and service proxy are always created, connected to the right pods, and configured properly. But KRM can manage more than just container infrastructure. You can use KRM to deploy and manage resources such as cloud databases, storage, and networks. It can also manage your custom-developed apps and services using <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://kubernetes.io" track-metadata-module="post">custom resource definitions</a>. </p><h3>Create what you need from a single source of truth</h3><p>With Anthos Config Management, you declare and set configurations once and forget them. You don’t have to be an expert in KRM or GitOps-style configuration because the hosted Config Controller service takes care of it. Config Controller provisions infrastructure, apps, and cloud services; configures them to meet your desired intent; monitors them for configuration drift; and applies changes every time you push a new resource declaration to your Git repository. Config changes are as easy as a git push—and easily integrate with your development workflows. </p><p>Anthos Config Management uses <a href="https://cloud.google.com/anthos-config-management/docs/config-sync-overview" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/anthos-config-management/docs/config-sync-overview" track-metadata-module="post">Config Sync</a> to continuously reconcile the state of your registered clusters and resources—that means any GKE, Anthos, or <a href="https://cloud.google.com/anthos/multicluster-management/connect/registering-a-cluster" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/anthos/multicluster-management/connect/registering-a-cluster" track-metadata-module="post">other registered</a> cluster—and makes sure unvetted changes are never pushed to live clusters. Anthos Config Management reduces the risk of dev or ops teams making any changes outside the Git source of truth by requiring code reviews and rolling back any breaking changes to a good working state. In short, using Anthos Config Management both encourages and enforces best practices.</p><h3>Repair what breaks for automated compliance</h3><p>Anthos Config Management’s <a href="https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller" track-metadata-module="post">Policy Controller</a> makes it easier to create and enforce fully programmable policies across all connected clusters. Policies act as guardrails to prevent any changes to configuration from violating your custom security, operational, or compliance controls. For example, you can set policies to actively block any non-compliant API requests, require every namespace to have a label, prevent pods from running privileged containers, restrict the types of storage volumes a container can mount, and more.</p><p>Policy Controller is based on the open source <a href="https://open-policy-agent.github.io/gatekeeper/website/docs/" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://open-policy-agent.github.io" track-metadata-module="post">Open Policy Agent Gatekeeper</a> project, augmented by Google Cloud with a ready-to-use <a href="https://cloud.google.com/anthos-config-management/docs/reference/constraint-template-library" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/anthos-config-management/docs/reference/constraint-template-library" track-metadata-module="post">library of pre-built policies</a> for the most common security and compliance controls. Customers can establish a secure baseline easily without deep expertise and ACM applies policies to a single cluster (e.g. GKE) or to a distributed set of Anthos clusters on-prem or in other cloud platforms. You can audit and add your own custom policies by allowing your security-savvy experts to <a href="https://cloud.google.com/anthos-config-management/docs/how-to/write-a-constraint-template" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/anthos-config-management/docs/how-to/write-a-constraint-template" track-metadata-module="post">create constraint templates</a> which anyone <a href="https://cloud.google.com/anthos-config-management/docs/how-to/creating-constraints" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/anthos-config-management/docs/how-to/creating-constraints" track-metadata-module="post">can invoke</a> in different dev or production environments without learning how to write or manage policy code. The <a href="https://cloud.google.com/anthos-config-management/docs/how-to/auditing-constraints" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/anthos-config-management/docs/how-to/auditing-constraints" track-metadata-module="post">audit functionality</a> included allows platform admins to audit all violations, simplifying compliance reviews.</p><h3>Configure and control every cluster consistently</h3><p>The hosted service, <a href="https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview" track-metadata-module="post">Config Controller</a>, which runs Config Connector, Config Sync, and Policy Controller for you, is available in Preview. Config Controller leverages <a href="https://cloud.google.com/config-connector/docs/overview" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/config-connector/docs/overview" track-metadata-module="post">Config Connector,</a> which lets you manage <a href="https://cloud.google.com/config-connector/docs/reference/overview" track-type="inline link" track-name="17" track-metadata-eventdetail="https://cloud.google.com/config-connector/docs/reference/overview" track-metadata-module="post">Google Cloud resources</a> the same way you manage other Kubernetes resources, with continuous monitoring and self-healing. For example, you can ask Config Connector to create a Cloud SQL instance and a database. Config Connector can manage more than 60 Google Cloud resources, including Bigtable, BigQuery, Pub/Sub, Spanner, Cloud Storage, and Cloud Load Balancer.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Jeff Reed&lt;/name&gt;&lt;title&gt;VP of Product, GKE and Anthos&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/GCP_Kubernetes_A.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 03 Aug 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Securing the software development lifecycle with Cloud Build and SLSA</title>
      <link>https://cloud.google.com/blog/products/application-development/google-introduces-slsa-framework/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;One of the biggest challenges for software developers is the need to make informed choices about the external software and products they use in their own software systems. Evaluating whether a given system is appropriately secured can be challenging, especially if it’s external or owned by a third party.&lt;/p&gt;&lt;p&gt;This so-called software supply chain has been under increasing scrutiny in recent years, with attacks on software systems being responsible for damages to both public and private interests. In collaboration with the &lt;a href=&#34;https://openssf.org/&#34; target=&#34;_blank&#34;&gt;OpenSSF&lt;/a&gt;, Google has proposed Supply-chain Levels for Software Artifacts (SLSA). The new &lt;a href=&#34;https://slsa.dev/&#34; target=&#34;_blank&#34;&gt;SLSA&lt;/a&gt; framework formalizes criteria around software supply chain integrity, to help the industry and open-source ecosystem secure the software development lifecycle.&lt;/p&gt;&lt;h3&gt;Secure your own software development lifecycle&lt;/h3&gt;&lt;p&gt;SLSA is not just for the public software supply chain. You can also apply these same levels, originally inspired by Google’s internal framework for secure software delivery, to your own software development life cycle.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;software development lifecycle.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/software_development_life.0480027109600294.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Each level of SLSA represents an incremental step towards a more secure software supply chain, adding additional security guidelines to address the most common threats to source and build integrity. Nor are these guidelines Google-specific: they are developed by the security extended community and established by consensus to be adopted amongst the wider industry.&lt;/p&gt;&lt;p&gt;Although SLSA is a new framework, many of the security guidelines it advocates for are already available for and adopted by consumers today.&lt;/p&gt;&lt;h3&gt;Understanding SLSA&lt;/h3&gt;&lt;p&gt;The SLSA framework introduces a number of new tools and concepts for securing the software development lifecycle:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Artifact - any file produced as the result of a build pipeline, such as container images, language packages, compiled binaries, etc.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Provenance - metadata about how an artifact was built, including the build process, top-level source, and dependencies&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Digest - the result of a cryptographic hash function which produces a fixed-size value uniquely identifying an artifact, such as a SHA-256 hash of a container image&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Attestation - a cryptographically signed file recording the provenance of the build pipeline at the time a specific artifact or set of artifacts was produced&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Attestor - any system or process that produces an attestation, often included as part of a build pipeline after artifact creation and prior to deployment&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Immutable references - an identifier, such as a URL, that is guaranteed to always point to the same, immutable artifact, such as a specific container image or language package&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Build integrity - the verification of the output of a build pipeline via attestations&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;When used in combination, these represent a build pipeline that adheres to the standards set by the SLSA framework.&lt;/p&gt;&lt;h3&gt;Cloud Build supports SLSA 1&lt;/h3&gt;&lt;p&gt;If you use &lt;a href=&#34;https://cloud.google.com/build&#34;&gt;Cloud Build&lt;/a&gt;, Google Cloud’s hosted CI/CD platform, you are likely already operating a software development lifecycle at SLSA 1, the first step in securing your software delivery pipeline. That’s because by default, Cloud Build allows you to create an automated build pipeline, and because any Cloud Build pipeline automatically generates provenance. While provenance for Cloud Build has been available for quite some time, it hasn’t been widely used to verify build pipelines.&lt;/p&gt;&lt;p&gt;Having a software supply chain at SLSA 1 does not entirely protect against tampering, but it does offer a basic level of code source identification and may aid in vulnerability management, protecting against software delivery that is not a product of the CI/CD system. At the same time, Cloud Build represents a foundation for a hosted software build system upon which you can reach higher SLSA levels, using techniques like verifiable source control, automatically verified provenance, and tools like &lt;a href=&#34;https://cloud.google.com/binary-authorization&#34;&gt;Binary Authorization&lt;/a&gt;.   &lt;/p&gt;&lt;h3&gt;You can start now&lt;/h3&gt;&lt;p&gt;By creating a build process that’s fully automated, mandating the use of a build system for production workflows, and by building your software pipeline with Cloud Build, you can have a SLSA 1 supply chain right from the start. &lt;/p&gt;&lt;p&gt;To get started today, you can follow the Cloud Build quickstart for &lt;a href=&#34;https://cloud.google.com/build/docs/quickstart-build&#34;&gt;building a Docker image and pushing the image to Artifact Registry&lt;/a&gt;, followed by the quickstart for &lt;a href=&#34;https://cloud.google.com/build/docs/quickstart-deploy&#34;&gt;deploying that containerized application to Cloud Run&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For more details on SLSA, you can read more here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://slsa.dev/&#34; target=&#34;_blank&#34;&gt;SLSA: Supply-chain Levels for Software Artifacts&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html&#34; target=&#34;_blank&#34;&gt;Introducing SLSA, an End-to-End Framework for Supply Chain Integrity&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;i&gt;Want to learn more about how you as a developer can help improve the security of your software? Today, we’re hosting &lt;a href=&#34;https://cloudonair.withgoogle.com/events/container-security&#34; target=&#34;_blank&#34;&gt;Building trust in your software supply chain&lt;/a&gt;, which explores this topic in depth. Click here to &lt;a href=&#34;https://cloudonair.withgoogle.com/events/container-security&#34; target=&#34;_blank&#34;&gt;register&lt;/a&gt; for the live event or to watch it on demand.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/identity-security/cloud-ciso-perspectives-june-2021/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Datastorage_8NMQKRy.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Cloud CISO Perspectives: June 2021&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Google Cloud CISO Phil Venables shares his thoughts on ransomware, software supply chains, and RSA retrospectives.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Each level of SLSA represents an incremental step towards a more secure software supply chain, adding additional security guidelines to address the most common threats to source and build integrity. Nor are these guidelines Google-specific: they are developed by the security extended community and established by consensus to be adopted amongst the wider industry.&lt;/p&gt;&lt;p&gt;Although SLSA is a new framework, many of the security guidelines it advocates for are already available for and adopted by consumers today.&lt;/p&gt;&lt;h3&gt;Understanding SLSA&lt;/h3&gt;&lt;p&gt;The SLSA framework introduces a number of new tools and concepts for securing the software development lifecycle:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Artifact - any file produced as the result of a build pipeline, such as container images, language packages, compiled binaries, etc.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Provenance - metadata about how an artifact was built, including the build process, top-level source, and dependencies&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Digest - the result of a cryptographic hash function which produces a fixed-size value uniquely identifying an artifact, such as a SHA-256 hash of a container image&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Attestation - a cryptographically signed file recording the provenance of the build pipeline at the time a specific artifact or set of artifacts was produced&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Attestor - any system or process that produces an attestation, often included as part of a build pipeline after artifact creation and prior to deployment&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Immutable references - an identifier, such as a URL, that is guaranteed to always point to the same, immutable artifact, such as a specific container image or language package&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Build integrity - the verification of the output of a build pipeline via attestations&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;When used in combination, these represent a build pipeline that adheres to the standards set by the SLSA framework.&lt;/p&gt;&lt;h3&gt;Cloud Build supports SLSA 1&lt;/h3&gt;&lt;p&gt;If you use &lt;a href=&#34;https://cloud.google.com/build&#34;&gt;Cloud Build&lt;/a&gt;, Google Cloud&amp;#8217;s hosted CI/CD platform, you are likely already operating a software development lifecycle at SLSA 1, the first step in securing your software delivery pipeline. That&amp;#8217;s because by default, Cloud Build allows you to create an automated build pipeline, and because any Cloud Build pipeline automatically generates provenance. While provenance for Cloud Build has been available for quite some time, it hasn&amp;#8217;t been widely used to verify build pipelines.&lt;/p&gt;&lt;p&gt;Having a software supply chain at SLSA 1 does not entirely protect against tampering, but it does offer a basic level of code source identification and may aid in vulnerability management, protecting against software delivery that is not a product of the CI/CD system. At the same time, Cloud Build represents a foundation for a hosted software build system upon which you can reach higher SLSA levels, using techniques like verifiable source control, automatically verified provenance, and tools like &lt;a href=&#34;https://cloud.google.com/binary-authorization&#34;&gt;Binary Authorization&lt;/a&gt;.&amp;#160;&amp;#160;&amp;#160;&lt;/p&gt;&lt;h3&gt;You can start now&lt;/h3&gt;&lt;p&gt;By creating a build process that&amp;#8217;s fully automated, mandating the use of a build system for production workflows, and by building your software pipeline with Cloud Build, you can have a SLSA 1 supply chain right from the start.&amp;#160;&lt;/p&gt;&lt;p&gt;To get started today, you can follow the Cloud Build quickstart for &lt;a href=&#34;https://cloud.google.com/build/docs/quickstart-build&#34;&gt;building a Docker image and pushing the image to Artifact Registry&lt;/a&gt;, followed by the quickstart for &lt;a href=&#34;https://cloud.google.com/build/docs/quickstart-deploy&#34;&gt;deploying that containerized application to Cloud Run&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;For more details on SLSA, you can read more here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;https://slsa.dev/&#34; target=&#34;_blank&#34;&gt;SLSA: Supply-chain Levels for Software Artifacts&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html&#34; target=&#34;_blank&#34;&gt;Introducing SLSA, an End-to-End Framework for Supply Chain Integrity&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;i&gt;Want to learn more about how you as a developer can help improve the security of your software? Today, we&amp;#8217;re hosting &lt;a href=&#34;https://cloudonair.withgoogle.com/events/container-security&#34; target=&#34;_blank&#34;&gt;Building trust in your software supply chain&lt;/a&gt;, which explores this topic in depth. Click here to &lt;a href=&#34;https://cloudonair.withgoogle.com/events/container-security&#34; target=&#34;_blank&#34;&gt;register&lt;/a&gt; for the live event or to watch it on demand.&lt;/i&gt;&lt;/p&gt;"><p>Each level of SLSA represents an incremental step towards a more secure software supply chain, adding additional security guidelines to address the most common threats to source and build integrity. Nor are these guidelines Google-specific: they are developed by the security extended community and established by consensus to be adopted amongst the wider industry.</p><p>Although SLSA is a new framework, many of the security guidelines it advocates for are already available for and adopted by consumers today.</p><h3>Understanding SLSA</h3><p>The SLSA framework introduces a number of new tools and concepts for securing the software development lifecycle:</p><ul><li><p>Artifact - any file produced as the result of a build pipeline, such as container images, language packages, compiled binaries, etc.</p></li><li><p>Provenance - metadata about how an artifact was built, including the build process, top-level source, and dependencies</p></li><li><p>Digest - the result of a cryptographic hash function which produces a fixed-size value uniquely identifying an artifact, such as a SHA-256 hash of a container image</p></li><li><p>Attestation - a cryptographically signed file recording the provenance of the build pipeline at the time a specific artifact or set of artifacts was produced</p></li><li><p>Attestor - any system or process that produces an attestation, often included as part of a build pipeline after artifact creation and prior to deployment</p></li><li><p>Immutable references - an identifier, such as a URL, that is guaranteed to always point to the same, immutable artifact, such as a specific container image or language package</p></li><li><p>Build integrity - the verification of the output of a build pipeline via attestations</p></li></ul><p>When used in combination, these represent a build pipeline that adheres to the standards set by the SLSA framework.</p><h3>Cloud Build supports SLSA 1</h3><p>If you use <a href="https://cloud.google.com/build" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/build" track-metadata-module="post">Cloud Build</a>, Google Cloud’s hosted CI/CD platform, you are likely already operating a software development lifecycle at SLSA 1, the first step in securing your software delivery pipeline. That’s because by default, Cloud Build allows you to create an automated build pipeline, and because any Cloud Build pipeline automatically generates provenance. While provenance for Cloud Build has been available for quite some time, it hasn’t been widely used to verify build pipelines.</p><p>Having a software supply chain at SLSA 1 does not entirely protect against tampering, but it does offer a basic level of code source identification and may aid in vulnerability management, protecting against software delivery that is not a product of the CI/CD system. At the same time, Cloud Build represents a foundation for a hosted software build system upon which you can reach higher SLSA levels, using techniques like verifiable source control, automatically verified provenance, and tools like <a href="https://cloud.google.com/binary-authorization" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/binary-authorization" track-metadata-module="post">Binary Authorization</a>.   </p><h3>You can start now</h3><p>By creating a build process that’s fully automated, mandating the use of a build system for production workflows, and by building your software pipeline with Cloud Build, you can have a SLSA 1 supply chain right from the start. </p><p>To get started today, you can follow the Cloud Build quickstart for <a href="https://cloud.google.com/build/docs/quickstart-build" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/build/docs/quickstart-build" track-metadata-module="post">building a Docker image and pushing the image to Artifact Registry</a>, followed by the quickstart for <a href="https://cloud.google.com/build/docs/quickstart-deploy" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/build/docs/quickstart-deploy" track-metadata-module="post">deploying that containerized application to Cloud Run</a>.</p><p>For more details on SLSA, you can read more here:</p><ul><li><a href="https://slsa.dev/" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://slsa.dev" track-metadata-module="post">SLSA: Supply-chain Levels for Software Artifacts</a></li><li><a href="https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://security.googleblog.com" track-metadata-module="post">Introducing SLSA, an End-to-End Framework for Supply Chain Integrity</a></li></ul><p><i>Want to learn more about how you as a developer can help improve the security of your software? Today, we’re hosting <a href="https://cloudonair.withgoogle.com/events/container-security" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloudonair.withgoogle.com" track-metadata-module="post">Building trust in your software supply chain</a>, which explores this topic in depth. Click here to <a href="https://cloudonair.withgoogle.com/events/container-security" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloudonair.withgoogle.com" track-metadata-module="post">register</a> for the live event or to watch it on demand.</i></p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Dustin Ingram&lt;/name&gt;&lt;title&gt;Senior Developer Advocate&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Security_BlogHeader_B_epmyJP1.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 29 Jul 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Introducing Cloud Build private pools: Secure CI/CD for private networks</title>
      <link>https://cloud.google.com/blog/products/devops-sre/cloud-build-private-pools-offers-cicd-for-private-networks/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;A recent &lt;a href=&#34;https://devops.com/survey-shows-mounting-devops-frustration-and-costs/&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; found that developers spend 39% of their time managing the DevOps infrastructure that powers their continuous integration (CI) and continuous delivery (CD) pipelines. Unreliable availability, manual provisioning, limited scaling, breaking upgrades, long queue times, and high fixed costs all slow down development and take valuable time and focus away from DevOps teams. And while cloud-based CI/CD solutions can solve many of these friction points, they largely only work with cloud-hosted resources. &lt;/p&gt;&lt;p&gt;That’s why we’re excited to announce that starting today, you can take advantage of serverless build environments within your own private network, with new &lt;b&gt;Cloud Build private pools&lt;/b&gt;. Launched in 2018, &lt;a href=&#34;https://cloud.google.com/build&#34;&gt;Cloud Build&lt;/a&gt; has helped thousands of customers modernize their CI/CD workloads to run on fully managed, secure, pay-as-you-go ‘workers’ with no infrastructure to manage. &lt;/p&gt;&lt;p&gt;Cloud Build offers on-demand auto-scaling capabilities, active build minute billing, all with no infrastructure to manage. The new private pools feature augments Cloud Build with secure, fully managed CI/CD and DevOps workflow automation that uses network peering to connect into your private networks. Private pools also unlocks a host of new customization options such as new machine types, higher maximum concurrency, regional builds, and network configuration options.&lt;/p&gt;&lt;p&gt;With Cloud Build private pools, you get the benefits of a cloud-hosted, fully managed CI/CD product while meeting enterprise security and compliance requirements—even for highly regulated industries like finance, healthcare, retail, and others. For instance, you can trigger fully managed DevOps workflows from source-code repositories hosted in private networks, including Github Enterprise.&lt;/p&gt;&lt;p&gt;With private pools, Cloud Build now supports:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;VPC Peering&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;VPC-SC&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Static IP ranges&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;No public IPs&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Org policy enforcement&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cross-project builds&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Build from private source repositories with first class integrations, including Github Enterprise&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Regionalization in 15 regions across the US, EU, Asia, Australia, and South America&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hundreds of concurrent builds per pool&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;15 machine types&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;And while designed primarily for private networking use cases, private pools work just as well with resources in Google Cloud, if you’re interested in trying out new features like higher concurrency or additional machine types.&lt;/p&gt;&lt;h3&gt;Same Cloud Build, new build environment&lt;/h3&gt;&lt;p&gt;Private pools introduces a new build environment for executing your builds with Cloud Build while maintaining a consistent product and API experience. All the same great features of Cloud Build are available with private pools, including fully managed workers, pay-as-you-go pricing, Cloud Console UI, source repo integrations, IAM permissions, Secret Manager and Pub/Sub integrations, and native support for Google Cloud runtimes like Google Kubernetes Engine (GKE), Cloud Run, Cloud Functions, App Engine, and Firebase.&lt;/p&gt;&lt;p&gt;Running builds on a private pool is as easy as creating the pool and setting it as your &lt;a href=&#34;https://cloud.google.com/build/docs/private-pools/run-builds-in-private-pool&#34;&gt;build environment in your cloudbuild.yaml config file&lt;/a&gt;. Private networking is optionally configured via Service Networking by &lt;a href=&#34;https://cloud.google.com/build/docs/private-pools/set-up-private-pool-environment#setup-private-connection&#34;&gt;peering your private pool to your customer-managed VPC&lt;/a&gt; and supports both peered and shared VPCs.&lt;/p&gt;&lt;p&gt;Running your first build is easy:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We’re excited to share private pools with you, so you can enjoy the secure, fully managed Cloud Build developer automation platform from your private network. The private pools feature is generally available today, and we look forward to introducing per-trigger service accounts and approval gates soon. To get started, try the &lt;a href=&#34;https://cloud.google.com/build/docs/private-pools/quickstart-private-pools&#34;&gt;quickstart&lt;/a&gt; or read the &lt;a href=&#34;https://cloud.google.com/build/docs/private-pools/private-pools-overview&#34;&gt;overview documentation&lt;/a&gt; for more details.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;Want to learn more about Cloud Build, and how to use it to improve the security of your software supply chain? On July 29 event &lt;a href=&#34;https://cloudonair.withgoogle.com/events/container-security&#34; target=&#34;_blank&#34;&gt;Building trust in your software supply chain&lt;/a&gt; explores this topic in depth. Click here to &lt;a href=&#34;https://cloudonair.withgoogle.com/events/container-security&#34; target=&#34;_blank&#34;&gt;register&lt;/a&gt; for the live event or to watch it on demand.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/application-development/forgerock-developers-stay-productive-with-google-cloud/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/appdev.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;DevOps on Google Cloud: tools to speed up software development velocity&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Google Cloud’s application development and continuous integration/continuous delivery (CI/CD) tools help ForgeRock developers stay produc...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;A recent &lt;a href=&#34;https://devops.com/survey-shows-mounting-devops-frustration-and-costs/&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; found that developers spend 39% of their time managing the DevOps infrastructure that powers their continuous integration (CI) and continuous delivery (CD) pipelines. Unreliable availability, manual provisioning, limited scaling, breaking upgrades, long queue times, and high fixed costs all slow down development and take valuable time and focus away from DevOps teams. And while cloud-based CI/CD solutions can solve many of these friction points, they largely only work with cloud-hosted resources.&amp;#160;&lt;/p&gt;&lt;p&gt;That&amp;#8217;s why we&amp;#8217;re excited to announce that starting today, you can take advantage of serverless build environments within your own private network, with new &lt;b&gt;Cloud Build private pools&lt;/b&gt;. Launched in 2018, &lt;a href=&#34;https://cloud.google.com/build&#34;&gt;Cloud Build&lt;/a&gt;&amp;#160;has helped thousands of customers modernize their CI/CD workloads to run on fully managed, secure, pay-as-you-go &amp;#8216;workers&amp;#8217; with no infrastructure to manage.&amp;#160;&lt;/p&gt;&lt;p&gt;Cloud Build offers on-demand auto-scaling capabilities, active build minute billing, all with no infrastructure to manage. The new private pools feature augments Cloud Build with secure, fully managed CI/CD and DevOps workflow automation that uses network peering to connect into your private networks. Private pools also unlocks a host of new customization options such as new machine types, higher maximum concurrency, regional builds, and network configuration options.&lt;/p&gt;&lt;p&gt;With Cloud Build private pools, you get the benefits of a cloud-hosted, fully managed CI/CD product while meeting enterprise security and compliance requirements&amp;#8212;even for highly regulated industries like finance, healthcare, retail, and others. For instance, you can trigger fully managed DevOps workflows from source-code repositories hosted in private networks, including Github Enterprise.&lt;/p&gt;&lt;p&gt;With private pools, Cloud Build now supports:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;VPC Peering&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;VPC-SC&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Static IP ranges&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;No public IPs&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Org policy enforcement&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cross-project builds&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Build from private source repositories with first class integrations, including Github Enterprise&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Regionalization in 15 regions across the US, EU, Asia, Australia, and South America&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Hundreds of concurrent builds per pool&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;15 machine types&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;And while designed primarily for private networking use cases, private pools work just as well with resources in Google Cloud, if you&amp;#8217;re interested in trying out new features like higher concurrency or additional machine types.&lt;/p&gt;&lt;h3&gt;Same Cloud Build, new build environment&lt;/h3&gt;&lt;p&gt;Private pools introduces a new build environment for executing your builds with Cloud Build while maintaining a consistent product and API experience. All the same great features of Cloud Build are available with private pools, including fully managed workers, pay-as-you-go pricing, Cloud Console UI, source repo integrations, IAM permissions, Secret Manager and Pub/Sub integrations, and native support for Google Cloud runtimes like Google Kubernetes Engine (GKE), Cloud Run, Cloud Functions, App Engine, and Firebase.&lt;/p&gt;&lt;p&gt;Running builds on a private pool is as easy as creating the pool and setting it as your &lt;a href=&#34;https://cloud.google.com/build/docs/private-pools/run-builds-in-private-pool&#34;&gt;build environment in your cloudbuild.yaml config file&lt;/a&gt;. Private networking is optionally configured via Service Networking by &lt;a href=&#34;https://cloud.google.com/build/docs/private-pools/set-up-private-pool-environment#setup-private-connection&#34;&gt;peering your private pool to your customer-managed VPC&lt;/a&gt; and supports both peered and shared VPCs.&lt;/p&gt;&lt;p&gt;Running your first build is easy:&lt;/p&gt;"><p>A recent <a href="https://devops.com/survey-shows-mounting-devops-frustration-and-costs/" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://devops.com" track-metadata-module="post">survey</a> found that developers spend 39% of their time managing the DevOps infrastructure that powers their continuous integration (CI) and continuous delivery (CD) pipelines. Unreliable availability, manual provisioning, limited scaling, breaking upgrades, long queue times, and high fixed costs all slow down development and take valuable time and focus away from DevOps teams. And while cloud-based CI/CD solutions can solve many of these friction points, they largely only work with cloud-hosted resources. </p><p>That’s why we’re excited to announce that starting today, you can take advantage of serverless build environments within your own private network, with new <b>Cloud Build private pools</b>. Launched in 2018, <a href="https://cloud.google.com/build" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/build" track-metadata-module="post">Cloud Build</a> has helped thousands of customers modernize their CI/CD workloads to run on fully managed, secure, pay-as-you-go ‘workers’ with no infrastructure to manage. </p><p>Cloud Build offers on-demand auto-scaling capabilities, active build minute billing, all with no infrastructure to manage. The new private pools feature augments Cloud Build with secure, fully managed CI/CD and DevOps workflow automation that uses network peering to connect into your private networks. Private pools also unlocks a host of new customization options such as new machine types, higher maximum concurrency, regional builds, and network configuration options.</p><p>With Cloud Build private pools, you get the benefits of a cloud-hosted, fully managed CI/CD product while meeting enterprise security and compliance requirements—even for highly regulated industries like finance, healthcare, retail, and others. For instance, you can trigger fully managed DevOps workflows from source-code repositories hosted in private networks, including Github Enterprise.</p><p>With private pools, Cloud Build now supports:</p><ul><li><p>VPC Peering</p></li><li><p>VPC-SC</p></li><li><p>Static IP ranges</p></li><li><p>No public IPs</p></li><li><p>Org policy enforcement</p></li><li><p>Cross-project builds</p></li><li><p>Build from private source repositories with first class integrations, including Github Enterprise</p></li><li><p>Regionalization in 15 regions across the US, EU, Asia, Australia, and South America</p></li><li><p>Hundreds of concurrent builds per pool</p></li><li><p>15 machine types</p></li></ul><p>And while designed primarily for private networking use cases, private pools work just as well with resources in Google Cloud, if you’re interested in trying out new features like higher concurrency or additional machine types.</p><h3>Same Cloud Build, new build environment</h3><p>Private pools introduces a new build environment for executing your builds with Cloud Build while maintaining a consistent product and API experience. All the same great features of Cloud Build are available with private pools, including fully managed workers, pay-as-you-go pricing, Cloud Console UI, source repo integrations, IAM permissions, Secret Manager and Pub/Sub integrations, and native support for Google Cloud runtimes like Google Kubernetes Engine (GKE), Cloud Run, Cloud Functions, App Engine, and Firebase.</p><p>Running builds on a private pool is as easy as creating the pool and setting it as your <a href="https://cloud.google.com/build/docs/private-pools/run-builds-in-private-pool" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/build/docs/private-pools/run-builds-in-private-pool" track-metadata-module="post">build environment in your cloudbuild.yaml config file</a>. Private networking is optionally configured via Service Networking by <a href="https://cloud.google.com/build/docs/private-pools/set-up-private-pool-environment#setup-private-connection" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/build/docs/private-pools/set-up-private-pool-environment#setup-private-connection" track-metadata-module="post">peering your private pool to your customer-managed VPC</a> and supports both peered and shared VPCs.</p><p>Running your first build is easy:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Christopher Sanson&lt;/name&gt;&lt;title&gt;Product Manager, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/devops.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 29 Jul 2021 15:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Announcing the 2021 State of DevOps Report Sponsors</title>
      <link>https://cloud.google.com/blog/products/devops-sre/announcing-2021-state-devops-report-sponsors/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Google Cloud and the &lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DORA&lt;/a&gt; research team are excited to announce our eight sponsors for the 2021 State of DevOps report. We recently launched the &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n&#34; target=&#34;_blank&#34;&gt;2021 State of DevOps survey&lt;/a&gt;, a 25-min survey for the DevOps community to share how they are using DevOps to improve software delivery performance. So if you haven’t taken the survey yet, this is your chance! &lt;/p&gt;&lt;p&gt;For those unfamiliar with the State Of DevOps report, it is the largest and longest running research of its kind. It provides an independent view into how teams and companies can drive powerful business outcomes, no matter what stage of the DevOps journey.&lt;/p&gt;&lt;p&gt;With the &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;State of DevOps&lt;/a&gt; reports we aim to capture how teams and companies are driving excellence in technology delivery through the implementation of DevOps practices, no matter what stage of the DevOps journey your team is in. If you’re wondering how your team measures up in your industry take our &lt;a href=&#34;https://www.devops-research.com/quickcheck.html&#34; target=&#34;_blank&#34;&gt;DevOps Quick Check&lt;/a&gt; and discover which capabilities you should focus on to improve your performance.&lt;/p&gt;&lt;p&gt;To capture a diverse array of information on how those in the DevOps community are performing, our eight sponsors of the 2021 State of DevOps survey are focussed on helping organizations of all sizes and industries to develop and deliver software faster across the whole DevOps lifecycle. Captured below, you’ll find more information on our sponsors for this year and why they chose to sponsor the State Of the DevOps 2021 Report.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://www.armory.io/&#34; target=&#34;_blank&#34;&gt;Armory&lt;/a&gt; &lt;br/&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 &#34;&gt;&lt;img alt=&#34;armory&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/armory.max-1000x1000.png&#34;/&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;br/&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Armory enables enterprise companies to ship better software, faster through trusted, reliable, safe, and secure deployments -- at its core, Armory is powered by Spinnaker OSS.&lt;/p&gt;&lt;p&gt;“Armory exists to unlock innovation through software. To achieve this, we help enterprises reliably deploy software at scale, naturally aligning with DevOps practices to improve software delivery performance. Given this, we applaud efforts like the State of DevOps Report that provides an independent view into the practices and capabilities that organizations can employ to drive better performance.&#34; &lt;/p&gt;&lt;p&gt;Carl Timm, Senior Director of Product Marketing at Armory&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://circleci.com/&#34; target=&#34;_blank&#34;&gt;CircleCI&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 &#34;&gt;&lt;img alt=&#34;circleci&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/circelci.max-1000x1000.png&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The world’s best software teams deliver quality code, confidently, with CircleCI. The world’s best software teams use CircleCI to go from next-up to feature shipped, at the speed ambitious businesses require. &lt;/p&gt;&lt;p&gt;“Though the DevOps space is only over a decade old, it moves incredibly quickly. Google’s State of DevOps report is both a reflection and projection of the industry, capturing how DevOps culture drives business results and where DevOps practitioners can look to improve. At CircleCI, we rely heavily on this survey data to glean valuable insights into our market and our customers overall. We also build upon these insights to guide our own research into how engineering teams move code through pipelines in our annual &lt;a href=&#34;https://circleci.com/resources/2020-state-of-software-delivery/&#34; target=&#34;_blank&#34;&gt;State of Software Delivery report&lt;/a&gt;. Taken together, this research highlights teams’ reported and actual behavior and paints a vibrant picture of how technology-driven organizations drive for success.”&lt;/p&gt;&lt;p&gt;Emma Webb, VP, Corporate Communications, CircleCI &lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cd.foundation/&#34; target=&#34;_blank&#34;&gt;Continuous Delivery Foundation&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 &#34;&gt;&lt;img alt=&#34;cdf&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/cdf.max-1000x1000.png&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;CD Foundation is an open-source community improving the world&#39;s ability to deliver software with security and speed.&lt;/p&gt;&lt;p&gt;&#34;Good decision-making is based on good data. Open source is a critical piece of the technology roadmap for DevOps, and the 2021 State of DevOps Report will provide actionable information for high performing teams in organizations of all types and sizes. The report will show where open source and DevOps intersect, and in a space that is changing so quickly, new relevant data is critical,&#34; said Tracy Miranda, Continuous Delivery Foundation Executive Director. &#34;CD Foundation members are deeply involved with producing this annual report, with over 10 years of historic research. Thank you to Google and our other members who have worked so hard on it. This report is a tangible result of working together.&#34;&lt;/p&gt;&lt;p&gt;Tracy Miranda, Continuous Delivery Foundation Executive Director&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://www2.deloitte.com/us/en.html&#34; target=&#34;_blank&#34;&gt;Deloitte&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 &#34;&gt;&lt;img alt=&#34;deloitte&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/deloitte.max-1000x1000.png&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Deloitte provides audit and assurance, tax, consulting, and risk and financial advisory services to a broad cross-section of the largest corporations and governmental agencies. At Deloitte, they are continuously evolving how they work and how they look at marketplace challenges so they can continually deliver measurable, sustainable results for their clients and communities.&lt;/p&gt;&lt;p&gt;“Software delivery is evolving rapidly and we know our customers need unique and compelling insights to make the right decisions. State of DevOps is a widely used report and considered as an Industry standard to understand the drivers for excellence in Software Development and Delivery. Deloitte is excited to help the team at DORA and Google Cloud in publishing this report through our sponsoring since we believe the insights from this report will help make software delivery better.” &lt;/p&gt;&lt;p&gt;Manoj Mishra, Consulting Managing Director, Deloitte Consulting LLP&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://about.gitlab.com/&#34; target=&#34;_blank&#34;&gt;GitLab&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 &#34;&gt;&lt;img alt=&#34;gitlab&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/gitlab.max-1000x1000.jpeg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;GitLab is the open DevOps platform built from the ground up as a single application for all stages of the DevOps lifecycle enabling Product, Development, QA, Security, and Operations teams to work concurrently on the same project.&lt;/p&gt;&lt;p&gt;“We&#39;re happy to sponsor the DORA Report and the vital work behind it. The more we understand the DevOps journey, the better we and our GitLab community can contribute to its future. We&#39;re particularly interested to see this year&#39;s results and the impact COVID-19 and remote work have had on DevOps. We appreciate the chance to be part of this exciting, informative process.”&lt;/p&gt;&lt;p&gt;Brendon O’Leary, Senior Developer Evangelist&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://www.liquibase.com/&#34; target=&#34;_blank&#34;&gt;Liquibase&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 &#34;&gt;&lt;img alt=&#34;liquibase&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/liquibase.max-1000x1000.png&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Liquibase helps millions of developers easily manage database schema changes by enabling teams to track, version, and deploy database changes by delivering on the promise of CI/CD for the database.&lt;/p&gt;&lt;p&gt;“With their rigorous methodology, the DORA research team delivers actionable information with the simple goal of helping organizations of any size accelerate the development and delivery of software. Liquibase is honored to sponsor the 2021 State of DevOps Report and its role in helping so many organizations build value for their customers and shareholders”&lt;/p&gt;&lt;p&gt;Matt Geise, VP of Marketing at Liquibase&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://www.pagerduty.com/platform/&#34; target=&#34;_blank&#34;&gt;PagerDuty&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 &#34;&gt;&lt;img alt=&#34;pagerduty&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/pagerduty.max-1000x1000.png&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;PagerDuty is a digital operations management platform that empowers the right action, when seconds matter.&lt;/p&gt;&lt;p&gt;“As a leader in digital operations management, PagerDuty is proud to sponsor this year’s report as it aligns with our dedication to helping DevOps professionals make better decisions. This report will inform tech and business leaders about the trends/challenges developers are facing and the opportunities there are to accelerate their own DevOps transformation.” &lt;/p&gt;&lt;p&gt;Carolyn Guss, VP of Corporate Marketing&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://sysdig.com/resources/whitepapers/5-keys-to-a-secure-devops-workflow/?utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=10874493567&amp;amp;adgroupid=106662582683&amp;amp;utm_content=471144145325&amp;amp;utm_term=sysdig&amp;amp;utm_position=&amp;amp;utm_device=c&amp;amp;utm_type=e&amp;amp;utm_geo=9033320&amp;amp;gclid=CjwKCAjw_JuGBhBkEiwA1xmbRSyNOvpZ_6_BMU7Cd_NqwtoumsXkkVHoHcEHLNTQmh2sAWTCOP3WiRoC8ycQAvD_BwE&#34; target=&#34;_blank&#34;&gt;SysDig&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 &#34;&gt;&lt;img alt=&#34;sysdig&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/sysdig.max-1000x1000.png&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Sysdig is driving the secure DevOps movement, empowering organizations to confidently secure containers, Kubernetes and cloud.&lt;/p&gt;&lt;p&gt;“There is a learning curve with all new technology, cloud is no exception. Learning from mistakes and sharing best practices is how we will ultimately ship secure applications, faster. For seven years, DORA and Google have partnered to understand the State of DevOps to help vendors and cloud practitioners to learn from each other and implement standards for best practices,”&lt;/p&gt;&lt;p&gt;Loris Degioanni, CTO and founder of Sysdig&lt;/p&gt;&lt;p&gt;Google Cloud, the DORA team, and our sponsors are very excited about this year’s report. We look forward to hearing from you, your colleagues, and networks about how DevOps is integrated into your workflow and ways we can help to further improve your performance. Please share your experience with software delivery by &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n&#34; target=&#34;_blank&#34;&gt;completing our survey&lt;/a&gt; that will be used to foster the next generation of DevOps best practices. To provide ample time for the DevOps community to contribute to this industry wide report we have extended the deadline for the survey until July 2nd.&lt;/p&gt;&lt;p&gt;Thank you to everyone who has participated so far, and the Google Cloud DORA team looks forward to hearing from even more of you soon!&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>sodr</span></p></figure></article-aspect-image-block><div><article-cta _nghost-c17=""><div _ngcontent-c17=""><h4 _ngcontent-c17=""><span _ngcontent-c17="">Accelerate State of DevOps Report</span></h4><p _ngcontent-c17=""><span _ngcontent-c17="">Get a comprehensive view of the DevOps industry, providing actionable guidance for organizations of all sizes.</span></p><p><a _ngcontent-c17="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="DORA_2019" track-metadata-eventdetail="https://cloud.google.com/devops/state-of-devops?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY19-Q3-global-demandgen-website-wd-gcp_gtm_stateofdevops" href="https://cloud.google.com/devops/state-of-devops?utm_source=google&amp;utm_medium=blog&amp;utm_campaign=FY19-Q3-global-demandgen-website-wd-gcp_gtm_stateofdevops"><span _ngcontent-c17="">Download</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Google Cloud and the &lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DORA&lt;/a&gt; research team are excited to announce our eight sponsors for the 2021 State of DevOps report. We recently launched the &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n&#34; target=&#34;_blank&#34;&gt;2021 State of DevOps survey&lt;/a&gt;, a 25-min survey for the DevOps community to share how they are using DevOps to improve software delivery performance. So if you haven&amp;#8217;t taken the survey yet, this is your chance!&amp;#160;&lt;/p&gt;&lt;p&gt;For those unfamiliar with the State Of DevOps report, it is the largest and longest running research of its kind. It provides an independent view into how teams and companies can drive powerful business outcomes, no matter what stage of the DevOps journey.&lt;/p&gt;&lt;p&gt;With the &lt;a href=&#34;https://cloud.google.com/devops&#34;&gt;State of DevOps&lt;/a&gt; reports we aim to capture how teams and companies are driving excellence in technology delivery through the implementation of DevOps practices, no matter what stage of the DevOps journey your team is in. If you&amp;#8217;re wondering how your team measures up in your industry take our &lt;a href=&#34;https://www.devops-research.com/quickcheck.html&#34; target=&#34;_blank&#34;&gt;DevOps Quick Check&lt;/a&gt; and discover which capabilities you should focus on to improve your performance.&lt;/p&gt;&lt;p&gt;To capture a diverse array of information on how those in the DevOps community are performing, our eight sponsors of the 2021 State of DevOps survey are focussed on helping organizations of all sizes and industries to develop and deliver software faster across the whole DevOps lifecycle. Captured below, you&amp;#8217;ll find more information on our sponsors for this year and why they chose to sponsor the State Of the DevOps 2021 Report.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://www.armory.io/&#34; target=&#34;_blank&#34;&gt;Armory&lt;/a&gt; &lt;br&gt;&lt;/p&gt;"><p>Google Cloud and the <a href="https://www.devops-research.com/research.html" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://www.devops-research.com" track-metadata-module="post">DORA</a> research team are excited to announce our eight sponsors for the 2021 State of DevOps report. We recently launched the <a href="https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://google.qualtrics.com" track-metadata-module="post">2021 State of DevOps survey</a>, a 25-min survey for the DevOps community to share how they are using DevOps to improve software delivery performance. So if you haven’t taken the survey yet, this is your chance! </p><p>For those unfamiliar with the State Of DevOps report, it is the largest and longest running research of its kind. It provides an independent view into how teams and companies can drive powerful business outcomes, no matter what stage of the DevOps journey.</p><p>With the <a href="https://cloud.google.com/devops" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/devops" track-metadata-module="post">State of DevOps</a> reports we aim to capture how teams and companies are driving excellence in technology delivery through the implementation of DevOps practices, no matter what stage of the DevOps journey your team is in. If you’re wondering how your team measures up in your industry take our <a href="https://www.devops-research.com/quickcheck.html" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://www.devops-research.com" track-metadata-module="post">DevOps Quick Check</a> and discover which capabilities you should focus on to improve your performance.</p><p>To capture a diverse array of information on how those in the DevOps community are performing, our eight sponsors of the 2021 State of DevOps survey are focussed on helping organizations of all sizes and industries to develop and deliver software faster across the whole DevOps lifecycle. Captured below, you’ll find more information on our sponsors for this year and why they chose to sponsor the State Of the DevOps 2021 Report.</p><p><a href="https://www.armory.io/" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://www.armory.io" track-metadata-module="post">Armory</a> <br/></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Armory enables enterprise companies to ship better software, faster through trusted, reliable, safe, and secure deployments -- at its core, Armory is powered by Spinnaker OSS.&lt;/p&gt;&lt;p&gt;&amp;#8220;Armory exists to unlock innovation through software. To achieve this, we help enterprises reliably deploy software at scale, naturally aligning with DevOps practices to improve software delivery performance. Given this, we applaud efforts like the State of DevOps Report that provides an independent view into the practices and capabilities that organizations can employ to drive better performance.&amp;#34;&amp;#160;&lt;/p&gt;&lt;p&gt;Carl Timm, Senior Director of Product Marketing at Armory&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://circleci.com/&#34; target=&#34;_blank&#34;&gt;CircleCI&lt;/a&gt;&lt;/p&gt;"><p>Armory enables enterprise companies to ship better software, faster through trusted, reliable, safe, and secure deployments -- at its core, Armory is powered by Spinnaker OSS.</p><p>“Armory exists to unlock innovation through software. To achieve this, we help enterprises reliably deploy software at scale, naturally aligning with DevOps practices to improve software delivery performance. Given this, we applaud efforts like the State of DevOps Report that provides an independent view into the practices and capabilities that organizations can employ to drive better performance.&#34; </p><p>Carl Timm, Senior Director of Product Marketing at Armory</p><p><a href="https://circleci.com/" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://circleci.com" track-metadata-module="post">CircleCI</a></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;The world&amp;#8217;s best software teams deliver quality code, confidently, with CircleCI. The world&amp;#8217;s best software teams use CircleCI to go from next-up to feature shipped, at the speed ambitious businesses require.&amp;#160;&lt;/p&gt;&lt;p&gt;&amp;#8220;Though the DevOps space is only over a decade old, it moves incredibly quickly. Google&amp;#8217;s State of DevOps report is both a reflection and projection of the industry, capturing how DevOps culture drives business results and where DevOps practitioners can look to improve. At CircleCI, we rely heavily on this survey data to glean valuable insights into our market and our customers overall. We also build upon these insights to guide our own research into how engineering teams move code through pipelines in our annual &lt;a href=&#34;https://circleci.com/resources/2020-state-of-software-delivery/&#34; target=&#34;_blank&#34;&gt;State of Software Delivery report&lt;/a&gt;. Taken together, this research highlights teams&amp;#8217; reported and actual behavior and paints a vibrant picture of how technology-driven organizations drive for success.&amp;#8221;&lt;/p&gt;&lt;p&gt;Emma Webb, VP, Corporate Communications, CircleCI&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cd.foundation/&#34; target=&#34;_blank&#34;&gt;Continuous Delivery Foundation&lt;/a&gt;&lt;/p&gt;"><p>The world’s best software teams deliver quality code, confidently, with CircleCI. The world’s best software teams use CircleCI to go from next-up to feature shipped, at the speed ambitious businesses require. </p><p>“Though the DevOps space is only over a decade old, it moves incredibly quickly. Google’s State of DevOps report is both a reflection and projection of the industry, capturing how DevOps culture drives business results and where DevOps practitioners can look to improve. At CircleCI, we rely heavily on this survey data to glean valuable insights into our market and our customers overall. We also build upon these insights to guide our own research into how engineering teams move code through pipelines in our annual <a href="https://circleci.com/resources/2020-state-of-software-delivery/" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://circleci.com" track-metadata-module="post">State of Software Delivery report</a>. Taken together, this research highlights teams’ reported and actual behavior and paints a vibrant picture of how technology-driven organizations drive for success.”</p><p>Emma Webb, VP, Corporate Communications, CircleCI </p><p><a href="https://cd.foundation/" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cd.foundation" track-metadata-module="post">Continuous Delivery Foundation</a></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;CD Foundation is an open-source community improving the world&#39;s ability to deliver software with security and speed.&lt;/p&gt;&lt;p&gt;&amp;#34;Good decision-making is based on good data. Open source is a critical piece of the technology roadmap for DevOps, and the 2021 State of DevOps Report will provide actionable information for high performing teams in organizations of all types and sizes. The report will show where open source and DevOps intersect, and in a space that is changing so quickly, new relevant data is critical,&amp;#34; said Tracy Miranda, Continuous Delivery Foundation Executive Director. &amp;#34;CD Foundation members are deeply involved with producing this annual report, with over 10 years of historic research. Thank you to Google and our other members who have worked so hard on it. This report is a tangible result of working together.&amp;#34;&lt;/p&gt;&lt;p&gt;Tracy Miranda, Continuous Delivery Foundation Executive Director&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://www2.deloitte.com/us/en.html&#34; target=&#34;_blank&#34;&gt;Deloitte&lt;/a&gt;&lt;/p&gt;"><p>CD Foundation is an open-source community improving the world&#39;s ability to deliver software with security and speed.</p><p>&#34;Good decision-making is based on good data. Open source is a critical piece of the technology roadmap for DevOps, and the 2021 State of DevOps Report will provide actionable information for high performing teams in organizations of all types and sizes. The report will show where open source and DevOps intersect, and in a space that is changing so quickly, new relevant data is critical,&#34; said Tracy Miranda, Continuous Delivery Foundation Executive Director. &#34;CD Foundation members are deeply involved with producing this annual report, with over 10 years of historic research. Thank you to Google and our other members who have worked so hard on it. This report is a tangible result of working together.&#34;</p><p>Tracy Miranda, Continuous Delivery Foundation Executive Director</p><p><a href="https://www2.deloitte.com/us/en.html" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://www2.deloitte.com" track-metadata-module="post">Deloitte</a></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Deloitte provides audit and assurance, tax, consulting, and risk and financial advisory services to a broad cross-section of the largest corporations and governmental agencies. At Deloitte, they are continuously evolving how they work and how they look at marketplace challenges so they can continually deliver measurable, sustainable results for their clients and communities.&lt;/p&gt;&lt;p&gt;&amp;#8220;Software delivery is evolving rapidly and we know our customers need unique and compelling insights to make the right decisions. State of DevOps is a widely used report and considered as an Industry standard to understand the drivers for excellence in Software Development and Delivery. Deloitte is excited to help the team at DORA and Google Cloud in publishing this report through our sponsoring since we believe the insights from this report will help make software delivery better.&amp;#8221;&amp;#160;&lt;/p&gt;&lt;p&gt;Manoj Mishra, Consulting Managing Director, Deloitte Consulting LLP&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://about.gitlab.com/&#34; target=&#34;_blank&#34;&gt;GitLab&lt;/a&gt;&lt;/p&gt;"><p>Deloitte provides audit and assurance, tax, consulting, and risk and financial advisory services to a broad cross-section of the largest corporations and governmental agencies. At Deloitte, they are continuously evolving how they work and how they look at marketplace challenges so they can continually deliver measurable, sustainable results for their clients and communities.</p><p>“Software delivery is evolving rapidly and we know our customers need unique and compelling insights to make the right decisions. State of DevOps is a widely used report and considered as an Industry standard to understand the drivers for excellence in Software Development and Delivery. Deloitte is excited to help the team at DORA and Google Cloud in publishing this report through our sponsoring since we believe the insights from this report will help make software delivery better.” </p><p>Manoj Mishra, Consulting Managing Director, Deloitte Consulting LLP</p><p><a href="https://about.gitlab.com/" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://about.gitlab.com" track-metadata-module="post">GitLab</a></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;GitLab is the open DevOps platform built from the ground up as a single application for all stages of the DevOps lifecycle enabling Product, Development, QA, Security, and Operations teams to work concurrently on the same project.&lt;/p&gt;&lt;p&gt;&amp;#8220;We&#39;re happy to sponsor the DORA Report and the vital work behind it. The more we understand the DevOps journey, the better we and our GitLab community can contribute to its future. We&#39;re particularly interested to see this year&#39;s results and the impact COVID-19 and remote work have had on DevOps. We appreciate the chance to be part of this exciting, informative process.&amp;#8221;&lt;/p&gt;&lt;p&gt;Brendon O&amp;#8217;Leary, Senior Developer Evangelist&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://www.liquibase.com/&#34; target=&#34;_blank&#34;&gt;Liquibase&lt;/a&gt;&lt;/p&gt;"><p>GitLab is the open DevOps platform built from the ground up as a single application for all stages of the DevOps lifecycle enabling Product, Development, QA, Security, and Operations teams to work concurrently on the same project.</p><p>“We&#39;re happy to sponsor the DORA Report and the vital work behind it. The more we understand the DevOps journey, the better we and our GitLab community can contribute to its future. We&#39;re particularly interested to see this year&#39;s results and the impact COVID-19 and remote work have had on DevOps. We appreciate the chance to be part of this exciting, informative process.”</p><p>Brendon O’Leary, Senior Developer Evangelist</p><p><a href="https://www.liquibase.com/" target="_blank" track-type="inline link" track-name="11" track-metadata-eventdetail="https://www.liquibase.com" track-metadata-module="post">Liquibase</a></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Liquibase helps millions of developers easily manage database schema changes by enabling teams to track, version, and deploy database changes by delivering on the promise of CI/CD for the database.&lt;/p&gt;&lt;p&gt;&amp;#8220;With their rigorous methodology, the DORA research team delivers actionable information with the simple goal of helping organizations of any size accelerate the development and delivery of software. Liquibase is honored to sponsor the 2021 State of DevOps Report and its role in helping so many organizations build value for their customers and shareholders&amp;#8221;&lt;/p&gt;&lt;p&gt;Matt Geise, VP of Marketing at Liquibase&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://www.pagerduty.com/platform/&#34; target=&#34;_blank&#34;&gt;PagerDuty&lt;/a&gt;&lt;/p&gt;"><p>Liquibase helps millions of developers easily manage database schema changes by enabling teams to track, version, and deploy database changes by delivering on the promise of CI/CD for the database.</p><p>“With their rigorous methodology, the DORA research team delivers actionable information with the simple goal of helping organizations of any size accelerate the development and delivery of software. Liquibase is honored to sponsor the 2021 State of DevOps Report and its role in helping so many organizations build value for their customers and shareholders”</p><p>Matt Geise, VP of Marketing at Liquibase</p><p><a href="https://www.pagerduty.com/platform/" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="https://www.pagerduty.com" track-metadata-module="post">PagerDuty</a></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;PagerDuty is a digital operations management platform that empowers the right action, when seconds matter.&lt;/p&gt;&lt;p&gt;&amp;#8220;As a leader in digital operations management, PagerDuty is proud to sponsor this year&amp;#8217;s report as it aligns with our dedication to helping DevOps professionals make better decisions. This report will inform tech and business leaders about the trends/challenges developers are facing and the opportunities there are to accelerate their own DevOps transformation.&amp;#8221;&amp;#160;&lt;/p&gt;&lt;p&gt;Carolyn Guss, VP of Corporate Marketing&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://sysdig.com/resources/whitepapers/5-keys-to-a-secure-devops-workflow/?utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=10874493567&amp;amp;adgroupid=106662582683&amp;amp;utm_content=471144145325&amp;amp;utm_term=sysdig&amp;amp;utm_position=&amp;amp;utm_device=c&amp;amp;utm_type=e&amp;amp;utm_geo=9033320&amp;amp;gclid=CjwKCAjw_JuGBhBkEiwA1xmbRSyNOvpZ_6_BMU7Cd_NqwtoumsXkkVHoHcEHLNTQmh2sAWTCOP3WiRoC8ycQAvD_BwE&#34; target=&#34;_blank&#34;&gt;SysDig&lt;/a&gt;&lt;/p&gt;"><p>PagerDuty is a digital operations management platform that empowers the right action, when seconds matter.</p><p>“As a leader in digital operations management, PagerDuty is proud to sponsor this year’s report as it aligns with our dedication to helping DevOps professionals make better decisions. This report will inform tech and business leaders about the trends/challenges developers are facing and the opportunities there are to accelerate their own DevOps transformation.” </p><p>Carolyn Guss, VP of Corporate Marketing</p><p><a href="https://sysdig.com/resources/whitepapers/5-keys-to-a-secure-devops-workflow/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=10874493567&amp;adgroupid=106662582683&amp;utm_content=471144145325&amp;utm_term=sysdig&amp;utm_position=&amp;utm_device=c&amp;utm_type=e&amp;utm_geo=9033320&amp;gclid=CjwKCAjw_JuGBhBkEiwA1xmbRSyNOvpZ_6_BMU7Cd_NqwtoumsXkkVHoHcEHLNTQmh2sAWTCOP3WiRoC8ycQAvD_BwE" target="_blank" track-type="inline link" track-name="13" track-metadata-eventdetail="https://sysdig.com" track-metadata-module="post">SysDig</a></p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Sysdig is driving the secure DevOps movement, empowering organizations to confidently secure containers, Kubernetes and cloud.&lt;/p&gt;&lt;p&gt;&amp;#8220;There is a learning curve with all new technology, cloud is no exception. Learning from mistakes and sharing best practices is how we will ultimately ship secure applications, faster. For seven years, DORA and Google have partnered to understand the State of DevOps to help vendors and cloud practitioners to learn from each other and implement standards for best practices,&amp;#8221;&lt;/p&gt;&lt;p&gt;Loris Degioanni, CTO and founder of Sysdig&lt;/p&gt;&lt;p&gt;Google Cloud, the DORA team, and our sponsors are very excited about this year&amp;#8217;s report. We look forward to hearing from you, your colleagues, and networks about how DevOps is integrated into your workflow and ways we can help to further improve your performance. Please share your experience with software delivery by &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n&#34; target=&#34;_blank&#34;&gt;completing our survey&lt;/a&gt; that will be used to foster the next generation of DevOps best practices. To provide ample time for the DevOps community to contribute to this industry wide report we have extended the deadline for the survey until July 2nd.&lt;/p&gt;&lt;p&gt;Thank you to everyone who has participated so far, and the Google Cloud DORA team looks forward to hearing from even more of you soon!&lt;/p&gt;"><p>Sysdig is driving the secure DevOps movement, empowering organizations to confidently secure containers, Kubernetes and cloud.</p><p>“There is a learning curve with all new technology, cloud is no exception. Learning from mistakes and sharing best practices is how we will ultimately ship secure applications, faster. For seven years, DORA and Google have partnered to understand the State of DevOps to help vendors and cloud practitioners to learn from each other and implement standards for best practices,”</p><p>Loris Degioanni, CTO and founder of Sysdig</p><p>Google Cloud, the DORA team, and our sponsors are very excited about this year’s report. We look forward to hearing from you, your colleagues, and networks about how DevOps is integrated into your workflow and ways we can help to further improve your performance. Please share your experience with software delivery by <a href="https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n" target="_blank" track-type="inline link" track-name="14" track-metadata-eventdetail="https://google.qualtrics.com" track-metadata-module="post">completing our survey</a> that will be used to foster the next generation of DevOps best practices. To provide ample time for the DevOps community to contribute to this industry wide report we have extended the deadline for the survey until July 2nd.</p><p>Thank you to everyone who has participated so far, and the Google Cloud DORA team looks forward to hearing from even more of you soon!</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c18=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Brenna Washington&lt;/name&gt;&lt;title&gt;Product Marketing Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/sodr.max-1600x1600.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 30 Jun 2021 11:28:00 +0000</pubDate>
    </item>
    <item>
      <title>A blueprint for secure infrastructure on Google Cloud</title>
      <link>https://cloud.google.com/blog/topics/developers-practitioners/blueprint-secure-infrastructure-google-cloud/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;When it comes to infrastructure security, every stakeholder has the same goal: maintain the confidentiality and integrity of their company’s data and systems. Period.&lt;/p&gt;&lt;p&gt;Developing and operating in the Cloud provides the opportunity to achieve these goals by being more secure and having greater visibility and governance over your resources and data. This is due to the relatively uniform environment of cloud infrastructure (as compared with on-prem) and inherent service-centric architecture. In addition, cloud providers take on some of the key responsibilities for security doing their part in a shared responsibility model. &lt;/p&gt;&lt;p&gt;However, translating this shared goal into reality can be a complex endeavor for a few reasons. Firstly, administering security in public clouds is unlike what you may be used to as the infrastructure primitives (the building blocks available to you) and control abstractions (how you administer security policy) differ from on premise environments. Additionally, ensuring you make the right policy decisions in an area as high-stakes and ever-evolving as security means that you’ll likely spend hours researching and reading through documentation, perhaps even hiring experts. &lt;/p&gt;&lt;p&gt;To &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/delivering-the-industrys-most-trusted-cloud&#34;&gt;partner with you&lt;/a&gt; and help address these challenges, Google Cloud built the security foundations blueprint to identify core security decisions and guide you with opinionated best practices for deploying a secured GCP environment. &lt;/p&gt;&lt;h3&gt;What is the security foundations blueprint?&lt;/h3&gt;&lt;p&gt;The security foundations blueprint is made up of two resources: the &lt;a href=&#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf&#34; target=&#34;_blank&#34;&gt;security foundations guide&lt;/a&gt;, and the &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-example-foundation&#34; target=&#34;_blank&#34;&gt;Terraform automation repository&lt;/a&gt;. For each security decision, the security foundations guide provides opinionated best practices in order to help you build a secure starting point for your Google Cloud deployment, and can be read and used as a reference guide. The recommended policies and architecture outlined in the document can then be deployed through automation using the &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-example-foundation&#34; target=&#34;_blank&#34;&gt;Terraform repository&lt;/a&gt; available on GitHub. &lt;/p&gt;&lt;h3&gt;Who is it for?&lt;/h3&gt;&lt;p&gt;The security foundations blueprint was designed with the enterprise in mind, including those with the strongest security requirements. However, the best practices are applicable to any size cloud customer, and can be adapted or adopted in pieces as needed for your organization. As far as who in an organization is going to find it most useful, it is beneficial for many roles:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;CISOs and compliance officers&lt;/b&gt; will use the &lt;a href=&#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf&#34; target=&#34;_blank&#34;&gt;security foundations guide&lt;/a&gt; as a reference to understand Google’s key principles for Cloud Security and how they can be applied and implemented to their deployments.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Security practitioners&lt;/b&gt; and&lt;b&gt;platform teams&lt;/b&gt; will follow the guide’s detailed instructions and accompanying Terraform templates for applying best practices so that they can actively set-up, configure, deploy, and operate their own security-centric infrastructure. &lt;/li&gt;&lt;li&gt;&lt;b&gt;Application developers&lt;/b&gt; will deploy their workloads and applications on this foundational infrastructure through an automated application deployment pipeline provided in the blueprint. &lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;What topics does it cover?&lt;/h3&gt;&lt;p&gt;This security foundations blueprint continues to expand the topics it covers, with its most recent release in April 2021 including the following areas:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Google Cloud organization structure and policy&lt;/li&gt;&lt;li&gt;Authentication and authorization&lt;/li&gt;&lt;li&gt;Resource hierarchy and deployment&lt;/li&gt;&lt;li&gt;Networking (segmentation and security)&lt;/li&gt;&lt;li&gt;Key and secret management&lt;/li&gt;&lt;li&gt;Logging&lt;/li&gt;&lt;li&gt;Detective controls&lt;/li&gt;&lt;li&gt;Billing setup&lt;/li&gt;&lt;li&gt;Application security&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Each of the security decisions addressed in these topics come with background and discussion to support your own understanding of the concepts, which in turn enables you to customize the deployment to your own specific use case (if needed).&lt;/p&gt;&lt;p&gt;The topics are useful separately, which makes it possible to pick-and-choose areas where you need recommendations, but they also work together. For example, by following the best practices for project and resource naming conventions, you will be set up for advanced monitoring capabilities, such as real-time notifications for compliance to custom policies.&lt;/p&gt;&lt;h3&gt;How can I use it?&lt;/h3&gt;While the &lt;a href=&#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf&#34; target=&#34;_blank&#34;&gt;security foundations guide&lt;/a&gt; is incredibly valuable on its own, the real magic for a security practitioner or application developer comes from the ability to adopt, adapt, and deploy the best practices using templates in &lt;a href=&#34;https://www.terraform.io/&#34; target=&#34;_blank&#34;&gt;Terraform&lt;/a&gt;, a tool for managing Infrastructure as Code (IaC). For anyone new to IaC, simply put, it allows you to automate your infrastructure through writing code that configures and provisions your infrastructure. By using IaC to minimize the amount of manual configuration, you also benefit through limiting the possibility of human error in enforcing these components of your security policy.&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Untitled_2.max-1000x1000.png&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;The Security Foundations Blueprint&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Untitled_2.max-1000x1000.png&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The Security Foundations Blueprint as an automated deployment pipeline&lt;/p&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-example-foundation&#34; target=&#34;_blank&#34;&gt;Terraform automation repo&lt;/a&gt; includes configuration that defines the environment outlined in the &lt;a href=&#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf&#34; target=&#34;_blank&#34;&gt;guide&lt;/a&gt;. You can apply the repo end-to-end to deploy the full security foundations blueprint, or use the included modules individually and modify them so that you can adopt just portions of the blueprint. It’s important to note that there are a few differences between the policies for the sample organization outlined in the guide and what is deployed using the Terraform templates. Luckily, those few differences are outlined in the &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-example-foundation/blob/master/ERRATA.md&#34; target=&#34;_blank&#34;&gt;errata pages&lt;/a&gt; that are part of the &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-example-foundation&#34; target=&#34;_blank&#34;&gt;Terraform automation repo&lt;/a&gt;.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;So what should I do next? &lt;/h3&gt;&lt;p&gt;We hope you’ll continue following the journey of this blog series where we’ll dive deeper into the best practices provided throughout the topical sections of the &lt;a href=&#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf&#34; target=&#34;_blank&#34;&gt;guide&lt;/a&gt;, discuss the different ways in which the blueprints have helped enterprises secure their own Cloud deployment, and take a look inside the &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-example-foundation&#34; target=&#34;_blank&#34;&gt;Terraform templates&lt;/a&gt; to see how they can be adopted, adapted, and deployed. In the meantime, take a look at &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/google-cloud-security-foundations-guide&#34;&gt;this recent Cloud blog post&lt;/a&gt; which announces the launch of the blueprint’s latest version and discusses the key security principles that steer the best practices.&lt;/p&gt;&lt;p&gt; If you’re ready to dive straight into the &lt;a href=&#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf&#34; target=&#34;_blank&#34;&gt;security foundations guide&lt;/a&gt;, you can start at the beginning, or head to a topic in which you’re particularly interested. Reviewing the guide in this way, you will be able to see for yourself the level of detail and discussion, and most importantly, the direct path it provides to move beyond recommendations and into implementation. We don’t expect you to try and apply the blueprint to your security posture right away, but a great first step would be to fork the repo and deploy it in a new folder or organization. Go forth, deploy and stay safe out there.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/identity-security/google-cloud-security-foundations-guide/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_security.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Build security into Google Cloud deployments with our updated security foundations blueprint&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Get step by step guidance for creating a secured environment with Google Cloud with the security foundations guide and Terraform blueprin...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;&lt;/p&gt;&lt;p&gt;When it comes to infrastructure security, every stakeholder has the same goal: maintain the confidentiality and integrity of their company&amp;#8217;s data and systems. Period.&lt;/p&gt;&lt;p&gt;Developing and operating in the Cloud provides the opportunity to achieve these goals by being more secure and having greater visibility and governance over your resources and data. This is due to the relatively uniform environment of cloud infrastructure (as compared with on-prem) and inherent service-centric architecture. In addition, cloud providers take on some of the key responsibilities for security doing their part in a shared responsibility model.&amp;#160;&lt;/p&gt;&lt;p&gt;However, translating this shared goal into reality can be a complex endeavor for a few reasons. Firstly, administering security in public clouds is unlike what you may be used to as the infrastructure primitives (the building blocks available to you) and control abstractions (how you administer security policy) differ from on premise environments. Additionally, ensuring you make the right policy decisions in an area as high-stakes and ever-evolving as security means that you&amp;#8217;ll likely spend hours researching and reading through documentation, perhaps even hiring experts.&amp;#160;&lt;/p&gt;&lt;p&gt;To &lt;a href=&#34;https://cloud.google.com/blog/products/identity-security/delivering-the-industrys-most-trusted-cloud&#34;&gt;partner with you&lt;/a&gt; and help address these challenges, Google Cloud built the security foundations blueprint to identify core security decisions and guide you with opinionated best practices for deploying a secured GCP environment.&amp;#160;&lt;/p&gt;&lt;h3&gt;What is the security foundations blueprint?&lt;/h3&gt;&lt;p&gt;The security foundations blueprint is made up of two resources: the &lt;a href=&#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf&#34; target=&#34;_blank&#34;&gt;security foundations guide&lt;/a&gt;, and the &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-example-foundation&#34; target=&#34;_blank&#34;&gt;Terraform automation repository&lt;/a&gt;. For each security decision, the security foundations guide provides opinionated best practices in order to help you build a secure starting point for your Google Cloud deployment, and can be read and used as a reference guide. The recommended policies and architecture outlined in the document can then be deployed through automation using the &lt;a href=&#34;https://github.com/terraform-google-modules/terraform-example-foundation&#34; target=&#34;_blank&#34;&gt;Terraform repository&lt;/a&gt; available on GitHub.&amp;#160;&lt;/p&gt;&lt;h3&gt;Who is it for?&lt;/h3&gt;&lt;p&gt;The security foundations blueprint was designed with the enterprise in mind, including those with the strongest security requirements. However, the best practices are applicable to any size cloud customer, and can be adapted or adopted in pieces as needed for your organization. As far as who in an organization is going to find it most useful, it is beneficial for many roles:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;CISOs and compliance officers&lt;/b&gt; will use the &lt;a href=&#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf&#34; target=&#34;_blank&#34;&gt;security foundations guide&lt;/a&gt; as a reference to understand Google&amp;#8217;s key principles for Cloud Security and how they can be applied and implemented to their deployments.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Security practitioners&lt;/b&gt; and&lt;b&gt; platform teams&lt;/b&gt; will follow the guide&amp;#8217;s detailed instructions and accompanying Terraform templates for applying best practices so that they can actively set-up, configure, deploy, and operate their own security-centric infrastructure.&amp;#160;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Application developers&lt;/b&gt; will deploy their workloads and applications on this foundational infrastructure through an automated application deployment pipeline provided in the blueprint.&amp;#160;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;What topics does it cover?&lt;/h3&gt;&lt;p&gt;This security foundations blueprint continues to expand the topics it covers, with its most recent release in April 2021 including the following areas:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Google Cloud organization structure and policy&lt;/li&gt;&lt;li&gt;Authentication and authorization&lt;/li&gt;&lt;li&gt;Resource hierarchy and deployment&lt;/li&gt;&lt;li&gt;Networking (segmentation and security)&lt;/li&gt;&lt;li&gt;Key and secret management&lt;/li&gt;&lt;li&gt;Logging&lt;/li&gt;&lt;li&gt;Detective controls&lt;/li&gt;&lt;li&gt;Billing setup&lt;/li&gt;&lt;li&gt;Application security&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Each of the security decisions addressed in these topics come with background and discussion to support your own understanding of the concepts, which in turn enables you to customize the deployment to your own specific use case (if needed).&lt;/p&gt;&lt;p&gt;The topics are useful separately, which makes it possible to pick-and-choose areas where you need recommendations, but they also work together. For example, by following the best practices for project and resource naming conventions, you will be set up for advanced monitoring capabilities, such as real-time notifications for compliance to custom policies.&lt;/p&gt;&lt;h3&gt;How can I use it?&lt;/h3&gt;While the &lt;a href=&#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf&#34; target=&#34;_blank&#34;&gt;security foundations guide&lt;/a&gt; is incredibly valuable on its own, the real magic for a security practitioner or application developer comes from the ability to adopt, adapt, and deploy the best practices using templates in &lt;a href=&#34;https://www.terraform.io/&#34; target=&#34;_blank&#34;&gt;Terraform&lt;/a&gt;, a tool for managing Infrastructure as Code (IaC). For anyone new to IaC, simply put, it allows you to automate your infrastructure through writing code that configures and provisions your infrastructure. By using IaC to minimize the amount of manual configuration, you also benefit through limiting the possibility of human error in enforcing these components of your security policy."><p>When it comes to infrastructure security, every stakeholder has the same goal: maintain the confidentiality and integrity of their company’s data and systems. Period.</p><p>Developing and operating in the Cloud provides the opportunity to achieve these goals by being more secure and having greater visibility and governance over your resources and data. This is due to the relatively uniform environment of cloud infrastructure (as compared with on-prem) and inherent service-centric architecture. In addition, cloud providers take on some of the key responsibilities for security doing their part in a shared responsibility model. </p><p>However, translating this shared goal into reality can be a complex endeavor for a few reasons. Firstly, administering security in public clouds is unlike what you may be used to as the infrastructure primitives (the building blocks available to you) and control abstractions (how you administer security policy) differ from on premise environments. Additionally, ensuring you make the right policy decisions in an area as high-stakes and ever-evolving as security means that you’ll likely spend hours researching and reading through documentation, perhaps even hiring experts. </p><p>To <a href="https://cloud.google.com/blog/products/identity-security/delivering-the-industrys-most-trusted-cloud" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/products/identity-security/delivering-the-industrys-most-trusted-cloud" track-metadata-module="post">partner with you</a> and help address these challenges, Google Cloud built the security foundations blueprint to identify core security decisions and guide you with opinionated best practices for deploying a secured GCP environment. </p><h3>What is the security foundations blueprint?</h3><p>The security foundations blueprint is made up of two resources: the <a href="https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://services.google.com" track-metadata-module="post">security foundations guide</a>, and the <a href="https://github.com/terraform-google-modules/terraform-example-foundation" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://github.com" track-metadata-module="post">Terraform automation repository</a>. For each security decision, the security foundations guide provides opinionated best practices in order to help you build a secure starting point for your Google Cloud deployment, and can be read and used as a reference guide. The recommended policies and architecture outlined in the document can then be deployed through automation using the <a href="https://github.com/terraform-google-modules/terraform-example-foundation" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://github.com" track-metadata-module="post">Terraform repository</a> available on GitHub. </p><h3>Who is it for?</h3><p>The security foundations blueprint was designed with the enterprise in mind, including those with the strongest security requirements. However, the best practices are applicable to any size cloud customer, and can be adapted or adopted in pieces as needed for your organization. As far as who in an organization is going to find it most useful, it is beneficial for many roles:</p><ul><li><b>CISOs and compliance officers</b> will use the <a href="https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://services.google.com" track-metadata-module="post">security foundations guide</a> as a reference to understand Google’s key principles for Cloud Security and how they can be applied and implemented to their deployments.</li><li><b>Security practitioners</b> and<b> platform teams</b> will follow the guide’s detailed instructions and accompanying Terraform templates for applying best practices so that they can actively set-up, configure, deploy, and operate their own security-centric infrastructure. </li><li><b>Application developers</b> will deploy their workloads and applications on this foundational infrastructure through an automated application deployment pipeline provided in the blueprint. </li></ul><h3>What topics does it cover?</h3><p>This security foundations blueprint continues to expand the topics it covers, with its most recent release in April 2021 including the following areas:</p><ul><li>Google Cloud organization structure and policy</li><li>Authentication and authorization</li><li>Resource hierarchy and deployment</li><li>Networking (segmentation and security)</li><li>Key and secret management</li><li>Logging</li><li>Detective controls</li><li>Billing setup</li><li>Application security</li></ul><p>Each of the security decisions addressed in these topics come with background and discussion to support your own understanding of the concepts, which in turn enables you to customize the deployment to your own specific use case (if needed).</p><p>The topics are useful separately, which makes it possible to pick-and-choose areas where you need recommendations, but they also work together. For example, by following the best practices for project and resource naming conventions, you will be set up for advanced monitoring capabilities, such as real-time notifications for compliance to custom policies.</p><h3>How can I use it?</h3><p>While the <a href="https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://services.google.com" track-metadata-module="post">security foundations guide</a> is incredibly valuable on its own, the real magic for a security practitioner or application developer comes from the ability to adopt, adapt, and deploy the best practices using templates in <a href="https://www.terraform.io/" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://www.terraform.io" track-metadata-module="post">Terraform</a>, a tool for managing Infrastructure as Code (IaC). For anyone new to IaC, simply put, it allows you to automate your infrastructure through writing code that configures and provisions your infrastructure. By using IaC to minimize the amount of manual configuration, you also benefit through limiting the possibility of human error in enforcing these components of your security policy.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Alicia Williams&lt;/name&gt;&lt;title&gt;Developer Advocate&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 24 Jun 2021 19:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Are we there yet? Thoughts on assessing an SRE team’s maturity</title>
      <link>https://cloud.google.com/blog/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;One facet of our work as &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering&#34;&gt;Customer Reliability Engineers&lt;/a&gt;—Google Site Reliability Engineers (SREs) tapped to help Google Cloud customers develop that practice in their own organizations—is advising operations or SRE teams to improve their operational maturity. We&#39;ve noticed a recurring question cropping up across many of these discussions, usually phrased along the lines of &#34;is what we&#39;re currently doing &lt;i&gt;&#39;SRE work&#39;?&#34;&lt;/i&gt; or, with a little more existential dread, &#34;can we call ourselves SREs yet?&#34;&lt;/p&gt;&lt;p&gt;We&#39;ve answered this question before with a &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey&#34;&gt;list of practices&lt;/a&gt; from the &lt;a href=&#34;https://sre.google/workbook/table-of-contents/&#34; target=&#34;_blank&#34;&gt;SRE workbook&lt;/a&gt;. But the list is long on the &lt;i&gt;what&lt;/i&gt; and short on the &lt;i&gt;why&lt;/i&gt;, which can make it hard to digest for folks already suffering an identity crisis. Instead, we hope to help answer this question by discussing some principles we consider fundamental to how an SRE team operates. We&#39;ll examine why they&#39;re important and suggest questions that characterize a team&#39;s progress towards embodying them. &lt;/p&gt;&lt;h2&gt;Are we there yet?&lt;/h2&gt;&lt;p&gt;This question is asked in different ways, for a myriad of different reasons, and it can be quite hard to answer due to the wide range of &lt;a href=&#34;https://web.devopstopologies.com/&#34; target=&#34;_blank&#34;&gt;different circumstances&lt;/a&gt; that our customers operate in. Moreover, CRE, and Google in general, is not the final arbiter of what is and isn&#39;t &#34;SRE&#34; for your organization, so we can&#39;t provide an authoritative answer, if one even exists. We can only influence you and the community at large by expressing our opinions and experiences, in person or via our &lt;a href=&#34;https://sre.google/books/&#34; target=&#34;_blank&#34;&gt;books&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/blog/topics/cre-life-lessons&#34;&gt;blog posts&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Further, discussions of this topic tend to be complicated by the fact that the term &#34;SRE&#34; is used interchangeably to mean three things:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;job role&lt;/b&gt; primarily focused on maintaining the reliability of a service or product.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;group of people&lt;/b&gt;working within an organization, usually in the above job role.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;set of principles and practices&lt;/b&gt; that the above people can utilize to improve service reliability.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;When people ask &#34;can we call ourselves SREs yet?&#34; we could interpret it as a desire to link these three definitions together. A clearer restatement of this interpretation might be: &#34;Is our &lt;b&gt;group&lt;/b&gt; sufficiently advanced in our application of the &lt;b&gt;principles and practices&lt;/b&gt; that we can justifiably term our &lt;b&gt;job role&lt;/b&gt; SRE?&#34; &lt;/p&gt;&lt;p&gt;We should stress that we&#39;re not saying that you need a clearly defined &lt;b&gt;job role&lt;/b&gt;—or even a &lt;b&gt;team&lt;/b&gt;—before you can start utilizing the &lt;b&gt;principles and practices&lt;/b&gt; to do things that are recognizably SRE-like. Job roles and teams crystallize from a more fluid set of responsibilities as organizations grow larger. But as this process plays out, the people involved may feel less certain of the scope of their responsibilities, precipitating the ‘are we there yet?’ question. We suspect that&#39;s where the tone of existential dread comes from...&lt;/p&gt;&lt;h2&gt;Key SRE indicators&lt;/h2&gt;&lt;p&gt;Within the CRE team here at Google Cloud, the ‘are we there yet?’ question surfaced a wide variety of opinions about the core principles that should guide an SRE team. We did manage to reach a rough consensus, with one proviso—the answer is partially dependent on how a team engages with the services it supports.&lt;/p&gt;&lt;p&gt;We&#39;ve chosen to structure this post around a set of principles that we would broadly expect groups of people working as SREs that &lt;i&gt;directly support services in production&lt;/i&gt; to adhere to. As in a &lt;a href=&#34;https://en.wikipedia.org/wiki/Litmus#Uses&#34; target=&#34;_blank&#34;&gt;litmus test&lt;/a&gt;, this won&#39;t provide pin-point accuracy; but in our collective opinion at least, alignment with most of the principles laid out below is a good signal that a team is practicing something that can recognizably be termed &lt;i&gt;Site Reliability Engineering.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Directly engaged SRE teams are usually considered &lt;i&gt;Accountable&lt;/i&gt; (in &lt;a href=&#34;https://en.wikipedia.org/wiki/Responsibility_assignment_matrix&#34; target=&#34;_blank&#34;&gt;RACI terms&lt;/a&gt;) for the service’s reliability, with &lt;i&gt;Responsibility&lt;/i&gt; shared between the SRE and development teams. As a team provides less direct support these indicators may be less applicable. We hope those teams can still adapt the principles to their own circumstances. &lt;/p&gt;&lt;p&gt;To illustrate how you might do this, for each principle we&#39;ve given a counter-example of a team of SREs operating in an advisory capacity. They&#39;re subject-matter experts who are &lt;i&gt;Consulted&lt;/i&gt; by development teams who are themselves &lt;i&gt;Responsible&lt;/i&gt; and &lt;i&gt;Accountable&lt;/i&gt; for service reliability.&lt;/p&gt;&lt;p&gt;Wherever your engagement model lies on the spectrum, being perceived by the rest of the organization as jointly responsible for a service&#39;s reliability, or as reliability subject-matter experts, is a key indicator of SRE-hood.&lt;/p&gt;&lt;h3&gt;Principle #1: SREs mitigate present and &lt;i&gt;future&lt;/i&gt; incidents&lt;/h3&gt;&lt;p&gt;This principle is the one that usually underlies the perception of &lt;b&gt;responsibility and accountability&lt;/b&gt; for a service&#39;s reliability. All the careful engineering and active regulation in the world can&#39;t guarantee reliability, especially in complex distributed systems—sometimes, things go wrong unexpectedly and the only thing left to do is react, mitigate, and fix. SREs have both the authority and the technical capability to act fast to restore service in these situations.&lt;/p&gt;&lt;p&gt;But mitigating the immediate problem isn&#39;t enough. If it can happen again tomorrow, then tomorrow isn&#39;t better than today, so SREs should work to understand the precipitating factors of incidents and propose changes that remediate the entire class of problem in the infrastructure they are responsible for. Don&#39;t have the same outage again next month!&lt;/p&gt;&lt;p&gt;How unique are your outages? Ask yourself these questions:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Can you mitigate the majority of the incidents without needing specialist knowledge from the development team?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you maintain training materials and practice incident response scenarios?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;After a major outage happens to your service, are you a key participant in blamelessly figuring out what &lt;b&gt;really&lt;/b&gt; went wrong, and how to prevent future outages?&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Now, for a counter example. In many organizations, SREs are a scarce resource and may add more value by developing platforms and best practices to uplift large swathes of the company, rather than being primarily focused on incident response. Thus, a consulting SRE team would probably not be directly involved in mitigating most incidents, though they may be called on to coordinate incident response for a widespread outage. Rather than authoring training materials and postmortems, they would be responsible for reviewing those created by the teams they advise.&lt;/p&gt;&lt;h3&gt;Principle #2: SREs actively regulate service reliability&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Reliability goals and feedback signals are fundamental for both motivating SRE work and influencing the prioritization of development work.&lt;/b&gt; At Google, we call our reliability goals &lt;a href=&#34;https://sre.google/sre-book/service-level-objectives/&#34; target=&#34;_blank&#34;&gt;&lt;i&gt;Service Level Objectives&lt;/i&gt;&lt;/a&gt; and our feedback signals &lt;i&gt;Error Budgets&lt;/i&gt;, and you can read more about how we use them in the &lt;a href=&#34;https://sre.google/workbook/implementing-slos/&#34; target=&#34;_blank&#34;&gt;Site Reliability Workbook&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Do your reliability signals affect your organization&#39;s priorities? Ask yourself these questions:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Do you agree on goals for the reliability of the services you support with your organization, and track performance against those goals in real time?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you have an established feedback loop that moderates the behaviour of the organization based on recent service reliability?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you have the influence to effect change within the organization in pursuit of the reliability goals?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you have the &lt;a href=&#34;https://en.wikipedia.org/wiki/Agency_(sociology)&#34; target=&#34;_blank&#34;&gt;agency&lt;/a&gt; to refuse, or negotiate looser goals, when asked to make changes that may cause a service to miss its current reliability goals?&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Each question builds on the last. It is almost impossible to establish a data-driven feedback loop without a well-defined and measured reliability goal. For those goals to be meaningful, SREs must have the capability to defend them. Periods of lower service reliability should result in consequences that temporarily reduce the aggregate risk of future production changes and shift engineering priorities towards reliability. &lt;/p&gt;&lt;p&gt;When it comes down to a choice between service reliability and the rollout of new but unreliable features, SREs need to be able to say &#34;no&#34;. This should be a data driven decision—when there&#39;s not enough spare error budget, there needs to be a valid business reason for making users unhappy. Sometimes, of course, there will be, and this can be accommodated with new, lower SLO targets that reflect the relaxed reliability requirements.&lt;/p&gt;&lt;p&gt;Consultant SREs, in contrast, help teams draft their reliability goals and may develop shared monitoring infrastructure for measuring them across the organization. They are the de-facto regulators of the reliability feedback loop and maintain the policy documents that underpin it. Their connection to many teams and services gives them broader insights that can spark cross-functional reliability improvements. &lt;/p&gt;&lt;h3&gt;Principle #3: SREs engage early and comprehensively&lt;/h3&gt;&lt;p&gt;As we said earlier, &lt;b&gt;SREs should be empowered to make tomorrow better than today.&lt;/b&gt; Without the ability to change the code and configuration of the services they support, they cannot fix problems as they encounter them. Involving SREs earlier in the design process can head off common reliability anti-patterns that are costly to correct &lt;i&gt;post-facto&lt;/i&gt;. And, with the ability to influence architectural decision making, SREs can drive convergence across an organization so that work to increase the reliability of one service can benefit the entire company.&lt;/p&gt;&lt;p&gt;Is your team actively working to make tomorrow better than today? Ask yourself these questions, which go from fine detail to a broad, high-level scope:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Do you engineer your service &lt;b&gt;now&lt;/b&gt; to improve its reliability, e.g. by viewing and modifying the source code and/or configuration?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Are you involved in analysis and design of &lt;b&gt;future&lt;/b&gt; iterations of your service, providing a lens on reliability/operability/maintainability?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Can you influence your &lt;b&gt;organization’s&lt;/b&gt; wider architectural decision making?&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Advising other teams naturally shifts priorities away from directly modifying the configuration or code of individual services. But consultant SREs may still maintain frameworks or shared libraries providing core reliability features, like exporting common metrics or graceful service degradation. Their breadth of engagements across many teams makes them naturally suited for providing high-level architectural advice to improve reliability across an entire organization.&lt;/p&gt;&lt;h3&gt;Principle #4: SREs automate anything repetitive&lt;/h3&gt;&lt;p&gt;Finally, SREs believe that computers are fundamentally better suited to doing repetitive work than humans are. People often underestimate the &lt;a href=&#34;https://xkcd.com/1205/&#34; target=&#34;_blank&#34;&gt;returns on investment&lt;/a&gt; when considering whether to automate a routine task, and that&#39;s before factoring in the exponential growth curve that comes with running a large, successful service. Moreover, computers never become inattentive and make mistakes when doing the same task for the hundredth time, or become demoralized and quit. Hiring or &lt;a href=&#34;https://sre.google/resources/practices-and-processes/training-site-reliability-engineers/&#34; target=&#34;_blank&#34;&gt;training SREs&lt;/a&gt; is expensive and time-consuming, so a successful SRE organization depends heavily on making computers do the grunt work.&lt;/p&gt;&lt;p&gt;Are you sufficiently automating your work? Ask yourself these questions:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Do you use—or create—automation and other tools to ensure that operational load won&#39;t scale linearly with organic growth or the number of services you support?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you try to measure repetitive work on your team, and reduce it over time?&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;A call for reflection&lt;/h2&gt;&lt;p&gt;Most blog posts end with a call for action. We&#39;d rather you took time to reflect, instead of jumping up to make changes straight after reading. &lt;b&gt;There&#39;s a risk, when writing an opinionated piece like this, that the lines drawn in the sand are used to divide, not to grow and improve.&lt;/b&gt; We promise this isn&#39;t a deliberate effort to gatekeep SRE and exclude those who don&#39;t tick the boxes; we see no value in that. But in some ways gatekeeping is what job roles are &lt;i&gt;designed&lt;/i&gt; to do, because specialization and the division of labour is critical to the success of any organization, and this makes it hard to avoid drawing those lines.&lt;/p&gt;&lt;p&gt;For those who aspire to call themselves SREs, or are concerned that others may disagree with their characterization of themselves as SREs, perhaps these opinions can assuage some of that existential dread.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div _ngcontent-c19="" innerhtml="&lt;p&gt;One facet of our work as &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering&#34;&gt;Customer Reliability Engineers&lt;/a&gt;&amp;#8212;Google Site Reliability Engineers (SREs) tapped to help Google Cloud customers develop that practice in their own organizations&amp;#8212;is advising operations or SRE teams to improve their operational maturity. We&#39;ve noticed a recurring question cropping up across many of these discussions, usually phrased along the lines of &amp;#34;is what we&#39;re currently doing &lt;i&gt;&#39;SRE work&#39;?&amp;#34;&lt;/i&gt; or, with a little more existential dread, &amp;#34;can we call ourselves SREs yet?&amp;#34;&lt;/p&gt;&lt;p&gt;We&#39;ve answered this question before with a &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey&#34;&gt;list of practices&lt;/a&gt; from the &lt;a href=&#34;https://sre.google/workbook/table-of-contents/&#34; target=&#34;_blank&#34;&gt;SRE workbook&lt;/a&gt;. But the list is long on the &lt;i&gt;what&lt;/i&gt; and short on the &lt;i&gt;why&lt;/i&gt;, which can make it hard to digest for folks already suffering an identity crisis. Instead, we hope to help answer this question by discussing some principles we consider fundamental to how an SRE team operates. We&#39;ll examine why they&#39;re important and suggest questions that characterize a team&#39;s progress towards embodying them.&amp;#160;&lt;/p&gt;&lt;h2&gt;Are we there yet?&lt;/h2&gt;&lt;p&gt;This question is asked in different ways, for a myriad of different reasons, and it can be quite hard to answer due to the wide range of &lt;a href=&#34;https://web.devopstopologies.com/&#34; target=&#34;_blank&#34;&gt;different circumstances&lt;/a&gt; that our customers operate in. Moreover, CRE, and Google in general, is not the final arbiter of what is and isn&#39;t &amp;#34;SRE&amp;#34; for your organization, so we can&#39;t provide an authoritative answer, if one even exists. We can only influence you and the community at large by expressing our opinions and experiences, in person or via our &lt;a href=&#34;https://sre.google/books/&#34; target=&#34;_blank&#34;&gt;books&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/blog/topics/cre-life-lessons&#34;&gt;blog posts&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;p&gt;Further, discussions of this topic tend to be complicated by the fact that the term &amp;#34;SRE&amp;#34; is used interchangeably to mean three things:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;job role&lt;/b&gt; primarily focused on maintaining the reliability of a service or product.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;group of people &lt;/b&gt;working within an organization, usually in the above job role.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;set of principles and practices&lt;/b&gt; that the above people can utilize to improve service reliability.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;When people ask &amp;#34;can we call ourselves SREs yet?&amp;#34; we could interpret it as a desire to link these three definitions together. A clearer restatement of this interpretation might be: &amp;#34;Is our &lt;b&gt;group&lt;/b&gt; sufficiently advanced in our application of the &lt;b&gt;principles and practices&lt;/b&gt; that we can justifiably term our &lt;b&gt;job role&lt;/b&gt; SRE?&amp;#34;&amp;#160;&lt;/p&gt;&lt;p&gt;We should stress that we&#39;re not saying that you need a clearly defined &lt;b&gt;job role&lt;/b&gt;&amp;#8212;or even a &lt;b&gt;team&lt;/b&gt;&amp;#8212;before you can start utilizing the &lt;b&gt;principles and practices&lt;/b&gt; to do things that are recognizably SRE-like. Job roles and teams crystallize from a more fluid set of responsibilities as organizations grow larger. But as this process plays out, the people involved may feel less certain of the scope of their responsibilities, precipitating the &amp;#8216;are we there yet?&amp;#8217; question. We suspect that&#39;s where the tone of existential dread comes from...&lt;/p&gt;&lt;h2&gt;Key SRE indicators&lt;/h2&gt;&lt;p&gt;Within the CRE team here at Google Cloud, the &amp;#8216;are we there yet?&amp;#8217; question surfaced a wide variety of opinions about the core principles that should guide an SRE team. We did manage to reach a rough consensus, with one proviso&amp;#8212;the answer is partially dependent on how a team engages with the services it supports.&lt;/p&gt;&lt;p&gt;We&#39;ve chosen to structure this post around a set of principles that we would broadly expect groups of people working as SREs that &lt;i&gt;directly support services in production&lt;/i&gt; to adhere to. As in a &lt;a href=&#34;https://en.wikipedia.org/wiki/Litmus#Uses&#34; target=&#34;_blank&#34;&gt;litmus test&lt;/a&gt;, this won&#39;t provide pin-point accuracy; but in our collective opinion at least, alignment with most of the principles laid out below is a good signal that a team is practicing something that can recognizably be termed &lt;i&gt;Site Reliability Engineering.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;Directly engaged SRE teams are usually considered &lt;i&gt;Accountable&lt;/i&gt; (in &lt;a href=&#34;https://en.wikipedia.org/wiki/Responsibility_assignment_matrix&#34; target=&#34;_blank&#34;&gt;RACI terms&lt;/a&gt;) for the service&amp;#8217;s reliability, with &lt;i&gt;Responsibility&lt;/i&gt; shared between the SRE and development teams. As a team provides less direct support these indicators may be less applicable. We hope those teams can still adapt the principles to their own circumstances.&amp;#160;&lt;/p&gt;&lt;p&gt;To illustrate how you might do this, for each principle we&#39;ve given a counter-example of a team of SREs operating in an advisory capacity. They&#39;re subject-matter experts who are &lt;i&gt;Consulted&lt;/i&gt; by development teams who are themselves &lt;i&gt;Responsible&lt;/i&gt; and &lt;i&gt;Accountable&lt;/i&gt; for service reliability.&lt;/p&gt;&lt;p&gt;Wherever your engagement model lies on the spectrum, being perceived by the rest of the organization as jointly responsible for a service&#39;s reliability, or as reliability subject-matter experts, is a key indicator of SRE-hood.&lt;/p&gt;&lt;h3&gt;Principle #1: SREs mitigate present and &lt;i&gt;future&lt;/i&gt; incidents&lt;/h3&gt;&lt;p&gt;This principle is the one that usually underlies the perception of &lt;b&gt;responsibility and accountability&lt;/b&gt; for a service&#39;s reliability. All the careful engineering and active regulation in the world can&#39;t guarantee reliability, especially in complex distributed systems&amp;#8212;sometimes, things go wrong unexpectedly and the only thing left to do is react, mitigate, and fix. SREs have both the authority and the technical capability to act fast to restore service in these situations.&lt;/p&gt;&lt;p&gt;But mitigating the immediate problem isn&#39;t enough. If it can happen again tomorrow, then tomorrow isn&#39;t better than today, so SREs should work to understand the precipitating factors of incidents and propose changes that remediate the entire class of problem in the infrastructure they are responsible for. Don&#39;t have the same outage again next month!&lt;/p&gt;&lt;p&gt;How unique are your outages? Ask yourself these questions:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Can you mitigate the majority of the incidents without needing specialist knowledge from the development team?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you maintain training materials and practice incident response scenarios?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;After a major outage happens to your service, are you a key participant in blamelessly figuring out what &lt;b&gt;really&lt;/b&gt; went wrong, and how to prevent future outages?&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Now, for a counter example. In many organizations, SREs are a scarce resource and may add more value by developing platforms and best practices to uplift large swathes of the company, rather than being primarily focused on incident response. Thus, a consulting SRE team would probably not be directly involved in mitigating most incidents, though they may be called on to coordinate incident response for a widespread outage. Rather than authoring training materials and postmortems, they would be responsible for reviewing those created by the teams they advise.&lt;/p&gt;&lt;h3&gt;Principle #2: SREs actively regulate service reliability&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Reliability goals and feedback signals are fundamental for both motivating SRE work and influencing the prioritization of development work.&lt;/b&gt; At Google, we call our reliability goals &lt;a href=&#34;https://sre.google/sre-book/service-level-objectives/&#34; target=&#34;_blank&#34;&gt;&lt;i&gt;Service Level Objectives&lt;/i&gt;&lt;/a&gt; and our feedback signals &lt;i&gt;Error Budgets&lt;/i&gt;, and you can read more about how we use them in the &lt;a href=&#34;https://sre.google/workbook/implementing-slos/&#34; target=&#34;_blank&#34;&gt;Site Reliability Workbook&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Do your reliability signals affect your organization&#39;s priorities? Ask yourself these questions:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Do you agree on goals for the reliability of the services you support with your organization, and track performance against those goals in real time?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you have an established feedback loop that moderates the behaviour of the organization based on recent service reliability?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you have the influence to effect change within the organization in pursuit of the reliability goals?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you have the &lt;a href=&#34;https://en.wikipedia.org/wiki/Agency_(sociology)&#34; target=&#34;_blank&#34;&gt;agency&lt;/a&gt; to refuse, or negotiate looser goals, when asked to make changes that may cause a service to miss its current reliability goals?&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Each question builds on the last. It is almost impossible to establish a data-driven feedback loop without a well-defined and measured reliability goal. For those goals to be meaningful, SREs must have the capability to defend them. Periods of lower service reliability should result in consequences that temporarily reduce the aggregate risk of future production changes and shift engineering priorities towards reliability.&amp;#160;&lt;/p&gt;&lt;p&gt;When it comes down to a choice between service reliability and the rollout of new but unreliable features, SREs need to be able to say &amp;#34;no&amp;#34;. This should be a data driven decision&amp;#8212;when there&#39;s not enough spare error budget, there needs to be a valid business reason for making users unhappy. Sometimes, of course, there will be, and this can be accommodated with new, lower SLO targets that reflect the relaxed reliability requirements.&lt;/p&gt;&lt;p&gt;Consultant SREs, in contrast, help teams draft their reliability goals and may develop shared monitoring infrastructure for measuring them across the organization. They are the de-facto regulators of the reliability feedback loop and maintain the policy documents that underpin it. Their connection to many teams and services gives them broader insights that can spark cross-functional reliability improvements.&amp;#160;&lt;/p&gt;&lt;h3&gt;Principle #3: SREs engage early and comprehensively&lt;/h3&gt;&lt;p&gt;As we said earlier, &lt;b&gt;SREs should be empowered to make tomorrow better than today.&lt;/b&gt; Without the ability to change the code and configuration of the services they support, they cannot fix problems as they encounter them. Involving SREs earlier in the design process can head off common reliability anti-patterns that are costly to correct &lt;i&gt;post-facto&lt;/i&gt;. And, with the ability to influence architectural decision making, SREs can drive convergence across an organization so that work to increase the reliability of one service can benefit the entire company.&lt;/p&gt;&lt;p&gt;Is your team actively working to make tomorrow better than today? Ask yourself these questions, which go from fine detail to a broad, high-level scope:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Do you engineer your service &lt;b&gt;now&lt;/b&gt; to improve its reliability, e.g. by viewing and modifying the source code and/or configuration?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Are you involved in analysis and design of &lt;b&gt;future&lt;/b&gt; iterations of your service, providing a lens on reliability/operability/maintainability?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Can you influence your &lt;b&gt;organization&amp;#8217;s&lt;/b&gt; wider architectural decision making?&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Advising other teams naturally shifts priorities away from directly modifying the configuration or code of individual services. But consultant SREs may still maintain frameworks or shared libraries providing core reliability features, like exporting common metrics or graceful service degradation. Their breadth of engagements across many teams makes them naturally suited for providing high-level architectural advice to improve reliability across an entire organization.&lt;/p&gt;&lt;h3&gt;Principle #4: SREs automate anything repetitive&lt;/h3&gt;&lt;p&gt;Finally, SREs believe that computers are fundamentally better suited to doing repetitive work than humans are. People often underestimate the &lt;a href=&#34;https://xkcd.com/1205/&#34; target=&#34;_blank&#34;&gt;returns on investment&lt;/a&gt; when considering whether to automate a routine task, and that&#39;s before factoring in the exponential growth curve that comes with running a large, successful service. Moreover, computers never become inattentive and make mistakes when doing the same task for the hundredth time, or become demoralized and quit. Hiring or &lt;a href=&#34;https://sre.google/resources/practices-and-processes/training-site-reliability-engineers/&#34; target=&#34;_blank&#34;&gt;training SREs&lt;/a&gt; is expensive and time-consuming, so a successful SRE organization depends heavily on making computers do the grunt work.&lt;/p&gt;&lt;p&gt;Are you sufficiently automating your work? Ask yourself these questions:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Do you use&amp;#8212;or create&amp;#8212;automation and other tools to ensure that operational load won&#39;t scale linearly with organic growth or the number of services you support?&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Do you try to measure repetitive work on your team, and reduce it over time?&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;A call for reflection&lt;/h2&gt;&lt;p&gt;Most blog posts end with a call for action. We&#39;d rather you took time to reflect, instead of jumping up to make changes straight after reading. &lt;b&gt;There&#39;s a risk, when writing an opinionated piece like this, that the lines drawn in the sand are used to divide, not to grow and improve.&lt;/b&gt; We promise this isn&#39;t a deliberate effort to gatekeep SRE and exclude those who don&#39;t tick the boxes; we see no value in that. But in some ways gatekeeping is what job roles are &lt;i&gt;designed&lt;/i&gt; to do, because specialization and the division of labour is critical to the success of any organization, and this makes it hard to avoid drawing those lines.&lt;/p&gt;&lt;p&gt;For those who aspire to call themselves SREs, or are concerned that others may disagree with their characterization of themselves as SREs, perhaps these opinions can assuage some of that existential dread.&lt;/p&gt;" _nghost-c19=""><p>One facet of our work as <a href="https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering" track-metadata-module="post">Customer Reliability Engineers</a>—Google Site Reliability Engineers (SREs) tapped to help Google Cloud customers develop that practice in their own organizations—is advising operations or SRE teams to improve their operational maturity. We&#39;ve noticed a recurring question cropping up across many of these discussions, usually phrased along the lines of &#34;is what we&#39;re currently doing <i>&#39;SRE work&#39;?&#34;</i> or, with a little more existential dread, &#34;can we call ourselves SREs yet?&#34;</p><p>We&#39;ve answered this question before with a <a href="https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey" track-metadata-module="post">list of practices</a> from the <a href="https://sre.google/workbook/table-of-contents/" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">SRE workbook</a>. But the list is long on the <i>what</i> and short on the <i>why</i>, which can make it hard to digest for folks already suffering an identity crisis. Instead, we hope to help answer this question by discussing some principles we consider fundamental to how an SRE team operates. We&#39;ll examine why they&#39;re important and suggest questions that characterize a team&#39;s progress towards embodying them. </p><h2>Are we there yet?</h2><p>This question is asked in different ways, for a myriad of different reasons, and it can be quite hard to answer due to the wide range of <a href="https://web.devopstopologies.com/" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://web.devopstopologies.com" track-metadata-module="post">different circumstances</a> that our customers operate in. Moreover, CRE, and Google in general, is not the final arbiter of what is and isn&#39;t &#34;SRE&#34; for your organization, so we can&#39;t provide an authoritative answer, if one even exists. We can only influence you and the community at large by expressing our opinions and experiences, in person or via our <a href="https://sre.google/books/" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">books</a> and <a href="https://cloud.google.com/blog/topics/cre-life-lessons" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/blog/topics/cre-life-lessons" track-metadata-module="post">blog posts</a>. </p><p>Further, discussions of this topic tend to be complicated by the fact that the term &#34;SRE&#34; is used interchangeably to mean three things:</p><ol><li><p>A <b>job role</b> primarily focused on maintaining the reliability of a service or product.</p></li><li><p>A <b>group of people </b>working within an organization, usually in the above job role.</p></li><li><p>A <b>set of principles and practices</b> that the above people can utilize to improve service reliability.</p></li></ol><p>When people ask &#34;can we call ourselves SREs yet?&#34; we could interpret it as a desire to link these three definitions together. A clearer restatement of this interpretation might be: &#34;Is our <b>group</b> sufficiently advanced in our application of the <b>principles and practices</b> that we can justifiably term our <b>job role</b> SRE?&#34; </p><p>We should stress that we&#39;re not saying that you need a clearly defined <b>job role</b>—or even a <b>team</b>—before you can start utilizing the <b>principles and practices</b> to do things that are recognizably SRE-like. Job roles and teams crystallize from a more fluid set of responsibilities as organizations grow larger. But as this process plays out, the people involved may feel less certain of the scope of their responsibilities, precipitating the ‘are we there yet?’ question. We suspect that&#39;s where the tone of existential dread comes from...</p><h2>Key SRE indicators</h2><p>Within the CRE team here at Google Cloud, the ‘are we there yet?’ question surfaced a wide variety of opinions about the core principles that should guide an SRE team. We did manage to reach a rough consensus, with one proviso—the answer is partially dependent on how a team engages with the services it supports.</p><p>We&#39;ve chosen to structure this post around a set of principles that we would broadly expect groups of people working as SREs that <i>directly support services in production</i> to adhere to. As in a <a href="https://en.wikipedia.org/wiki/Litmus#Uses" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://en.wikipedia.org" track-metadata-module="post">litmus test</a>, this won&#39;t provide pin-point accuracy; but in our collective opinion at least, alignment with most of the principles laid out below is a good signal that a team is practicing something that can recognizably be termed <i>Site Reliability Engineering.</i></p><p>Directly engaged SRE teams are usually considered <i>Accountable</i> (in <a href="https://en.wikipedia.org/wiki/Responsibility_assignment_matrix" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://en.wikipedia.org" track-metadata-module="post">RACI terms</a>) for the service’s reliability, with <i>Responsibility</i> shared between the SRE and development teams. As a team provides less direct support these indicators may be less applicable. We hope those teams can still adapt the principles to their own circumstances. </p><p>To illustrate how you might do this, for each principle we&#39;ve given a counter-example of a team of SREs operating in an advisory capacity. They&#39;re subject-matter experts who are <i>Consulted</i> by development teams who are themselves <i>Responsible</i> and <i>Accountable</i> for service reliability.</p><p>Wherever your engagement model lies on the spectrum, being perceived by the rest of the organization as jointly responsible for a service&#39;s reliability, or as reliability subject-matter experts, is a key indicator of SRE-hood.</p><h3>Principle #1: SREs mitigate present and <i>future</i> incidents</h3><p>This principle is the one that usually underlies the perception of <b>responsibility and accountability</b> for a service&#39;s reliability. All the careful engineering and active regulation in the world can&#39;t guarantee reliability, especially in complex distributed systems—sometimes, things go wrong unexpectedly and the only thing left to do is react, mitigate, and fix. SREs have both the authority and the technical capability to act fast to restore service in these situations.</p><p>But mitigating the immediate problem isn&#39;t enough. If it can happen again tomorrow, then tomorrow isn&#39;t better than today, so SREs should work to understand the precipitating factors of incidents and propose changes that remediate the entire class of problem in the infrastructure they are responsible for. Don&#39;t have the same outage again next month!</p><p>How unique are your outages? Ask yourself these questions:</p><ul><li><p>Can you mitigate the majority of the incidents without needing specialist knowledge from the development team?</p></li><li><p>Do you maintain training materials and practice incident response scenarios?</p></li><li><p>After a major outage happens to your service, are you a key participant in blamelessly figuring out what <b>really</b> went wrong, and how to prevent future outages?</p></li></ul><p>Now, for a counter example. In many organizations, SREs are a scarce resource and may add more value by developing platforms and best practices to uplift large swathes of the company, rather than being primarily focused on incident response. Thus, a consulting SRE team would probably not be directly involved in mitigating most incidents, though they may be called on to coordinate incident response for a widespread outage. Rather than authoring training materials and postmortems, they would be responsible for reviewing those created by the teams they advise.</p><h3>Principle #2: SREs actively regulate service reliability</h3><p><b>Reliability goals and feedback signals are fundamental for both motivating SRE work and influencing the prioritization of development work.</b> At Google, we call our reliability goals <a href="https://sre.google/sre-book/service-level-objectives/" target="_blank" track-type="inline link" track-name="9" track-metadata-eventdetail="https://sre.google" track-metadata-module="post"><i>Service Level Objectives</i></a> and our feedback signals <i>Error Budgets</i>, and you can read more about how we use them in the <a href="https://sre.google/workbook/implementing-slos/" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">Site Reliability Workbook</a>.</p><p>Do your reliability signals affect your organization&#39;s priorities? Ask yourself these questions:</p><ul><li><p>Do you agree on goals for the reliability of the services you support with your organization, and track performance against those goals in real time?</p></li><li><p>Do you have an established feedback loop that moderates the behaviour of the organization based on recent service reliability?</p></li><li><p>Do you have the influence to effect change within the organization in pursuit of the reliability goals?</p></li><li><p>Do you have the <a href="https://en.wikipedia.org/wiki/Agency_(sociology)" target="_blank" track-type="inline link" track-name="11" track-metadata-eventdetail="https://en.wikipedia.org" track-metadata-module="post">agency</a> to refuse, or negotiate looser goals, when asked to make changes that may cause a service to miss its current reliability goals?</p></li></ul><p>Each question builds on the last. It is almost impossible to establish a data-driven feedback loop without a well-defined and measured reliability goal. For those goals to be meaningful, SREs must have the capability to defend them. Periods of lower service reliability should result in consequences that temporarily reduce the aggregate risk of future production changes and shift engineering priorities towards reliability. </p><p>When it comes down to a choice between service reliability and the rollout of new but unreliable features, SREs need to be able to say &#34;no&#34;. This should be a data driven decision—when there&#39;s not enough spare error budget, there needs to be a valid business reason for making users unhappy. Sometimes, of course, there will be, and this can be accommodated with new, lower SLO targets that reflect the relaxed reliability requirements.</p><p>Consultant SREs, in contrast, help teams draft their reliability goals and may develop shared monitoring infrastructure for measuring them across the organization. They are the de-facto regulators of the reliability feedback loop and maintain the policy documents that underpin it. Their connection to many teams and services gives them broader insights that can spark cross-functional reliability improvements. </p><h3>Principle #3: SREs engage early and comprehensively</h3><p>As we said earlier, <b>SREs should be empowered to make tomorrow better than today.</b> Without the ability to change the code and configuration of the services they support, they cannot fix problems as they encounter them. Involving SREs earlier in the design process can head off common reliability anti-patterns that are costly to correct <i>post-facto</i>. And, with the ability to influence architectural decision making, SREs can drive convergence across an organization so that work to increase the reliability of one service can benefit the entire company.</p><p>Is your team actively working to make tomorrow better than today? Ask yourself these questions, which go from fine detail to a broad, high-level scope:</p><ul><li><p>Do you engineer your service <b>now</b> to improve its reliability, e.g. by viewing and modifying the source code and/or configuration?</p></li><li><p>Are you involved in analysis and design of <b>future</b> iterations of your service, providing a lens on reliability/operability/maintainability?</p></li><li><p>Can you influence your <b>organization’s</b> wider architectural decision making?</p></li></ul><p>Advising other teams naturally shifts priorities away from directly modifying the configuration or code of individual services. But consultant SREs may still maintain frameworks or shared libraries providing core reliability features, like exporting common metrics or graceful service degradation. Their breadth of engagements across many teams makes them naturally suited for providing high-level architectural advice to improve reliability across an entire organization.</p><h3>Principle #4: SREs automate anything repetitive</h3><p>Finally, SREs believe that computers are fundamentally better suited to doing repetitive work than humans are. People often underestimate the <a href="https://xkcd.com/1205/" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="https://xkcd.com" track-metadata-module="post">returns on investment</a> when considering whether to automate a routine task, and that&#39;s before factoring in the exponential growth curve that comes with running a large, successful service. Moreover, computers never become inattentive and make mistakes when doing the same task for the hundredth time, or become demoralized and quit. Hiring or <a href="https://sre.google/resources/practices-and-processes/training-site-reliability-engineers/" target="_blank" track-type="inline link" track-name="13" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">training SREs</a> is expensive and time-consuming, so a successful SRE organization depends heavily on making computers do the grunt work.</p><p>Are you sufficiently automating your work? Ask yourself these questions:</p><ul><li><p>Do you use—or create—automation and other tools to ensure that operational load won&#39;t scale linearly with organic growth or the number of services you support?</p></li><li><p>Do you try to measure repetitive work on your team, and reduce it over time?</p></li></ul><h2>A call for reflection</h2><p>Most blog posts end with a call for action. We&#39;d rather you took time to reflect, instead of jumping up to make changes straight after reading. <b>There&#39;s a risk, when writing an opinionated piece like this, that the lines drawn in the sand are used to divide, not to grow and improve.</b> We promise this isn&#39;t a deliberate effort to gatekeep SRE and exclude those who don&#39;t tick the boxes; we see no value in that. But in some ways gatekeeping is what job roles are <i>designed</i> to do, because specialization and the division of labour is critical to the success of any organization, and this makes it hard to avoid drawing those lines.</p><p>For those who aspire to call themselves SREs, or are concerned that others may disagree with their characterization of themselves as SREs, perhaps these opinions can assuage some of that existential dread.</p></div></div>]]></content:encoded>
      <author>&lt;name&gt;Alex Bramley&lt;/name&gt;&lt;title&gt;Customer Reliability Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 18 Jun 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Node, Python and Java repositories now available in Artifact Registry</title>
      <link>https://cloud.google.com/blog/products/application-development/artifact-registry-adds-node-python-and-java-repositories/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;As a developer, you need a secure place to store all your stuff: container images of course, but also language packages that can enable code reuse across multiple applications. Today, we’re pleased to announce support for Node.js, Python and Java repositories for &lt;a href=&#34;https://cloud.google.com/artifact-registry&#34;&gt;Artifact Registry&lt;/a&gt; in Preview. With today’s announcement, you can not only use Artifact Registry to secure and distribute container images, but also manage and secure your other software artifacts. &lt;/p&gt;&lt;p&gt;At the same time, the Artifact Registry managed service provides advantages over on-premises registries. As a fully serverless platform, it scales based on demand, so you only pay for what you actually use. Enterprise security features such as VPC-SC, CMEK, and granular IAM ensure you get greater control and security features for both container and non-container artifacts. You can also connect to tools you are already using as a part of a CI/CD workflow. &lt;/p&gt;&lt;p&gt;Let’s take a closer look at the features you’ll find in Artifact Registry, giving you a fully-managed tool to store, manage, and secure all your artifacts. &lt;/p&gt;&lt;h3&gt;Expanded repository formats&lt;/h3&gt;&lt;p&gt;With support for new repository formats, you can streamline and get a consistent view across all your artifacts. Now, supported artifacts include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/java&#34;&gt;Java&lt;/a&gt; packages  (using the Maven repository format)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/nodejs&#34;&gt;Node.js&lt;/a&gt; packages (using the npm repository format)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/python&#34;&gt;Python&lt;/a&gt; packages (using the PyPI repository format)&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In addition to existing container images and Helm charts (using the Docker repository format). &lt;/p&gt;&lt;h3&gt;Easy integration with your CI/CD toolchain&lt;/h3&gt;&lt;p&gt;You can also integrate Artifact Registry, including the new repository formats, with Google Cloud’s build and runtime services or your existing build system. The following are just some of the use cases that are made possible by this integration:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Deployment to Google Kubernetes Engine (GKE), Cloud Run, Compute Engine and other runtime services &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CI/CD with Cloud Build, with automatic vulnerability scanning for OCI images &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Compatibility with Jenkins, Circle CI, TeamCity and other CI tools &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Native support for &lt;a href=&#34;https://cloud.google.com/binary-authorization&#34;&gt;Binary Authorization&lt;/a&gt; to ensure only approved artifact images are deployed&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Storage and management of artifacts in a variety of formats&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Streamlined authentication and access control across repositories using Google Cloud IAM&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;A more secure software supply chain&lt;/h3&gt;&lt;p&gt;Storing trusted artifacts in private repositories is a key part of a secure software supply chain and helps mitigate the risks associated with using artifacts directly from public repositories. With Artifact Registry, you can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Scan container images for vulnerabilities&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Protect repositories via a security perimeter (VPC-SC support)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Configure access control at the repository level using Cloud IAM&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use customer managed encryption keys (CMEK) instead of the default Google-managed encryption&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use Cloud Audit Logging to track and review repository usage&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Optimize your infrastructure and maintain data compliance&lt;/h3&gt;&lt;p&gt;Artifact Registry provides regional support, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.&lt;/p&gt;&lt;h2&gt;Get started today&lt;/h2&gt;&lt;p&gt;These new features are available to all Artifact Registry customers. Pricing for language packages is the same as container pricing; see the &lt;a href=&#34;https://cloud.google.com/artifact-registry/pricing&#34;&gt;pricing documentation&lt;/a&gt; for details.To get started using Node.js, Python and Java repositories, try the quickstarts in the Artifact Registry documentation.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/nodejs&#34;&gt;Node.js&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/python&#34;&gt;Python&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/java&#34;&gt;Java&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://youtu.be/2-P4cSCk1VM&#34; target=&#34;_blank&#34;&gt;Video Overview: using Maven in Artifact Registry&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/identity-security/how-were-helping-reshape-software-supply-chain-ecosystem-securely/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Security_BlogHeader_B_epmyJP1.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;How we’re helping to reshape the software supply chain ecosystem securely&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;We’re sharing some of the security best practices we employ and investments we make in secure software development and supply chain risk ...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;As a developer, you need a secure place to store all your stuff: container images of course, but also language packages that can enable code reuse across multiple applications. Today, we&amp;#8217;re pleased to announce support for Node.js, Python and Java repositories for &lt;a href=&#34;https://cloud.google.com/artifact-registry&#34;&gt;Artifact Registry&lt;/a&gt; in Preview. With today&amp;#8217;s announcement, you can not only use Artifact Registry to secure and distribute container images, but also manage and secure your other software artifacts.&amp;#160;&lt;/p&gt;&lt;p&gt;At the same time, the Artifact Registry managed service provides advantages over on-premises registries. As a fully serverless platform, it scales based on demand, so you only pay for what you actually use. Enterprise security features such as VPC-SC, CMEK, and granular IAM ensure you get greater control and security features for both container and non-container artifacts. You can also connect to tools you are already using as a part of a CI/CD workflow.&amp;#160;&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s take a closer look at the features you&amp;#8217;ll find in Artifact Registry, giving you a fully-managed tool to store, manage, and secure all your artifacts.&amp;#160;&lt;/p&gt;&lt;h3&gt;Expanded repository formats&lt;/h3&gt;&lt;p&gt;With support for new repository formats, you can streamline and get a consistent view across all your artifacts. Now, supported artifacts include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/java&#34;&gt;Java&lt;/a&gt; packages&amp;#160; (using the Maven repository format)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/nodejs&#34;&gt;Node.js&lt;/a&gt; packages (using the npm repository format)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/python&#34;&gt;Python&lt;/a&gt; packages (using the PyPI repository format)&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In addition to existing container images and Helm charts (using the Docker repository format).&amp;#160;&lt;/p&gt;&lt;h3&gt;Easy integration with your CI/CD toolchain&lt;/h3&gt;&lt;p&gt;You can also integrate Artifact Registry, including the new repository formats, with Google Cloud&amp;#8217;s build and runtime services or your existing build system. The following are just some of the use cases that are made possible by this integration:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Deployment to Google Kubernetes Engine (GKE), Cloud Run, Compute Engine and other runtime services&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;CI/CD with Cloud Build, with automatic vulnerability scanning for OCI images&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Compatibility with Jenkins, Circle CI, TeamCity and other CI tools&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Native support for &lt;a href=&#34;https://cloud.google.com/binary-authorization&#34;&gt;Binary Authorization&lt;/a&gt; to ensure only approved artifact images are deployed&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Storage and management of artifacts in a variety of formats&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Streamlined authentication and access control across repositories using Google Cloud IAM&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;A more secure software supply chain&lt;/h3&gt;&lt;p&gt;Storing trusted artifacts in private repositories is a key part of a secure software supply chain and helps mitigate the risks associated with using artifacts directly from public repositories. With Artifact Registry, you can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Scan container images for vulnerabilities&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Protect repositories via a security perimeter (VPC-SC support)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Configure access control at the repository level using Cloud IAM&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use customer managed encryption keys (CMEK) instead of the default Google-managed encryption&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use Cloud Audit Logging to track and review repository usage&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Optimize your infrastructure and maintain data compliance&lt;/h3&gt;&lt;p&gt;Artifact Registry provides regional support, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.&lt;/p&gt;&lt;h2&gt;Get started today&lt;/h2&gt;&lt;p&gt;These new features are available to all Artifact Registry customers. Pricing for language packages is the same as container pricing; see the &lt;a href=&#34;https://cloud.google.com/artifact-registry/pricing&#34;&gt;pricing documentation&lt;/a&gt; for details.To get started using Node.js, Python and Java repositories, try the quickstarts in the Artifact Registry documentation.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/nodejs&#34;&gt;Node.js&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/python&#34;&gt;Python&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/artifact-registry/docs/java&#34;&gt;Java&lt;/a&gt; Quickstart Guide&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://youtu.be/2-P4cSCk1VM&#34; target=&#34;_blank&#34;&gt;Video Overview: using Maven in Artifact Registry&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;"><p>As a developer, you need a secure place to store all your stuff: container images of course, but also language packages that can enable code reuse across multiple applications. Today, we’re pleased to announce support for Node.js, Python and Java repositories for <a href="https://cloud.google.com/artifact-registry" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/artifact-registry" track-metadata-module="post">Artifact Registry</a> in Preview. With today’s announcement, you can not only use Artifact Registry to secure and distribute container images, but also manage and secure your other software artifacts. </p><p>At the same time, the Artifact Registry managed service provides advantages over on-premises registries. As a fully serverless platform, it scales based on demand, so you only pay for what you actually use. Enterprise security features such as VPC-SC, CMEK, and granular IAM ensure you get greater control and security features for both container and non-container artifacts. You can also connect to tools you are already using as a part of a CI/CD workflow. </p><p>Let’s take a closer look at the features you’ll find in Artifact Registry, giving you a fully-managed tool to store, manage, and secure all your artifacts. </p><h3>Expanded repository formats</h3><p>With support for new repository formats, you can streamline and get a consistent view across all your artifacts. Now, supported artifacts include:</p><ul><li><p><a href="https://cloud.google.com/artifact-registry/docs/java" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/java" track-metadata-module="post">Java</a> packages  (using the Maven repository format)</p></li><li><p><a href="https://cloud.google.com/artifact-registry/docs/nodejs" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/nodejs" track-metadata-module="post">Node.js</a> packages (using the npm repository format)</p></li><li><p><a href="https://cloud.google.com/artifact-registry/docs/python" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/python" track-metadata-module="post">Python</a> packages (using the PyPI repository format)</p></li></ul><p>In addition to existing container images and Helm charts (using the Docker repository format). </p><h3>Easy integration with your CI/CD toolchain</h3><p>You can also integrate Artifact Registry, including the new repository formats, with Google Cloud’s build and runtime services or your existing build system. The following are just some of the use cases that are made possible by this integration:</p><ul><li><p>Deployment to Google Kubernetes Engine (GKE), Cloud Run, Compute Engine and other runtime services </p></li><li><p>CI/CD with Cloud Build, with automatic vulnerability scanning for OCI images </p></li><li><p>Compatibility with Jenkins, Circle CI, TeamCity and other CI tools </p></li><li><p>Native support for <a href="https://cloud.google.com/binary-authorization" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/binary-authorization" track-metadata-module="post">Binary Authorization</a> to ensure only approved artifact images are deployed</p></li><li><p>Storage and management of artifacts in a variety of formats</p></li><li><p>Streamlined authentication and access control across repositories using Google Cloud IAM</p></li></ul><h3>A more secure software supply chain</h3><p>Storing trusted artifacts in private repositories is a key part of a secure software supply chain and helps mitigate the risks associated with using artifacts directly from public repositories. With Artifact Registry, you can:</p><ul><li><p>Scan container images for vulnerabilities</p></li><li><p>Protect repositories via a security perimeter (VPC-SC support)</p></li><li><p>Configure access control at the repository level using Cloud IAM</p></li><li><p>Use customer managed encryption keys (CMEK) instead of the default Google-managed encryption</p></li><li><p>Use Cloud Audit Logging to track and review repository usage</p></li></ul><h3>Optimize your infrastructure and maintain data compliance</h3><p>Artifact Registry provides regional support, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.</p><h2>Get started today</h2><p>These new features are available to all Artifact Registry customers. Pricing for language packages is the same as container pricing; see the <a href="https://cloud.google.com/artifact-registry/pricing" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/pricing" track-metadata-module="post">pricing documentation</a> for details.To get started using Node.js, Python and Java repositories, try the quickstarts in the Artifact Registry documentation.</p><ul><li><p><a href="https://cloud.google.com/artifact-registry/docs/nodejs" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/nodejs" track-metadata-module="post">Node.js</a> Quickstart Guide</p></li><li><p><a href="https://cloud.google.com/artifact-registry/docs/python" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/python" track-metadata-module="post">Python</a> Quickstart Guide</p></li><li><p><a href="https://cloud.google.com/artifact-registry/docs/java" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/artifact-registry/docs/java" track-metadata-module="post">Java</a> Quickstart Guide</p></li><li><p><a href="https://youtu.be/2-P4cSCk1VM" target="_blank" track-type="inline link" track-name="10" track-metadata-eventdetail="https://youtu.be" track-metadata-module="post">Video Overview: using Maven in Artifact Registry</a></p></li></ul></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Patrick Faucher&lt;/name&gt;&lt;title&gt;Senior Product Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Artifact_Registry_ubFsgrO.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 07 Jun 2021 16:30:00 +0000</pubDate>
    </item>
    <item>
      <title>How Lowe’s meets customer demand with Google SRE practices</title>
      <link>https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;Editor’s note:&lt;/b&gt; Today we hear from the Lowe’s SRE team. They share about how they have been able to increase the number of releases they can support by adopting Google’s &lt;a href=&#34;https://sre.google/&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering&lt;/a&gt; (SRE) framework and leveraging their partnership with Google Cloud. &lt;/i&gt;&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;At Lowe’s, we’ve made significant progress in our multiyear technology transformation. To modernize our systems and build new capabilities for our customers and associates, we leverage Google’s SRE framework and Google Cloud, which helps us meet their needs faster and more effectively. With these efforts, we’ve been able to go from one release every two weeks to 20+ releases daily—a 300x increase. &lt;/p&gt;&lt;p&gt;Our SRE transformation didn’t happen overnight, though. Every step along the way brought some challenges. But looking back, we are excited to see how much we have accomplished for our customers as a result. &lt;/p&gt;&lt;p&gt;Back in 2018, before adopting SRE practices, we were more reactive than proactive, following an “eyes on glass” approach. On-call structures and incident management efficiency were not at optimal levels with too many repetitive and manual tasks, resulting in operational toil. Production concerns were not surfaced into the product roadmap, which resulted in delays in making fixes.&lt;/p&gt;&lt;h3&gt;Bootstrapping SRE at Lowe’s&lt;/h3&gt;&lt;p&gt;As we moved from on-prem to &lt;a href=&#34;https://cloud.google.com/&#34;&gt;Google Cloud&lt;/a&gt;, we decided to move from a monolithic- to microservices-based architecture. And to better manage this new architecture, we embarked on an SRE journey. &lt;/p&gt;&lt;p&gt;Then as COVID-19 hit, we really had to accelerate this journey as customers increasingly moved to online ordering and delivery to meet their Total Home Improvement needs. To do so, we followed four key principles that allowed us to meet changing customer needs quickly and release fast and reliably.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Automate away toil &lt;br/&gt;&lt;/b&gt;As we moved from traditional Ops to an SRE ecosystem, our biggest opportunity was &lt;a href=&#34;https://sre.google/sre-book/eliminating-toil/&#34; target=&#34;_blank&#34;&gt;reducing toil&lt;/a&gt;, so that engineers can spend time on activities that drive business impact and customer outcomes. We think of toil as work that is manual, repetitive,  tactical, devoid of enduring value—but automatable. So, to tackle toil, we focused on automating away the need for manual intervention. As an example, we made sure engineers were not the first point of contact for any alert. Any triage or resolution that an engineer can perform, a machine can be trained to do the same. We used supervised and unsupervised learning techniques to automate our toil. With a long-term goal of “no toil,” our SREs work on identifying and reducing toil to a manageable level across the organization.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Engineer alignment through roadmaps&lt;br/&gt;&lt;/b&gt;Our goal is to maximize the engineering velocity of developer teams while keeping products reliable. We want an engagement model where product, SRE and development teams are closely aligned. A key way we’ve been able to create this alignment is by having our SREs embedded into domain and product teams. Each domain has an SRE, who is  involved at the beginning stages of product development to ensure that the domain stakeholders are in alignment with the SRE initiatives. As such, SREs are able to improve the reliability, performance, scalability and launch velocity of the services throughout all phases of the service lifecycle. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Adopt one-touch releases&lt;br/&gt;&lt;/b&gt;Our path to production used to contain many manual steps and validations, slowing the rate at which we released features. Additionally, we used to bulk all our releases together to deploy at once, which increased the risk of failure and created a longer feedback loop from production. To tackle this with an SRE mindset, we created a one-touch release process in which SREs review the product team’s pull requests. When approved, this triggers a DevSecOps pipeline that deploys the approved changes to production securely. This process created a safe, reliable and sustainable continuous delivery pipeline with quick feedback loops. Striking the right balance between speed, innovation and stability, we were able to increase our releases exponentially for the year, taking less than 30 minutes per release to deliver quality code, including various automated quality checks and processes, all in just one click. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Embrace capacity planning&lt;br/&gt;&lt;/b&gt;To ensure our services have enough spare capacity to handle any surge in traffic patterns, our SREs emphasize capacity planning, making recommended capacity changes in the continuous delivery (CD) pipeline. They constantly &lt;a href=&#34;https://cloud.google.com/monitoring&#34;&gt;monitor performance&lt;/a&gt; to make sure the service is robust, stable and available. And when there’s a sudden surge beyond the forecasted volume, SREs change the capacity on demand and document changes for the performance and domain teams. &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Capacity planning is especially important for us during peak holiday times such as Black Friday and Cyber Monday (BFCM). We lay out our SRE stability plan three months in advance and surface into the domain team’s product roadmap. This way development teams are able to allocate sufficient engineering time to reliability. We do performance testing to ensure the environment is able to sustain increased load over long periods of time and also handle sudden surges in traffic. We also do region failover testing at a global scale to validate the automatic failover duration of service level agreements  (SLAs), SRE and domain readiness. Additionally, we conduct Black Friday and Cyber Monday-specific destructive testing to validate customer experience, reliability and more.&lt;/p&gt;&lt;p&gt;Google Cloud’s Black Friday and Cyber Monday &lt;a href=&#34;https://cloud.google.com/solutions/retail&#34;&gt;white-glove service&lt;/a&gt; played a key role in ensuring our success in both BFCM 2019 and BFCM 2020. This service included on-site visits from Google’s Customer Reliability Engineering (CRE) team who reviewed Lowe’s web architecture, capacity planning, operations practices for event risks, and presented workshops on topics such as incident response best practices. &lt;/p&gt;&lt;h3&gt;Looking ahead&lt;/h3&gt;&lt;p&gt;There is always room for improvement, and at Lowe’s we aim to continuously improve our SRE practices. One thing that has worked well for us, which we plan to continue, has been our road shows, where senior SRE leads present to other SREs and application domain teams on the latest SRE principles and best practices, and to get input in real-time from them. &lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;Google’s tools&lt;/a&gt; and methodology have played an instrumental role in helping reshape our SRE practices and better serve our customers. We look forward to building on the momentum and partnership as we continue our SRE journey at Lowe’s. &lt;/p&gt;&lt;p&gt;&lt;i&gt;If you want to learn more about how to adopt SRE best practices on Google Cloud, &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring&#34;&gt;check out our documentation&lt;/a&gt;. If you want to learn more about Google SRE, &lt;a href=&#34;https://sre.google/&#34; target=&#34;_blank&#34;&gt;visit our website&lt;/a&gt;. Stay tuned for the next blogs with Lowe’s discussing how they trained their engineering talent to adopt SRE practices and tooling, and how they improved MTTR using SRE principles.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Four steps to jumpstarting your SRE practice&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Once you have leadership buy-in, there are some things you can do to get the SRE ball rolling, fast.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;&lt;i&gt;&lt;b&gt;Editor&amp;#8217;s note:&lt;/b&gt; Today we hear from the Lowe&amp;#8217;s SRE team. They share about how they have been able to increase the number of releases they can support by adopting Google&amp;#8217;s &lt;a href=&#34;https://sre.google/&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering&lt;/a&gt; (SRE) framework and leveraging their partnership with Google Cloud.&amp;#160;&lt;/i&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;At Lowe&amp;#8217;s, we&amp;#8217;ve made significant progress in our multiyear technology transformation. To modernize our systems and build new capabilities for our customers and associates, we leverage Google&amp;#8217;s SRE framework and Google Cloud, which helps us meet their needs faster and more effectively. With these efforts, we&amp;#8217;ve been able to go from one release every two weeks to 20+ releases daily&amp;#8212;a 300x increase.&amp;#160;&lt;/p&gt;&lt;p&gt;Our SRE transformation didn&amp;#8217;t happen overnight, though. Every step along the way brought some challenges. But looking back, we are excited to see how much we have accomplished for our customers as a result.&amp;#160;&lt;/p&gt;&lt;p&gt;Back in 2018, before adopting SRE practices, we were more reactive than proactive, following an &amp;#8220;eyes on glass&amp;#8221; approach. On-call structures and incident management efficiency were not at optimal levels with too many repetitive and manual tasks, resulting in operational toil. Production concerns were not surfaced into the product roadmap, which resulted in delays in making fixes.&lt;/p&gt;&lt;h3&gt;Bootstrapping SRE at Lowe&amp;#8217;s&lt;/h3&gt;&lt;p&gt;As we moved from on-prem to &lt;a href=&#34;https://cloud.google.com/&#34;&gt;Google Cloud&lt;/a&gt;, we decided to move from a monolithic- to microservices-based architecture. And to better manage this new architecture, we embarked on an SRE journey.&amp;#160;&lt;/p&gt;&lt;p&gt;Then as COVID-19 hit, we really had to accelerate this journey as customers increasingly moved to online ordering and delivery to meet their Total Home Improvement needs. To do so, we followed four key principles that allowed us to meet changing customer needs quickly and release fast and reliably.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Automate away toil&amp;#160;&lt;br&gt;&lt;/b&gt;As we moved from traditional Ops to an SRE ecosystem, our biggest opportunity was &lt;a href=&#34;https://sre.google/sre-book/eliminating-toil/&#34; target=&#34;_blank&#34;&gt;reducing toil&lt;/a&gt;, so that engineers can spend time on activities that drive business impact and customer outcomes. We think of toil as work that is manual, repetitive,&amp;#160; tactical, devoid of enduring value&amp;#8212;but automatable. So, to tackle toil, we focused on automating away the need for manual intervention. As an example, we made sure engineers were not the first point of contact for any alert. Any triage or resolution that an engineer can perform, a machine can be trained to do the same. We used supervised and unsupervised learning techniques to automate our toil. With a long-term goal of &amp;#8220;no toil,&amp;#8221; our SREs work on identifying and reducing toil to a manageable level across the organization.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Engineer alignment through roadmaps&lt;br&gt;&lt;/b&gt;Our goal is to maximize the engineering velocity of developer teams while keeping products reliable. We want an engagement model where product, SRE and development teams are closely aligned. A key way we&amp;#8217;ve been able to create this alignment is by having our SREs embedded into domain and product teams. Each domain has an SRE, who is&amp;#160; involved at the beginning stages of product development to ensure that the domain stakeholders are in alignment with the SRE initiatives. As such, SREs are able to improve the reliability, performance, scalability and launch velocity of the services throughout all phases of the service lifecycle.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Adopt one-touch releases&lt;br&gt;&lt;/b&gt;Our path to production used to contain many manual steps and validations, slowing the rate at which we released features. Additionally, we used to bulk all our releases together to deploy at once, which increased the risk of failure and created a longer feedback loop from production. To tackle this with an SRE mindset, we created a one-touch release process in which SREs review the product team&amp;#8217;s pull requests. When approved, this triggers a DevSecOps pipeline that deploys the approved changes to production securely. This process created a safe, reliable and sustainable continuous delivery pipeline with quick feedback loops. Striking the right balance between speed, innovation and stability, we were able to increase our releases exponentially for the year, taking less than 30 minutes per release to deliver quality code, including various automated quality checks and processes, all in just one click.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Embrace capacity planning&lt;br&gt;&lt;/b&gt;To ensure our services have enough spare capacity to handle any surge in traffic patterns, our SREs emphasize capacity planning, making recommended capacity changes in the continuous delivery (CD) pipeline. They constantly &lt;a href=&#34;https://cloud.google.com/monitoring&#34;&gt;monitor performance&lt;/a&gt; to make sure the service is robust, stable and available. And when there&amp;#8217;s a sudden surge beyond the forecasted volume, SREs change the capacity on demand and document changes for the performance and domain teams.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Capacity planning is especially important for us during peak holiday times such as Black Friday and Cyber Monday (BFCM). We lay out our SRE stability plan three months in advance and surface into the domain team&amp;#8217;s product roadmap. This way development teams are able to allocate sufficient engineering time to reliability. We do performance testing to ensure the environment is able to sustain increased load over long periods of time and also handle sudden surges in traffic. We also do region failover testing at a global scale to validate the automatic failover duration of service level agreements&amp;#160; (SLAs), SRE and domain readiness. Additionally, we conduct Black Friday and Cyber Monday-specific destructive testing to validate customer experience, reliability and more.&lt;/p&gt;&lt;p&gt;Google Cloud&amp;#8217;s Black Friday and Cyber Monday &lt;a href=&#34;https://cloud.google.com/solutions/retail&#34;&gt;white-glove service&lt;/a&gt; played a key role in ensuring our success in both BFCM 2019 and BFCM 2020. This service included on-site visits from Google&amp;#8217;s Customer Reliability Engineering (CRE) team who reviewed Lowe&amp;#8217;s web architecture, capacity planning, operations practices for event risks, and presented workshops on topics such as incident response best practices.&amp;#160;&lt;/p&gt;&lt;h3&gt;Looking ahead&lt;/h3&gt;&lt;p&gt;There is always room for improvement, and at Lowe&amp;#8217;s we aim to continuously improve our SRE practices. One thing that has worked well for us, which we plan to continue, has been our road shows, where senior SRE leads present to other SREs and application domain teams on the latest SRE principles and best practices, and to get input in real-time from them.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;Google&amp;#8217;s tools&lt;/a&gt; and methodology have played an instrumental role in helping reshape our SRE practices and better serve our customers. We look forward to building on the momentum and partnership as we continue our SRE journey at Lowe&amp;#8217;s.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;If you want to learn more about how to adopt SRE best practices on Google Cloud, &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring&#34;&gt;check out our documentation&lt;/a&gt;. If you want to learn more about Google SRE, &lt;a href=&#34;https://sre.google/&#34; target=&#34;_blank&#34;&gt;visit our website&lt;/a&gt;. Stay tuned for the next blogs with Lowe&amp;#8217;s discussing how they trained their engineering talent to adopt SRE practices and tooling, and how they improved MTTR using SRE principles.&lt;/i&gt;&lt;/p&gt;"><p><i><b>Editor’s note:</b> Today we hear from the Lowe’s SRE team. They share about how they have been able to increase the number of releases they can support by adopting Google’s <a href="https://sre.google/" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">Site Reliability Engineering</a> (SRE) framework and leveraging their partnership with Google Cloud. </i></p><hr/><p>At Lowe’s, we’ve made significant progress in our multiyear technology transformation. To modernize our systems and build new capabilities for our customers and associates, we leverage Google’s SRE framework and Google Cloud, which helps us meet their needs faster and more effectively. With these efforts, we’ve been able to go from one release every two weeks to 20+ releases daily—a 300x increase. </p><p>Our SRE transformation didn’t happen overnight, though. Every step along the way brought some challenges. But looking back, we are excited to see how much we have accomplished for our customers as a result. </p><p>Back in 2018, before adopting SRE practices, we were more reactive than proactive, following an “eyes on glass” approach. On-call structures and incident management efficiency were not at optimal levels with too many repetitive and manual tasks, resulting in operational toil. Production concerns were not surfaced into the product roadmap, which resulted in delays in making fixes.</p><h3>Bootstrapping SRE at Lowe’s</h3><p>As we moved from on-prem to <a href="https://cloud.google.com/" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/" track-metadata-module="post">Google Cloud</a>, we decided to move from a monolithic- to microservices-based architecture. And to better manage this new architecture, we embarked on an SRE journey. </p><p>Then as COVID-19 hit, we really had to accelerate this journey as customers increasingly moved to online ordering and delivery to meet their Total Home Improvement needs. To do so, we followed four key principles that allowed us to meet changing customer needs quickly and release fast and reliably.</p><ol><li><p><b>Automate away toil <br/></b>As we moved from traditional Ops to an SRE ecosystem, our biggest opportunity was <a href="https://sre.google/sre-book/eliminating-toil/" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">reducing toil</a>, so that engineers can spend time on activities that drive business impact and customer outcomes. We think of toil as work that is manual, repetitive,  tactical, devoid of enduring value—but automatable. So, to tackle toil, we focused on automating away the need for manual intervention. As an example, we made sure engineers were not the first point of contact for any alert. Any triage or resolution that an engineer can perform, a machine can be trained to do the same. We used supervised and unsupervised learning techniques to automate our toil. With a long-term goal of “no toil,” our SREs work on identifying and reducing toil to a manageable level across the organization.</p></li><li><p><b>Engineer alignment through roadmaps<br/></b>Our goal is to maximize the engineering velocity of developer teams while keeping products reliable. We want an engagement model where product, SRE and development teams are closely aligned. A key way we’ve been able to create this alignment is by having our SREs embedded into domain and product teams. Each domain has an SRE, who is  involved at the beginning stages of product development to ensure that the domain stakeholders are in alignment with the SRE initiatives. As such, SREs are able to improve the reliability, performance, scalability and launch velocity of the services throughout all phases of the service lifecycle. </p></li><li><p><b>Adopt one-touch releases<br/></b>Our path to production used to contain many manual steps and validations, slowing the rate at which we released features. Additionally, we used to bulk all our releases together to deploy at once, which increased the risk of failure and created a longer feedback loop from production. To tackle this with an SRE mindset, we created a one-touch release process in which SREs review the product team’s pull requests. When approved, this triggers a DevSecOps pipeline that deploys the approved changes to production securely. This process created a safe, reliable and sustainable continuous delivery pipeline with quick feedback loops. Striking the right balance between speed, innovation and stability, we were able to increase our releases exponentially for the year, taking less than 30 minutes per release to deliver quality code, including various automated quality checks and processes, all in just one click. </p></li><li><p><b>Embrace capacity planning<br/></b>To ensure our services have enough spare capacity to handle any surge in traffic patterns, our SREs emphasize capacity planning, making recommended capacity changes in the continuous delivery (CD) pipeline. They constantly <a href="https://cloud.google.com/monitoring" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/monitoring" track-metadata-module="post">monitor performance</a> to make sure the service is robust, stable and available. And when there’s a sudden surge beyond the forecasted volume, SREs change the capacity on demand and document changes for the performance and domain teams. </p></li></ol><p>Capacity planning is especially important for us during peak holiday times such as Black Friday and Cyber Monday (BFCM). We lay out our SRE stability plan three months in advance and surface into the domain team’s product roadmap. This way development teams are able to allocate sufficient engineering time to reliability. We do performance testing to ensure the environment is able to sustain increased load over long periods of time and also handle sudden surges in traffic. We also do region failover testing at a global scale to validate the automatic failover duration of service level agreements  (SLAs), SRE and domain readiness. Additionally, we conduct Black Friday and Cyber Monday-specific destructive testing to validate customer experience, reliability and more.</p><p>Google Cloud’s Black Friday and Cyber Monday <a href="https://cloud.google.com/solutions/retail" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/solutions/retail" track-metadata-module="post">white-glove service</a> played a key role in ensuring our success in both BFCM 2019 and BFCM 2020. This service included on-site visits from Google’s Customer Reliability Engineering (CRE) team who reviewed Lowe’s web architecture, capacity planning, operations practices for event risks, and presented workshops on topics such as incident response best practices. </p><h3>Looking ahead</h3><p>There is always room for improvement, and at Lowe’s we aim to continuously improve our SRE practices. One thing that has worked well for us, which we plan to continue, has been our road shows, where senior SRE leads present to other SREs and application domain teams on the latest SRE principles and best practices, and to get input in real-time from them. </p><p><a href="https://cloud.google.com/products/operations" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/products/operations" track-metadata-module="post">Google’s tools</a> and methodology have played an instrumental role in helping reshape our SRE practices and better serve our customers. We look forward to building on the momentum and partnership as we continue our SRE journey at Lowe’s. </p><p><i>If you want to learn more about how to adopt SRE best practices on Google Cloud, <a href="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring" track-metadata-module="post">check out our documentation</a>. If you want to learn more about Google SRE, <a href="https://sre.google/" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">visit our website</a>. Stay tuned for the next blogs with Lowe’s discussing how they trained their engineering talent to adopt SRE practices and tooling, and how they improved MTTR using SRE principles.</i></p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Rahul Mohan Kola Kandy&lt;/name&gt;&lt;title&gt;Sr. Manager, Digital SRE, Lowe’s Companies, Inc.&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/devops.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 07 Jun 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>DevOps on Google Cloud: tools to speed up software development velocity</title>
      <link>https://cloud.google.com/blog/products/application-development/forgerock-developers-stay-productive-with-google-cloud/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;Editor’s note&lt;/b&gt;: Today we hear from &lt;a href=&#34;https://www.forgerock.com/&#34; target=&#34;_blank&#34;&gt;ForgeRock&lt;/a&gt;, a multinational &lt;a href=&#34;https://en.wikipedia.org/wiki/Identity_and_access_management&#34; target=&#34;_blank&#34;&gt;identity and access management&lt;/a&gt; software company with more than 1,100 enterprise customers, including a major public broadcaster. In total, customers use the ForgeRock Identity Platform to authenticate and log in over 45 million users daily, helping them manage identity, governance, and access management across all platforms, including on-premises and multicloud environments. &lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Operating at that kind of scale isn’t easy. In this blog post, ForgeRock Engineering Director, Warren Strange discusses the three things that help make their developers efficient and productive, and the Google Cloud tools they use along the way. &lt;/i&gt;&lt;/p&gt;&lt;hr/&gt;&lt;p&gt;At ForgeRock, we’ve been an early adopter of Kubernetes, viewing it as a strategic platform. Running on Kubernetes allows us to drive multicloud support across &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE), Amazon (EKS), and Azure (AKS). So no matter which cloud our customers are running on, we are able to seamlessly integrate our products into customers&#39; environments. &lt;/p&gt;&lt;p&gt;Making it easier for ForgeRock&#39;s developers and operators to build, deploy and manage applications has been crucial in our ability to continually provide high quality solutions for our customers. We’re always looking for tools to improve productivity and keep our developers focused on coding instead of configuration. Google Cloud’s suite of DevOps tools have streamlined three specific practices to help keep our developers productive: &lt;/p&gt;&lt;h3&gt;1. Make developers productive within IDEs&lt;/h3&gt;&lt;p&gt;Developer productivity is core to the success of any organization, including ForgeRock. Since developers spend most of their time within their IDE of choice, our goal at ForgeRock has been to make it easier for our developers to write Kubernetes applications within the IDEs they know and love. &lt;a href=&#34;https://cloud.google.com/code&#34;&gt;Cloud Code&lt;/a&gt; helps us precisely with that: it makes the process of building, deploying, scaling, and managing Kubernetes infrastructure and applications a breeze. &lt;/p&gt;&lt;p&gt;In particular, working with the Kubernetes YAML syntax and schema takes time, and a lot of trial and error to master. Thanks to &lt;a href=&#34;https://cloud.google.com/code/docs/vscode/yaml-editing&#34;&gt;YAML authoring support&lt;/a&gt; within Cloud Code, we can easily avoid the complicated and time consuming task of writing YAML files at ForgeRock. With YAML authoring support, developers save time on every bug. Cloud Code’s inline  snippets, completions, and schema validation, a.k.a. “linting,” further streamline working with YAML files. &lt;/p&gt;&lt;p&gt;The benefits of Cloud Code extend to local development as well. Iterating locally on Kubernetes applications often requires multiple manual steps, including building container images, updating Kubernetes manifests, and redeploying applications. Doing these steps over and over again can be a chore. Cloud Code &lt;a href=&#34;https://cloud.google.com/code/docs/intellij/deploying-a-k8-app&#34;&gt;supports Skaffold under the hood,&lt;/a&gt; which tracks changes as they come and automatically rebuilds and redeploys—reducing repetitive development tasks. &lt;/p&gt;&lt;p&gt;Finally, developing for Kubernetes usually involves jumping between the IDE, documentation, samples etc. Cloud Code reduces this context switching with Kubernetes &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-code-samples&#34; target=&#34;_blank&#34;&gt;code samples&lt;/a&gt;. With samples, we can get new developers up and running quickly. They spend less time learning about configuration and management of the application—and spend more time on writing and evolving the code.&lt;/p&gt;&lt;h3&gt;2. Drive end-to-end automation&lt;/h3&gt;&lt;p&gt;To further improve developer productivity, we’ve focused on end-to-end automation: from writing code within IDEs, to automatically triggering CI/CD pipelines and running the code in production. In particular, &lt;a href=&#34;https://cloud.google.com/tekton&#34;&gt;Tekton&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/cloud-build&#34;&gt;Cloud Build&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/container-registry&#34;&gt;Container Registry,&lt;/a&gt; and GKE have been critical to Forgerock as we streamline the flow of code, feedback and remediation through the build and deployment processes. The process looks something like this:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/ForgeRock__Google.0997050516950896.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;ForgeRock + Google.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/ForgeRock__Google.0997050516950896.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;We begin by developing Kubernetes manifests and dockerfiles using Cloud Code. Then we use &lt;a href=&#34;https://cloud.google.com/blog/products/application-development/kubernetes-development-simplified-skaffold-is-now-ga&#34;&gt;Skaffold&lt;/a&gt; to build containers locally, while Cloud Build helps with continuous integration (CI). The &lt;a href=&#34;https://github.com/marketplace/google-cloud-build&#34; target=&#34;_blank&#34;&gt;Cloud Build GitHub app&lt;/a&gt; allows us to automate builds and tests as part of our GitHub workflow. Cloud Build is differentiated from other continuous integration tools since it is fully serverless. It scales up and scales down in response to load, with no need for us to pre-provision servers or pay in advance for additional capacity. We pay for the exact resources we use. &lt;/p&gt;&lt;p&gt;Once the image is built by Cloud Build, it is stored, managed, and secured in Google’s &lt;a href=&#34;https://cloud.google.com/container-registry&#34;&gt;Container Registry&lt;/a&gt;. Just like Cloud Build, Container Registry is serverless, so we only pay for what we  use. Additionally, since Container Registry comes with automatic vulnerability scanning, every time we upload a new image to Container Registry, we can also scan it for vulnerabilities. &lt;/p&gt;&lt;p&gt;Next, a Tekton pipeline is triggered, which deploys the docker images stored in Container Registry and Kubernetes manifests to a running GKE cluster. Along with Cloud Build, Tekton is a critical part of our CI/CD process at ForgeRock. Most importantly, since Tekton comes with standardized Kubernetes-native primitives, we can create continuous delivery workflows very quickly.&lt;/p&gt;&lt;p&gt;After deployment, Tekton triggers a functional test suite to ensure that the applications we deploy perform as expected. The test results are posted to our team Slack channel so all developers have instant access and insights about each cluster. From there, we are able to provide our customers with their finished product request.&lt;/p&gt;&lt;h3&gt;3.  Leverage multicloud patterns and practices&lt;/h3&gt;&lt;p&gt;The industry has seen a shift towards &lt;a href=&#34;https://cloud.google.com/multicloud&#34;&gt;multicloud&lt;/a&gt;. Organizations have adopted multicloud strategies to minimize vendor lock-in, take advantage of best-in-class solutions, improve cost-efficiencies, and increase flexibility through choice. &lt;/p&gt;&lt;p&gt;At ForgeRock, we’re big proponents of multicloud. Part of that comes from the fact that our identity and access management product works across Google Cloud, AWS, and Azure. Developing products using open-source technologies such as Kubernetes has been particularly helpful in driving this interoperability. &lt;/p&gt;&lt;p&gt;Tekton has been another critical project that has allowed us to prevent vendor lock-in. Thanks to Tekton, our continuous delivery pipelines can deploy across any Kubernetes cluster. Most importantly, since Tekton pipelines run on Kubernetes, these pipelines can be decoupled from the runtime. Like Tekton and Kubernetes, both Cloud Build and Container Registry are based on open technologies. &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-builders-community&#34; target=&#34;_blank&#34;&gt;Community-contributed builders&lt;/a&gt;and &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-builders&#34; target=&#34;_blank&#34;&gt;official builder images&lt;/a&gt; allow us to connect to a variety of tools as a part of the build process. And finally, with support for open technologies like &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/google-cloud-now-supports-buildpacks&#34;&gt;Google Cloud buildpacks&lt;/a&gt; within Cloud Build, we can build containers without even knowing Docker. &lt;/p&gt;&lt;p&gt;Making it easier for developers and operators to build, deploy and manage applications is critical for the success of any organization. Driving developer productivity within IDEs, leveraging end-to-end automation, and support for multi-cloud patterns and practices are just some of the ways we are trying to achieve this at ForgeRock. To learn more about ForgeRock, and to deploy the ForgeRock Identity Platform into your Kubernetes cluster, check out our open-source &lt;a href=&#34;https://github.com/ForgeRock/forgeops&#34; target=&#34;_blank&#34;&gt;ForgeOps&lt;/a&gt; repository on GitHub.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;&lt;i&gt;&lt;b&gt;Editor&amp;#8217;s note&lt;/b&gt;: Today we hear from &lt;a href=&#34;https://www.forgerock.com/&#34; target=&#34;_blank&#34;&gt;ForgeRock&lt;/a&gt;, a multinational &lt;a href=&#34;https://en.wikipedia.org/wiki/Identity_and_access_management&#34; target=&#34;_blank&#34;&gt;identity and access management&lt;/a&gt; software company with more than 1,100 enterprise customers, including a major public broadcaster. In total, customers use the ForgeRock Identity Platform to authenticate and log in over 45 million users daily, helping them manage identity, governance, and access management across all platforms, including on-premises and multicloud environments.&amp;#160;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Operating at that kind of scale isn&amp;#8217;t easy. In this blog post, ForgeRock Engineering Director, Warren Strange discusses the three things that help make their developers efficient and productive, and the Google Cloud tools they use along the way.&amp;#160;&lt;/i&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;At ForgeRock, we&amp;#8217;ve been an early adopter of Kubernetes, viewing it as a strategic platform. Running on Kubernetes allows us to drive multicloud support across &lt;a href=&#34;https://cloud.google.com/kubernetes-engine&#34;&gt;Google Kubernetes Engine&lt;/a&gt; (GKE), Amazon (EKS), and Azure (AKS). So no matter which cloud our customers are running on, we are able to seamlessly integrate our products into customers&#39; environments.&amp;#160;&lt;/p&gt;&lt;p&gt;Making it easier for ForgeRock&#39;s developers and operators to build, deploy and manage applications has been crucial in our ability to continually provide high quality solutions for our customers. We&amp;#8217;re always looking for tools to improve productivity and keep our developers focused on coding instead of configuration. Google Cloud&amp;#8217;s suite of DevOps tools have streamlined three specific practices to help keep our developers productive:&amp;#160;&lt;/p&gt;&lt;h3&gt;1. Make developers productive within IDEs&lt;/h3&gt;&lt;p&gt;Developer productivity is core to the success of any organization, including ForgeRock. Since developers spend most of their time within their IDE of choice, our goal at ForgeRock has been to make it easier for our developers to write Kubernetes applications within the IDEs they know and love. &lt;a href=&#34;https://cloud.google.com/code&#34;&gt;Cloud Code&lt;/a&gt; helps us precisely with that: it makes the process of building, deploying, scaling, and managing Kubernetes infrastructure and applications a breeze.&amp;#160;&lt;/p&gt;&lt;p&gt;In particular, working with the Kubernetes YAML syntax and schema takes time, and a lot of trial and error to master. Thanks to &lt;a href=&#34;https://cloud.google.com/code/docs/vscode/yaml-editing&#34;&gt;YAML authoring support&lt;/a&gt; within Cloud Code, we can easily avoid the complicated and time consuming task of writing YAML files at ForgeRock. With YAML authoring support, developers save time on every bug. Cloud Code&amp;#8217;s inline&amp;#160; snippets, completions, and schema validation, a.k.a. &amp;#8220;linting,&amp;#8221; further streamline working with YAML files.&amp;#160;&lt;/p&gt;&lt;p&gt;The benefits of Cloud Code extend to local development as well. Iterating locally on Kubernetes applications often requires multiple manual steps, including building container images, updating Kubernetes manifests, and redeploying applications. Doing these steps over and over again can be a chore. Cloud Code &lt;a href=&#34;https://cloud.google.com/code/docs/intellij/deploying-a-k8-app&#34;&gt;supports Skaffold under the hood,&lt;/a&gt; which tracks changes as they come and automatically rebuilds and redeploys&amp;#8212;reducing repetitive development tasks.&amp;#160;&lt;/p&gt;&lt;p&gt;Finally, developing for Kubernetes usually involves jumping between the IDE, documentation, samples etc. Cloud Code reduces this context switching with Kubernetes &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-code-samples&#34; target=&#34;_blank&#34;&gt;code samples&lt;/a&gt;. With samples, we can get new developers up and running quickly. They spend less time learning about configuration and management of the application&amp;#8212;and spend more time on writing and evolving the code.&lt;/p&gt;&lt;h3&gt;2. Drive end-to-end automation&lt;/h3&gt;&lt;p&gt;To further improve developer productivity, we&amp;#8217;ve focused on end-to-end automation: from writing code within IDEs, to automatically triggering CI/CD pipelines and running the code in production. In particular, &lt;a href=&#34;https://cloud.google.com/tekton&#34;&gt;Tekton&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/cloud-build&#34;&gt;Cloud Build&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/container-registry&#34;&gt;Container Registry,&lt;/a&gt; and GKE have been critical to Forgerock as we streamline the flow of code, feedback and remediation through the build and deployment processes. The process looks something like this:&lt;/p&gt;"><p><i><b>Editor’s note</b>: Today we hear from <a href="https://www.forgerock.com/" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://www.forgerock.com" track-metadata-module="post">ForgeRock</a>, a multinational <a href="https://en.wikipedia.org/wiki/Identity_and_access_management" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://en.wikipedia.org" track-metadata-module="post">identity and access management</a> software company with more than 1,100 enterprise customers, including a major public broadcaster. In total, customers use the ForgeRock Identity Platform to authenticate and log in over 45 million users daily, helping them manage identity, governance, and access management across all platforms, including on-premises and multicloud environments. </i></p><p><i>Operating at that kind of scale isn’t easy. In this blog post, ForgeRock Engineering Director, Warren Strange discusses the three things that help make their developers efficient and productive, and the Google Cloud tools they use along the way. </i></p><hr/><p>At ForgeRock, we’ve been an early adopter of Kubernetes, viewing it as a strategic platform. Running on Kubernetes allows us to drive multicloud support across <a href="https://cloud.google.com/kubernetes-engine" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/kubernetes-engine" track-metadata-module="post">Google Kubernetes Engine</a> (GKE), Amazon (EKS), and Azure (AKS). So no matter which cloud our customers are running on, we are able to seamlessly integrate our products into customers&#39; environments. </p><p>Making it easier for ForgeRock&#39;s developers and operators to build, deploy and manage applications has been crucial in our ability to continually provide high quality solutions for our customers. We’re always looking for tools to improve productivity and keep our developers focused on coding instead of configuration. Google Cloud’s suite of DevOps tools have streamlined three specific practices to help keep our developers productive: </p><h3>1. Make developers productive within IDEs</h3><p>Developer productivity is core to the success of any organization, including ForgeRock. Since developers spend most of their time within their IDE of choice, our goal at ForgeRock has been to make it easier for our developers to write Kubernetes applications within the IDEs they know and love. <a href="https://cloud.google.com/code" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/code" track-metadata-module="post">Cloud Code</a> helps us precisely with that: it makes the process of building, deploying, scaling, and managing Kubernetes infrastructure and applications a breeze. </p><p>In particular, working with the Kubernetes YAML syntax and schema takes time, and a lot of trial and error to master. Thanks to <a href="https://cloud.google.com/code/docs/vscode/yaml-editing" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/code/docs/vscode/yaml-editing" track-metadata-module="post">YAML authoring support</a> within Cloud Code, we can easily avoid the complicated and time consuming task of writing YAML files at ForgeRock. With YAML authoring support, developers save time on every bug. Cloud Code’s inline  snippets, completions, and schema validation, a.k.a. “linting,” further streamline working with YAML files. </p><p>The benefits of Cloud Code extend to local development as well. Iterating locally on Kubernetes applications often requires multiple manual steps, including building container images, updating Kubernetes manifests, and redeploying applications. Doing these steps over and over again can be a chore. Cloud Code <a href="https://cloud.google.com/code/docs/intellij/deploying-a-k8-app" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/code/docs/intellij/deploying-a-k8-app" track-metadata-module="post">supports Skaffold under the hood,</a> which tracks changes as they come and automatically rebuilds and redeploys—reducing repetitive development tasks. </p><p>Finally, developing for Kubernetes usually involves jumping between the IDE, documentation, samples etc. Cloud Code reduces this context switching with Kubernetes <a href="https://github.com/GoogleCloudPlatform/cloud-code-samples" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://github.com" track-metadata-module="post">code samples</a>. With samples, we can get new developers up and running quickly. They spend less time learning about configuration and management of the application—and spend more time on writing and evolving the code.</p><h3>2. Drive end-to-end automation</h3><p>To further improve developer productivity, we’ve focused on end-to-end automation: from writing code within IDEs, to automatically triggering CI/CD pipelines and running the code in production. In particular, <a href="https://cloud.google.com/tekton" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/tekton" track-metadata-module="post">Tekton</a>, <a href="https://cloud.google.com/cloud-build" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/cloud-build" track-metadata-module="post">Cloud Build</a>, <a href="https://cloud.google.com/container-registry" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/container-registry" track-metadata-module="post">Container Registry,</a> and GKE have been critical to Forgerock as we streamline the flow of code, feedback and remediation through the build and deployment processes. The process looks something like this:</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Warren Strange&lt;/name&gt;&lt;title&gt;Engineering Director, ForgeRock&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/appdev.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 01 Jun 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Four steps to jumpstarting your SRE practice</title>
      <link>https://cloud.google.com/blog/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;A few months ago, we wrote about how the first step to implementing Site Reliability Engineering (SRE) in an organization is &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board&#34;&gt;getting leadership on board&lt;/a&gt;. So, let’s assume that you’ve gone ahead and done that. Now what? What are some concrete steps you can take to get the SRE ball rolling? In this blog post, we’ll take a look at what you as an IT leader can do to fast-track SRE within your team. &lt;/p&gt;&lt;h2&gt;Step 1: Start small and iterate &lt;/h2&gt;&lt;p&gt;&#34;Rome wasn&#39;t built in a day,&#34; the saying goes, but you do need to start somewhere. When it comes to implementing SRE principles, the approach that I (and my team) found to be the most effective is to start with a proof of concept, learn from our mistakes, and iterate!&lt;/p&gt;&lt;h3&gt;Start by identifying a relevant application and/or team &lt;/h3&gt;&lt;p&gt;There are many factors that go into choosing a specific team or application for your SRE proof of concept. Most of the time, though, this is a strategic decision for the organization, which is outside the scope of this article. Possible candidates can be a team shifting from traditional operations or DevOps to SRE, or a need to increase reliability to a business-critical product. No matter the reason, it’s crucial to select an application that is:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Critical to the business. Your customers should care deeply about its uptime and reliability. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Currently in development. Pick an application in which the business is actively investing resources. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In a perfect world, the application provides data and metrics regarding its behaviour. &lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Conversely, stay away from proprietary software. If the application wasn’t built by you, it&#39;s not a good candidate for SRE! You need the ability to make strategic decisions about—and engineering changes to—the application as needed. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Pro tip&lt;/b&gt;: In general, if you have workloads both on-premises and in the cloud, try to start with the cloud-based app. If your engineers come from a traditional operations environment, changing their thinking away from &#39;bare metal&#39; and infrastructure metrics will be easier for a cloud-based app, as managed infrastructure turns practitioners into users and forces them to consume it like developers (APIs, infrastructure as code, etc.)&lt;/p&gt;&lt;p&gt;&lt;b&gt;Remember&lt;/b&gt;: Set realistic goals. Discouraging your team with unrealistic expectations early on will have a negative effect on the initiative. &lt;/p&gt;&lt;h2&gt;Step 2: Empower your teams&lt;/h2&gt;&lt;p&gt;Implementing SRE principles requires fostering a learning culture, and in that regard, &lt;b&gt;team enablement&lt;/b&gt; means both training them, i.e., in regards to knowledge, as well as &lt;i&gt;empowering&lt;/i&gt; them.&lt;/p&gt;&lt;p&gt;Building a training program is a topic in and of itself, but it’s important to think about an &lt;b&gt;enablement strategy&lt;/b&gt; at an early stage. Especially in large organizations, you need to address topics like internal upskilling, hiring and scaling the team as well as onboarding and creating a learning community. &lt;/p&gt;&lt;p&gt;Your enablement strategy should also accommodate employees at different levels and in different functions. For example, higher leadership&#39;s training will look very different from practitioners’ training. Leadership&#39;s education should be sufficient to get buy-in and to be able to make organizational decisions. To drive change in the entire organization, additional training to leadership on cultural concepts and practices might be required.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout_external&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;When it comes to engineering leadership and/or middle management (managers that manage managers), training should be a combination of  high-level cultural concepts to help foster the required culture, and technical SRE practices that are deep enough to understand prioritization, resource allocation, process creation, and future needs.&lt;/p&gt;&lt;p&gt;When it comes to practitioners, ideally you want the entire organization to be aligned both from a knowledge perspective as well as culturally. But as we’ve mentioned earlier, it’s best to start simple, with just one team.&lt;/p&gt;&lt;p&gt;The starting point for those teams should be to understand reliability and key concepts like &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos&#34;&gt;SLAs, SLOs, SLIs&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows&#34;&gt;error budgets&lt;/a&gt;. These are important because SRE is focused on the customer experience. Measuring whether systems meet customer expectations requires a shift in mindset and can take time.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout_external&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;After identifying your first application and/or the team responsible for it, it&#39;s time to identify the app’s user journeys, the set of interactions a user has with a service to achieve a single goal—for example, a single click or a multi-step pipeline, and rank them according to business impact. The most critical ones are called Critical User Journeys (CUJ), and these are where you should start  drafting SLO/SLIs.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_kqBt6Vr.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;image1.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_kqBt6Vr.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Pro tip&lt;/b&gt;: There are some general technical practices that can help you embrace SRE faster. For example, using less repos rather than more can help you reduce silos within the organization and better utilize resources. &lt;/p&gt;&lt;p&gt;Likewise, prioritizing automatic processes and self-healing systems can benefit reliability, but also team satisfaction, helping the organization retain talent.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout_external&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;b&gt;Final note&lt;/b&gt;: Similar to the way that you make architecture decisions, your chosen technology, solutions and implementation tools should enable you to do what you are trying to do and not vice versa. &lt;/p&gt;&lt;h2&gt;Step 3: Scale those learnings &lt;/h2&gt;&lt;p&gt;After you establish these SRE practices with one or a few teams, the next step is to think about building an SRE community and formalized processes across the organization. In some organizations, you can do this in parallel to the end of step 2, and in some organizations, only after you have a few successful implementations under your belt.&lt;/p&gt;&lt;p&gt;In this phase, you’ll probably want to address &lt;b&gt;community, culture, enablement and processes.&lt;/b&gt; You will need to address them all, especially as they are intertwined, but which one you prioritize will depend on your organization.&lt;/p&gt;&lt;p&gt;Creating an SRE &lt;b&gt;community&lt;/b&gt; in the organization is important both from a learning perspective, but also to establish a knowledge base of best practices, train subject-matter experts, help create needed guardrails, and align processes. &lt;/p&gt;&lt;p&gt;Building a community goes hand in hand with fostering an &lt;b&gt;empowered culture and training teams&lt;/b&gt;. The idea is that early adopters are ambassadors for SRE who share their learnings and train other teams in the organization. &lt;/p&gt;&lt;p&gt;It is also useful to identify potential ambassadors or champions in individual development teams who are passionate about SRE and will help with the adoption of those practices.&lt;/p&gt;&lt;p&gt;It is also crucial to create repeatable trainings for each functional role, including onboarding sessions. Onboarding new team members is a critical aspect of training and fostering an empowered SRE culture. Therefore it is vital to be mindful about your onboarding process and make sure that the knowledge is not lost when team members change roles.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout_external&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;During this phase, you also want to foster an org-wide culture that promotes psychological safety, accepts failure as normal and enables the team to learn from mistakes. For that, leadership must model the desired culture and promote transparency. &lt;/p&gt;&lt;p&gt;Finally, having structured and formalized processes can help reduce the stress around emergency response—especially being on-call. Processes can also provide clarity and make teams more collaborative and effective. &lt;/p&gt;&lt;p&gt;In order to have the most impact, start by prioritizing the most painful areas under your team’s remit—for example, clean up noisy alerts to avoid (or address) alert fatigue, automate your change management processes and involve only the necessary people to save team bandwidth. Team members shouldn&#39;t work on software engineering projects while doing on-call incident management, and vice-versa. Make sure they have enough bandwidth to do both, separately.  Similar to other areas, you want to use data to drive your decisions.  As such, identify where your teams spend the most time, and for how long. &lt;/p&gt;&lt;p&gt;If you find that it is challenging to collect this kind of data, be it quantitative or qualitative, a good starting point is often your emergency response processes, as those have a direct impact on the business, especially around the escalation process, incident management and related policies. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Pro tip&lt;/b&gt;: All the above practices contribute to reducing silos and align goals across the organization; those should include also your vendors and engineering partners. To that end, make sure your contracts with them capture those goals as well.&lt;/p&gt;&lt;h2&gt;Step 4: Embody a data-driven mindset&lt;/h2&gt;&lt;p&gt;Starting the SRE journey can take time, even if you&#39;re just implementing it for one team. Two quick wins that you can start with that will make a positive impact are collecting data and doing blameless postmortems.&lt;/p&gt;&lt;p&gt;In SRE we try to be as &lt;b&gt;data-driven&lt;/b&gt; as possible, so creating a measurement culture in your organization is crucial. When prioritizing data collection, ideally look for data that represents the customer experience. Collecting that data will help you identify your gaps and help you prioritize according to business needs and by extension your customer expectations.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout_external&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Another thing that you can do is run or improve &lt;a href=&#34;https://sre.google/sre-book/postmortem-culture/&#34; target=&#34;_blank&#34;&gt;postmortems&lt;/a&gt;, which are an essential way of learning from failure and fostering a strong SRE culture. From our experience, even organizations that do run postmortems can benefit from them much more with a few minor improvements. It is important to remember that postmortems should be blameless in order to make the team feel safe to share and learn from failures. And to make tomorrow better than today, i.e., not repeat the same problems, it’s important that postmortems include action items and are assigned to an owner. &lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons&#34;&gt;Creating a shared repository&lt;/a&gt; for postmortems can have a tremendous impact on the team: it increases transparency, reduces silos, and contributes to the &lt;a href=&#34;https://cloud.google.com/solutions/devops/devops-culture-learning-culture&#34;&gt;learning culture&lt;/a&gt;. It also shows the team that the organization “practices what it preaches.” Implementing a repository can be as easy as creating a shared drive.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Pro ti&lt;/b&gt;p: Postmortems should be blameless and actionable.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout_external&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h2&gt;On the SRE fast track&lt;/h2&gt;&lt;p&gt;Of course, no two organizations are alike, and no two SRE teams are either. But by following these steps, you can help get your team on the path to SRE success faster. To learn more about developing an effective SRE practice, check out the following resources. &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@ayeletsachti/sre-public-resources-for-gcp-customers-bab039444ad3&#34; target=&#34;_blank&#34;&gt;Collection of SRE Public resources&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/consulting&#34;&gt;Google Professional Services SRE packages&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/sre-success-starts-with-getting-leadership-on-board/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;With SRE, failing to plan is planning to fail&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;The process of becoming a successful Site Reliability Engineering shop starts well before you take your first class or read your first ma...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>#DevOps</span></p></figure></article-aspect-image-block><div><article-cta _nghost-c17=""><div _ngcontent-c17=""><h4 _ngcontent-c17=""><span _ngcontent-c17="">Try GCP</span></h4><p _ngcontent-c17=""><span _ngcontent-c17="">Start building on Google Cloud with $300 in free credits and 20+ always free products.</span></p><p><a _ngcontent-c17="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="free trial" track-metadata-eventdetail="https://cloud.google.com/free/" href="https://cloud.google.com/free/"><span _ngcontent-c17="">Free Trial</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;A few months ago, we wrote about how the first step to implementing Site Reliability Engineering (SRE) in an organization is &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board&#34;&gt;getting leadership on board&lt;/a&gt;. So, let&amp;#8217;s assume that you&amp;#8217;ve gone ahead and done that. Now what? What are some concrete steps you can take to get the SRE ball rolling? In this blog post, we&amp;#8217;ll take a look at what you as an IT leader can do to fast-track SRE within your team.&amp;#160;&lt;/p&gt;&lt;h2&gt;Step 1: Start small and iterate&amp;#160;&lt;/h2&gt;&lt;p&gt;&amp;#34;Rome wasn&#39;t built in a day,&amp;#34; the saying goes, but you do need to start somewhere. When it comes to implementing SRE principles, the approach that I (and my team) found to be the most effective is to start with a proof of concept, learn from our mistakes, and iterate!&lt;/p&gt;&lt;h3&gt;Start by identifying a relevant application and/or team&amp;#160;&lt;/h3&gt;&lt;p&gt;There are many factors that go into choosing a specific team or application for your SRE proof of concept. Most of the time, though, this is a strategic decision for the organization, which is outside the scope of this article. Possible candidates can be a team shifting from traditional operations or DevOps to SRE, or a need to increase reliability to a business-critical product. No matter the reason, it&amp;#8217;s crucial to select an application that is:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Critical to the business. Your customers should care deeply about its uptime and reliability.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Currently in development. Pick an application in which the business is actively investing resources.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;In a perfect world, the application provides data and metrics regarding its behaviour.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Conversely, stay away from proprietary software. If the application wasn&amp;#8217;t built by you, it&#39;s not a good candidate for SRE! You need the ability to make strategic decisions about&amp;#8212;and engineering changes to&amp;#8212;the application as needed.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Pro tip&lt;/b&gt;: In general, if you have workloads both on-premises and in the cloud, try to start with the cloud-based app. If your engineers come from a traditional operations environment, changing their thinking away from &#39;bare metal&#39; and infrastructure metrics will be easier for a cloud-based app, as managed infrastructure turns practitioners into users and forces them to consume it like developers (APIs, infrastructure as code, etc.)&lt;/p&gt;&lt;p&gt;&lt;b&gt;Remember&lt;/b&gt;: Set realistic goals. Discouraging your team with unrealistic expectations early on will have a negative effect on the initiative.&amp;#160;&lt;/p&gt;&lt;h2&gt;Step 2: Empower your teams&lt;/h2&gt;&lt;p&gt;Implementing SRE principles requires fostering a learning culture, and in that regard, &lt;b&gt;team enablement&lt;/b&gt; means both training them, i.e., in regards to knowledge, as well as &lt;i&gt;empowering&lt;/i&gt; them.&lt;/p&gt;&lt;p&gt;Building a training program is a topic in and of itself, but it&amp;#8217;s important to think about an &lt;b&gt;enablement strategy&lt;/b&gt; at an early stage. Especially in large organizations, you need to address topics like internal upskilling, hiring and scaling the team as well as onboarding and creating a learning community.&amp;#160;&lt;/p&gt;&lt;p&gt;Your enablement strategy should also accommodate employees at different levels and in different functions. For example, higher leadership&#39;s training will look very different from practitioners&amp;#8217; training. Leadership&#39;s education should be sufficient to get buy-in and to be able to make organizational decisions. To drive change in the entire organization, additional training to leadership on cultural concepts and practices might be required.&lt;/p&gt;"><p>A few months ago, we wrote about how the first step to implementing Site Reliability Engineering (SRE) in an organization is <a href="https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board" track-metadata-module="post">getting leadership on board</a>. So, let’s assume that you’ve gone ahead and done that. Now what? What are some concrete steps you can take to get the SRE ball rolling? In this blog post, we’ll take a look at what you as an IT leader can do to fast-track SRE within your team. </p><h2>Step 1: Start small and iterate </h2><p>&#34;Rome wasn&#39;t built in a day,&#34; the saying goes, but you do need to start somewhere. When it comes to implementing SRE principles, the approach that I (and my team) found to be the most effective is to start with a proof of concept, learn from our mistakes, and iterate!</p><h3>Start by identifying a relevant application and/or team </h3><p>There are many factors that go into choosing a specific team or application for your SRE proof of concept. Most of the time, though, this is a strategic decision for the organization, which is outside the scope of this article. Possible candidates can be a team shifting from traditional operations or DevOps to SRE, or a need to increase reliability to a business-critical product. No matter the reason, it’s crucial to select an application that is:</p><ol><li><p>Critical to the business. Your customers should care deeply about its uptime and reliability. </p></li><li><p>Currently in development. Pick an application in which the business is actively investing resources. </p></li><li><p>In a perfect world, the application provides data and metrics regarding its behaviour. </p></li></ol><p>Conversely, stay away from proprietary software. If the application wasn’t built by you, it&#39;s not a good candidate for SRE! You need the ability to make strategic decisions about—and engineering changes to—the application as needed. </p><p><b>Pro tip</b>: In general, if you have workloads both on-premises and in the cloud, try to start with the cloud-based app. If your engineers come from a traditional operations environment, changing their thinking away from &#39;bare metal&#39; and infrastructure metrics will be easier for a cloud-based app, as managed infrastructure turns practitioners into users and forces them to consume it like developers (APIs, infrastructure as code, etc.)</p><p><b>Remember</b>: Set realistic goals. Discouraging your team with unrealistic expectations early on will have a negative effect on the initiative. </p><h2>Step 2: Empower your teams</h2><p>Implementing SRE principles requires fostering a learning culture, and in that regard, <b>team enablement</b> means both training them, i.e., in regards to knowledge, as well as <i>empowering</i> them.</p><p>Building a training program is a topic in and of itself, but it’s important to think about an <b>enablement strategy</b> at an early stage. Especially in large organizations, you need to address topics like internal upskilling, hiring and scaling the team as well as onboarding and creating a learning community. </p><p>Your enablement strategy should also accommodate employees at different levels and in different functions. For example, higher leadership&#39;s training will look very different from practitioners’ training. Leadership&#39;s education should be sufficient to get buy-in and to be able to make organizational decisions. To drive change in the entire organization, additional training to leadership on cultural concepts and practices might be required.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;When it comes to engineering leadership and/or middle management (managers that manage managers), training should be a combination of&amp;#160; high-level cultural concepts to help foster the required culture, and technical SRE practices that are deep enough to understand prioritization, resource allocation, process creation, and future needs.&lt;/p&gt;&lt;p&gt;When it comes to practitioners, ideally you want the entire organization to be aligned both from a knowledge perspective as well as culturally. But as we&amp;#8217;ve mentioned earlier, it&amp;#8217;s best to start simple, with just one team.&lt;/p&gt;&lt;p&gt;The starting point for those teams should be to understand reliability and key concepts like &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos&#34;&gt;SLAs, SLOs, SLIs&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows&#34;&gt;error budgets&lt;/a&gt;. These are important because SRE is focused on the customer experience. Measuring whether systems meet customer expectations requires a shift in mindset and can take time.&lt;/p&gt;"><p>When it comes to engineering leadership and/or middle management (managers that manage managers), training should be a combination of  high-level cultural concepts to help foster the required culture, and technical SRE practices that are deep enough to understand prioritization, resource allocation, process creation, and future needs.</p><p>When it comes to practitioners, ideally you want the entire organization to be aligned both from a knowledge perspective as well as culturally. But as we’ve mentioned earlier, it’s best to start simple, with just one team.</p><p>The starting point for those teams should be to understand reliability and key concepts like <a href="https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos" track-metadata-module="post">SLAs, SLOs, SLIs</a> and <a href="https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows" track-metadata-module="post">error budgets</a>. These are important because SRE is focused on the customer experience. Measuring whether systems meet customer expectations requires a shift in mindset and can take time.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><p>After identifying your first application and/or the team responsible for it, it&#39;s time to identify the app’s user journeys, the set of interactions a user has with a service to achieve a single goal—for example, a single click or a multi-step pipeline, and rank them according to business impact. The most critical ones are called Critical User Journeys (CUJ), and these are where you should start  drafting SLO/SLIs.</p></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;&lt;b&gt;Pro tip&lt;/b&gt;: There are some general technical practices that can help you embrace SRE faster. For example, using less repos rather than more can help you reduce silos within the organization and better utilize resources.&amp;#160;&lt;/p&gt;&lt;p&gt;Likewise, prioritizing automatic processes and self-healing systems can benefit reliability, but also team satisfaction, helping the organization retain talent.&lt;/p&gt;"><p><b>Pro tip</b>: There are some general technical practices that can help you embrace SRE faster. For example, using less repos rather than more can help you reduce silos within the organization and better utilize resources. </p><p>Likewise, prioritizing automatic processes and self-healing systems can benefit reliability, but also team satisfaction, helping the organization retain talent.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;&lt;b&gt;Final note&lt;/b&gt;: Similar to the way that you make architecture decisions, your chosen technology, solutions and implementation tools should enable you to do what you are trying to do and not vice versa.&amp;#160;&lt;/p&gt;&lt;h2&gt;Step 3: Scale those learnings&amp;#160;&lt;/h2&gt;&lt;p&gt;After you establish these SRE practices with one or a few teams, the next step is to think about building an SRE community and formalized processes across the organization. In some organizations, you can do this in parallel to the end of step 2, and in some organizations, only after you have a few successful implementations under your belt.&lt;/p&gt;&lt;p&gt;In this phase, you&amp;#8217;ll probably want to address &lt;b&gt;community, culture, enablement and processes.&lt;/b&gt; You will need to address them all, especially as they are intertwined, but which one you prioritize will depend on your organization.&lt;/p&gt;&lt;p&gt;Creating an SRE &lt;b&gt;community&lt;/b&gt; in the organization is important both from a learning perspective, but also to establish a knowledge base of best practices, train subject-matter experts, help create needed guardrails, and align processes.&amp;#160;&lt;/p&gt;&lt;p&gt;Building a community goes hand in hand with fostering an &lt;b&gt;empowered culture and training teams&lt;/b&gt;. The idea is that early adopters are ambassadors for SRE who share their learnings and train other teams in the organization.&amp;#160;&lt;/p&gt;&lt;p&gt;It is also useful to identify potential ambassadors or champions in individual development teams who are passionate about SRE and will help with the adoption of those practices.&lt;/p&gt;&lt;p&gt;It is also crucial to create repeatable trainings for each functional role, including onboarding sessions. Onboarding new team members is a critical aspect of training and fostering an empowered SRE culture. Therefore it is vital to be mindful about your onboarding process and make sure that the knowledge is not lost when team members change roles.&lt;/p&gt;"><p><b>Final note</b>: Similar to the way that you make architecture decisions, your chosen technology, solutions and implementation tools should enable you to do what you are trying to do and not vice versa. </p><h2>Step 3: Scale those learnings </h2><p>After you establish these SRE practices with one or a few teams, the next step is to think about building an SRE community and formalized processes across the organization. In some organizations, you can do this in parallel to the end of step 2, and in some organizations, only after you have a few successful implementations under your belt.</p><p>In this phase, you’ll probably want to address <b>community, culture, enablement and processes.</b> You will need to address them all, especially as they are intertwined, but which one you prioritize will depend on your organization.</p><p>Creating an SRE <b>community</b> in the organization is important both from a learning perspective, but also to establish a knowledge base of best practices, train subject-matter experts, help create needed guardrails, and align processes. </p><p>Building a community goes hand in hand with fostering an <b>empowered culture and training teams</b>. The idea is that early adopters are ambassadors for SRE who share their learnings and train other teams in the organization. </p><p>It is also useful to identify potential ambassadors or champions in individual development teams who are passionate about SRE and will help with the adoption of those practices.</p><p>It is also crucial to create repeatable trainings for each functional role, including onboarding sessions. Onboarding new team members is a critical aspect of training and fostering an empowered SRE culture. Therefore it is vital to be mindful about your onboarding process and make sure that the knowledge is not lost when team members change roles.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;During this phase, you also want to foster an org-wide culture that promotes psychological safety, accepts failure as normal and enables the team to learn from mistakes. For that, leadership must model the desired culture and promote transparency.&amp;#160;&lt;/p&gt;&lt;p&gt;Finally, having structured and formalized processes can help reduce the stress around emergency response&amp;#8212;especially being on-call. Processes can also provide clarity and make teams more collaborative and effective.&amp;#160;&lt;/p&gt;&lt;p&gt;In order to have the most impact, start by prioritizing the most painful areas under your team&amp;#8217;s remit&amp;#8212;for example, clean up noisy alerts to avoid (or address) alert fatigue, automate your change management processes and involve only the necessary people to save team bandwidth. Team members shouldn&#39;t work on software engineering projects while doing on-call incident management, and vice-versa. Make sure they have enough bandwidth to do both, separately.&amp;#160; Similar to other areas, you want to use data to drive your decisions.&amp;#160; As such, identify where your teams spend the most time, and for how long.&amp;#160;&lt;/p&gt;&lt;p&gt;If you find that it is challenging to collect this kind of data, be it quantitative or qualitative, a good starting point is often your emergency response processes, as those have a direct impact on the business, especially around the escalation process, incident management and related policies.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Pro tip&lt;/b&gt;: All the above practices contribute to reducing silos and align goals across the organization; those should include also your vendors and engineering partners. To that end, make sure your contracts with them capture those goals as well.&lt;/p&gt;&lt;h2&gt;Step 4: Embody a data-driven mindset&lt;/h2&gt;&lt;p&gt;Starting the SRE journey can take time, even if you&#39;re just implementing it for one team. Two quick wins that you can start with that will make a positive impact are collecting data and doing blameless postmortems.&lt;/p&gt;&lt;p&gt;In SRE we try to be as &lt;b&gt;data-driven&lt;/b&gt; as possible, so creating a measurement culture in your organization is crucial. When prioritizing data collection, ideally look for data that represents the customer experience. Collecting that data will help you identify your gaps and help you prioritize according to business needs and by extension your customer expectations.&lt;/p&gt;"><p>During this phase, you also want to foster an org-wide culture that promotes psychological safety, accepts failure as normal and enables the team to learn from mistakes. For that, leadership must model the desired culture and promote transparency. </p><p>Finally, having structured and formalized processes can help reduce the stress around emergency response—especially being on-call. Processes can also provide clarity and make teams more collaborative and effective. </p><p>In order to have the most impact, start by prioritizing the most painful areas under your team’s remit—for example, clean up noisy alerts to avoid (or address) alert fatigue, automate your change management processes and involve only the necessary people to save team bandwidth. Team members shouldn&#39;t work on software engineering projects while doing on-call incident management, and vice-versa. Make sure they have enough bandwidth to do both, separately.  Similar to other areas, you want to use data to drive your decisions.  As such, identify where your teams spend the most time, and for how long. </p><p>If you find that it is challenging to collect this kind of data, be it quantitative or qualitative, a good starting point is often your emergency response processes, as those have a direct impact on the business, especially around the escalation process, incident management and related policies. </p><p><b>Pro tip</b>: All the above practices contribute to reducing silos and align goals across the organization; those should include also your vendors and engineering partners. To that end, make sure your contracts with them capture those goals as well.</p><h2>Step 4: Embody a data-driven mindset</h2><p>Starting the SRE journey can take time, even if you&#39;re just implementing it for one team. Two quick wins that you can start with that will make a positive impact are collecting data and doing blameless postmortems.</p><p>In SRE we try to be as <b>data-driven</b> as possible, so creating a measurement culture in your organization is crucial. When prioritizing data collection, ideally look for data that represents the customer experience. Collecting that data will help you identify your gaps and help you prioritize according to business needs and by extension your customer expectations.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Another thing that you can do is run or improve &lt;a href=&#34;https://sre.google/sre-book/postmortem-culture/&#34; target=&#34;_blank&#34;&gt;postmortems&lt;/a&gt;, which are an essential way of learning from failure and fostering a strong SRE culture. From our experience, even organizations that do run postmortems can benefit from them much more with a few minor improvements. It is important to remember that postmortems should be blameless in order to make the team feel safe to share and learn from failures. And to make tomorrow better than today, i.e., not repeat the same problems, it&amp;#8217;s important that postmortems include action items and are assigned to an owner.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons&#34;&gt;Creating a shared repository&lt;/a&gt; for postmortems can have a tremendous impact on the team: it increases transparency, reduces silos, and contributes to the &lt;a href=&#34;https://cloud.google.com/solutions/devops/devops-culture-learning-culture&#34;&gt;learning culture&lt;/a&gt;. It also shows the team that the organization &amp;#8220;practices what it preaches.&amp;#8221; Implementing a repository can be as easy as creating a shared drive.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Pro ti&lt;/b&gt;p: Postmortems should be blameless and actionable.&lt;/p&gt;"><p>Another thing that you can do is run or improve <a href="https://sre.google/sre-book/postmortem-culture/" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">postmortems</a>, which are an essential way of learning from failure and fostering a strong SRE culture. From our experience, even organizations that do run postmortems can benefit from them much more with a few minor improvements. It is important to remember that postmortems should be blameless in order to make the team feel safe to share and learn from failures. And to make tomorrow better than today, i.e., not repeat the same problems, it’s important that postmortems include action items and are assigned to an owner. </p><p><a href="https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons" track-metadata-module="post">Creating a shared repository</a> for postmortems can have a tremendous impact on the team: it increases transparency, reduces silos, and contributes to the <a href="https://cloud.google.com/solutions/devops/devops-culture-learning-culture" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/solutions/devops/devops-culture-learning-culture" track-metadata-module="post">learning culture</a>. It also shows the team that the organization “practices what it preaches.” Implementing a repository can be as easy as creating a shared drive.</p><p><b>Pro ti</b>p: Postmortems should be blameless and actionable.</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c18=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Ayelet Sachto&lt;/name&gt;&lt;title&gt;Strategic Cloud Engineer, Infra, AppMod, SRE&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 25 May 2021 18:30:00 +0000</pubDate>
    </item>
    <item>
      <title>SRE fundamentals 2021: SLIs vs SLAs vs SLOs</title>
      <link>https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-sli-vs-slo-vs-sla/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;A big part of &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons&#34;&gt;ensuring the availability of your applications&lt;/a&gt; is establishing and monitoring service-level metrics—something that our &lt;a href=&#34;https://sre.google/&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering&lt;/a&gt; (SRE) team does every day here at Google Cloud. The end goal of our SRE principles is to improve services and in turn the user experience.&lt;/p&gt;&lt;p&gt;The concept of SRE starts with the idea that metrics should be closely tied to business objectives. In addition to business-level SLAs, we also use &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons&#34;&gt;SLOs and SLIs&lt;/a&gt; in SRE planning and practice. &lt;/p&gt;&lt;h2&gt;Defining the terms of site reliability engineering&lt;/h2&gt;&lt;p&gt;These tools aren’t just useful abstractions. Without them, you won’t know if your system is reliable, available, or even useful. If the tools don’t tie back to your business objectives, then you’ll be missing data on whether your choices are helping or hurting your business.&lt;/p&gt;&lt;p&gt;As a refresher, here’s a look at SLOs, SLAs, and SLIS, as discussed by our Customer Reliability Engineering team in their blog post, &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons&#34;&gt;SLOs, SLIs, SLAs, oh my - CRE life lessons&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;1. Service-Level Objective (SLO)&lt;/h3&gt;&lt;p&gt;SRE begins with the idea that &lt;a href=&#34;https://sre.google/sre-book/embracing-risk/&#34; target=&#34;_blank&#34;&gt;availability is a prerequisite for success&lt;/a&gt;. An unavailable system can’t perform its function and will fail by default. Availability, in SRE terms, defines whether a system is able to fulfill its intended function at a point in time. In addition to its use as a reporting tool, the historical availability measurement can also describe the probability that your system will perform as expected in the future.&lt;/p&gt;&lt;p&gt;When we set out to define the terms of SRE, we wanted to set a precise numerical target for system availability. We term this target the availability &lt;a href=&#34;https://sre.google/sre-book/service-level-objectives/&#34; target=&#34;_blank&#34;&gt;Service-Level Objective&lt;/a&gt; (SLO) of our system. Any future discussion about whether the system is running reliably and if any design or architectural changes to it are needed must be framed in terms of our system continuing to meet this SLO.&lt;/p&gt;&lt;p&gt;Keep in mind that the more reliable the service, the more it costs to operate. Define the lowest level of reliability that is acceptable for users of each service, then state that as your SLO. Every service should have an availability SLO—without it, your team and your stakeholders can’t make principled judgments about whether your service needs to be made more reliable (increasing cost and slowing development) or less reliable (allowing greater velocity of development). Excessive availability has become the expectation, which can lead to problems. Don’t make your system overly reliable if the user experience doesn’t necessitate it, and especially if you don’t intend to commit to always reaching that level. You can learn more about this by participating in &lt;a href=&#34;https://sre.google/resources/practices-and-processes/art-of-slos/&#34; target=&#34;_blank&#34;&gt;The Art of SLOs&lt;/a&gt; training.&lt;/p&gt;&lt;p&gt; Within Google Cloud, we implement periodic downtime in some services to prevent a service from being overly available. You could also try experimenting with occasional planned-downtime exercises with front-end servers, as we did with one of our internal systems. We found that these exercises can uncover services that are using those servers inappropriately. With that information, you can then move workloads to a more suitable place and keep servers at the right availability level.&lt;/p&gt;&lt;h3&gt;2. Service-Level Agreement (SLA)&lt;/h3&gt;&lt;p&gt;At Google Cloud, &lt;a href=&#34;https://sre.google/sre-book/service-level-objectives/&#34; target=&#34;_blank&#34;&gt;we distinguish between an SLO and a Service-Level Agreement (SLA)&lt;/a&gt;. An SLA normally involves a promise to a service user that the service availability SLO should meet a certain level over a certain period. Failing to do so then results in some kind of penalty. This might be a partial refund of the service subscription fee paid by customers for that period, or additional subscription time added for free. Going out of SLO will hurt the service team, so they will push hard to stay within SLO. If you’re charging your customers money, you’ll probably need an SLA.&lt;/p&gt;&lt;p&gt;Because of this, and because of the principle that availability shouldn’t be much better than the SLO, the availability SLO in the SLA is normally a looser objective than the internal availability SLO. This might be expressed in availability numbers: for instance, an availability SLO of 99.9% over one month, with an internal availability SLO of 99.95%. Alternatively, the SLA might only specify a subset of the metrics that make up the internal SLO.&lt;/p&gt;&lt;p&gt;If you have an SLO in your SLA that is different from your internal SLO (as it almost always is), it’s important for your monitoring to explicitly measure SLO compliance. You want to be able to view your system’s availability over the SLA calendar period, and quickly see if it appears to be in danger of going out of SLO. &lt;/p&gt;&lt;p&gt;You’ll also need a precise measurement of compliance, usually from logs analysis. Since we have an extra set of obligations (described in the SLA) to paying customers, we need to measure queries received from them separately from other queries. This is another benefit of establishing an SLA—it’s an unambiguous way to prioritize traffic.&lt;/p&gt;&lt;p&gt;When you define your SLA’s availability SLO, be careful about which queries you count as legitimate. For example, if a customer goes over quota because they released a buggy version of their mobile client, you may consider excluding all “out of quota” response codes from your SLA accounting.&lt;/p&gt;&lt;h3&gt;3. Service-Level Indicator (SLI)&lt;/h3&gt;&lt;p&gt;Our Service-Level Indicator (SLI) is a direct measurement of a service’s behavior, defined as the frequency of successful probes of our system. When we evaluate whether our system has been running within SLO for the past week, &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons&#34;&gt;we look at the SLI&lt;/a&gt; to get the service availability percentage. If it goes below the specified SLO, we have a problem and may need to make the system more available in some way, such as by running a second instance of the service in a different city and load-balancing between the two. If you want to know how reliable your service is, you must be able to measure the rates of successful and unsuccessful queries as your SLIs.&lt;/p&gt;&lt;p&gt;If you’re building a system from scratch, make sure that SLIs and SLOs are part of your system requirements. If you already have a production system but don’t have them clearly defined, then that’s your highest priority work.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_sZG2gQO.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Cloud Monitoring.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_sZG2gQO.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-image__caption &#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;i&gt;&lt;a href=&#34;https://cloud.google.com/monitoring&#34;&gt;Cloud Monitoring&lt;/a&gt; provides predefined dashboards for the Google Cloud services that you use. These dashboards require no setup or configuration effort. Learn how to set SLOs in Cloud Monitoring &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/ui/create-slo&#34;&gt;here&lt;/a&gt;.&lt;/i&gt;&lt;/div&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Learn more about these concepts in our &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos&#34;&gt;practical guide to setting SLOs&lt;/a&gt;, and make use of our &lt;a href=&#34;https://sre.google/resources/practices-and-processes/art-of-slos/&#34; target=&#34;_blank&#34;&gt;shared training materials&lt;/a&gt; to teach others in your organization.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/sre-at-google-our-complete-list-of-cre-life-lessons/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;SRE at Google: Our complete list of CRE life lessons&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Find links to blog posts that share Google’s SRE best practices in one handy location.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;A big part of &lt;a href=&#34;https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons&#34;&gt;ensuring the availability of your applications&lt;/a&gt; is establishing and monitoring service-level metrics&amp;#8212;something that our &lt;a href=&#34;https://sre.google/&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering&lt;/a&gt; (SRE) team does every day here at Google Cloud. The end goal of our SRE principles is to improve services and in turn the user experience.&lt;/p&gt;&lt;p&gt;The concept of SRE starts with the idea that metrics should be closely tied to business objectives. In addition to business-level SLAs, we also use &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons&#34;&gt;SLOs and SLIs&lt;/a&gt; in SRE planning and practice.&amp;#160;&lt;/p&gt;&lt;h2&gt;Defining the terms of site reliability engineering&lt;/h2&gt;&lt;p&gt;These tools aren&amp;#8217;t just useful abstractions. Without them, you won&amp;#8217;t know if your system is reliable, available, or even useful. If the tools don&amp;#8217;t tie back to your business objectives, then you&amp;#8217;ll be missing data on whether your choices are helping or hurting your business.&lt;/p&gt;&lt;p&gt;As a refresher, here&amp;#8217;s a look at SLOs, SLAs, and SLIS, as discussed by our Customer Reliability Engineering team in their blog post, &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons&#34;&gt;SLOs, SLIs, SLAs, oh my - CRE life lessons&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;1. Service-Level Objective (SLO)&lt;/h3&gt;&lt;p&gt;SRE begins with the idea that &lt;a href=&#34;https://sre.google/sre-book/embracing-risk/&#34; target=&#34;_blank&#34;&gt;availability is a prerequisite for success&lt;/a&gt;. An unavailable system can&amp;#8217;t perform its function and will fail by default. Availability, in SRE terms, defines whether a system is able to fulfill its intended function at a point in time. In addition to its use as a reporting tool, the historical availability measurement can also describe the probability that your system will perform as expected in the future.&lt;/p&gt;&lt;p&gt;When we set out to define the terms of SRE, we wanted to set a precise numerical target for system availability. We term this target the availability &lt;a href=&#34;https://sre.google/sre-book/service-level-objectives/&#34; target=&#34;_blank&#34;&gt;Service-Level Objective&lt;/a&gt; (SLO) of our system. Any future discussion about whether the system is running reliably and if any design or architectural changes to it are needed must be framed in terms of our system continuing to meet this SLO.&lt;/p&gt;&lt;p&gt;Keep in mind that the more reliable the service, the more it costs to operate. Define the lowest level of reliability that is acceptable for users of each service, then state that as your SLO. Every service should have an availability SLO&amp;#8212;without it, your team and your stakeholders can&amp;#8217;t make principled judgments about whether your service needs to be made more reliable (increasing cost and slowing development) or less reliable (allowing greater velocity of development). Excessive availability has become the expectation, which can lead to problems. Don&amp;#8217;t make your system overly reliable if the user experience doesn&amp;#8217;t necessitate it, and especially if you don&amp;#8217;t intend to commit to always reaching that level. You can learn more about this by participating in &lt;a href=&#34;https://sre.google/resources/practices-and-processes/art-of-slos/&#34; target=&#34;_blank&#34;&gt;The Art of SLOs&lt;/a&gt; training.&lt;/p&gt;&lt;p&gt;&amp;#160;Within Google Cloud, we implement periodic downtime in some services to prevent a service from being overly available. You could also try experimenting with occasional planned-downtime exercises with front-end servers, as we did with one of our internal systems. We found that these exercises can uncover services that are using those servers inappropriately. With that information, you can then move workloads to a more suitable place and keep servers at the right availability level.&lt;/p&gt;&lt;h3&gt;2. Service-Level Agreement (SLA)&lt;/h3&gt;&lt;p&gt;At Google Cloud, &lt;a href=&#34;https://sre.google/sre-book/service-level-objectives/&#34; target=&#34;_blank&#34;&gt;we distinguish between an SLO and a Service-Level Agreement (SLA)&lt;/a&gt;. An SLA normally involves a promise to a service user that the service availability SLO should meet a certain level over a certain period. Failing to do so then results in some kind of penalty. This might be a partial refund of the service subscription fee paid by customers for that period, or additional subscription time added for free. Going out of SLO will hurt the service team, so they will push hard to stay within SLO. If you&amp;#8217;re charging your customers money, you&amp;#8217;ll probably need an SLA.&lt;/p&gt;&lt;p&gt;Because of this, and because of the principle that availability shouldn&amp;#8217;t be much better than the SLO, the availability SLO in the SLA is normally a looser objective than the internal availability SLO. This might be expressed in availability numbers: for instance, an availability SLO of 99.9% over one month, with an internal availability SLO of 99.95%. Alternatively, the SLA might only specify a subset of the metrics that make up the internal SLO.&lt;/p&gt;&lt;p&gt;If you have an SLO in your SLA that is different from your internal SLO (as it almost always is), it&amp;#8217;s important for your monitoring to explicitly measure SLO compliance. You want to be able to view your system&amp;#8217;s availability over the SLA calendar period, and quickly see if it appears to be in danger of going out of SLO.&amp;#160;&lt;/p&gt;&lt;p&gt;You&amp;#8217;ll also need a precise measurement of compliance, usually from logs analysis. Since we have an extra set of obligations (described in the SLA) to paying customers, we need to measure queries received from them separately from other queries. This is another benefit of establishing an SLA&amp;#8212;it&amp;#8217;s an unambiguous way to prioritize traffic.&lt;/p&gt;&lt;p&gt;When you define your SLA&amp;#8217;s availability SLO, be careful about which queries you count as legitimate. For example, if a customer goes over quota because they released a buggy version of their mobile client, you may consider excluding all &amp;#8220;out of quota&amp;#8221; response codes from your SLA accounting.&lt;/p&gt;&lt;h3&gt;3. Service-Level Indicator (SLI)&lt;/h3&gt;&lt;p&gt;Our Service-Level Indicator (SLI) is a direct measurement of a service&amp;#8217;s behavior, defined as the frequency of successful probes of our system. When we evaluate whether our system has been running within SLO for the past week, &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons&#34;&gt;we look at the SLI&lt;/a&gt; to get the service availability percentage. If it goes below the specified SLO, we have a problem and may need to make the system more available in some way, such as by running a second instance of the service in a different city and load-balancing between the two. If you want to know how reliable your service is, you must be able to measure the rates of successful and unsuccessful queries as your SLIs.&lt;/p&gt;&lt;p&gt;If you&amp;#8217;re building a system from scratch, make sure that SLIs and SLOs are part of your system requirements. If you already have a production system but don&amp;#8217;t have them clearly defined, then that&amp;#8217;s your highest priority work.&lt;/p&gt;"><p>A big part of <a href="https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons" track-metadata-module="post">ensuring the availability of your applications</a> is establishing and monitoring service-level metrics—something that our <a href="https://sre.google/" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">Site Reliability Engineering</a> (SRE) team does every day here at Google Cloud. The end goal of our SRE principles is to improve services and in turn the user experience.</p><p>The concept of SRE starts with the idea that metrics should be closely tied to business objectives. In addition to business-level SLAs, we also use <a href="https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons" track-metadata-module="post">SLOs and SLIs</a> in SRE planning and practice. </p><h2>Defining the terms of site reliability engineering</h2><p>These tools aren’t just useful abstractions. Without them, you won’t know if your system is reliable, available, or even useful. If the tools don’t tie back to your business objectives, then you’ll be missing data on whether your choices are helping or hurting your business.</p><p>As a refresher, here’s a look at SLOs, SLAs, and SLIS, as discussed by our Customer Reliability Engineering team in their blog post, <a href="https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons" track-metadata-module="post">SLOs, SLIs, SLAs, oh my - CRE life lessons</a>.</p><h3>1. Service-Level Objective (SLO)</h3><p>SRE begins with the idea that <a href="https://sre.google/sre-book/embracing-risk/" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">availability is a prerequisite for success</a>. An unavailable system can’t perform its function and will fail by default. Availability, in SRE terms, defines whether a system is able to fulfill its intended function at a point in time. In addition to its use as a reporting tool, the historical availability measurement can also describe the probability that your system will perform as expected in the future.</p><p>When we set out to define the terms of SRE, we wanted to set a precise numerical target for system availability. We term this target the availability <a href="https://sre.google/sre-book/service-level-objectives/" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">Service-Level Objective</a> (SLO) of our system. Any future discussion about whether the system is running reliably and if any design or architectural changes to it are needed must be framed in terms of our system continuing to meet this SLO.</p><p>Keep in mind that the more reliable the service, the more it costs to operate. Define the lowest level of reliability that is acceptable for users of each service, then state that as your SLO. Every service should have an availability SLO—without it, your team and your stakeholders can’t make principled judgments about whether your service needs to be made more reliable (increasing cost and slowing development) or less reliable (allowing greater velocity of development). Excessive availability has become the expectation, which can lead to problems. Don’t make your system overly reliable if the user experience doesn’t necessitate it, and especially if you don’t intend to commit to always reaching that level. You can learn more about this by participating in <a href="https://sre.google/resources/practices-and-processes/art-of-slos/" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">The Art of SLOs</a> training.</p><p> Within Google Cloud, we implement periodic downtime in some services to prevent a service from being overly available. You could also try experimenting with occasional planned-downtime exercises with front-end servers, as we did with one of our internal systems. We found that these exercises can uncover services that are using those servers inappropriately. With that information, you can then move workloads to a more suitable place and keep servers at the right availability level.</p><h3>2. Service-Level Agreement (SLA)</h3><p>At Google Cloud, <a href="https://sre.google/sre-book/service-level-objectives/" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">we distinguish between an SLO and a Service-Level Agreement (SLA)</a>. An SLA normally involves a promise to a service user that the service availability SLO should meet a certain level over a certain period. Failing to do so then results in some kind of penalty. This might be a partial refund of the service subscription fee paid by customers for that period, or additional subscription time added for free. Going out of SLO will hurt the service team, so they will push hard to stay within SLO. If you’re charging your customers money, you’ll probably need an SLA.</p><p>Because of this, and because of the principle that availability shouldn’t be much better than the SLO, the availability SLO in the SLA is normally a looser objective than the internal availability SLO. This might be expressed in availability numbers: for instance, an availability SLO of 99.9% over one month, with an internal availability SLO of 99.95%. Alternatively, the SLA might only specify a subset of the metrics that make up the internal SLO.</p><p>If you have an SLO in your SLA that is different from your internal SLO (as it almost always is), it’s important for your monitoring to explicitly measure SLO compliance. You want to be able to view your system’s availability over the SLA calendar period, and quickly see if it appears to be in danger of going out of SLO. </p><p>You’ll also need a precise measurement of compliance, usually from logs analysis. Since we have an extra set of obligations (described in the SLA) to paying customers, we need to measure queries received from them separately from other queries. This is another benefit of establishing an SLA—it’s an unambiguous way to prioritize traffic.</p><p>When you define your SLA’s availability SLO, be careful about which queries you count as legitimate. For example, if a customer goes over quota because they released a buggy version of their mobile client, you may consider excluding all “out of quota” response codes from your SLA accounting.</p><h3>3. Service-Level Indicator (SLI)</h3><p>Our Service-Level Indicator (SLI) is a direct measurement of a service’s behavior, defined as the frequency of successful probes of our system. When we evaluate whether our system has been running within SLO for the past week, <a href="https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons" track-metadata-module="post">we look at the SLI</a> to get the service availability percentage. If it goes below the specified SLO, we have a problem and may need to make the system more available in some way, such as by running a second instance of the service in a different city and load-balancing between the two. If you want to know how reliable your service is, you must be able to measure the rates of successful and unsuccessful queries as your SLIs.</p><p>If you’re building a system from scratch, make sure that SLIs and SLOs are part of your system requirements. If you already have a production system but don’t have them clearly defined, then that’s your highest priority work.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Adrian Hilton&lt;/name&gt;&lt;title&gt;Customer Reliability Engineer, SRE&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 07 May 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Take the 2021 State of DevOps survey: Shape the future of DevOps</title>
      <link>https://cloud.google.com/blog/products/devops-sre/take-2021-state-devops-survey-shape-future-devops/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Today, Google Cloud and the &lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DORA&lt;/a&gt; research team are excited to announce the launch of the &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n&#34; target=&#34;_blank&#34;&gt;2021 State of DevOps survey&lt;/a&gt;. The &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; takes approximately 25 minutes to complete and we’d love to hear from you. Your answers will allow us to better understand the practices that teams are employing to improve software delivery performance and inturn generate powerful business outcomes.&lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper&#34;&gt;State of DevOps report&lt;/a&gt; by Google Cloud and the DORA research team is the largest and longest running research of its kind. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region can employ to drive better performance.  &lt;/p&gt;&lt;p&gt;Like the past six research reports, our goal this year is to perform detailed analysis to help various teams benchmark their performance against the industry as elite, high, medium, or low performers. We also look to show specific strategies that teams can employ to improve their performance. &lt;/p&gt;&lt;p&gt;The table below highlights elite, high, medium, and low performers at a glance from the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;last report.&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;dora2&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Screen_Shot_2021-05-03_at_12.43.32_AM.max-1000x1000.png&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Achieving elite performance is a team endeavor and diverse, inclusive teams drive the best performance. The research program benefits from the participation of a diverse group of people. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. &lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; is for everyone, regardless of where you are on your DevOps journey, the size of your organization, or your organization&#39;s industry. There are no right or wrong answers, in fact we often hear feedback that questions in the survey prompt ideas for improvement. Many of these ideas can be put into practice immediately. &lt;/p&gt;&lt;p&gt;Some of the key topics we look to deep dive into this year include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Metrics and Measurement: Practices employed by high performing teams &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;SRE and DevOps: How do they fit together and how they impact performance&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;How to best integrate security &amp;amp; compliance as a part of your app development &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The impact of cloud, monitoring &amp;amp; observability, open source, and documentation on performance&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Distributed teams: Practices to improve work/life balance and reduce burnout&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The state of multi-cloud computing &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Help us shape the future of DevOps and make your voice heard by completing the &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n&#34; target=&#34;_blank&#34;&gt;survey now&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n&#34; target=&#34;_blank&#34;&gt;The survey&lt;/a&gt; will remain open until midnight PST on July 2, 2021. We look forward to hearing from you. &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Today, Google Cloud and the &lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DORA&lt;/a&gt; research team are excited to announce the launch of the &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n&#34; target=&#34;_blank&#34;&gt;2021 State of DevOps survey&lt;/a&gt;. The &lt;a href=&#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n&#34; target=&#34;_blank&#34;&gt;survey&lt;/a&gt; takes approximately 25 minutes to complete and we&amp;#8217;d love to hear from you. Your answers will allow us to better understand the practices that teams are employing to improve software delivery performance and inturn generate powerful business outcomes.&lt;/p&gt;&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper&#34;&gt;State of DevOps report&lt;/a&gt; by Google Cloud and the DORA research team is the largest and longest running research of its kind. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region can employ to drive better performance.&amp;#160;&amp;#160;&lt;/p&gt;&lt;p&gt;Like the past six research reports, our goal this year is to perform detailed analysis to help various teams benchmark their performance against the industry as elite, high, medium, or low performers. We also look to show specific strategies that teams can employ to improve their performance.&amp;#160;&lt;/p&gt;&lt;p&gt;The table below highlights elite, high, medium, and low performers at a glance from the &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;last report.&lt;/a&gt;&lt;/p&gt;"><p>Today, Google Cloud and the <a href="https://www.devops-research.com/research.html" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://www.devops-research.com" track-metadata-module="post">DORA</a> research team are excited to announce the launch of the <a href="https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://google.qualtrics.com" track-metadata-module="post">2021 State of DevOps survey</a>. The <a href="https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://google.qualtrics.com" track-metadata-module="post">survey</a> takes approximately 25 minutes to complete and we’d love to hear from you. Your answers will allow us to better understand the practices that teams are employing to improve software delivery performance and inturn generate powerful business outcomes.</p><p>The <a href="https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper" track-metadata-module="post">State of DevOps report</a> by Google Cloud and the DORA research team is the largest and longest running research of its kind. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region can employ to drive better performance.  </p><p>Like the past six research reports, our goal this year is to perform detailed analysis to help various teams benchmark their performance against the industry as elite, high, medium, or low performers. We also look to show specific strategies that teams can employ to improve their performance. </p><p>The table below highlights elite, high, medium, and low performers at a glance from the <a href="https://cloud.google.com/devops/state-of-devops" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/devops/state-of-devops" track-metadata-module="post">last report.</a></p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Dustin Smith&lt;/name&gt;&lt;title&gt;DORA Research Lead&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/WorkplaceTransformation-01_TjiwJTd.max-1000x1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 03 May 2021 13:50:00 +0000</pubDate>
    </item>
    <item>
      <title>SRE at Google: Our complete list of CRE life lessons</title>
      <link>https://cloud.google.com/blog/products/devops-sre/sre-at-google-our-complete-list-of-cre-life-lessons/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;In 2016 we announced a new discipline at Google, &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering&#34;&gt;Customer Reliability Engineering&lt;/a&gt;, an offshoot of &lt;a href=&#34;https://sre.google/&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering&lt;/a&gt; (SRE). Our goal with CRE was (and still is) to create a shared operational fate between Google and our Google Cloud customers, to give you more control over the critical applications you&#39;re entrusting to us. Since then, here on the Google Cloud blog, we’ve published a wealth of resources to help you take the best practices we’ve learned from SRE teams at Google and apply them in your own environments. &lt;/p&gt;&lt;p&gt;Below is the complete list of CRE life lessons posts we’ve published &lt;a href=&#34;https://cloud.google.com/blog/topics/cre-life-lessons&#34;&gt;in the past five years&lt;/a&gt; in one convenient location.&lt;/p&gt;&lt;h3&gt;Common pitfalls&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons&#34;&gt;Know thy enemy: How to prioritize and communicate risks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/how-to-avoid-a-self-inflicted-ddos-attack-cre-life-lessons&#34;&gt;How to avoid a self-inflicted DDoS Attack&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/using-load-shedding-to-survive-a-success-disaster-cre-life-lessons&#34;&gt;Using load shedding to survive a success disaster&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Service-level metrics&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons&#34;&gt;Available . . . or not? That is the question&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons&#34;&gt;SLOs, SLIs, SLAs, oh my&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/building-good-slos-cre-life-lessons&#34;&gt;Building good SLOs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/consequences-of-slo-violations-cre-life-lessons&#34;&gt;Consequences of SLO violations&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/an-example-escalation-policy-cre-life-lessons&#34;&gt;An example escalation policy&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/applying-the-escalation-policy-cre-life-lessons&#34;&gt;Applying the escalation policy&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/defining-slos-for-services-with-dependencies-cre-life-lessons&#34;&gt;Defining SLOs for services with dependencies&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons&#34;&gt;Tune up your SLI metrics&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/learn-how-to-set-slos-for-an-sre-or-cre-practice&#34;&gt;Learning—and teaching—the art of service-level objectives&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/using-deemed-slis-to-measure-customer-reliability&#34;&gt;Using deemed SLIs to measure customer reliability&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Releases&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/reliable-releases-and-rollbacks-cre-life-lessons&#34;&gt;Reliable releases and rollbacks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/how-release-canaries-can-save-your-bacon-cre-life-lessons&#34;&gt;How release canaries can save your bacon&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;SRE support&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/why-should-your-app-get-sre-support-cre-life-lessons&#34;&gt;Why should your app get SRE support?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/how-sres-find-the-landmines-in-a-service-cre-life-lessons&#34;&gt;How SREs find the landmines in a service&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/making-the-most-of-an-sre-service-takeover-cre-life-lessons&#34;&gt;Making the most of an SRE service takeover&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Dark launches&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/cre-life-lessons-what-is-a-dark-launch-and-what-does-it-do-for-me&#34;&gt;What is a dark launch, and what does it do for me?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/cre-life-lessons-practicalities-of-dark-launches&#34;&gt;The practicalities of dark launching&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Postmortems&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons&#34;&gt;Fearless shared postmortems&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/getting-the-most-out-of-shared-postmortems-cre-life-lessons&#34;&gt;Getting the most out of shared postmortems&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Error budgets&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/good-housekeeping-error-budgetscre-life-lessons&#34;&gt;Good housekeeping for error budgets&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons&#34;&gt;Understanding error budget overspend&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Production incidents&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/shrinking-the-impact-of-production-incidents-using-sre-principles-cre-life-lessons&#34;&gt;Shrinking the impact of production incidents using SRE principles&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents&#34;&gt;Shrinking the time to mitigate production incidents&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Becoming an SRE team&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board&#34;&gt;How to implement a successful SRE practice from the outset&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice&#34;&gt;Jumpstarting your SRE practice&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum&#34;&gt;Assessing an SRE team’s maturity&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;We still have plenty more articles to come, so keep your eye on our &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre&#34;&gt;DevOps &amp;amp; SRE channel&lt;/a&gt;. You can also check out &lt;a href=&#34;http://sre.google&#34; target=&#34;_blank&#34;&gt;sre.google&lt;/a&gt; or &lt;a href=&#34;https://sre.google/books/&#34; target=&#34;_blank&#34;&gt;read our SRE books online&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/sre-fundamentals-sli-vs-slo-vs-sla/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;SRE fundamentals 2021: SLIs vs SLAs vs SLOs&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;What’s the difference between an SLI, an SLO and an SLA? Google Site Reliability Engineers (SRE) explain.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;In 2016 we announced a new discipline at Google, &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering&#34;&gt;Customer Reliability Engineering&lt;/a&gt;, an offshoot of &lt;a href=&#34;https://sre.google/&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering&lt;/a&gt; (SRE). Our goal with CRE was (and still is) to create a shared operational fate between Google and our Google Cloud customers, to give you more control over the critical applications you&#39;re entrusting to us. Since then, here on the Google Cloud blog, we&amp;#8217;ve published a wealth of resources to help you take the best practices we&amp;#8217;ve learned from SRE teams at Google and apply them in your own environments.&amp;#160;&lt;/p&gt;&lt;p&gt;Below is the complete list of CRE life lessons posts we&amp;#8217;ve published &lt;a href=&#34;https://cloud.google.com/blog/topics/cre-life-lessons&#34;&gt;in the past five years&lt;/a&gt; in one convenient location.&lt;/p&gt;&lt;h3&gt;Common pitfalls&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons&#34;&gt;Know thy enemy: How to prioritize and communicate risks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/how-to-avoid-a-self-inflicted-ddos-attack-cre-life-lessons&#34;&gt;How to avoid a self-inflicted DDoS Attack&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/using-load-shedding-to-survive-a-success-disaster-cre-life-lessons&#34;&gt;Using load shedding to survive a success disaster&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Service-level metrics&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons&#34;&gt;Available . . . or not? That is the question&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons&#34;&gt;SLOs, SLIs, SLAs, oh my&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/building-good-slos-cre-life-lessons&#34;&gt;Building good SLOs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/consequences-of-slo-violations-cre-life-lessons&#34;&gt;Consequences of SLO violations&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/an-example-escalation-policy-cre-life-lessons&#34;&gt;An example escalation policy&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/applying-the-escalation-policy-cre-life-lessons&#34;&gt;Applying the escalation policy&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/defining-slos-for-services-with-dependencies-cre-life-lessons&#34;&gt;Defining SLOs for services with dependencies&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons&#34;&gt;Tune up your SLI metrics&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/learn-how-to-set-slos-for-an-sre-or-cre-practice&#34;&gt;Learning&amp;#8212;and teaching&amp;#8212;the art of service-level objectives&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/using-deemed-slis-to-measure-customer-reliability&#34;&gt;Using deemed SLIs to measure customer reliability&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Releases&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/reliable-releases-and-rollbacks-cre-life-lessons&#34;&gt;Reliable releases and rollbacks&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/how-release-canaries-can-save-your-bacon-cre-life-lessons&#34;&gt;How release canaries can save your bacon&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;SRE support&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/why-should-your-app-get-sre-support-cre-life-lessons&#34;&gt;Why should your app get SRE support?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/how-sres-find-the-landmines-in-a-service-cre-life-lessons&#34;&gt;How SREs find the landmines in a service&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/making-the-most-of-an-sre-service-takeover-cre-life-lessons&#34;&gt;Making the most of an SRE service takeover&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Dark launches&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/cre-life-lessons-what-is-a-dark-launch-and-what-does-it-do-for-me&#34;&gt;What is a dark launch, and what does it do for me?&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/cre-life-lessons-practicalities-of-dark-launches&#34;&gt;The practicalities of dark launching&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Postmortems&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons&#34;&gt;Fearless shared postmortems&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/getting-the-most-out-of-shared-postmortems-cre-life-lessons&#34;&gt;Getting the most out of shared postmortems&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Error budgets&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/good-housekeeping-error-budgetscre-life-lessons&#34;&gt;Good housekeeping for error budgets&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons&#34;&gt;Understanding error budget overspend&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Production incidents&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/shrinking-the-impact-of-production-incidents-using-sre-principles-cre-life-lessons&#34;&gt;Shrinking the impact of production incidents using SRE principles&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents&#34;&gt;Shrinking the time to mitigate production incidents&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;Becoming an SRE team&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board&#34;&gt;How to implement a successful SRE practice from the outset&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice&#34;&gt;Jumpstarting your SRE practice&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum&#34;&gt;Assessing an SRE team&amp;#8217;s maturity&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;We still have plenty more articles to come, so keep your eye on our &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre&#34;&gt;DevOps &amp;amp; SRE channel&lt;/a&gt;. You can also check out &lt;a href=&#34;http://sre.google&#34; target=&#34;_blank&#34;&gt;sre.google&lt;/a&gt; or &lt;a href=&#34;https://sre.google/books/&#34; target=&#34;_blank&#34;&gt;read our SRE books online&lt;/a&gt;.&lt;/p&gt;"><p>In 2016 we announced a new discipline at Google, <a href="https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering" track-metadata-module="post">Customer Reliability Engineering</a>, an offshoot of <a href="https://sre.google/" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">Site Reliability Engineering</a> (SRE). Our goal with CRE was (and still is) to create a shared operational fate between Google and our Google Cloud customers, to give you more control over the critical applications you&#39;re entrusting to us. Since then, here on the Google Cloud blog, we’ve published a wealth of resources to help you take the best practices we’ve learned from SRE teams at Google and apply them in your own environments. </p><p>Below is the complete list of CRE life lessons posts we’ve published <a href="https://cloud.google.com/blog/topics/cre-life-lessons" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/blog/topics/cre-life-lessons" track-metadata-module="post">in the past five years</a> in one convenient location.</p><h3>Common pitfalls</h3><ul><li><p><a href="https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons" track-metadata-module="post">Know thy enemy: How to prioritize and communicate risks</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/how-to-avoid-a-self-inflicted-ddos-attack-cre-life-lessons" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/how-to-avoid-a-self-inflicted-ddos-attack-cre-life-lessons" track-metadata-module="post">How to avoid a self-inflicted DDoS Attack</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/using-load-shedding-to-survive-a-success-disaster-cre-life-lessons" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/using-load-shedding-to-survive-a-success-disaster-cre-life-lessons" track-metadata-module="post">Using load shedding to survive a success disaster</a></p></li></ul><h3>Service-level metrics</h3><ul><li><p><a href="https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons" track-metadata-module="post">Available . . . or not? That is the question</a></p></li><li><p><a href="https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons" track-type="inline link" track-name="8" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons" track-metadata-module="post">SLOs, SLIs, SLAs, oh my</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/building-good-slos-cre-life-lessons" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/building-good-slos-cre-life-lessons" track-metadata-module="post">Building good SLOs</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/consequences-of-slo-violations-cre-life-lessons" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/consequences-of-slo-violations-cre-life-lessons" track-metadata-module="post">Consequences of SLO violations</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/an-example-escalation-policy-cre-life-lessons" track-type="inline link" track-name="11" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/an-example-escalation-policy-cre-life-lessons" track-metadata-module="post">An example escalation policy</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/applying-the-escalation-policy-cre-life-lessons" track-type="inline link" track-name="12" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/applying-the-escalation-policy-cre-life-lessons" track-metadata-module="post">Applying the escalation policy</a></p></li><li><p><a href="https://cloud.google.com/blog/products/devops-sre/defining-slos-for-services-with-dependencies-cre-life-lessons" track-type="inline link" track-name="13" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/defining-slos-for-services-with-dependencies-cre-life-lessons" track-metadata-module="post">Defining SLOs for services with dependencies</a></p></li><li><p><a href="https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons" track-type="inline link" track-name="14" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons" track-metadata-module="post">Tune up your SLI metrics</a></p></li><li><p><a href="https://cloud.google.com/blog/products/management-tools/learn-how-to-set-slos-for-an-sre-or-cre-practice" track-type="inline link" track-name="15" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/learn-how-to-set-slos-for-an-sre-or-cre-practice" track-metadata-module="post">Learning—and teaching—the art of service-level objectives</a></p></li><li><p><a href="https://cloud.google.com/blog/products/management-tools/using-deemed-slis-to-measure-customer-reliability" track-type="inline link" track-name="16" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/using-deemed-slis-to-measure-customer-reliability" track-metadata-module="post">Using deemed SLIs to measure customer reliability</a></p></li></ul><h3>Releases</h3><ul><li><p><a href="https://cloud.google.com/blog/products/gcp/reliable-releases-and-rollbacks-cre-life-lessons" track-type="inline link" track-name="17" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/reliable-releases-and-rollbacks-cre-life-lessons" track-metadata-module="post">Reliable releases and rollbacks</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/how-release-canaries-can-save-your-bacon-cre-life-lessons" track-type="inline link" track-name="18" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/how-release-canaries-can-save-your-bacon-cre-life-lessons" track-metadata-module="post">How release canaries can save your bacon</a></p></li></ul><h3>SRE support</h3><ul><li><p><a href="https://cloud.google.com/blog/products/gcp/why-should-your-app-get-sre-support-cre-life-lessons" track-type="inline link" track-name="19" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/why-should-your-app-get-sre-support-cre-life-lessons" track-metadata-module="post">Why should your app get SRE support?</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/how-sres-find-the-landmines-in-a-service-cre-life-lessons" track-type="inline link" track-name="20" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/how-sres-find-the-landmines-in-a-service-cre-life-lessons" track-metadata-module="post">How SREs find the landmines in a service</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/making-the-most-of-an-sre-service-takeover-cre-life-lessons" track-type="inline link" track-name="21" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/making-the-most-of-an-sre-service-takeover-cre-life-lessons" track-metadata-module="post">Making the most of an SRE service takeover</a></p></li></ul><h3>Dark launches</h3><ul><li><p><a href="https://cloud.google.com/blog/products/gcp/cre-life-lessons-what-is-a-dark-launch-and-what-does-it-do-for-me" track-type="inline link" track-name="22" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/cre-life-lessons-what-is-a-dark-launch-and-what-does-it-do-for-me" track-metadata-module="post">What is a dark launch, and what does it do for me?</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/cre-life-lessons-practicalities-of-dark-launches" track-type="inline link" track-name="23" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/cre-life-lessons-practicalities-of-dark-launches" track-metadata-module="post">The practicalities of dark launching</a></p></li></ul><h3>Postmortems</h3><ul><li><p><a href="https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons" track-type="inline link" track-name="24" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons" track-metadata-module="post">Fearless shared postmortems</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/getting-the-most-out-of-shared-postmortems-cre-life-lessons" track-type="inline link" track-name="25" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/getting-the-most-out-of-shared-postmortems-cre-life-lessons" track-metadata-module="post">Getting the most out of shared postmortems</a></p></li></ul><h3>Error budgets</h3><ul><li><p><a href="https://cloud.google.com/blog/products/devops-sre/good-housekeeping-error-budgetscre-life-lessons" track-type="inline link" track-name="26" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/good-housekeeping-error-budgetscre-life-lessons" track-metadata-module="post">Good housekeeping for error budgets</a></p></li><li><p><a href="https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons" track-type="inline link" track-name="27" track-metadata-eventdetail="https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons" track-metadata-module="post">Understanding error budget overspend</a></p></li></ul><h3>Production incidents</h3><ul><li><p><a href="https://cloud.google.com/blog/products/devops-sre/shrinking-the-impact-of-production-incidents-using-sre-principles-cre-life-lessons" track-type="inline link" track-name="28" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/shrinking-the-impact-of-production-incidents-using-sre-principles-cre-life-lessons" track-metadata-module="post">Shrinking the impact of production incidents using SRE principles</a></p></li><li><p><a href="https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents" track-type="inline link" track-name="29" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents" track-metadata-module="post">Shrinking the time to mitigate production incidents</a></p></li></ul><h3>Becoming an SRE team</h3><ul><li><p><a href="https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board" track-type="inline link" track-name="30" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board" track-metadata-module="post">How to implement a successful SRE practice from the outset</a></p></li><li><p><a href="https://cloud.google.com/blog/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice" track-type="inline link" track-name="31" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice" track-metadata-module="post">Jumpstarting your SRE practice</a></p></li><li><p><a href="https://cloud.google.com/blog/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum" track-type="inline link" track-name="32" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum" track-metadata-module="post">Assessing an SRE team’s maturity</a></p></li></ul><p>We still have plenty more articles to come, so keep your eye on our <a href="https://cloud.google.com/blog/products/devops-sre" track-type="inline link" track-name="33" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre" track-metadata-module="post">DevOps &amp; SRE channel</a>. You can also check out <a href="http://sre.google" target="_blank" track-type="inline link" track-name="34" track-metadata-eventdetail="http://sre.google" track-metadata-module="post">sre.google</a> or <a href="https://sre.google/books/" target="_blank" track-type="inline link" track-name="35" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">read our SRE books online</a>.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;The Google Cloud content marketing team &lt;/name&gt;&lt;title&gt;&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 27 Apr 2021 20:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Sign here! Creating a policy contract with Configuration as Data</title>
      <link>https://cloud.google.com/blog/products/containers-kubernetes/how-configuration-as-data-impacts-policy/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Configuration as Data is an emerging cloud infrastructure management paradigm that allows developers to &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes&#34;&gt;declare the desired state&lt;/a&gt; of their applications and infrastructure, without specifying the precise actions or steps for how to achieve it. However, declaring a configuration is only half the battle: you also want policy that defines how a configuration is to be used. &lt;/p&gt;&lt;p&gt;Configuration as Data enables a normalized policy contract across all your cloud resources. That contract, knowing how your deployment will operate, can be inspected and enforced throughout a CI/CD pipeline, from upstream in your development environment to deployment time, and ongoing in the live runtime environment. This consistency is possible by expressing configuration as data throughout the development and operations lifecycle.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/config-connector/docs/overview&#34;&gt;Config Connector&lt;/a&gt; is the tool that allows you to express configuration as data in Google Cloud. In this model, configuration is what you want to deploy, such as “a storage bucket named &lt;code&gt;my-bucket&lt;/code&gt; with a standard storage class and uniform access control.” &lt;/p&gt;&lt;p&gt;Policy, meanwhile, typically specifies what you’re allowed to deploy, usually in conformance with your organization’s compliance needs. For example, “all resources must be deployed in Google Cloud’s &lt;code&gt;LONDON&lt;/code&gt; region.” &lt;/p&gt;&lt;p&gt;When each stage in your pipeline treats configuration as data, you can use any tool or language to manipulate configuration as data, knowing they will interoperate and that policy can be consistently enforced at any or all stages. And while a policy engine won’t be able to understand every tool, it can validate the data generated by each tool. It’s just like data in a database can be inspected by anyone who knows the schema regardless of the tool that wrote into the database.&lt;/p&gt;&lt;p&gt;Contrast that with pipelines today, where policy is manually validated, hard coded in scripts within the pipeline logic itself, or post-processed on raw deployment artifacts after rendering configuration templates into specific instances. In each case, policy is siloed—you can’t take the same policy and apply it anywhere in your pipeline because formats differ from tool to tool. &lt;/p&gt;&lt;p&gt;Helm, for example, contains code &lt;a href=&#34;https://helm.sh/docs/chart_template_guide/control_structures/&#34; target=&#34;_blank&#34;&gt;specific to its own format&lt;/a&gt;.&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.terraform.io/docs/language/syntax/configuration.html&#34; target=&#34;_blank&#34;&gt;Terraform HCL&lt;/a&gt; may then deploy the Helm chart.&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The HCL becomes a &lt;a href=&#34;https://www.terraform.io/docs/internals/json-format.html&#34; target=&#34;_blank&#34;&gt;JSON plan&lt;/a&gt;, where the deployment-ready configuration may be validated before being applied to the live environment.&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;These examples show three disparate data formats across two different tools representing different portions of a desired end state. Add in Python scripting, gcloud CLI, or &lt;code&gt;kubectl&lt;/code&gt; commands and you start approaching ten different formats—all for the same deployment!  Reliably enforcing a policy contract requires you to inject tool- and format-specific validation logic on case-by-case basis. If you decide to move a config step from Python to Terraform or from Terraform to &lt;code&gt;kubectl&lt;/code&gt;, you’ll need to re-evaluate your contract and probably re-implement some of that policy validation. &lt;/p&gt;&lt;p&gt;Why don’t these tools work together cleanly? Why does policy validation change depending on the development tools you’re using? Each tool can do a good job enforcing policy within itself. As long as you use that tool everywhere, things will probably work ok. But we all know that’s not how development works. People tend to choose tools that fit their needs and figure out integration later on.&lt;/p&gt;&lt;h3&gt;A Rosetta Stone for policy contracts&lt;/h3&gt;&lt;p&gt;Imagine that everyone is defining their configuration as data, while using tools and formats of their choice. Terraform or Python for orchestration. Helm for application packaging. Java or Go for data transformation and validation. Once the data format is understood (because it is open source and extensible), your pipeline becomes a bus that anyone can push configuration onto and pull configuration from.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/policy_contracts.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;policy contracts.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/policy_contracts.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Policies can be automatically validated at commit or build time using custom and &lt;a href=&#34;https://googlecontainertools.github.io/kpt/guides/consumer/function/catalog/validators/&#34; target=&#34;_blank&#34;&gt;off-the-shelf functions&lt;/a&gt; that operate on YAML. You can manage commit and merge permissions separately for config and policy to separate these distinct concerns. You can have folders and unique permissions for org-wide policy, team-wide policy, or app-specific policy. Therein lies the dream. &lt;/p&gt;&lt;p&gt;The most common way to generate configuration is to simply write a YAML file describing how Kubernetes should create a resource for you. The resulting YAML file is then stored in a git repository where it can be versioned and picked up by another tool and applied to a Kubernetes cluster. Policies can be enforced on the git repo side to limit who can push changes to the repository and ultimately reference them at deploy time.&lt;/p&gt;&lt;p&gt;For most users this is not where policy enforcement ends. While code reviews can catch a lot of things, it’s considered best practice to “trust but verify” at all layers in the stack. That’s where admission controllers come in, which can be considered to be the last mile of policy enforcement. &lt;a href=&#34;https://www.openpolicyagent.org/&#34; target=&#34;_blank&#34;&gt;Gatekeeper&lt;/a&gt; serves as an admission controller inside of a Kubernetes cluster. Only configurations that meet defined constraints will be admitted to the live cloud environment.&lt;/p&gt;&lt;p&gt;Let’s tie these concepts together &lt;a href=&#34;https://github.com/kelseyhightower/config-connector-policy-demo&#34; target=&#34;_blank&#34;&gt;with an example&lt;/a&gt;. Imagine you want to enable users to create Cloud Storage buckets, but you don’t want them doing so using the Google Cloud Console or the gcloud command line tool. You want all users to declare what they want and push those changes to a git repository for review before the underlying Cloud Storage buckets are created with &lt;a href=&#34;https://cloud.google.com/config-connector/docs/overview&#34;&gt;Config Connector&lt;/a&gt;. Essentially you want users to be able to submit a YAML file that looks like this:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;This creates a storage bucket in a default location. There is one problem with this: users can create buckets in any location even if company policy dictates otherwise. Sure, you can catch people using forbidden bucket locations during code review, but that’s prone to human error.&lt;/p&gt;&lt;p&gt;This is where Gatekeeper comes in. You want the ability to limit which Cloud Storage bucket location can be used. Ideally you can write policies that look like this:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;The above &lt;code&gt;StorageBucketAllowedLocation&lt;/code&gt; policy rejects &lt;code&gt;StorageBucket&lt;/code&gt; objects with the &lt;code&gt;spec.location&lt;/code&gt; field set to any value other than one of the Cloud Storage multi-region locations: ASIA, EU, US. You decide where to validate policy without being limited by your tool of choice and anywhere in your pipeline.&lt;/p&gt;&lt;p&gt;Now you have the last stage of your configuration pipeline. &lt;/p&gt;&lt;h3&gt;Testing the contract&lt;/h3&gt;&lt;p&gt;How does this work in practice? Let’s say someone managed to check in &lt;code&gt;StorageBucket&lt;/code&gt; resource with the following config:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Our policy would reject the bucket because an empty location is not allowed. What happens if configuration was set to a Cloud Storage location not allowed by the policy, &lt;code&gt;US-WEST1&lt;/code&gt; for example?&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Ideally you would catch this during the code review process before the config is committed to a git repo, but as mentioned above, that’s error prone. &lt;/p&gt;&lt;p&gt;Luckily, the configuration will fail because the &lt;code&gt;allowmultiregions&lt;/code&gt; policy constraint only allows multi-region bucket locations including ASIA, EU, and US, and will reject the configuration. So, now, if you set location to “US” you can deploy the Cloud Storage bucket. You can also apply this type of location policy or any other like it to all of your resource types—Redis instances, Compute Engine virtual machines, even Google Kubernetes Engine (GKE) clusters. Beyond admission control, you can apply the same constraint anywhere in your pipeline, by ”shifting left” policy validation at any stage. &lt;/p&gt;&lt;h3&gt;One contract to rule them all&lt;/h3&gt;&lt;p&gt;When config is managed in silos—whether across many tools, pipelines, graphical interfaces, and command lines—you can’t inject logic without building bespoke tools for every interface. You may be able to define policies built for your front-end tools and hope nothing changes on the backend. Or you can wait until deployment time to scan for deviations and hope nothing appears during crunch time. &lt;/p&gt;&lt;p&gt;Compare that with configuration as data contracts, which are transparent and normalized across resource types, which has facilitated a rich ecosystem of tooling built around Kubernetes with varied syntax (YAML, JSON) and languages including Ruby, Typescript, Go, Jinja, Mustache, Jsonnet, Starlark, and many others. This isn’t possible without a data model. &lt;/p&gt;&lt;p&gt;Configuration-as-Data-inspired tools such as &lt;a href=&#34;https://cloud.google.com/config-connector/docs/overview&#34;&gt;Config Connector&lt;/a&gt; and Gatekeeper let you enforce policy and governance as natural parts of your existing git-based workflow rather than creating manual processes and approvals. Configuration as data normalizes your contract across resource types and even cloud providers. You don’t need to reverse engineer scripts and code paths to know if your contract is being met—just look at the data.&lt;/p&gt;&lt;hr/&gt;&lt;sup&gt;&lt;i&gt;1. &lt;a href=&#34;https://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml&#34; target=&#34;_blank&#34;&gt;https://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml&lt;/a&gt;&lt;/i&gt;&lt;/sup&gt;&lt;p&gt;&lt;sup&gt;&lt;i&gt;2. &lt;a href=&#34;https://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55&#34; target=&#34;_blank&#34;&gt;https://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55&lt;/a&gt;&lt;br/&gt;&lt;/i&gt;&lt;/sup&gt;&lt;sup&gt;&lt;i&gt;3. &lt;/i&gt;&lt;/sup&gt;&lt;a href=&#34;https://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md&#34; target=&#34;_blank&#34;&gt;&lt;sup&gt;&lt;i&gt;https://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md&lt;/i&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Blog_header_opensource_2.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;I do declare! Infrastructure automation with Configuration as Data&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Configuration as Data enables operational consistency, security, and velocity on Google Cloud with products like Config Connector.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><router-outlet></router-outlet><dynamic-page><article-page><main id="jump-content"><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>#containers</span></p></figure></article-aspect-image-block><div><div><article-author-block><div><div><p> Mark Balch </p><p> Senior Product Manager, Google Cloud </p></div><p><span> April 26, 2021 </span></p></div></article-author-block></div><article-cta _nghost-c17=""><div _ngcontent-c17=""><h4 _ngcontent-c17=""><span _ngcontent-c17="">Try GCP</span></h4><p _ngcontent-c17=""><span _ngcontent-c17="">Start building on Google Cloud with $300 in free credits and 20+ always free products.</span></p><p><a _ngcontent-c17="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="free trial" track-metadata-eventdetail="https://cloud.google.com/free/" href="https://cloud.google.com/free/"><span _ngcontent-c17="">Free Trial</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Configuration as Data is an emerging cloud infrastructure management paradigm that allows developers to &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes&#34;&gt;declare the desired state&lt;/a&gt; of their applications and infrastructure, without specifying the precise actions or steps for how to achieve it. However, declaring a configuration is only half the battle: you also want policy that defines how a configuration is to be used.&amp;#160;&lt;/p&gt;&lt;p&gt;Configuration as Data enables a normalized policy contract across all your cloud resources. That contract, knowing how your deployment will operate, can be inspected and enforced throughout a CI/CD pipeline, from upstream in your development environment to deployment time, and ongoing in the live runtime environment. This consistency is possible by expressing configuration as data throughout the development and operations lifecycle.&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/config-connector/docs/overview&#34;&gt;Config Connector&lt;/a&gt; is the tool that allows you to express configuration as data in Google Cloud. In this model, configuration is what you want to deploy, such as &amp;#8220;a storage bucket named &lt;code&gt;my-bucket&lt;/code&gt; with a standard storage class and uniform access control.&amp;#8221;&amp;#160;&lt;/p&gt;&lt;p&gt;Policy, meanwhile, typically specifies what you&amp;#8217;re allowed to deploy, usually in conformance with your organization&amp;#8217;s compliance needs. For example, &amp;#8220;all resources must be deployed in Google Cloud&amp;#8217;s &lt;code&gt;LONDON&lt;/code&gt; region.&amp;#8221;&amp;#160;&lt;/p&gt;&lt;p&gt;When each stage in your pipeline treats configuration as data, you can use any tool or language to manipulate configuration as data, knowing they will interoperate and that policy can be consistently enforced at any or all stages. And while a policy engine won&amp;#8217;t be able to understand every tool, it can validate the data generated by each tool. It&amp;#8217;s just like data in a database can be inspected by anyone who knows the schema regardless of the tool that wrote into the database.&lt;/p&gt;&lt;p&gt;Contrast that with pipelines today, where policy is manually validated, hard coded in scripts within the pipeline logic itself, or post-processed on raw deployment artifacts after rendering configuration templates into specific instances. In each case, policy is siloed&amp;#8212;you can&amp;#8217;t take the same policy and apply it anywhere in your pipeline because formats differ from tool to tool.&amp;#160;&lt;/p&gt;&lt;p&gt;Helm, for example, contains code &lt;a href=&#34;https://helm.sh/docs/chart_template_guide/control_structures/&#34; target=&#34;_blank&#34;&gt;specific to its own format&lt;/a&gt;.&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;"><p>Configuration as Data is an emerging cloud infrastructure management paradigm that allows developers to <a href="https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes" track-metadata-module="post">declare the desired state</a> of their applications and infrastructure, without specifying the precise actions or steps for how to achieve it. However, declaring a configuration is only half the battle: you also want policy that defines how a configuration is to be used. </p><p>Configuration as Data enables a normalized policy contract across all your cloud resources. That contract, knowing how your deployment will operate, can be inspected and enforced throughout a CI/CD pipeline, from upstream in your development environment to deployment time, and ongoing in the live runtime environment. This consistency is possible by expressing configuration as data throughout the development and operations lifecycle.</p><p><a href="https://cloud.google.com/config-connector/docs/overview" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/config-connector/docs/overview" track-metadata-module="post">Config Connector</a> is the tool that allows you to express configuration as data in Google Cloud. In this model, configuration is what you want to deploy, such as “a storage bucket named <code>my-bucket</code> with a standard storage class and uniform access control.” </p><p>Policy, meanwhile, typically specifies what you’re allowed to deploy, usually in conformance with your organization’s compliance needs. For example, “all resources must be deployed in Google Cloud’s <code>LONDON</code> region.” </p><p>When each stage in your pipeline treats configuration as data, you can use any tool or language to manipulate configuration as data, knowing they will interoperate and that policy can be consistently enforced at any or all stages. And while a policy engine won’t be able to understand every tool, it can validate the data generated by each tool. It’s just like data in a database can be inspected by anyone who knows the schema regardless of the tool that wrote into the database.</p><p>Contrast that with pipelines today, where policy is manually validated, hard coded in scripts within the pipeline logic itself, or post-processed on raw deployment artifacts after rendering configuration templates into specific instances. In each case, policy is siloed—you can’t take the same policy and apply it anywhere in your pipeline because formats differ from tool to tool. </p><p>Helm, for example, contains code <a href="https://helm.sh/docs/chart_template_guide/control_structures/" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://helm.sh" track-metadata-module="post">specific to its own format</a>.<sup>1</sup></p></div></paragraph-block></div><div><article-code-block _nghost-c20=""><pre _ngcontent-c20="">  <code _ngcontent-c20="">{{- if .Values.master.usePodSecurityContext }}
</code><code _ngcontent-c20="">      securityContext:
</code><code _ngcontent-c20="">        runAsUser: {{ default 0 .Values.master.runAsUser }}
</code><code _ngcontent-c20="">{{- if and (.Values.master.runAsUser) (.Values.master.fsGroup) }}
</code><code _ngcontent-c20="">{{- if not (eq (int .Values.master.runAsUser) 0) }}
</code><code _ngcontent-c20="">        fsGroup: {{ .Values.master.fsGroup }}
</code><code _ngcontent-c20="">{{- end }}
</code><code _ngcontent-c20="">{{- end }}
</code><code _ngcontent-c20="">{{- end }}</code>
</pre></article-code-block></div><div><article-code-block _nghost-c20=""><pre _ngcontent-c20="">  <code _ngcontent-c20="">data &#34;helm_repository&#34; &#34;stable&#34; {
</code><code _ngcontent-c20="">  name = &#34;stable&#34;
</code><code _ngcontent-c20="">  url  = &#34;https://kubernetes-charts.storage.googleapis.com/&#34;
</code><code _ngcontent-c20="">}
</code><code _ngcontent-c20="">resource &#34;helm_release&#34; &#34;default&#34; {
</code><code _ngcontent-c20="">  name  = &#34;spin&#34;
</code><code _ngcontent-c20="">  chart = &#34;stable/spinnaker&#34;
</code><code _ngcontent-c20="">
</code><code _ngcontent-c20="">  values = [local.helm_chart_values]
</code><code _ngcontent-c20="">
</code><code _ngcontent-c20="">  timeout = 1200
</code><code _ngcontent-c20="">}</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c19=""><p>The HCL becomes a <a href="https://www.terraform.io/docs/internals/json-format.html" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://www.terraform.io" track-metadata-module="post">JSON plan</a>, where the deployment-ready configuration may be validated before being applied to the live environment.<sup>3</sup></p></paragraph-block></div><div><article-code-block _nghost-c20=""><pre _ngcontent-c20="">  <code _ngcontent-c20=""># google_compute_instance.vm_instance will be created
</code><code _ngcontent-c20="">  + resource &#34;google_compute_instance&#34; &#34;vm_instance&#34; {
</code><code _ngcontent-c20="">      + can_ip_forward       = false
</code><code _ngcontent-c20="">      + cpu_platform         = (known after apply)
</code><code _ngcontent-c20="">      + deletion_protection  = false
</code><code _ngcontent-c20="">      + guest_accelerator    = (known after apply)
</code><code _ngcontent-c20="">      + id                   = (known after apply)
</code><code _ngcontent-c20="">      + instance_id          = (known after apply)
</code><code _ngcontent-c20="">      + label_fingerprint    = (known after apply)
</code><code _ngcontent-c20="">      + machine_type         = &#34;f1-micro&#34;
</code><code _ngcontent-c20="">...
</code><code _ngcontent-c20="">        }</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;These examples show three disparate data formats across two different tools representing different portions of a desired end state. Add in Python scripting, gcloud CLI, or &lt;code&gt;kubectl&lt;/code&gt; commands and you start approaching ten different formats&amp;#8212;all for the same deployment!&amp;#160; Reliably enforcing a policy contract requires you to inject tool- and format-specific validation logic on case-by-case basis. If you decide to move a config step from Python to Terraform or from Terraform to &lt;code&gt;kubectl&lt;/code&gt;, you&amp;#8217;ll need to re-evaluate your contract and probably re-implement some of that policy validation.&amp;#160;&lt;/p&gt;&lt;p&gt;Why don&amp;#8217;t these tools work together cleanly? Why does policy validation change depending on the development tools you&amp;#8217;re using? Each tool can do a good job enforcing policy within itself. As long as you use that tool everywhere, things will probably work ok. But we all know that&amp;#8217;s not how development works. People tend to choose tools that fit their needs and figure out integration later on.&lt;/p&gt;&lt;h3&gt;A Rosetta Stone for policy contracts&lt;/h3&gt;&lt;p&gt;Imagine that everyone is defining their configuration as data, while using tools and formats of their choice. Terraform or Python for orchestration. Helm for application packaging. Java or Go for data transformation and validation. Once the data format is understood (because it is open source and extensible), your pipeline becomes a bus that anyone can push configuration onto and pull configuration from.&lt;/p&gt;"><p>These examples show three disparate data formats across two different tools representing different portions of a desired end state. Add in Python scripting, gcloud CLI, or <code>kubectl</code> commands and you start approaching ten different formats—all for the same deployment!  Reliably enforcing a policy contract requires you to inject tool- and format-specific validation logic on case-by-case basis. If you decide to move a config step from Python to Terraform or from Terraform to <code>kubectl</code>, you’ll need to re-evaluate your contract and probably re-implement some of that policy validation. </p><p>Why don’t these tools work together cleanly? Why does policy validation change depending on the development tools you’re using? Each tool can do a good job enforcing policy within itself. As long as you use that tool everywhere, things will probably work ok. But we all know that’s not how development works. People tend to choose tools that fit their needs and figure out integration later on.</p><h3>A Rosetta Stone for policy contracts</h3><p>Imagine that everyone is defining their configuration as data, while using tools and formats of their choice. Terraform or Python for orchestration. Helm for application packaging. Java or Go for data transformation and validation. Once the data format is understood (because it is open source and extensible), your pipeline becomes a bus that anyone can push configuration onto and pull configuration from.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Policies can be automatically validated at commit or build time using custom and &lt;a href=&#34;https://googlecontainertools.github.io/kpt/guides/consumer/function/catalog/validators/&#34; target=&#34;_blank&#34;&gt;off-the-shelf functions&lt;/a&gt; that operate on YAML. You can manage commit and merge permissions separately for config and policy to separate these distinct concerns. You can have folders and unique permissions for org-wide policy, team-wide policy, or app-specific policy. Therein lies the dream.&amp;#160;&lt;/p&gt;&lt;p&gt;The most common way to generate configuration is to simply write a YAML file describing how Kubernetes should create a resource for you. The resulting YAML file is then stored in a git repository where it can be versioned and picked up by another tool and applied to a Kubernetes cluster. Policies can be enforced on the git repo side to limit who can push changes to the repository and ultimately reference them at deploy time.&lt;/p&gt;&lt;p&gt;For most users this is not where policy enforcement ends. While code reviews can catch a lot of things, it&amp;#8217;s considered best practice to &amp;#8220;trust but verify&amp;#8221; at all layers in the stack. That&amp;#8217;s where admission controllers come in, which can be considered to be the last mile of policy enforcement. &lt;a href=&#34;https://www.openpolicyagent.org/&#34; target=&#34;_blank&#34;&gt;Gatekeeper&lt;/a&gt; serves as an admission controller inside of a Kubernetes cluster. Only configurations that meet defined constraints will be admitted to the live cloud environment.&lt;/p&gt;&lt;p&gt;Let&amp;#8217;s tie these concepts together &lt;a href=&#34;https://github.com/kelseyhightower/config-connector-policy-demo&#34; target=&#34;_blank&#34;&gt;with an example&lt;/a&gt;. Imagine you want to enable users to create Cloud Storage buckets, but you don&amp;#8217;t want them doing so using the Google Cloud Console or the gcloud command line tool. You want all users to declare what they want and push those changes to a git repository for review before the underlying Cloud Storage buckets are created with &lt;a href=&#34;https://cloud.google.com/config-connector/docs/overview&#34;&gt;Config Connector&lt;/a&gt;. Essentially you want users to be able to submit a YAML file that looks like this:&lt;/p&gt;"><p>Policies can be automatically validated at commit or build time using custom and <a href="https://googlecontainertools.github.io/kpt/guides/consumer/function/catalog/validators/" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://googlecontainertools.github.io" track-metadata-module="post">off-the-shelf functions</a> that operate on YAML. You can manage commit and merge permissions separately for config and policy to separate these distinct concerns. You can have folders and unique permissions for org-wide policy, team-wide policy, or app-specific policy. Therein lies the dream. </p><p>The most common way to generate configuration is to simply write a YAML file describing how Kubernetes should create a resource for you. The resulting YAML file is then stored in a git repository where it can be versioned and picked up by another tool and applied to a Kubernetes cluster. Policies can be enforced on the git repo side to limit who can push changes to the repository and ultimately reference them at deploy time.</p><p>For most users this is not where policy enforcement ends. While code reviews can catch a lot of things, it’s considered best practice to “trust but verify” at all layers in the stack. That’s where admission controllers come in, which can be considered to be the last mile of policy enforcement. <a href="https://www.openpolicyagent.org/" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://www.openpolicyagent.org" track-metadata-module="post">Gatekeeper</a> serves as an admission controller inside of a Kubernetes cluster. Only configurations that meet defined constraints will be admitted to the live cloud environment.</p><p>Let’s tie these concepts together <a href="https://github.com/kelseyhightower/config-connector-policy-demo" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://github.com" track-metadata-module="post">with an example</a>. Imagine you want to enable users to create Cloud Storage buckets, but you don’t want them doing so using the Google Cloud Console or the gcloud command line tool. You want all users to declare what they want and push those changes to a git repository for review before the underlying Cloud Storage buckets are created with <a href="https://cloud.google.com/config-connector/docs/overview" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/config-connector/docs/overview" track-metadata-module="post">Config Connector</a>. Essentially you want users to be able to submit a YAML file that looks like this:</p></div></paragraph-block></div><div><article-code-block _nghost-c20=""><pre _ngcontent-c20="">  <code _ngcontent-c20="">apiVersion: storage.cnrm.cloud.google.com/v1beta1
</code><code _ngcontent-c20="">kind: StorageBucket
</code><code _ngcontent-c20="">metadata:
</code><code _ngcontent-c20="">  name: ${BUCKET_NAME}</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;This creates a storage bucket in a default location. There is one problem with this: users can create buckets in any location even if company policy dictates otherwise. Sure, you can catch people using forbidden bucket locations during code review, but that&amp;#8217;s prone to human error.&lt;/p&gt;&lt;p&gt;This is where Gatekeeper comes in. You want the ability to limit which Cloud Storage bucket location can be used. Ideally you can write policies that look like this:&lt;/p&gt;"><p>This creates a storage bucket in a default location. There is one problem with this: users can create buckets in any location even if company policy dictates otherwise. Sure, you can catch people using forbidden bucket locations during code review, but that’s prone to human error.</p><p>This is where Gatekeeper comes in. You want the ability to limit which Cloud Storage bucket location can be used. Ideally you can write policies that look like this:</p></div></paragraph-block></div><div><article-code-block _nghost-c20=""><pre _ngcontent-c20="">  <code _ngcontent-c20="">apiVersion: constraints.gatekeeper.sh/v1beta1
</code><code _ngcontent-c20="">kind: StorageBucketAllowedLocations
</code><code _ngcontent-c20="">metadata:
</code><code _ngcontent-c20="">  name: allowmultiregions
</code><code _ngcontent-c20="">spec:
</code><code _ngcontent-c20="">  match:
</code><code _ngcontent-c20="">    kinds:
</code><code _ngcontent-c20="">      - apiGroups: [&#34;storage.cnrm.cloud.google.com&#34;]
</code><code _ngcontent-c20="">        kinds: [&#34;StorageBucket&#34;]
</code><code _ngcontent-c20="">  parameters:
</code><code _ngcontent-c20="">    locations:
</code><code _ngcontent-c20="">      - &#34;ASIA&#34;
</code><code _ngcontent-c20="">      - &#34;EU&#34;
</code><code _ngcontent-c20="">      - &#34;US&#34;</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;The above &lt;code&gt;StorageBucketAllowedLocation&lt;/code&gt; policy rejects &lt;code&gt;StorageBucket&lt;/code&gt; objects with the &lt;code&gt;spec.location&lt;/code&gt; field set to any value other than one of the Cloud Storage multi-region locations: ASIA, EU, US. You decide where to validate policy without being limited by your tool of choice and anywhere in your pipeline.&lt;/p&gt;&lt;p&gt;Now you have the last stage of your configuration pipeline.&amp;#160;&lt;/p&gt;&lt;h3&gt;Testing the contract&lt;/h3&gt;&lt;p&gt;How does this work in practice? Let&amp;#8217;s say someone managed to check in &lt;code&gt;StorageBucket&lt;/code&gt; resource with the following config:&lt;/p&gt;"><p>The above <code>StorageBucketAllowedLocation</code> policy rejects <code>StorageBucket</code> objects with the <code>spec.location</code> field set to any value other than one of the Cloud Storage multi-region locations: ASIA, EU, US. You decide where to validate policy without being limited by your tool of choice and anywhere in your pipeline.</p><p>Now you have the last stage of your configuration pipeline. </p><h3>Testing the contract</h3><p>How does this work in practice? Let’s say someone managed to check in <code>StorageBucket</code> resource with the following config:</p></div></paragraph-block></div><div><article-code-block _nghost-c20=""><pre _ngcontent-c20="">  <code _ngcontent-c20="">apiVersion: storage.cnrm.cloud.google.com/v1beta1
</code><code _ngcontent-c20="">kind: StorageBucket
</code><code _ngcontent-c20="">metadata:
</code><code _ngcontent-c20="">  annotations:
</code><code _ngcontent-c20="">    cnrm.cloud.google.com/force-destroy: &#34;false&#34;
</code><code _ngcontent-c20="">  name: ${BUCKET_NAME}
</code><code _ngcontent-c20="">spec:</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c19=""><p>Our policy would reject the bucket because an empty location is not allowed. What happens if configuration was set to a Cloud Storage location not allowed by the policy, <code>US-WEST1</code> for example?</p></paragraph-block></div><div><article-code-block _nghost-c20=""><pre _ngcontent-c20="">  <code _ngcontent-c20="">apiVersion: storage.cnrm.cloud.google.com/v1beta1
</code><code _ngcontent-c20="">kind: StorageBucket
</code><code _ngcontent-c20="">metadata:
</code><code _ngcontent-c20="">  annotations:
</code><code _ngcontent-c20="">    cnrm.cloud.google.com/force-destroy: &#34;false&#34;
</code><code _ngcontent-c20="">  name: ${BUCKET_NAME}
</code><code _ngcontent-c20="">spec:
</code><code _ngcontent-c20="">  location: &#34;US-WEST1&#34;</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Ideally you would catch this during the code review process before the config is committed to a git repo, but as mentioned above, that&amp;#8217;s error prone.&amp;#160;&lt;/p&gt;&lt;p&gt;Luckily, the configuration will fail because the &lt;code&gt;allowmultiregions&lt;/code&gt; policy constraint only allows multi-region bucket locations including ASIA, EU, and US, and will reject the configuration. So, now, if you set location to &amp;#8220;US&amp;#8221; you can deploy the Cloud Storage bucket. You can also apply this type of location policy or any other like it to all of your resource types&amp;#8212;Redis instances, Compute Engine virtual machines, even Google Kubernetes Engine (GKE) clusters. Beyond admission control, you can apply the same constraint anywhere in your pipeline, by &amp;#8221;shifting left&amp;#8221; policy validation at any stage.&amp;#160;&lt;/p&gt;&lt;h3&gt;One contract to rule them all&lt;/h3&gt;&lt;p&gt;When config is managed in silos&amp;#8212;whether across many tools, pipelines, graphical interfaces, and command lines&amp;#8212;you can&amp;#8217;t inject logic without building bespoke tools for every interface. You may be able to define policies built for your front-end tools and hope nothing changes on the backend. Or you can wait until deployment time to scan for deviations and hope nothing appears during crunch time.&amp;#160;&lt;/p&gt;&lt;p&gt;Compare that with configuration as data contracts, which are transparent and normalized across resource types, which has facilitated a rich ecosystem of tooling built around Kubernetes with varied syntax (YAML, JSON) and languages including Ruby, Typescript, Go, Jinja, Mustache, Jsonnet, Starlark, and many others. This isn&amp;#8217;t possible without a data model.&amp;#160;&lt;/p&gt;&lt;p&gt;Configuration-as-Data-inspired tools such as &lt;a href=&#34;https://cloud.google.com/config-connector/docs/overview&#34;&gt;Config Connector&lt;/a&gt; and Gatekeeper let you enforce policy and governance as natural parts of your existing git-based workflow rather than creating manual processes and approvals. Configuration as data normalizes your contract across resource types and even cloud providers. You don&amp;#8217;t need to reverse engineer scripts and code paths to know if your contract is being met&amp;#8212;just look at the data.&lt;/p&gt;&lt;hr&gt;&lt;sup&gt;&lt;i&gt;1.&amp;#160;&lt;a href=&#34;https://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml&#34; target=&#34;_blank&#34;&gt;https://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml&lt;/a&gt;&lt;/i&gt;&lt;/sup&gt;&lt;p&gt;&lt;sup&gt;&lt;i&gt;2.&amp;#160;&lt;a href=&#34;https://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55&#34; target=&#34;_blank&#34;&gt;https://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55&lt;/a&gt;&lt;br&gt;&lt;/i&gt;&lt;/sup&gt;&lt;sup&gt;&lt;i&gt;3.&amp;#160;&lt;/i&gt;&lt;/sup&gt;&lt;a href=&#34;https://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md&#34; target=&#34;_blank&#34;&gt;&lt;sup&gt;&lt;i&gt;https://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md&lt;/i&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;"><p>Ideally you would catch this during the code review process before the config is committed to a git repo, but as mentioned above, that’s error prone. </p><p>Luckily, the configuration will fail because the <code>allowmultiregions</code> policy constraint only allows multi-region bucket locations including ASIA, EU, and US, and will reject the configuration. So, now, if you set location to “US” you can deploy the Cloud Storage bucket. You can also apply this type of location policy or any other like it to all of your resource types—Redis instances, Compute Engine virtual machines, even Google Kubernetes Engine (GKE) clusters. Beyond admission control, you can apply the same constraint anywhere in your pipeline, by ”shifting left” policy validation at any stage. </p><h3>One contract to rule them all</h3><p>When config is managed in silos—whether across many tools, pipelines, graphical interfaces, and command lines—you can’t inject logic without building bespoke tools for every interface. You may be able to define policies built for your front-end tools and hope nothing changes on the backend. Or you can wait until deployment time to scan for deviations and hope nothing appears during crunch time. </p><p>Compare that with configuration as data contracts, which are transparent and normalized across resource types, which has facilitated a rich ecosystem of tooling built around Kubernetes with varied syntax (YAML, JSON) and languages including Ruby, Typescript, Go, Jinja, Mustache, Jsonnet, Starlark, and many others. This isn’t possible without a data model. </p><p>Configuration-as-Data-inspired tools such as <a href="https://cloud.google.com/config-connector/docs/overview" track-type="inline link" track-name="10" track-metadata-eventdetail="https://cloud.google.com/config-connector/docs/overview" track-metadata-module="post">Config Connector</a> and Gatekeeper let you enforce policy and governance as natural parts of your existing git-based workflow rather than creating manual processes and approvals. Configuration as data normalizes your contract across resource types and even cloud providers. You don’t need to reverse engineer scripts and code paths to know if your contract is being met—just look at the data.</p><hr/><p><sup><i>1. <a href="https://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml" target="_blank" track-type="inline link" track-name="11" track-metadata-eventdetail="https://github.com" track-metadata-module="post">https://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml</a></i></sup></p><p><sup><i>2. <a href="https://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55" target="_blank" track-type="inline link" track-name="12" track-metadata-eventdetail="https://medium.com" track-metadata-module="post">https://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55</a><br/></i></sup><sup><i>3. </i></sup><a href="https://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md" target="_blank" track-type="inline link" track-name="13" track-metadata-eventdetail="https://github.com" track-metadata-module="post"><sup><i>https://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md</i></sup></a></p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c18=""></article-up-1to3-block></section></div></article></main></article-page></dynamic-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Mark Balch&lt;/name&gt;&lt;title&gt;Senior Product Manager, Google Cloud&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_x_GKE.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 26 Apr 2021 16:00:00 +0000</pubDate>
    </item>
    <item>
      <title>5 resources to help you get started with SRE</title>
      <link>https://cloud.google.com/blog/products/devops-sre/5-google-sre-resources-to-get-started/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://sre.google/&#34; target=&#34;_blank&#34;&gt;Site reliability engineering&lt;/a&gt; (SRE) is an essential part of engineering at Google—it’s a mindset, and a set of practices, metrics, and prescriptive ways to ensure systems reliability. But not everyone knows the best places to start to implement SRE in their own organizations. Here are our top resources at Google Cloud for getting started.&lt;/p&gt;&lt;h3&gt;1. Do you have an SRE team yet? How to start and assess your journey&lt;/h3&gt;&lt;p&gt;We’re often asked what implementing SRE means in practice, since our customers face challenges quantifying their success when setting up their own SRE practices. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey&#34;&gt;In this post&lt;/a&gt;, we share a couple of checklists to be used by members of an organization responsible for any high-reliability services. These will be useful when you’re trying to move your team toward an SRE model. Implementing this model at your organization can benefit both your services and teams due to higher service reliability, lower operational cost, and higher-value work for everyone on the team.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/how-to-start-and-assess-your-sre-journey/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Do you have an SRE team yet? How to start and assess your journey&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;This post shares checklists you can use when you’re trying to move your team toward an SRE model. These checklists can be useful as a for...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;2. SRE fundamentals: SLIs, SLAs and SLOs&lt;/h3&gt;&lt;p&gt;Core to the definition of SRE is the idea that metrics should be closely tied to business objectives. Thus, a big part of the day-to-day of SREs is establishing and monitoring these service-level metrics. At Google, we use several essential measurements—SLO, SLA and SLI—in SRE planning and practice. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos&#34;&gt;This post&lt;/a&gt; gives you an overview of what each of these acronyms are, what they mean, and how to incorporate them.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/sre-fundamentals-slis-slas-and-slos/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;SRE fundamentals: SLIs, SLAs and SLOs&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;A big part of SRE is establishing and monitoring service-level metrics like SLOs, SLAs and SLIs. This post gives you an overview of what ...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;3. How SRE teams are organized, and how to get started&lt;/h3&gt;&lt;p&gt;You know what SREs do and understand which best practices should be implemented at various levels of SRE maturity. Now you’re ready to take the next step by setting up your own SRE team. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started&#34;&gt;In this post&lt;/a&gt;, we’ll cover how different implementations of SRE teams establish boundaries to achieve their goals. We describe six different implementations that we’ve experienced, and what we have observed to be their most important pros and cons.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_A_Rnd3.max-2800x2800.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;How SRE teams are organized, and how to get started&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Learn six different implementations of SRE teams you can apply in your organization, as well as how to establish boundaries to achieve th...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;4. Meeting reliability challenges with SRE principles&lt;/h3&gt;&lt;p&gt;Through years of work using SRE principles, we’ve found there are a few common challenges that teams face, and some important ways to meet or avoid those challenges. Learn what we at Google think are the &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/meeting-reliability-challenges-with-sre-principles&#34;&gt;three top sources of production stress&lt;/a&gt; and how we recommend addressing them.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/management-tools/meeting-reliability-challenges-with-sre-principles/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_GCP.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Meeting reliability challenges with SRE principles&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Following SRE principles can help you build reliable production systems. When getting started, you may encounter three common challenges....&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;5. Transitioning a typical engineering ops team into an SRE powerhouse&lt;/h3&gt;&lt;p&gt;Perpetually adding engineers to ops teams to meet customer growth doesn’t scale. Google’s SRE principles can help, bringing software engineering solutions to operational problems. &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/transitioning-a-typical-engineering-ops-team-into-an-sre-powerhouse&#34;&gt;In this post&lt;/a&gt;, we’ll take a look at how we transformed our global network ops team by abandoning traditional network engineering orthodoxy and replacing it with SRE. You’ll learn how Google’s production networking team tackled this problem and consider how you might incorporate SRE principles in your own organization.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/management-tools/transitioning-a-typical-engineering-ops-team-into-an-sre-powerhouse/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Transitioning a typical engineering ops team into an SRE powerhouse&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Moving a network operations team to an SRE-driven model took some time, but was well worth the effort, as teams can focus on reliability ...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Lots more to read&lt;/h3&gt;&lt;p&gt;Can’t wait to read more about SRE? We wrote &lt;a href=&#34;https://sre.google/sre-book/table-of-contents/&#34; target=&#34;_blank&#34;&gt;an entire book on SRE&lt;/a&gt; to help you get started (actually, we’ve written &lt;a href=&#34;https://sre.google/books/&#34; target=&#34;_blank&#34;&gt;more than one&lt;/a&gt;). You can also find all our &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre&#34;&gt;DevOps and SRE blog content&lt;/a&gt; or follow our columns on &lt;a href=&#34;https://cloud.google.com/blog/topics/cre-life-lessons&#34;&gt;Customer Reliability Engineering&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/gcp/sre-vs-devops-competing-standards-or-close-friends/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/SREvsDevOps.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;SRE vs. DevOps: Competing standards or close friends?&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;What exactly is SRE and how does it relate to DevOps? This post helps answer questions and reduce friction between the communities.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-content-stream-block><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;&lt;a href=&#34;https://sre.google/&#34; target=&#34;_blank&#34;&gt;Site reliability engineering&lt;/a&gt; (SRE) is an essential part of engineering at Google&amp;#8212;it&amp;#8217;s a mindset, and a set of practices, metrics, and prescriptive ways to ensure systems reliability. But not everyone knows the best places to start to implement SRE in their own organizations. Here are our top resources at Google Cloud for getting started.&lt;/p&gt;&lt;h3&gt;1. Do you have an SRE team yet? How to start and assess your journey&lt;/h3&gt;&lt;p&gt;We&amp;#8217;re often asked what implementing SRE means in practice, since our customers face challenges quantifying their success when setting up their own SRE practices. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey&#34;&gt;In this post&lt;/a&gt;, we share a couple of checklists to be used by members of an organization responsible for any high-reliability services. These will be useful when you&amp;#8217;re trying to move your team toward an SRE model. Implementing this model at your organization can benefit both your services and teams due to higher service reliability, lower operational cost, and higher-value work for everyone on the team.&lt;/p&gt;"><p><a href="https://sre.google/" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">Site reliability engineering</a> (SRE) is an essential part of engineering at Google—it’s a mindset, and a set of practices, metrics, and prescriptive ways to ensure systems reliability. But not everyone knows the best places to start to implement SRE in their own organizations. Here are our top resources at Google Cloud for getting started.</p><h3>1. Do you have an SRE team yet? How to start and assess your journey</h3><p>We’re often asked what implementing SRE means in practice, since our customers face challenges quantifying their success when setting up their own SRE practices. <a href="https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey" track-metadata-module="post">In this post</a>, we share a couple of checklists to be used by members of an organization responsible for any high-reliability services. These will be useful when you’re trying to move your team toward an SRE model. Implementing this model at your organization can benefit both your services and teams due to higher service reliability, lower operational cost, and higher-value work for everyone on the team.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;h3&gt;2. SRE fundamentals: SLIs, SLAs and SLOs&lt;/h3&gt;&lt;p&gt;Core to the definition of SRE is the idea that metrics should be closely tied to business objectives. Thus, a big part of the day-to-day of SREs is establishing and monitoring these service-level metrics. At Google, we use several essential measurements&amp;#8212;SLO, SLA and SLI&amp;#8212;in SRE planning and practice. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos&#34;&gt;This post&lt;/a&gt; gives you an overview of what each of these acronyms are, what they mean, and how to incorporate them.&lt;/p&gt;"><h3>2. SRE fundamentals: SLIs, SLAs and SLOs</h3><p>Core to the definition of SRE is the idea that metrics should be closely tied to business objectives. Thus, a big part of the day-to-day of SREs is establishing and monitoring these service-level metrics. At Google, we use several essential measurements—SLO, SLA and SLI—in SRE planning and practice. <a href="https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos" track-metadata-module="post">This post</a> gives you an overview of what each of these acronyms are, what they mean, and how to incorporate them.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;h3&gt;3. How SRE teams are organized, and how to get started&lt;/h3&gt;&lt;p&gt;You know what SREs do and understand which best practices should be implemented at various levels of SRE maturity. Now you&amp;#8217;re ready to take the next step by setting up your own SRE team. &lt;a href=&#34;https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started&#34;&gt;In this post&lt;/a&gt;, we&amp;#8217;ll cover how different implementations of SRE teams establish boundaries to achieve their goals. We describe six different implementations that we&amp;#8217;ve experienced, and what we have observed to be their most important pros and cons.&lt;/p&gt;"><h3>3. How SRE teams are organized, and how to get started</h3><p>You know what SREs do and understand which best practices should be implemented at various levels of SRE maturity. Now you’re ready to take the next step by setting up your own SRE team. <a href="https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started" track-metadata-module="post">In this post</a>, we’ll cover how different implementations of SRE teams establish boundaries to achieve their goals. We describe six different implementations that we’ve experienced, and what we have observed to be their most important pros and cons.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;h3&gt;4. Meeting reliability challenges with SRE principles&lt;/h3&gt;&lt;p&gt;Through years of work using SRE principles, we&amp;#8217;ve found there are a few common challenges that teams face, and some important ways to meet or avoid those challenges. Learn what we at Google think are the &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/meeting-reliability-challenges-with-sre-principles&#34;&gt;three top sources of production stress&lt;/a&gt; and how we recommend addressing them.&lt;/p&gt;"><h3>4. Meeting reliability challenges with SRE principles</h3><p>Through years of work using SRE principles, we’ve found there are a few common challenges that teams face, and some important ways to meet or avoid those challenges. Learn what we at Google think are the <a href="https://cloud.google.com/blog/products/management-tools/meeting-reliability-challenges-with-sre-principles" track-type="inline link" track-name="5" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/meeting-reliability-challenges-with-sre-principles" track-metadata-module="post">three top sources of production stress</a> and how we recommend addressing them.</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;h3&gt;5. Transitioning a typical engineering ops team into an SRE powerhouse&lt;/h3&gt;&lt;p&gt;Perpetually adding engineers to ops teams to meet customer growth doesn&amp;#8217;t scale. Google&amp;#8217;s SRE principles can help, bringing software engineering solutions to operational problems. &lt;a href=&#34;https://cloud.google.com/blog/products/management-tools/transitioning-a-typical-engineering-ops-team-into-an-sre-powerhouse&#34;&gt;In this post&lt;/a&gt;, we&amp;#8217;ll take a look at how we transformed our global network ops team by abandoning traditional network engineering orthodoxy and replacing it with SRE. You&amp;#8217;ll learn how Google&amp;#8217;s production networking team tackled this problem and consider how you might incorporate SRE principles in your own organization.&lt;/p&gt;"><h3>5. Transitioning a typical engineering ops team into an SRE powerhouse</h3><p>Perpetually adding engineers to ops teams to meet customer growth doesn’t scale. Google’s SRE principles can help, bringing software engineering solutions to operational problems. <a href="https://cloud.google.com/blog/products/management-tools/transitioning-a-typical-engineering-ops-team-into-an-sre-powerhouse" track-type="inline link" track-name="6" track-metadata-eventdetail="https://cloud.google.com/blog/products/management-tools/transitioning-a-typical-engineering-ops-team-into-an-sre-powerhouse" track-metadata-module="post">In this post</a>, we’ll take a look at how we transformed our global network ops team by abandoning traditional network engineering orthodoxy and replacing it with SRE. You’ll learn how Google’s production networking team tackled this problem and consider how you might incorporate SRE principles in your own organization.</p></div></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;The Google Cloud content marketing team &lt;/name&gt;&lt;title&gt;&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_C_Rnd3_n7MW7mI.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 23 Apr 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>How do you eat an elephant? Google SREs talk digital transformation</title>
      <link>https://cloud.google.com/blog/products/devops-sre/a-practical-guide-to-cloud-migration-from-google-cloud-sres/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Today, everything from payroll software to specialized machine-learning systems is available “as a service” in the cloud, addressing a vast range of needs across businesses, enabling rapid growth and scale while allowing a business to focus on its core competencies.&lt;/p&gt;&lt;p&gt;But moving to the cloud can generate tension, which is inevitably challenging for everyone involved—especially if that transformation creates &#34;winners&#34; and &#34;losers&#34; or frames individuals as &#34;old&#34; or &#34;new.&#34; The good news; however, is that a cloud transformation doesn&#39;t have to be this way.&lt;/p&gt;&lt;p&gt;As Google Cloud has grown, so too has the team of Googlers who build and support the platform, and many of us have sat in the same seat as our customers. We&#39;ve experienced firsthand how empowering it can be to shape the future of an organization, help one another grow, as well as unlock the business opportunities that a transformation provides. Our own personal experiences, and those of our peers, have led us to conclude one thing we know to be true for every company - the story of digital transformation is a human story—one that involves as much cultural transformation as technological transformation. It’s with this realization we have identified the deeper factors behind a successful transformation. That&#39;s why we recently published a guide, reflecting on the nature of these changes and how you can take action in your own organization to drive a migration to the cloud. &lt;/p&gt;&lt;p&gt;Posing challenging questions helps you reflect on your own organization’s journey and the unique path you will need to take to lead to meaningful change. We wrote this guide to share key tenets that underpin the change philosophy you need to instill in your own organization. In &lt;a href=&#34;https://googlesre.page.link/cloud-migration-guide&#34; target=&#34;_blank&#34;&gt;A Practical Guide to Moving to Cloud&lt;/a&gt;, we present the following calls to action:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Understand who in the organization you need to enlist to move to cloud.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Create a psychologically safe culture in which you can grow together.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Define clear objectives for your organization. Document measurable steps towards these goals and understand that each step must, in and of itself, deliver value.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Review your existing organizational behaviors and set principles/policies which influence and direct every future decision related to your transformation.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use your new culture to refine how decisions are made, and provide meaningful autonomy across the organization.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Build structures that empower practitioners to share best practices and solve common problems. Use these structures to empower your peers.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Build guardrails into your cloud platform that support transformation, at pace, without negatively impacting others. Support safe experimentation.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Understand what types of cloud platforms are the best fit for your business needs and determine your multi-cloud strategy in anticipation of your evolving business needs (e.g. acquisitions, new revenue streams, competitive responses).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Recognize that everything is now software, and understand what this means for your existing IT infrastructure functions.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Don’t be afraid to revisit existing, hallowed, security policies. Making them fit-for-purpose is crucial.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Continuously measure and apply your new policies through software.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Be bold; build a new way of operating your business products with a customer-centric perspective. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Love your developers.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;At Google Cloud, we’re here to help you craft the right migration for you and your business. &lt;a href=&#34;https://googlesre.page.link/cloud-migration-guide&#34; target=&#34;_blank&#34;&gt;A Practical Guide to Moving to Cloud&lt;/a&gt; is available as a free download. You can also learn more about our &lt;a href=&#34;https://cloud.google.com/solutions/migration-center&#34;&gt;data center migration solutions&lt;/a&gt; or &lt;a href=&#34;https://inthecloud.withgoogle.com/tco-assessment-19/form.html&#34; target=&#34;_blank&#34;&gt;sign up for a free migration cost assessment&lt;/a&gt;. Let’s get migrating! &lt;/p&gt;&lt;p&gt;&lt;i&gt;Visit &lt;a href=&#34;https://sre.google&#34; target=&#34;_blank&#34;&gt;sre.google&lt;/a&gt; to learn more about SRE and industry-leading practices for service reliability.&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;How SRE teams are organized, and how to get started&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Learn six different implementations of SRE teams you can apply in your organization, as well as how to establish boundaries to achieve th...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Today, everything from payroll software to specialized machine-learning systems is available &amp;#8220;as a service&amp;#8221; in the cloud, addressing a vast range of needs across businesses, enabling rapid growth and scale while allowing a business to focus on its core competencies.&lt;/p&gt;&lt;p&gt;But moving to the cloud can generate tension, which is inevitably challenging for everyone involved&amp;#8212;especially if that transformation creates &amp;#34;winners&amp;#34; and &amp;#34;losers&amp;#34; or frames individuals as &amp;#34;old&amp;#34; or &amp;#34;new.&amp;#34; The good news; however, is that a cloud transformation doesn&#39;t have to be this way.&lt;/p&gt;&lt;p&gt;As Google Cloud has grown, so too has the team of Googlers who build and support the platform, and many of us have sat in the same seat as our customers. We&#39;ve experienced firsthand how empowering it can be to shape the future of an organization, help one another grow, as well as unlock the business opportunities that a transformation provides. Our own personal experiences, and those of our peers, have led us to conclude one thing we know to be true for every company - the story of digital transformation is a human story&amp;#8212;one that involves as much cultural transformation as technological transformation. It&amp;#8217;s with this realization we have identified the deeper factors behind a successful transformation. That&#39;s why we recently published a guide, reflecting on the nature of these changes and how you can take action in your own organization to drive a migration to the cloud.&amp;#160;&lt;/p&gt;&lt;p&gt;Posing challenging questions helps you reflect on your own organization&amp;#8217;s journey and the unique path you will need to take to lead to meaningful change. We wrote this guide to share key tenets that underpin the change philosophy you need to instill in your own organization. In &lt;a href=&#34;https://googlesre.page.link/cloud-migration-guide&#34; target=&#34;_blank&#34;&gt;A Practical Guide to Moving to Cloud&lt;/a&gt;, we present the following calls to action:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;Understand who in the organization you need to enlist to move to cloud.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Create a psychologically safe culture in which you can grow together.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Define clear objectives for your organization. Document measurable steps towards these goals and understand that each step must, in and of itself, deliver value.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Review your existing organizational behaviors and set principles/policies which influence and direct every future decision related to your transformation.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Use your new culture to refine how decisions are made, and provide meaningful autonomy across the organization.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Build structures that empower practitioners to share best practices and solve common problems. Use these structures to empower your peers.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Build guardrails into your cloud platform that support transformation, at pace, without negatively impacting others. Support safe experimentation.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Understand what types of cloud platforms are the best fit for your business needs and determine your multi-cloud strategy in anticipation of your evolving business needs (e.g. acquisitions, new revenue streams, competitive responses).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Recognize that everything is now software, and understand what this means for your existing IT infrastructure functions.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Don&amp;#8217;t be afraid to revisit existing, hallowed, security policies. Making them fit-for-purpose is crucial.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Continuously measure and apply your new policies through software.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Be bold; build a new way of operating your business products with a customer-centric perspective.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Love your developers.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;At Google Cloud, we&amp;#8217;re here to help you craft the right migration for you and your business. &lt;a href=&#34;https://googlesre.page.link/cloud-migration-guide&#34; target=&#34;_blank&#34;&gt;A Practical Guide to Moving to Cloud&lt;/a&gt; is available as a free download. You can also learn more about our &lt;a href=&#34;https://cloud.google.com/solutions/migration-center&#34;&gt;data center migration solutions&lt;/a&gt; or &lt;a href=&#34;https://inthecloud.withgoogle.com/tco-assessment-19/form.html&#34; target=&#34;_blank&#34;&gt;sign up for a free migration cost assessment&lt;/a&gt;. Let&amp;#8217;s get migrating!&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Visit &lt;a href=&#34;https://sre.google&#34; target=&#34;_blank&#34;&gt;sre.google&lt;/a&gt; to learn more about SRE and industry-leading practices for service reliability.&lt;/i&gt;&lt;/p&gt;"><p>Today, everything from payroll software to specialized machine-learning systems is available “as a service” in the cloud, addressing a vast range of needs across businesses, enabling rapid growth and scale while allowing a business to focus on its core competencies.</p><p>But moving to the cloud can generate tension, which is inevitably challenging for everyone involved—especially if that transformation creates &#34;winners&#34; and &#34;losers&#34; or frames individuals as &#34;old&#34; or &#34;new.&#34; The good news; however, is that a cloud transformation doesn&#39;t have to be this way.</p><p>As Google Cloud has grown, so too has the team of Googlers who build and support the platform, and many of us have sat in the same seat as our customers. We&#39;ve experienced firsthand how empowering it can be to shape the future of an organization, help one another grow, as well as unlock the business opportunities that a transformation provides. Our own personal experiences, and those of our peers, have led us to conclude one thing we know to be true for every company - the story of digital transformation is a human story—one that involves as much cultural transformation as technological transformation. It’s with this realization we have identified the deeper factors behind a successful transformation. That&#39;s why we recently published a guide, reflecting on the nature of these changes and how you can take action in your own organization to drive a migration to the cloud. </p><p>Posing challenging questions helps you reflect on your own organization’s journey and the unique path you will need to take to lead to meaningful change. We wrote this guide to share key tenets that underpin the change philosophy you need to instill in your own organization. In <a href="https://googlesre.page.link/cloud-migration-guide" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="https://googlesre.page.link" track-metadata-module="post">A Practical Guide to Moving to Cloud</a>, we present the following calls to action:</p><ol><li><p>Understand who in the organization you need to enlist to move to cloud.</p></li><li><p>Create a psychologically safe culture in which you can grow together.</p></li><li><p>Define clear objectives for your organization. Document measurable steps towards these goals and understand that each step must, in and of itself, deliver value.</p></li><li><p>Review your existing organizational behaviors and set principles/policies which influence and direct every future decision related to your transformation.</p></li><li><p>Use your new culture to refine how decisions are made, and provide meaningful autonomy across the organization.</p></li><li><p>Build structures that empower practitioners to share best practices and solve common problems. Use these structures to empower your peers.</p></li><li><p>Build guardrails into your cloud platform that support transformation, at pace, without negatively impacting others. Support safe experimentation.</p></li><li><p>Understand what types of cloud platforms are the best fit for your business needs and determine your multi-cloud strategy in anticipation of your evolving business needs (e.g. acquisitions, new revenue streams, competitive responses).</p></li><li><p>Recognize that everything is now software, and understand what this means for your existing IT infrastructure functions.</p></li><li><p>Don’t be afraid to revisit existing, hallowed, security policies. Making them fit-for-purpose is crucial.</p></li><li><p>Continuously measure and apply your new policies through software.</p></li><li><p>Be bold; build a new way of operating your business products with a customer-centric perspective. </p></li><li><p>Love your developers.</p></li></ol><p>At Google Cloud, we’re here to help you craft the right migration for you and your business. <a href="https://googlesre.page.link/cloud-migration-guide" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://googlesre.page.link" track-metadata-module="post">A Practical Guide to Moving to Cloud</a> is available as a free download. You can also learn more about our <a href="https://cloud.google.com/solutions/migration-center" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/solutions/migration-center" track-metadata-module="post">data center migration solutions</a> or <a href="https://inthecloud.withgoogle.com/tco-assessment-19/form.html" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://inthecloud.withgoogle.com" track-metadata-module="post">sign up for a free migration cost assessment</a>. Let’s get migrating! </p><p><i>Visit <a href="https://sre.google" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://sre.google" track-metadata-module="post">sre.google</a> to learn more about SRE and industry-leading practices for service reliability.</i></p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Kieran Broadfoot&lt;/name&gt;&lt;title&gt;Director, Site Reliability Engineering&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Blog_CloudMigration_D.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 12 Mar 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>With SRE, failing to plan is planning to fail</title>
      <link>https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;People sometimes think that implementing &lt;a href=&#34;http://sre.google&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering&lt;/a&gt; (or DevOps for that matter) will magically make everything better. Just sprinkle a little bit of SRE fairy dust on your organization and your services will be more reliable, more profitable, and your IT, product and engineering teams will be happy.&lt;/p&gt;&lt;p&gt;It’s easy to see why people think this way. Some of the world’s most reliable and scalable services run with the help of an SRE team, Google being the prime example. &lt;/p&gt;&lt;p&gt;For almost two decades, I’ve lived and breathed running production systems at large scale. I had to think about tradeoffs, reliability, costs, implementing a variety of architectures with different constraints and requirements—all while getting paged in the middle of the night. More recently, I’ve had the privilege to leverage that experience and knowledge to help Google Cloud customers modernize their infrastructure and applications, including implementing an SRE practice. While these learnings look different from organization to organization, there are common lessons learned that will impact the success of your deployment.&lt;/p&gt;&lt;p&gt;When problems do arise, it’s usually not because of technical challenges. A stalled SRE culture is usually a business process failure—goals weren’t properly defined up front and stakeholders weren’t properly engaged. After watching this play out repeatedly, I’ve developed some advice for technology leaders about how to implement a successful SRE practice. &lt;/p&gt;&lt;h3&gt;Before you start&lt;/h3&gt;&lt;p&gt;Your SRE journey should start well before you read your first manual, or put in your first call to an SRE advisor. As a technology leader within your organization, your first job is to answer a few key questions and gather some basic facts. &lt;/p&gt;&lt;p&gt;&lt;b&gt;What problem are you trying to solve? &lt;/b&gt;&lt;/p&gt;&lt;p&gt;Most organizations will readily admit they’re not perfect. Perhaps you need to &lt;a href=&#34;https://www.youtube.com/watch?v=IvQ-15-yE_c&#34; target=&#34;_blank&#34;&gt;reduce toil&lt;/a&gt;, be more innovative, or release software faster. SRE, as a framework for operating large scale systems reliably, can certainly help with those goals. To do that, it’s important to understand your motivations and what gaps or needs exist in your organization.&lt;/p&gt;&lt;p&gt;Ask yourself what the organization is trying to achieve from the transformation. What worries the organization about reliability? For SRE to be successful and efficient, it is crucial to &lt;b&gt;start with the pain&lt;/b&gt;. Starting by identifying what you are trying to solve will not just help you solve it; it will help your organization be more focused, align the relevant parties to a common goal, and make it easier to gain decision-makers’ buy-in (and much more).&lt;/p&gt;&lt;p&gt;Once you understand the problem you are trying to solve, you need to know when you have “solved” it (e.g. how you will define success). Setting goals is critical—otherwise, how will you know if you have improved? We’ll discuss how to set up metrics to help in this self-evaluation in a later post.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Who are the key decision-makers in the organization?&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Even though implementing SRE principles involves engineering at its core, it’s actually more of a transformation process than a technological challenge. As such, it will likely require procedural and cultural changes.&lt;/p&gt;&lt;p&gt;As with any business transformation, you need to identify the relevant decision-makers up front. Who those people are depends on the organization, but it usually includes stakeholders from product, operations, and engineering leadership, though these can be named differently in various organizations and can even be separated under multiple organizations. Identifying those decision makers can be especially difficult in a siloed organization. It is important to take the time and reach out to different groups to identify the key stakeholders and influencers (it will save you a lot of time later on). Make sure that you are throwing a wide enough net. It is important to get input from different groups with different requirements (e.g., security).  &lt;/p&gt;&lt;p&gt;At the same time, try to be &lt;b&gt;flexible&lt;/b&gt;. It’s okay if your list of decision makers gets updated and fine-tuned during the process. Like in other engineering domains, the goal is to start simple and iterate. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Get buy-in and build trust&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Once you’ve identified the relevant decision makers, make sure you have support from your colleagues, and the rest of the organization’s leaders. &lt;b&gt;Creating an empowered culture&lt;/b&gt; is critical for implementing the core principles of SRE: a &lt;a href=&#34;https://cloud.google.com/solutions/devops/devops-culture-learning-culture&#34;&gt;learning culture&lt;/a&gt; that accepts failures, that facilitates blamelessness and creates psychological safety, all while prioritizing gradual changes and automation.&lt;/p&gt;&lt;p&gt;From my experience, you cannot drive real change in an organization without widespread support and buy-in from leadership and decision-makers—and that’s especially true for SRE.  Implementing SRE, similar to DevOps, requires collaboration between different functions in the organization (product, operations and development). In most organizations, those functions fall under separate leadership chains, each with its own processes. If you’re going to align those goals and procedures, leadership needs to prioritize the change. At the same time, driving cultural change from the bottom up can be more challenging and take longer than top-down mandates, and in some cultures will be impossible. In short, leading by example and enabling the people in the organization are critical for driving change and fostering the &#39;right&#39; culture.&lt;/p&gt;&lt;h3&gt;Remember: it&#39;s a marathon, not a sprint &lt;/h3&gt;&lt;p&gt;The journey to SRE combines several challenges, both from technical and human (culture, process, extra) perspectives, and those are intertwined. To be successful, leadership needs to prioritize organizational changes, allocating resources for &lt;b&gt;engineering excellence&lt;/b&gt; (quality and reliability) and &lt;a href=&#34;https://cloud.google.com/solutions/devops/devops-culture-westrum-organizational-culture#how_to_implement_organizational_culture&#34;&gt;&lt;b&gt;fostering cultural principles&lt;/b&gt;&lt;/a&gt; like reducing silos, blamelessness and accepting failure as normal.&lt;/p&gt;&lt;p&gt;Align expectations! All parties involved in an SRE implementation—from product and engineering to leadership—will need to recognize that change takes time and effort, and in the short term—resources. Daunting as it may be, SRE’s goal is to solve hard problems and build for a better tomorrow. &lt;/p&gt;&lt;p&gt;&lt;i&gt;Interested in getting deeper with SRE principles? Check out this Coursera course for leaders, &lt;a href=&#34;https://www.coursera.org/learn/developing-a-google-sre-culture&#34; target=&#34;_blank&#34;&gt;Developing a Google SRE Culture&lt;/a&gt;.&lt;/i&gt; And stay tuned for my next post, where I outline some tactical considerations for teams that are early on their SRE journey, from identifying the right teams to start with, enablement and building community.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_A_Rnd3.max-2800x2800.max-500x500.jpg&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Four steps to jumpstarting your SRE practice&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;Once you have leadership buy-in, there are some things you can do to get the SRE ball rolling, fast.&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;People sometimes think that implementing &lt;a href=&#34;http://sre.google&#34; target=&#34;_blank&#34;&gt;Site Reliability Engineering&lt;/a&gt; (or DevOps for that matter) will magically make everything better. Just sprinkle a little bit of SRE fairy dust on your organization and your services will be more reliable, more profitable, and your IT, product and engineering teams will be happy.&lt;/p&gt;&lt;p&gt;It&amp;#8217;s easy to see why people think this way. Some of the world&amp;#8217;s most reliable and scalable services run with the help of an SRE team, Google being the prime example.&amp;#160;&lt;/p&gt;&lt;p&gt;For almost two decades, I&amp;#8217;ve lived and breathed running production systems at large scale. I had to think about tradeoffs, reliability, costs, implementing a variety of architectures with different constraints and requirements&amp;#8212;all while getting paged in the middle of the night. More recently, I&amp;#8217;ve had the privilege to leverage that experience and knowledge to help Google Cloud customers modernize their infrastructure and applications, including implementing an SRE practice. While these learnings look different from organization to organization, there are common lessons learned that will impact the success of your deployment.&lt;/p&gt;&lt;p&gt;When problems do arise, it&amp;#8217;s usually not because of technical challenges. A stalled SRE culture is usually a business process failure&amp;#8212;goals weren&amp;#8217;t properly defined up front and stakeholders weren&amp;#8217;t properly engaged. After watching this play out repeatedly, I&amp;#8217;ve developed some advice for technology leaders about how to implement a successful SRE practice.&amp;#160;&lt;/p&gt;&lt;h3&gt;Before you start&lt;/h3&gt;&lt;p&gt;Your SRE journey should start well before you read your first manual, or put in your first call to an SRE advisor. As a technology leader within your organization, your first job is to answer a few key questions and gather some basic facts.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;What problem are you trying to solve?&amp;#160;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Most organizations will readily admit they&amp;#8217;re not perfect. Perhaps you need to &lt;a href=&#34;https://www.youtube.com/watch?v=IvQ-15-yE_c&#34; target=&#34;_blank&#34;&gt;reduce toil&lt;/a&gt;, be more innovative, or release software faster. SRE, as a framework for operating large scale systems reliably, can certainly help with those goals. To do that, it&amp;#8217;s important to understand your motivations and what gaps or needs exist in your organization.&lt;/p&gt;&lt;p&gt;Ask yourself what the organization is trying to achieve from the transformation. What worries the organization about reliability? For SRE to be successful and efficient, it is crucial to &lt;b&gt;start with the pain&lt;/b&gt;. Starting by identifying what you are trying to solve will not just help you solve it; it will help your organization be more focused, align the relevant parties to a common goal, and make it easier to gain decision-makers&amp;#8217; buy-in (and much more).&lt;/p&gt;&lt;p&gt;Once you understand the problem you are trying to solve, you need to know when you have &amp;#8220;solved&amp;#8221; it (e.g. how you will define success). Setting goals is critical&amp;#8212;otherwise, how will you know if you have improved? We&amp;#8217;ll discuss how to set up metrics to help in this self-evaluation in a later post.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Who are the key decision-makers in the organization?&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Even though implementing SRE principles involves engineering at its core, it&amp;#8217;s actually more of a transformation process than a technological challenge. As such, it will likely require procedural and cultural changes.&lt;/p&gt;&lt;p&gt;As with any business transformation, you need to identify the relevant decision-makers up front. Who those people are depends on the organization, but it usually includes stakeholders from product, operations, and engineering leadership, though these can be named differently in various organizations and can even be separated under multiple organizations. Identifying those decision makers can be especially difficult in a siloed organization. It is important to take the time and reach out to different groups to identify the key stakeholders and influencers (it will save you a lot of time later on). Make sure that you are throwing a wide enough net. It is important to get input from different groups with different requirements (e.g., security).&amp;#160;&amp;#160;&lt;/p&gt;&lt;p&gt;At the same time, try to be &lt;b&gt;flexible&lt;/b&gt;. It&amp;#8217;s okay if your list of decision makers gets updated and fine-tuned during the process. Like in other engineering domains, the goal is to start simple and iterate.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Get buy-in and build trust&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Once you&amp;#8217;ve identified the relevant decision makers, make sure you have support from your colleagues, and the rest of the organization&amp;#8217;s leaders. &lt;b&gt;Creating an empowered culture&lt;/b&gt; is critical for implementing the core principles of SRE: a &lt;a href=&#34;https://cloud.google.com/solutions/devops/devops-culture-learning-culture&#34;&gt;learning culture&lt;/a&gt; that accepts failures, that facilitates blamelessness and creates psychological safety, all while prioritizing gradual changes and automation.&lt;/p&gt;&lt;p&gt;From my experience, you cannot drive real change in an organization without widespread support and buy-in from leadership and decision-makers&amp;#8212;and that&amp;#8217;s especially true for SRE.&amp;#160; Implementing SRE, similar to DevOps, requires collaboration between different functions in the organization (product, operations and development). In most organizations, those functions fall under separate leadership chains, each with its own processes. If you&amp;#8217;re going to align those goals and procedures, leadership needs to prioritize the change. At the same time, driving cultural change from the bottom up can be more challenging and take longer than top-down mandates, and in some cultures will be impossible. In short, leading by example and enabling the people in the organization are critical for driving change and fostering the &#39;right&#39; culture.&lt;/p&gt;&lt;h3&gt;Remember: it&#39;s a marathon, not a sprint&amp;#160;&lt;/h3&gt;&lt;p&gt;The journey to SRE combines several challenges, both from technical and human (culture, process, extra) perspectives, and those are intertwined. To be successful, leadership needs to prioritize organizational changes, allocating resources for &lt;b&gt;engineering excellence&lt;/b&gt; (quality and reliability) and &lt;a href=&#34;https://cloud.google.com/solutions/devops/devops-culture-westrum-organizational-culture#how_to_implement_organizational_culture&#34;&gt;&lt;b&gt;fostering cultural principles&lt;/b&gt;&lt;/a&gt; like reducing silos, blamelessness and accepting failure as normal.&lt;/p&gt;&lt;p&gt;Align expectations! All parties involved in an SRE implementation&amp;#8212;from product and engineering to leadership&amp;#8212;will need to recognize that change takes time and effort, and in the short term&amp;#8212;resources. Daunting as it may be, SRE&amp;#8217;s goal is to solve hard problems and build for a better tomorrow.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Interested in getting deeper with SRE principles? Check out this Coursera course for leaders, &lt;a href=&#34;https://www.coursera.org/learn/developing-a-google-sre-culture&#34; target=&#34;_blank&#34;&gt;Developing a Google SRE Culture&lt;/a&gt;.&lt;/i&gt; And stay tuned for my next post, where I outline some tactical considerations for teams that are early on their SRE journey, from identifying the right teams to start with, enablement and building community.&lt;/p&gt;"><p>People sometimes think that implementing <a href="http://sre.google" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="http://sre.google" track-metadata-module="post">Site Reliability Engineering</a> (or DevOps for that matter) will magically make everything better. Just sprinkle a little bit of SRE fairy dust on your organization and your services will be more reliable, more profitable, and your IT, product and engineering teams will be happy.</p><p>It’s easy to see why people think this way. Some of the world’s most reliable and scalable services run with the help of an SRE team, Google being the prime example. </p><p>For almost two decades, I’ve lived and breathed running production systems at large scale. I had to think about tradeoffs, reliability, costs, implementing a variety of architectures with different constraints and requirements—all while getting paged in the middle of the night. More recently, I’ve had the privilege to leverage that experience and knowledge to help Google Cloud customers modernize their infrastructure and applications, including implementing an SRE practice. While these learnings look different from organization to organization, there are common lessons learned that will impact the success of your deployment.</p><p>When problems do arise, it’s usually not because of technical challenges. A stalled SRE culture is usually a business process failure—goals weren’t properly defined up front and stakeholders weren’t properly engaged. After watching this play out repeatedly, I’ve developed some advice for technology leaders about how to implement a successful SRE practice. </p><h3>Before you start</h3><p>Your SRE journey should start well before you read your first manual, or put in your first call to an SRE advisor. As a technology leader within your organization, your first job is to answer a few key questions and gather some basic facts. </p><p><b>What problem are you trying to solve? </b></p><p>Most organizations will readily admit they’re not perfect. Perhaps you need to <a href="https://www.youtube.com/watch?v=IvQ-15-yE_c" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://www.youtube.com" track-metadata-module="post">reduce toil</a>, be more innovative, or release software faster. SRE, as a framework for operating large scale systems reliably, can certainly help with those goals. To do that, it’s important to understand your motivations and what gaps or needs exist in your organization.</p><p>Ask yourself what the organization is trying to achieve from the transformation. What worries the organization about reliability? For SRE to be successful and efficient, it is crucial to <b>start with the pain</b>. Starting by identifying what you are trying to solve will not just help you solve it; it will help your organization be more focused, align the relevant parties to a common goal, and make it easier to gain decision-makers’ buy-in (and much more).</p><p>Once you understand the problem you are trying to solve, you need to know when you have “solved” it (e.g. how you will define success). Setting goals is critical—otherwise, how will you know if you have improved? We’ll discuss how to set up metrics to help in this self-evaluation in a later post.</p><p><b>Who are the key decision-makers in the organization?</b></p><p>Even though implementing SRE principles involves engineering at its core, it’s actually more of a transformation process than a technological challenge. As such, it will likely require procedural and cultural changes.</p><p>As with any business transformation, you need to identify the relevant decision-makers up front. Who those people are depends on the organization, but it usually includes stakeholders from product, operations, and engineering leadership, though these can be named differently in various organizations and can even be separated under multiple organizations. Identifying those decision makers can be especially difficult in a siloed organization. It is important to take the time and reach out to different groups to identify the key stakeholders and influencers (it will save you a lot of time later on). Make sure that you are throwing a wide enough net. It is important to get input from different groups with different requirements (e.g., security).  </p><p>At the same time, try to be <b>flexible</b>. It’s okay if your list of decision makers gets updated and fine-tuned during the process. Like in other engineering domains, the goal is to start simple and iterate. </p><p><b>Get buy-in and build trust</b></p><p>Once you’ve identified the relevant decision makers, make sure you have support from your colleagues, and the rest of the organization’s leaders. <b>Creating an empowered culture</b> is critical for implementing the core principles of SRE: a <a href="https://cloud.google.com/solutions/devops/devops-culture-learning-culture" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/solutions/devops/devops-culture-learning-culture" track-metadata-module="post">learning culture</a> that accepts failures, that facilitates blamelessness and creates psychological safety, all while prioritizing gradual changes and automation.</p><p>From my experience, you cannot drive real change in an organization without widespread support and buy-in from leadership and decision-makers—and that’s especially true for SRE.  Implementing SRE, similar to DevOps, requires collaboration between different functions in the organization (product, operations and development). In most organizations, those functions fall under separate leadership chains, each with its own processes. If you’re going to align those goals and procedures, leadership needs to prioritize the change. At the same time, driving cultural change from the bottom up can be more challenging and take longer than top-down mandates, and in some cultures will be impossible. In short, leading by example and enabling the people in the organization are critical for driving change and fostering the &#39;right&#39; culture.</p><h3>Remember: it&#39;s a marathon, not a sprint </h3><p>The journey to SRE combines several challenges, both from technical and human (culture, process, extra) perspectives, and those are intertwined. To be successful, leadership needs to prioritize organizational changes, allocating resources for <b>engineering excellence</b> (quality and reliability) and <a href="https://cloud.google.com/solutions/devops/devops-culture-westrum-organizational-culture#how_to_implement_organizational_culture" track-type="inline link" track-name="4" track-metadata-eventdetail="https://cloud.google.com/solutions/devops/devops-culture-westrum-organizational-culture#how_to_implement_organizational_culture" track-metadata-module="post"><b>fostering cultural principles</b></a> like reducing silos, blamelessness and accepting failure as normal.</p><p>Align expectations! All parties involved in an SRE implementation—from product and engineering to leadership—will need to recognize that change takes time and effort, and in the short term—resources. Daunting as it may be, SRE’s goal is to solve hard problems and build for a better tomorrow. </p><p><i>Interested in getting deeper with SRE principles? Check out this Coursera course for leaders, <a href="https://www.coursera.org/learn/developing-a-google-sre-culture" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://www.coursera.org" track-metadata-module="post">Developing a Google SRE Culture</a>.</i> And stay tuned for my next post, where I outline some tactical considerations for teams that are early on their SRE journey, from identifying the right teams to start with, enablement and building community.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Ayelet Sachto&lt;/name&gt;&lt;title&gt;Strategic Cloud Engineer, Infra, AppMod, SRE&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 26 Feb 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Troubleshooting services on Google Kubernetes Engine by example</title>
      <link>https://cloud.google.com/blog/products/operations/troubleshooting-services-on-google-kubernetes-engine/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Applications fail. Containers crash. It’s a fact of life that SRE and DevOps teams know all too well. To help navigate life’s hiccups, we’ve previously shared &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/tools-for-debugging-apps-on-google-kubernetes-engine&#34;&gt;how to debug applications running on Google Kubernetes Engine (GKE)&lt;/a&gt;. We’ve also updated the GKE &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/observing&#34;&gt;dashboard&lt;/a&gt; with new easier-to-use troubleshooting flows. Today, we go one step further and show you how you can use these flows to quickly find and resolve issues in your applications and infrastructure. &lt;/p&gt;&lt;p&gt;In this blog, we&#39;ll walk through deploying a sample app to your cluster and configuring an alerting policy that will notify you if there are any container restarts observed. From there, we&#39;ll trigger the alert and explore how the new GKE dashboard makes it easy to identify the issue and determine exactly what&#39;s going on with your workload or infrastructure that may be causing it.&lt;/p&gt;&lt;h3&gt;Setting up&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Deploy the app&lt;br/&gt;&lt;/b&gt;This example uses a &lt;a href=&#34;https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/main.go&#34; target=&#34;_blank&#34;&gt;demo app&lt;/a&gt; that exposes two endpoints: an endpoint at /, which is just a &#34;hello world&#34;, and a /crashme endpoint, which uses Go&#39;s &lt;code&gt;os.Exit(1)&lt;/code&gt; to terminate the process. To deploy the app in your own cluster, create a container image using Cloud Build and &lt;a href=&#34;https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml&#34; target=&#34;_blank&#34;&gt;deploy&lt;/a&gt; it to GKE. Then, expose the service with a load balancer. &lt;/p&gt;&lt;p&gt;Once the service is deployed, check the running pods:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Notice that RESTARTS is initially at zero for each pod. Use a browser or a command line tool like curl to access the /crashme endpoint. At this point, you should see a restart:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Each request to that endpoint will result in a restart. However, be careful to not do this more often than every 30 seconds or so, otherwise, the containers will go into CrashLoopBackOff, and it will take time for the service to be available again. You can use this simple shell script to trigger restarts when as needed:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;where $IP_ADDRESS is the IP address of the load balancer you&#39;ve already created. &lt;/p&gt;&lt;p&gt;Why do container restarts matter? Well, restarts, to a certain degree, are an expected part of a container’s typical lifecycle in Kubernetes. Too many container restarts, however, could affect the availability of your service, especially when expanded over a larger number of replicas for a given Pod. Not only do excessive restarts degrade the service in question, but they also risks affecting other services downstream that use it as a dependency.&lt;/p&gt;&lt;p&gt;In real life,the culprit for a large number of restarts could be a poorly designed liveness probe, issues like &lt;a href=&#34;https://en.wikipedia.org/wiki/Deadlock&#34; target=&#34;_blank&#34;&gt;deadlocks&lt;/a&gt; in the application itself, or misconfigured memory requests that result in &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;OOMkilled&lt;/code&gt;&lt;/a&gt; errors. So, it is important for you to proactively alert on container restarts to preempt potential degradation that can cascade across multiple services. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Configure the alert&lt;br/&gt;&lt;/b&gt;Now, you&#39;re ready to configure the alert that will notify you when restarts are detected. Here&#39;s how to set up your &lt;a href=&#34;https://cloud.google.com/monitoring/alerts&#34;&gt;alerting policy&lt;/a&gt;:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;1 Configure the alert.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Configure_the_alert.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;You can use the &lt;code&gt;kubernetes.io/container/restart_count&lt;/code&gt; metric, filtered to the specific container name (as specified in the deployment yaml &lt;a href=&#34;https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml&#34; target=&#34;_blank&#34;&gt;file&lt;/a&gt;). Configure the alert to trigger if any timeseries exceeded zero—meaning if any container restarts are observed. &lt;/p&gt;&lt;p&gt;With the setup done, you are ready to test and see what happens!&lt;/p&gt;&lt;h3&gt;Testing the alert&lt;/h3&gt;&lt;p&gt;When you’re ready, start the looped script to hit the /crashme endpoint every 45 seconds. The restart_count metric is sampled every 60 seconds, so it shouldn&#39;t take very long for an alert to show up on the dashboard:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;2 Testing the alert.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Testing_the_alert.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;You can mouse-over the incident to get more information about it:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;3 incident.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/3_incident.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Then click on &#34;View Incident&#34;. This takes you to the Incident details screen, where you can see the specific resources that triggered it—in this case, the incident is generated by the container.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;4 View Incident.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/4_View_Incident.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Next, you can click on View Logs to see the logs (in the &lt;a href=&#34;https://cloud.google.com/logging/docs/view/logs-viewer-preview&#34;&gt;new Logs Viewer&lt;/a&gt;!)—it&#39;s immediately apparently that the alert is triggered by the containers restarting:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;5 View Logs.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/5_View_Logs.max-1000x1000.jpg&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;This is all very nicely tied together and makes troubleshooting during an incident much easier!&lt;/p&gt;&lt;h3&gt;In summary….&lt;/h3&gt;&lt;p&gt;The latest GKE dashboard includes many improvements over previous iterations. The new alerts timeline is intuitive, and incidents are clearly marked so that you can interact with them to get the full details of exactly what happened, all the way down to the container logs that tell you the actual problem.&lt;/p&gt;&lt;p&gt;As an oncall SRE or DevOps engineer for a service running on GKE, the GKE dashboard makes it easier for you to respond to incidents. You&#39;re now able to go from an incident all the way to debug logs quickly and easily and reduce the time it takes to triage and mitigate incidents. For a short overview on how to troubleshoot services on GKE, check out this video:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-video&#34;&gt;&lt;div class=&#34;article-module article-video &#34;&gt;&lt;figure&gt;&lt;a class=&#34;h-c-video h-c-video--marquee&#34; data-glue-modal-disabled-on-mobile=&#34;true&#34; data-glue-modal-trigger=&#34;uni-modal---4WWwx4Log-&#34; href=&#34;https://youtube.com/watch?v=--4WWwx4Log&#34;&gt;&lt;img alt=&#34;In previous episodes, we’ve shown you how to set up monitoring and alerting for your GKE services, but what do you do when an alert fires? In this episode of Stack Doctor, we show you how to use the alerts timeline on your GKE monitoring dashboards to troubleshoot your services. Watch to learn how you can easily spot and resolve issues in your applications and infrastructure!&#34; src=&#34;//img.youtube.com/vi/--4WWwx4Log/maxresdefault.jpg&#34;/&gt;&lt;svg class=&#34;h-c-video__play h-c-icon h-c-icon--color-white&#34; role=&#34;img&#34;&gt;&lt;use xlink:href=&#34;#mi-youtube-icon&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;div class=&#34;h-c-modal--video&#34; data-glue-modal=&#34;uni-modal---4WWwx4Log-&#34; data-glue-modal-close-label=&#34;Close Dialog&#34;&gt;&lt;a class=&#34;glue-yt-video&#34; data-glue-yt-video-autoplay=&#34;true&#34; data-glue-yt-video-height=&#34;99%&#34; data-glue-yt-video-vid=&#34;--4WWwx4Log&#34; data-glue-yt-video-width=&#34;100%&#34; href=&#34;https://youtube.com/watch?v=--4WWwx4Log&#34; ng-cloak=&#34;&#34;&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;&lt;i&gt;&lt;sup&gt;A special thanks to Anthony Bushong, Specialist Customer Engineer, for his contributions to this blog post.&lt;/sup&gt;&lt;/i&gt;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;https://gweb-cloudblog-publish.appspot.com/products/management-tools/shrinking-the-time-to-mitigate-production-incidents/&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;Shrinking the time to mitigate production incidents—CRE life lessons&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;See how you can use SRE and CRE principles and tests from Google, including Wheel of Misfortune and DiRT, to reduce the time needed to mi...&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><article-page><main id="jump-content"><article><article-header-block></article-header-block><article-aspect-image-block><figure><p><span>#containers</span></p></figure></article-aspect-image-block><div><article-cta _nghost-c17=""><div _ngcontent-c17=""><h4 _ngcontent-c17=""><span _ngcontent-c17="">Try GCP</span></h4><p _ngcontent-c17=""><span _ngcontent-c17="">Start building on Google Cloud with $300 in free credits and 20+ always free products.</span></p><p><a _ngcontent-c17="" clicktracker="" rel="external" track-metadata-module="article cta" track-type="button" track-name="free trial" track-metadata-eventdetail="https://cloud.google.com/free/" href="https://cloud.google.com/free/"><span _ngcontent-c17="">Free Trial</span></a></p></div></article-cta></div><article-share-block></article-share-block><article-sticky-share-block></article-sticky-share-block><div><div><article-content-stream-block><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;Applications fail. Containers crash. It&amp;#8217;s a fact of life that SRE and DevOps teams know all too well. To help navigate life&amp;#8217;s hiccups, we&amp;#8217;ve previously shared &lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/tools-for-debugging-apps-on-google-kubernetes-engine&#34;&gt;how to debug applications running on Google Kubernetes Engine (GKE)&lt;/a&gt;. We&amp;#8217;ve also updated the GKE &lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/gke/observing&#34;&gt;dashboard&lt;/a&gt; with new easier-to-use troubleshooting flows. Today, we go one step further and show you how you can use these flows to quickly find and resolve issues in your applications and infrastructure.&amp;#160;&lt;/p&gt;&lt;p&gt;In this blog, we&#39;ll walk through deploying a sample app to your cluster and configuring an alerting policy that will notify you if there are any container restarts observed. From there, we&#39;ll trigger the alert and explore how the new GKE dashboard makes it easy to identify the issue and determine exactly what&#39;s going on with your workload or infrastructure that may be causing it.&lt;/p&gt;&lt;h3&gt;Setting up&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Deploy the app&lt;br&gt;&lt;/b&gt;This example uses a &lt;a href=&#34;https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/main.go&#34; target=&#34;_blank&#34;&gt;demo app&lt;/a&gt; that exposes two endpoints: an endpoint at /, which is just a &amp;#34;hello world&amp;#34;, and a /crashme endpoint, which uses Go&#39;s &lt;code&gt;os.Exit(1)&lt;/code&gt; to terminate the process. To deploy the app in your own cluster, create a container image using Cloud Build and &lt;a href=&#34;https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml&#34; target=&#34;_blank&#34;&gt;deploy&lt;/a&gt; it to GKE. Then, expose the service with a load balancer.&amp;#160;&lt;/p&gt;&lt;p&gt;Once the service is deployed, check the running pods:&lt;/p&gt;"><p>Applications fail. Containers crash. It’s a fact of life that SRE and DevOps teams know all too well. To help navigate life’s hiccups, we’ve previously shared <a href="https://cloud.google.com/blog/products/containers-kubernetes/tools-for-debugging-apps-on-google-kubernetes-engine" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/blog/products/containers-kubernetes/tools-for-debugging-apps-on-google-kubernetes-engine" track-metadata-module="post">how to debug applications running on Google Kubernetes Engine (GKE)</a>. We’ve also updated the GKE <a href="https://cloud.google.com/stackdriver/docs/solutions/gke/observing" track-type="inline link" track-name="2" track-metadata-eventdetail="https://cloud.google.com/stackdriver/docs/solutions/gke/observing" track-metadata-module="post">dashboard</a> with new easier-to-use troubleshooting flows. Today, we go one step further and show you how you can use these flows to quickly find and resolve issues in your applications and infrastructure. </p><p>In this blog, we&#39;ll walk through deploying a sample app to your cluster and configuring an alerting policy that will notify you if there are any container restarts observed. From there, we&#39;ll trigger the alert and explore how the new GKE dashboard makes it easy to identify the issue and determine exactly what&#39;s going on with your workload or infrastructure that may be causing it.</p><h3>Setting up</h3><p><b>Deploy the app<br/></b>This example uses a <a href="https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/main.go" target="_blank" track-type="inline link" track-name="3" track-metadata-eventdetail="https://github.com" track-metadata-module="post">demo app</a> that exposes two endpoints: an endpoint at /, which is just a &#34;hello world&#34;, and a /crashme endpoint, which uses Go&#39;s <code>os.Exit(1)</code> to terminate the process. To deploy the app in your own cluster, create a container image using Cloud Build and <a href="https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://github.com" track-metadata-module="post">deploy</a> it to GKE. Then, expose the service with a load balancer. </p><p>Once the service is deployed, check the running pods:</p></div></paragraph-block></div><div><article-code-block _nghost-c20=""><pre _ngcontent-c20="">  <code _ngcontent-c20="">✗ kubectl get pods
</code><code _ngcontent-c20="">NAME                                     READY   STATUS    RESTARTS   AGE
</code><code _ngcontent-c20="">restarting-deployment-54c8678f79-gjh2v   1/1     Running   0          6m38s
</code><code _ngcontent-c20="">restarting-deployment-54c8678f79-l8tsm   1/1     Running   0          6m38s
</code><code _ngcontent-c20="">restarting-deployment-54c8678f79-qjrcb   1/1     Running   0          6m38s</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c19=""><p>Notice that RESTARTS is initially at zero for each pod. Use a browser or a command line tool like curl to access the /crashme endpoint. At this point, you should see a restart:</p></paragraph-block></div><div><article-code-block _nghost-c20=""><pre _ngcontent-c20="">  <code _ngcontent-c20="">✗ kubectl get pods
</code><code _ngcontent-c20="">NAME                                     READY   STATUS    RESTARTS   AGE
</code><code _ngcontent-c20="">restarting-deployment-54c8678f79-gjh2v   1/1     Running   1          9m28s
</code><code _ngcontent-c20="">restarting-deployment-54c8678f79-l8tsm   1/1     Running   0          9m28s
</code><code _ngcontent-c20="">restarting-deployment-54c8678f79-qjrcb   1/1     Running   0          9m28s</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c19=""><p>Each request to that endpoint will result in a restart. However, be careful to not do this more often than every 30 seconds or so, otherwise, the containers will go into CrashLoopBackOff, and it will take time for the service to be available again. You can use this simple shell script to trigger restarts when as needed:</p></paragraph-block></div><div><article-code-block _nghost-c20=""><pre _ngcontent-c20="">  <code _ngcontent-c20="">while true;
</code><code _ngcontent-c20="">do 
</code><code _ngcontent-c20="">curl http://$IP_ADDRESS:8080/crashme; 
</code><code _ngcontent-c20="">sleep 45; 
</code><code _ngcontent-c20="">done</code>
</pre></article-code-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;where $IP_ADDRESS is the IP address of the load balancer you&#39;ve already created.&amp;#160;&lt;/p&gt;&lt;p&gt;Why do container restarts matter? Well, restarts, to a certain degree, are an expected part of a container&amp;#8217;s typical lifecycle in Kubernetes. Too many container restarts, however, could affect the availability of your service, especially when expanded over a larger number of replicas for a given Pod. Not only do excessive restarts degrade the service in question, but they also risks affecting other services downstream that use it as a dependency.&lt;/p&gt;&lt;p&gt;In real life,the culprit for a large number of restarts could be a poorly designed liveness probe, issues like &lt;a href=&#34;https://en.wikipedia.org/wiki/Deadlock&#34; target=&#34;_blank&#34;&gt;deadlocks&lt;/a&gt; in the application itself, or misconfigured memory requests that result in &lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;OOMkilled&lt;/code&gt;&lt;/a&gt; errors. So, it is important for you to proactively alert on container restarts to preempt potential degradation that can cascade across multiple services.&amp;#160;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Configure the alert&lt;br&gt;&lt;/b&gt;Now, you&#39;re ready to configure the alert that will notify you when restarts are detected. Here&#39;s how to set up your &lt;a href=&#34;https://cloud.google.com/monitoring/alerts&#34;&gt;alerting policy&lt;/a&gt;:&lt;/p&gt;"><p>where $IP_ADDRESS is the IP address of the load balancer you&#39;ve already created. </p><p>Why do container restarts matter? Well, restarts, to a certain degree, are an expected part of a container’s typical lifecycle in Kubernetes. Too many container restarts, however, could affect the availability of your service, especially when expanded over a larger number of replicas for a given Pod. Not only do excessive restarts degrade the service in question, but they also risks affecting other services downstream that use it as a dependency.</p><p>In real life,the culprit for a large number of restarts could be a poorly designed liveness probe, issues like <a href="https://en.wikipedia.org/wiki/Deadlock" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="https://en.wikipedia.org" track-metadata-module="post">deadlocks</a> in the application itself, or misconfigured memory requests that result in <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://kubernetes.io" track-metadata-module="post"><code>OOMkilled</code></a> errors. So, it is important for you to proactively alert on container restarts to preempt potential degradation that can cascade across multiple services. </p><p><b>Configure the alert<br/></b>Now, you&#39;re ready to configure the alert that will notify you when restarts are detected. Here&#39;s how to set up your <a href="https://cloud.google.com/monitoring/alerts" track-type="inline link" track-name="7" track-metadata-eventdetail="https://cloud.google.com/monitoring/alerts" track-metadata-module="post">alerting policy</a>:</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;You can use the &lt;code&gt;kubernetes.io/container/restart_count&lt;/code&gt; metric, filtered to the specific container name (as specified in the deployment yaml &lt;a href=&#34;https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml&#34; target=&#34;_blank&#34;&gt;file&lt;/a&gt;). Configure the alert to trigger if any timeseries exceeded zero&amp;#8212;meaning if any container restarts are observed.&amp;#160;&lt;/p&gt;&lt;p&gt;With the setup done, you are ready to test and see what happens!&lt;/p&gt;&lt;h3&gt;Testing the alert&lt;/h3&gt;&lt;p&gt;When you&amp;#8217;re ready, start the looped script to hit the /crashme endpoint every 45 seconds. The restart_count metric is sampled every 60 seconds, so it shouldn&#39;t take very long for an alert to show up on the dashboard:&lt;/p&gt;"><p>You can use the <code>kubernetes.io/container/restart_count</code> metric, filtered to the specific container name (as specified in the deployment yaml <a href="https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml" target="_blank" track-type="inline link" track-name="8" track-metadata-eventdetail="https://github.com" track-metadata-module="post">file</a>). Configure the alert to trigger if any timeseries exceeded zero—meaning if any container restarts are observed. </p><p>With the setup done, you are ready to test and see what happens!</p><h3>Testing the alert</h3><p>When you’re ready, start the looped script to hit the /crashme endpoint every 45 seconds. The restart_count metric is sampled every 60 seconds, so it shouldn&#39;t take very long for an alert to show up on the dashboard:</p></div></paragraph-block></div><div><paragraph-block _nghost-c19=""><p>You can mouse-over the incident to get more information about it:</p></paragraph-block></div><div><paragraph-block _nghost-c19=""><p>Then click on &#34;View Incident&#34;. This takes you to the Incident details screen, where you can see the specific resources that triggered it—in this case, the incident is generated by the container.</p></paragraph-block></div><div><paragraph-block _nghost-c19=""><p>Next, you can click on View Logs to see the logs (in the <a href="https://cloud.google.com/logging/docs/view/logs-viewer-preview" track-type="inline link" track-name="9" track-metadata-eventdetail="https://cloud.google.com/logging/docs/view/logs-viewer-preview" track-metadata-module="post">new Logs Viewer</a>!)—it&#39;s immediately apparently that the alert is triggered by the containers restarting:</p></paragraph-block></div><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;This is all very nicely tied together and makes troubleshooting during an incident much easier!&lt;/p&gt;&lt;h3&gt;In summary&amp;#8230;.&lt;/h3&gt;&lt;p&gt;The latest GKE dashboard includes many improvements over previous iterations. The new alerts timeline is intuitive, and incidents are clearly marked so that you can interact with them to get the full details of exactly what happened, all the way down to the container logs that tell you the actual problem.&lt;/p&gt;&lt;p&gt;As an oncall SRE or DevOps engineer for a service running on GKE, the GKE dashboard makes it easier for you to respond to incidents. You&#39;re now able to go from an incident all the way to debug logs quickly and easily and reduce the time it takes to triage and mitigate incidents. For a short overview on how to troubleshoot services on GKE, check out this video:&lt;/p&gt;"><p>This is all very nicely tied together and makes troubleshooting during an incident much easier!</p><h3>In summary….</h3><p>The latest GKE dashboard includes many improvements over previous iterations. The new alerts timeline is intuitive, and incidents are clearly marked so that you can interact with them to get the full details of exactly what happened, all the way down to the container logs that tell you the actual problem.</p><p>As an oncall SRE or DevOps engineer for a service running on GKE, the GKE dashboard makes it easier for you to respond to incidents. You&#39;re now able to go from an incident all the way to debug logs quickly and easily and reduce the time it takes to triage and mitigate incidents. For a short overview on how to troubleshoot services on GKE, check out this video:</p></div></paragraph-block></div><div><article-video-block _nghost-c16=""><p _ngcontent-c16=""><iframe _ngcontent-c16="" allow="encrypted-media" allowfullscreen="" frameborder="0" height="100%" position="absolute" width="100%" src="https://www.youtube.com/embed/--4WWwx4Log?enablejsapi=1&amp;"></iframe></p></article-video-block></div><div><paragraph-block _nghost-c19=""><p><i><sup>A special thanks to Anthony Bushong, Specialist Customer Engineer, for his contributions to this blog post.</sup></i></p></paragraph-block></div></article-content-stream-block><article-tag-list-block></article-tag-list-block></div><section><article-up-1to3-block _nghost-c18=""></article-up-1to3-block></section></div></article></main></article-page></div></div>]]></content:encoded>
      <author>&lt;name&gt;Yuri Grinshteyn&lt;/name&gt;&lt;title&gt;Site Reliability Engineer, CRE&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_Kubernetes_tJPVpVo.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 26 Feb 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Show me the money! How you can see returns up to $259M with a DevOps transformation</title>
      <link>https://cloud.google.com/blog/products/application-development/show-me-the-money-how-you-can-see-returns-up-to-259m-with-a-devops-transformation/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;2020 challenged some of the best laid plans by enterprises. With nearly everything moving online, Covid-19 pushed forward years of digital transformation. DevOps was at the heart of this transformation journey. After all, delivering software quickly, reliably, and safely to meet the changing needs of customers was crucial to adapt to this new normal.&lt;/p&gt;&lt;p&gt;It is unlikely that the pace of modernization will slow down in 2021. As IT and business leaders further drive digital adoption within their organizations via DevOps, the need to quantify the business benefit from a digital transformation remains top of mind. A reliable model is imperative to drive the right level of investments and measure the returns. This is precisely why we wrote &lt;a href=&#34;https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper&#34;&gt;How to Measure ROI of DevOps Transformation&lt;/a&gt;. This white paper is backed with scientific studies conducted by &lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DevOps Research and Assessment, DORA&lt;/a&gt;, with 31,000 professionals worldwide over 6 years to provide clear guidance based on impartial industry data. We found the financial savings of DevOps transformation varies from from $10M to $259M a year.&lt;/p&gt;&lt;h3&gt;Looking beyond cost to value&lt;/h3&gt;&lt;p&gt;The most innovative companies undertake their technology transformations with a focus on the value they can deliver to their customers. Hence, in addition to measuring cost savings, we show how DevOps done right can be a value driver and innovation engine. Let&#39;s look deeper into how we quantify the cost and value-generating power of DevOps. &lt;/p&gt;&lt;h3&gt;Cost-driven category&lt;/h3&gt;&lt;p&gt;Here, we focus on quantifying the cost savings and efficiencies realized by implementing DevOps—for example, how an investment in DevOps reduces costs by cutting the time it takes to resolve outages and avoiding downtime as much as possible. &lt;/p&gt;&lt;p&gt;However, focusing solely on reducing costs can rarely yield systemic, long-term gains; thereby increasing the importance of going beyond cost-driven strategies. The cost savings achieved in year one “no longer count” beyond year two as the organization adjusts to a new baseline of costs and performance. Worse, only focusing on cost savings signals to technical staff their job is potentially at risk due to automation rather than being liberated from drudge work to better drive business growth. This leads to negative effects on morale and productivity. &lt;/p&gt;&lt;h3&gt;Value-driven category&lt;/h3&gt;&lt;p&gt;There are two value drivers in a DevOps transformation, (1) improved efficiency through the reduction of unnecessary rework, and (2) the potential revenue gained by reinvesting the time saved in new offer capabilities.&lt;/p&gt;&lt;p&gt;Adding these cost and value driven categories together, IT and business decision makers can get an estimate of the potential value their organizations can expect to gain from a DevOps transformation. This helps justify the investment needed to implement the required changes. To quantify the impact, we leverage industry benchmark data across low, medium, high, and elite DevOps teams, as described by DORA in its annual &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;Accelerate: State of DevOps report&lt;/a&gt;. &lt;/p&gt;&lt;h3&gt;Combining cost and value&lt;/h3&gt;&lt;p&gt;As an example, let&#39;s consider the impact of a DevOps transformation on a large organization with 8,500 technical staff and a medium IT performer. Using the data gained from the DevOps report, we can calculate both the cost and value driven categories along with total impact. &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;img alt=&#34;roi table&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Screen_Shot_2021-01-25_at_6.08.51_PM.max-1000x1000.png&#34;/&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;While this example represents what a medium IT performer at a large organization might expect by investing in DevOps, companies of all sizes and performance profiles can leverage DevOps to drive performance. In the &lt;a href=&#34;https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper&#34;&gt;white paper&lt;/a&gt;, we calculate the impact of DevOps across organizations of different sizes—small, medium, and large—as well as across four distinct performance profiles—low, medium, high, elite. &lt;/p&gt;&lt;p&gt;There will be variation in these measurements based on your team’s current performance, compensation, change fail rate, benefits multiplier, and deployments per year, so we share our methodology in the white paper and invite you to customize the approach based on your specific needs and constraints. &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-video&#34;&gt;&lt;div class=&#34;article-module article-video &#34;&gt;&lt;figure&gt;&lt;a class=&#34;h-c-video h-c-video--marquee&#34; data-glue-modal-disabled-on-mobile=&#34;true&#34; data-glue-modal-trigger=&#34;uni-modal-gZ7GtQ8XzYo-&#34; href=&#34;https://youtube.com/watch?v=gZ7GtQ8XzYo&#34;&gt;&lt;div class=&#34;article-video__aspect-image&#34; style=&#34;background-image: url(https://storage.googleapis.com/gweb-cloudblog-publish/images/Screen_Shot_2021-01-12_at_2.49.11_PM.max-1000x1000.png);&#34;&gt;&lt;span class=&#34;h-u-visually-hidden&#34;&gt;ROI of DevOps Transformation&lt;/span&gt;&lt;/div&gt;&lt;svg class=&#34;h-c-video__play h-c-icon h-c-icon--color-white&#34; role=&#34;img&#34;&gt;&lt;use xlink:href=&#34;#mi-youtube-icon&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/a&gt;&lt;figcaption class=&#34;article-video__caption h-c-page&#34;&gt;&lt;h4 class=&#34;h-c-headline h-c-headline--four h-u-font-weight-medium h-u-mt-std&#34;&gt;ROI of DevOps Transformation: How to quantify the impact of your modernization initiatives&lt;/h4&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;div class=&#34;h-c-modal--video&#34; data-glue-modal=&#34;uni-modal-gZ7GtQ8XzYo-&#34; data-glue-modal-close-label=&#34;Close Dialog&#34;&gt;&lt;a class=&#34;glue-yt-video&#34; data-glue-yt-video-autoplay=&#34;true&#34; data-glue-yt-video-height=&#34;99%&#34; data-glue-yt-video-vid=&#34;gZ7GtQ8XzYo&#34; data-glue-yt-video-width=&#34;100%&#34; href=&#34;https://youtube.com/watch?v=gZ7GtQ8XzYo&#34; ng-cloak=&#34;&#34;&gt;&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;Years of &lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DORA research&lt;/a&gt; show that undertaking a technology transformation initiative can produce sizable returns for any organization. Our goal &lt;a href=&#34;https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper&#34;&gt;with the white paper&lt;/a&gt; is to provide IT and business decision makers an industry backed, data driven foundational basis for determining their investment in DevOps. Download the white paper &lt;a href=&#34;https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper&#34;&gt;here&lt;/a&gt; to calculate the impact of DevOps on your organization, while driving your digital transformation. &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-related_article_tout_external&#34;&gt;&lt;div class=&#34;uni-related-article-tout h-c-page&#34;&gt;&lt;section class=&#34;h-c-grid&#34;&gt;&lt;a class=&#34;uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker&#34; data-analytics=&#39;{&#xA;                       &#34;event&#34;: &#34;page interaction&#34;,&#xA;                       &#34;category&#34;: &#34;article lead&#34;,&#xA;                       &#34;action&#34;: &#34;related article - inline&#34;,&#xA;                       &#34;label&#34;: &#34;article: {slug}&#34;&#xA;                     }&#39; href=&#34;&#34;&gt;&lt;div class=&#34;uni-related-article-tout__inner-wrapper&#34;&gt;&lt;p class=&#34;uni-related-article-tout__eyebrow h-c-eyebrow&#34;&gt;Related Article&lt;/p&gt;&lt;div class=&#34;uni-related-article-tout__content-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image-wrapper&#34;&gt;&lt;div class=&#34;uni-related-article-tout__image&#34; style=&#34;background-image: url(&#39;&#39;)&#34;&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;uni-related-article-tout__content&#34;&gt;&lt;h4 class=&#34;uni-related-article-tout__header h-has-bottom-margin&#34;&gt;&lt;/h4&gt;&lt;p class=&#34;uni-related-article-tout__body&#34;&gt;&lt;/p&gt;&lt;div class=&#34;cta module-cta h-c-copy uni-related-article-tout__cta muted&#34;&gt;&lt;span class=&#34;nowrap&#34;&gt;Read Article&lt;svg class=&#34;icon h-c-icon&#34; role=&#34;presentation&#34;&gt;&lt;use xlink:href=&#34;#mi-arrow-forward&#34; xmlns:xlink=&#34;http://www.w3.org/1999/xlink&#34;&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt;&lt;/section&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;2020 challenged some of the best laid plans by enterprises. With nearly everything moving online, Covid-19 pushed forward years of digital transformation. DevOps was at the heart of this transformation journey. After all, delivering software quickly, reliably, and safely to meet the changing needs of customers was crucial to adapt to this new normal.&lt;/p&gt;&lt;p&gt;It is unlikely that the pace of modernization will slow down in 2021. As IT and business leaders further drive digital adoption within their organizations via DevOps, the need to quantify the business benefit from a digital transformation remains top of mind. A reliable model is imperative to drive the right level of investments and measure the returns. This is precisely why we wrote &lt;a href=&#34;https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper&#34;&gt;How to Measure ROI of DevOps Transformation&lt;/a&gt;. This white paper is backed with scientific studies conducted by &lt;a href=&#34;https://www.devops-research.com/research.html&#34; target=&#34;_blank&#34;&gt;DevOps Research and Assessment, DORA&lt;/a&gt;, with 31,000 professionals worldwide over 6 years to provide clear guidance based on impartial industry data. We found the financial savings of DevOps transformation varies from from $10M to $259M a year.&lt;/p&gt;&lt;h3&gt;Looking beyond cost to value&lt;/h3&gt;&lt;p&gt;The most innovative companies undertake their technology transformations with a focus on the value they can deliver to their customers. Hence, in addition to measuring cost savings, we show how DevOps done right can be a value driver and innovation engine. Let&#39;s look deeper into how we quantify the cost and value-generating power of DevOps.&amp;#160;&lt;/p&gt;&lt;h3&gt;Cost-driven category&lt;/h3&gt;&lt;p&gt;Here, we focus on quantifying the cost savings and efficiencies realized by implementing DevOps&amp;#8212;for example, how an investment in DevOps reduces costs by cutting the time it takes to resolve outages and avoiding downtime as much as possible.&amp;#160;&lt;/p&gt;&lt;p&gt;However, focusing solely on reducing costs can rarely yield systemic, long-term gains; thereby increasing the importance of going beyond cost-driven strategies. The cost savings achieved in year one &amp;#8220;no longer count&amp;#8221; beyond year two as the organization adjusts to a new baseline of costs and performance. Worse, only focusing on cost savings signals to technical staff their job is potentially at risk due to automation rather than being liberated from drudge work to better drive business growth. This leads to negative effects on morale and productivity.&amp;#160;&lt;/p&gt;&lt;h3&gt;Value-driven category&lt;/h3&gt;&lt;p&gt;There are two value drivers in a DevOps transformation, (1) improved efficiency through the reduction of unnecessary rework, and (2) the potential revenue gained by reinvesting the time saved in new offer capabilities.&lt;/p&gt;&lt;p&gt;Adding these cost and value driven categories together, IT and business decision makers can get an estimate of the potential value their organizations can expect to gain from a DevOps transformation. This helps justify the investment needed to implement the required changes. To quantify the impact, we leverage industry benchmark data across low, medium, high, and elite DevOps teams, as described by DORA in its annual &lt;a href=&#34;https://cloud.google.com/devops/state-of-devops&#34;&gt;Accelerate: State of DevOps report&lt;/a&gt;.&amp;#160;&lt;/p&gt;&lt;h3&gt;Combining cost and value&lt;/h3&gt;&lt;p&gt;As an example, let&#39;s consider the impact of a DevOps transformation on a large organization with 8,500 technical staff and a medium IT performer. Using the data gained from the DevOps report, we can calculate both the cost and value driven categories along with total impact.&amp;#160;&lt;/p&gt;"><p>2020 challenged some of the best laid plans by enterprises. With nearly everything moving online, Covid-19 pushed forward years of digital transformation. DevOps was at the heart of this transformation journey. After all, delivering software quickly, reliably, and safely to meet the changing needs of customers was crucial to adapt to this new normal.</p><p>It is unlikely that the pace of modernization will slow down in 2021. As IT and business leaders further drive digital adoption within their organizations via DevOps, the need to quantify the business benefit from a digital transformation remains top of mind. A reliable model is imperative to drive the right level of investments and measure the returns. This is precisely why we wrote <a href="https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper" track-type="inline link" track-name="1" track-metadata-eventdetail="https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper" track-metadata-module="post">How to Measure ROI of DevOps Transformation</a>. This white paper is backed with scientific studies conducted by <a href="https://www.devops-research.com/research.html" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://www.devops-research.com" track-metadata-module="post">DevOps Research and Assessment, DORA</a>, with 31,000 professionals worldwide over 6 years to provide clear guidance based on impartial industry data. We found the financial savings of DevOps transformation varies from from $10M to $259M a year.</p><h3>Looking beyond cost to value</h3><p>The most innovative companies undertake their technology transformations with a focus on the value they can deliver to their customers. Hence, in addition to measuring cost savings, we show how DevOps done right can be a value driver and innovation engine. Let&#39;s look deeper into how we quantify the cost and value-generating power of DevOps. </p><h3>Cost-driven category</h3><p>Here, we focus on quantifying the cost savings and efficiencies realized by implementing DevOps—for example, how an investment in DevOps reduces costs by cutting the time it takes to resolve outages and avoiding downtime as much as possible. </p><p>However, focusing solely on reducing costs can rarely yield systemic, long-term gains; thereby increasing the importance of going beyond cost-driven strategies. The cost savings achieved in year one “no longer count” beyond year two as the organization adjusts to a new baseline of costs and performance. Worse, only focusing on cost savings signals to technical staff their job is potentially at risk due to automation rather than being liberated from drudge work to better drive business growth. This leads to negative effects on morale and productivity. </p><h3>Value-driven category</h3><p>There are two value drivers in a DevOps transformation, (1) improved efficiency through the reduction of unnecessary rework, and (2) the potential revenue gained by reinvesting the time saved in new offer capabilities.</p><p>Adding these cost and value driven categories together, IT and business decision makers can get an estimate of the potential value their organizations can expect to gain from a DevOps transformation. This helps justify the investment needed to implement the required changes. To quantify the impact, we leverage industry benchmark data across low, medium, high, and elite DevOps teams, as described by DORA in its annual <a href="https://cloud.google.com/devops/state-of-devops" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/devops/state-of-devops" track-metadata-module="post">Accelerate: State of DevOps report</a>. </p><h3>Combining cost and value</h3><p>As an example, let&#39;s consider the impact of a DevOps transformation on a large organization with 8,500 technical staff and a medium IT performer. Using the data gained from the DevOps report, we can calculate both the cost and value driven categories along with total impact. </p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Brenna Washington&lt;/name&gt;&lt;title&gt;Product Marketing Manager&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_BigQuery_KHi78bE.max-2200x2200.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 26 Jan 2021 17:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Take the first step toward SRE with Cloud Operations Sandbox</title>
      <link>https://cloud.google.com/blog/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox/</link>
      <description>&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;At Google Cloud, we strive to bring Site Reliability Engineering (SRE) culture to our customers not only through training on organizational best practices, but also with the tools you need to run successful cloud services. Part and parcel of that is comprehensive observability tooling—logging, monitoring, tracing, profiling and debugging—which can help you troubleshoot production issues faster, increase release velocity and improve service reliability. &lt;/p&gt;&lt;p&gt;We often hear that implementing observability is hard, especially for complex distributed applications that are implemented in different programming languages, deployed in a variety of environments, that have different operational costs, and many other factors. As a result, when migrating and modernizing workloads onto Google Cloud, observability is often an afterthought. &lt;/p&gt;&lt;p&gt;Nevertheless, being able to debug the system and gain insights into the system’s behavior is important for running reliable production systems. Customers want to learn how to instrument services for observability and implement SRE best practices using tools Google Cloud has to offer, but without risking production environments. With &lt;a href=&#34;http://cloud-ops-sandbox.dev&#34; target=&#34;_blank&#34;&gt;Cloud Operations Sandbox&lt;/a&gt;, you can learn in practice how to kickstart your observability journey and answer the question, “Will it work for my use-case?”&lt;/p&gt;&lt;p&gt;Cloud Operations Sandbox is an &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox&#34; target=&#34;_blank&#34;&gt;open-source tool&lt;/a&gt; that helps you learn SRE practices from Google and apply them on cloud services using &lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;Google Cloud’s operations suite&lt;/a&gt; (formerly Stackdriver). Cloud Operations Sandbox has everything you need to get started in one click:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Demo service&lt;/b&gt; - an application built using microservices architecture on modern, cloud-native stack (a modified fork of a &lt;a href=&#34;https://github.com/GoogleCloudPlatform/microservices-demo&#34; target=&#34;_blank&#34;&gt;Online Boutique microservices&lt;/a&gt; demo app)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;One-click deployment&lt;/b&gt; - automated script that deploys and configures the service to Google Cloud, including:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Service Monitoring configuration&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Tracing with OpenTelemetry&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Profiling, Logging, Error Reporting, Debugging and more&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Load generator&lt;/b&gt; - a component that produces synthetic traffic on the demo service&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;SRE recipes&lt;/b&gt; - pre-built tasks that manufacture intentional errors in the demo app so you can use Cloud Operations tools to find the root cause of problems like you would in production&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;An &lt;b&gt;interactive walkthrough&lt;/b&gt; to get started with Cloud Operations &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Getting started&lt;/h3&gt;&lt;p&gt;Launching the Cloud Operations Sandbox is as easy as can be. Simply:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Go to &lt;a href=&#34;http://cloud-ops-sandbox.dev&#34; target=&#34;_blank&#34;&gt;cloud-ops-sandbox.dev&lt;/a&gt; &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click on the “Open in Google Cloud Shell” button. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This creates a new Google Cloud project. Within that project, a Terraform script creates a Google Kubernetes Engine (GKE) cluster and deploys a sample application to it. &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src&#34; target=&#34;_blank&#34;&gt;The microservices that make up the demo app&lt;/a&gt; are pre-instrumented with logging, monitoring, tracing, debugging and profiling as appropriate for each microservices language runtime. As such, sending traffic to the demo app generates telemetry that can be useful for diagnosing the cloud service’s operation. In order to generate production-like traffic to the demo app, &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src/loadgenerator&#34; target=&#34;_blank&#34;&gt;an automated script&lt;/a&gt; deploys a synthetic load generator in a different geo-location than the demo app.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Terraform_script_creates_a_GKE_cluste.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Terraform script creates a GKE cluste.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/Terraform_script_creates_a_GKE_cluste.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;It creates 11 custom dashboards (one for each microservice) to illustrate &lt;a href=&#34;https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals&#34; target=&#34;_blank&#34;&gt;the four golden signals&lt;/a&gt; of monitoring &lt;a href=&#34;https://sre.google/sre-book/monitoring-distributed-systems/&#34; target=&#34;_blank&#34;&gt;as described in Google’s SRE book&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/creates_11_custom_dashboards.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;creates 11 custom dashboards.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/creates_11_custom_dashboards.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;It also adds and automatically configures uptime checks, service monitoring (&lt;a href=&#34;https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring&#34;&gt;SLOs and SLIs&lt;/a&gt;), log-based metrics, alerting policies and more.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/checkout_service.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;checkout service.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/checkout_service.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;At the end of the provisioning script you’ll get a few URLs of the newly created project:&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-image_full_width&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid&#34;&gt;&lt;figure class=&#34;article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 &#34;&gt;&lt;a href=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/provisioning_script.max-2800x2800.jpg&#34; rel=&#34;external&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;provisioning script.jpg&#34; src=&#34;https://storage.googleapis.com/gweb-cloudblog-publish/images/provisioning_script.max-1000x1000.jpg&#34;/&gt;&lt;/a&gt;&lt;/figure&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;p&gt;You can &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/blob/master/docs/README.md&#34; target=&#34;_blank&#34;&gt;follow the user guide&lt;/a&gt; to learn about the entire Cloud Operations suite of tools, including tracking microservices interactions in &lt;a href=&#34;https://cloud.google.com/trace&#34;&gt;Cloud Trace&lt;/a&gt; (thanks to the &lt;a href=&#34;https://cloud.google.com/learn/what-is-opentelemetry&#34;&gt;OpenTelemetry&lt;/a&gt; instrumentation of the demo app) and see how to &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/terraform/monitoring&#34; target=&#34;_blank&#34;&gt;apply the learnings to your scenario&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Finally, to remove the Sandbox once you’re finished using it, you can run&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-code&#34;&gt;&lt;div class=&#34;article-module h-c-page&#34;&gt;&lt;div class=&#34;h-c-grid uni-paragraph-wrap&#34;&gt;&lt;div class=&#34;uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3&#34;&gt;&lt;pre&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&#34;block-paragraph&#34;&gt;&lt;div class=&#34;rich-text&#34;&gt;&lt;h3&gt;Next steps&lt;/h3&gt;&lt;p&gt;Following SRE principles is a proven method for running highly reliable applications in the cloud. We hope that the Cloud Operations Sandbox gives you the understanding and confidence you need to jumpstart your SRE practice. &lt;/p&gt;&lt;p&gt;To get started, visit  &lt;a href=&#34;http://cloud-ops-sandbox.dev&#34; target=&#34;_blank&#34;&gt;cloud-ops-sandbox.dev&lt;/a&gt;, explore the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox&#34; target=&#34;_blank&#34;&gt;project repo&lt;/a&gt;, and follow along in the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/blob/master/docs/README.md&#34; target=&#34;_blank&#34;&gt;user guide&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><paragraph-block _nghost-c19=""><div _ngcontent-c19="" innerhtml="&lt;p&gt;At Google Cloud, we strive to bring Site Reliability Engineering (SRE) culture to our customers not only through training on organizational best practices, but also with the tools you need to run successful cloud services. Part and parcel of that is comprehensive observability tooling&amp;#8212;logging, monitoring, tracing, profiling and debugging&amp;#8212;which can help you troubleshoot production issues faster, increase release velocity and improve service reliability.&amp;#160;&lt;/p&gt;&lt;p&gt;We often hear that implementing observability is hard, especially for complex distributed applications that are implemented in different programming languages, deployed in a variety of environments, that have different operational costs, and many other factors. As a result, when migrating and modernizing workloads onto Google Cloud, observability is often an afterthought.&amp;#160;&lt;/p&gt;&lt;p&gt;Nevertheless, being able to debug the system and gain insights into the system&amp;#8217;s behavior is important for running reliable production systems. Customers want to learn how to instrument services for observability and implement SRE best practices using tools Google Cloud has to offer, but without risking production environments. With &lt;a href=&#34;http://cloud-ops-sandbox.dev&#34; target=&#34;_blank&#34;&gt;Cloud Operations Sandbox&lt;/a&gt;, you can learn in practice how to kickstart your observability journey and answer the question, &amp;#8220;Will it work for my use-case?&amp;#8221;&lt;/p&gt;&lt;p&gt;Cloud Operations Sandbox is an &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox&#34; target=&#34;_blank&#34;&gt;open-source tool&lt;/a&gt; that helps you learn SRE practices from Google and apply them on cloud services using &lt;a href=&#34;https://cloud.google.com/products/operations&#34;&gt;Google Cloud&amp;#8217;s operations suite&lt;/a&gt; (formerly Stackdriver). Cloud Operations Sandbox has everything you need to get started in one click:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Demo service&lt;/b&gt; - an application built using microservices architecture on modern, cloud-native stack (a modified fork of a &lt;a href=&#34;https://github.com/GoogleCloudPlatform/microservices-demo&#34; target=&#34;_blank&#34;&gt;Online Boutique microservices&lt;/a&gt; demo app)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;One-click deployment&lt;/b&gt; - automated script that deploys and configures the service to Google Cloud, including:&lt;/p&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Service Monitoring configuration&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Tracing with OpenTelemetry&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Cloud Profiling, Logging, Error Reporting, Debugging and more&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Load generator&lt;/b&gt; - a component that produces synthetic traffic on the demo service&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;SRE recipes&lt;/b&gt; - pre-built tasks that manufacture intentional errors in the demo app so you can use Cloud Operations tools to find the root cause of problems like you would in production&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;An &lt;b&gt;interactive walkthrough&lt;/b&gt; to get started with Cloud Operations&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Getting started&lt;/h3&gt;&lt;p&gt;Launching the Cloud Operations Sandbox is as easy as can be. Simply:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Go to &lt;a href=&#34;http://cloud-ops-sandbox.dev&#34; target=&#34;_blank&#34;&gt;cloud-ops-sandbox.dev&lt;/a&gt;&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Click on the &amp;#8220;Open in Google Cloud Shell&amp;#8221; button.&amp;#160;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This creates a new Google Cloud project. Within that project, a Terraform script creates a Google Kubernetes Engine (GKE) cluster and deploys a sample application to it. &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src&#34; target=&#34;_blank&#34;&gt;The microservices that make up the demo app&lt;/a&gt; are pre-instrumented with logging, monitoring, tracing, debugging and profiling as appropriate for each microservices language runtime. As such, sending traffic to the demo app generates telemetry that can be useful for diagnosing the cloud service&amp;#8217;s operation. In order to generate production-like traffic to the demo app, &lt;a href=&#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src/loadgenerator&#34; target=&#34;_blank&#34;&gt;an automated script&lt;/a&gt; deploys a synthetic load generator in a different geo-location than the demo app.&lt;/p&gt;"><p>At Google Cloud, we strive to bring Site Reliability Engineering (SRE) culture to our customers not only through training on organizational best practices, but also with the tools you need to run successful cloud services. Part and parcel of that is comprehensive observability tooling—logging, monitoring, tracing, profiling and debugging—which can help you troubleshoot production issues faster, increase release velocity and improve service reliability. </p><p>We often hear that implementing observability is hard, especially for complex distributed applications that are implemented in different programming languages, deployed in a variety of environments, that have different operational costs, and many other factors. As a result, when migrating and modernizing workloads onto Google Cloud, observability is often an afterthought. </p><p>Nevertheless, being able to debug the system and gain insights into the system’s behavior is important for running reliable production systems. Customers want to learn how to instrument services for observability and implement SRE best practices using tools Google Cloud has to offer, but without risking production environments. With <a href="http://cloud-ops-sandbox.dev" target="_blank" track-type="inline link" track-name="1" track-metadata-eventdetail="http://cloud-ops-sandbox.dev" track-metadata-module="post">Cloud Operations Sandbox</a>, you can learn in practice how to kickstart your observability journey and answer the question, “Will it work for my use-case?”</p><p>Cloud Operations Sandbox is an <a href="https://github.com/GoogleCloudPlatform/cloud-ops-sandbox" target="_blank" track-type="inline link" track-name="2" track-metadata-eventdetail="https://github.com" track-metadata-module="post">open-source tool</a> that helps you learn SRE practices from Google and apply them on cloud services using <a href="https://cloud.google.com/products/operations" track-type="inline link" track-name="3" track-metadata-eventdetail="https://cloud.google.com/products/operations" track-metadata-module="post">Google Cloud’s operations suite</a> (formerly Stackdriver). Cloud Operations Sandbox has everything you need to get started in one click:</p><ul><li><p><b>Demo service</b> - an application built using microservices architecture on modern, cloud-native stack (a modified fork of a <a href="https://github.com/GoogleCloudPlatform/microservices-demo" target="_blank" track-type="inline link" track-name="4" track-metadata-eventdetail="https://github.com" track-metadata-module="post">Online Boutique microservices</a> demo app)</p></li><li><p><b>One-click deployment</b> - automated script that deploys and configures the service to Google Cloud, including:</p></li><ul><li><p>Service Monitoring configuration</p></li><li><p>Tracing with OpenTelemetry</p></li><li><p>Cloud Profiling, Logging, Error Reporting, Debugging and more</p></li></ul><li><p><b>Load generator</b> - a component that produces synthetic traffic on the demo service</p></li><li><p><b>SRE recipes</b> - pre-built tasks that manufacture intentional errors in the demo app so you can use Cloud Operations tools to find the root cause of problems like you would in production</p></li><li><p>An <b>interactive walkthrough</b> to get started with Cloud Operations </p></li></ul><h3>Getting started</h3><p>Launching the Cloud Operations Sandbox is as easy as can be. Simply:</p><ul><li><p>Go to <a href="http://cloud-ops-sandbox.dev" target="_blank" track-type="inline link" track-name="5" track-metadata-eventdetail="http://cloud-ops-sandbox.dev" track-metadata-module="post">cloud-ops-sandbox.dev</a> </p></li><li><p>Click on the “Open in Google Cloud Shell” button. </p></li></ul><p>This creates a new Google Cloud project. Within that project, a Terraform script creates a Google Kubernetes Engine (GKE) cluster and deploys a sample application to it. <a href="https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src" target="_blank" track-type="inline link" track-name="6" track-metadata-eventdetail="https://github.com" track-metadata-module="post">The microservices that make up the demo app</a> are pre-instrumented with logging, monitoring, tracing, debugging and profiling as appropriate for each microservices language runtime. As such, sending traffic to the demo app generates telemetry that can be useful for diagnosing the cloud service’s operation. In order to generate production-like traffic to the demo app, <a href="https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src/loadgenerator" target="_blank" track-type="inline link" track-name="7" track-metadata-eventdetail="https://github.com" track-metadata-module="post">an automated script</a> deploys a synthetic load generator in a different geo-location than the demo app.</p></div></paragraph-block></div></div>]]></content:encoded>
      <author>&lt;name&gt;Daniel Sanche&lt;/name&gt;&lt;title&gt;Developer Programs Engineer&lt;/title&gt;&lt;department&gt;&lt;/department&gt;&lt;company&gt;&lt;/company&gt;</author>
      <enclosure url="https://storage.googleapis.com/gweb-cloudblog-publish/images/Public-Sector-Momentum.max-1000x1000.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 22 Jan 2021 17:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>